{
  "html": "<section id=\"S1\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Recent advances in Multimodal Large Language Models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2024</a>; Zhang et al., <a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2024g</a>; Comanici et al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2025</a>; Bai et al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2025</a>; Zhang et al., <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2025a</a>)</cite> enable the capability to reason across textual and visual contents.\nDespite fast improvements, their capabilities to capture fine-grained details of actions, motions, object locations, interactions between objects, and spatial-temporal orders of events in long videos are still limited <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2025c</a>)</cite>.\nThere are two main reasons for this.\nFirstly, encoding the frames in a long video often generates a large number of visual tokens, exceeding the context length of the underlying Large Language Model (LLM).\nSecondly, storing the KV cache of these large number of visual tokens and attending to them during decoding poses significant memory and computational overhead.\nWhile the first issue has been alleviated by recent progress in long-context LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiong et al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2023</a>; Su et al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2024</a>; Zhang et al., <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2024b</a>)</cite>, the memory and compute efficiency of dealing with long videos remains a challenge, especially for real-world applications on edge devices.</p>\n</div>\n<figure id=\"S1.F1\" class=\"ltx_figure\"><img src=\"./assets/x1.png\" id=\"S1.F1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"498\" height=\"323\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\">Query-agnostic key-value (KV) cache compression in streaming video.</span>\nStreamMem addresses the challenge of streaming video processing under a memory budget by introducing a query-agnostic KV compression strategy.\n</span></figcaption>\n</figure>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">A number of recent works explored video token compression strategies to tackle long video understanding, including temporal compression <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2025</a>; Tan et al., <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, spatial compression <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2024a</a>; Zhang et al., <a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite>, and hybrid methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2024d</a>; He et al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2024</a>; Shen et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2024</a>; Tao et al., <a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>.\nThese approaches can suffer significant information loss.\nFor example, the action information in the video often cannot be captured with any single frame in the video.\nMany such methods also rely on having access to the text query for visual compression <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2024b</a>; Liang et al., <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2024</a>; Hu et al., <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, which is often unknown at the time of video processing in real-world applications <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite>.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">In parallel to these efforts, recent works start to explore streaming video processing with MLLMs, a paradigm in which video frames are incrementally encoded as they arrive, without prior knowledge of the video’s full length or the downstream query. Compared to offline video processing, the streaming video processing setup is much more flexible, as the model does not need to know the text query or the length of the video when encoding visual information. ReKV <cite class=\"ltx_cite ltx_citemacro_cite\">Di et al. (<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> is a leading work in this direction. It encodes new video frames in the stream with sliding window attention and stores the KV cache. When the model receives a question, it retrieves the most relevant KV cache in each layer with in-context retrieval. While this method is shown to be effective, it consumes significant memory to store all the KV cache. Offloading the KV cache to memory or disk and reloading them upon retrieval could also be very inefficient as the video becomes longer. LiveVLM <cite class=\"ltx_cite ltx_citemacro_cite\">Ning et al. (<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> proposes a KV compression mechanism to reduce the KV cache size by 70%. While LiveVLM alleviates the issue of memory consumption, it simply throws out the KV cache of earlier tokens when the memory upper bound is reached, which can lead to complete forgetting of earlier parts of the video.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">To enable efficient long video processing in memory-constrained environments, we introduce StreamMem, a training-free and query-agnostic KV cache memory system for streaming video understanding with MLLMs.\nStreamMem maintains a bounded memory footprint by continuously compressing the KV cache after each incoming video clip, thus preventing out-of-memory (OOM) errors and avoiding costly memory offloading regardless of video length.\nTo achieve effective and efficient memory retention, StreamMem leverages a novel saliency metric based on cross-attention scores between visual tokens and <span class=\"ltx_text ltx_font_italic\">chat template</span> tokens, allowing it to select and preserve informative visual content in a query-agnostic manner.\nIn addition, it incorporates an input frame compression module to reduce frame-level redundancy prior to MLLM encoding, and a frame-wise KV merging mechanism that constructs prototype representations for each observed frame.\nTogether, these components produce a diverse yet compact KV cache that supports accurate and memory-efficient streaming question answering.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\">We evaluate StreamMem across three offline and two streaming long video understanding benchmarks (EgoSchema <cite class=\"ltx_cite ltx_citemacro_citep\">(Mangalam et al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, VideoMME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>; RVS-Ego and RVS-Movie <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite>) using three open-source pre-trained MLLMs (LLaVA-OneVision <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite>, Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite>, and Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>).\nResults show that StreamMem consistently retains high utility while keeping the KV cache compact across videos of varying lengths and question types.\nIt not only surpasses state-of-the-art streaming video models, but also achieves competitive performance with methods that rely on significantly larger memory budgets.\nComprehensive ablation studies confirm the contribution of each component in the StreamMem framework.\nBy enabling continuous, scalable memory compression without fine-tuning, StreamMem provides a crucial step toward building real-time MLLM agents capable of continuous video understanding in open-world settings.</p>\n</div>\n<figure id=\"S1.F2\" class=\"ltx_figure\"><img src=\"./assets/x2.png\" id=\"S1.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"747\" height=\"530\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n(a) The overall workflow of StreamMem for streaming video understanding.\nIncoming frames are first filtered to reduce redundancy, then passed through the vision encoder and integrated with the existing KV memory via cross-attention.\nThe resulting KV cache is compressed to maintain a fixed memory budget, enabling continual processing of future frames or downstream question answering.\n(b) Detailed illustration of the KV compression module.\nSome KV cache in the memory and the new frames are pruned according to the attention score between the keys and the proxy queries.\nIn addition, we aggregated the key-value pairs for each new frame into a single frame-level prototype via weighted merging (shown in darker squares).\nThis combination of pruning and merging ensures compact yet expressive memory representations for long video sequences.\n</span></figcaption>\n</figure>\n</section>\n<section id=\"S2\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n\n<section id=\"S2.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Streaming video understanding with MLLMs.</h4>\n\n<div id=\"S2.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Streaming video understanding refers to the setting where the model continuously processes video frames in real-time.\nThe model does not know the length of the video beforehand and therefore cannot sample a fixed number of frames uniformly from the video.\nVideoLLM-online <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite> presents an MLLM that supports efficient streaming video processing and real-time dialogues.\nHowever, it aggressively down-samples each video frame to only include 10 visual tokens, limiting its understanding of fine-grained details in the video.\nFlash-VStream <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite> and Dispider <cite class=\"ltx_cite ltx_citemacro_citep\">(Qian et al., <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> use external memory modules to compress and organize visual tokens.\nUpon receiving a question, the model retrieves relevant visual tokens, combines them with the text tokens, and feeds them through the MLLM.\nRecent works start to explore KV cache compression and retrieval for video understanding.\nReKV <cite class=\"ltx_cite ltx_citemacro_citep\">(Di et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> encodes the video in streaming fashion and stores all the KV cache by offloading to memory or disk, and performs in-context retrieval of the relevant KV cache for each layer when answering a question.\nThe offloading of KV cache could incur a lot of memory and is not scalable to ultra-long videos.\nLiveVLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> designs a KV cache compression strategy for MLLMs to significantly reduce memory usage and improve question answering speed compared to ReKV. However, it uses a fixed compression ratio throughout the video and relies on first-in-first-out (FIFO) strategy to maintain a constrained memory, which leads to forgetting of earlier information in long videos, even though they might be informative. StreamMem resolves this issue by compressing the KV cache memory and the KVs from the new frames together and ensures a fixed-size KV cache memory throughout the video stream.\nConcurrent work InfiniPot-V <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et al. (<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite> also studies streaming video processing with constrained memory consumption. Different from StreamMem, they used a combination of two compression mechanisms, temporal-axis redundancy reduction and value norm-based selection.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Long video understanding with MLLMs.</h4>\n\n<div id=\"S2.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Long video understanding has been a great challenge for MLLMs given their constrained context length.\nEarly models such as LLaVA <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2023</a>, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> can only process a very small number of frames, leading to significant information loss.\nA number of training-based methods <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et al. (<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2024b</a>); Shen et al. (<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2024</a>); Shu et al. (<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">2025</a>); Liu et al. (<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2025a</a>)</cite> have been proposed to reduce the number of visual tokens needed to represent each video frame.\nIn addition, recent foundation models like Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> and Qwen2.5-VL <cite class=\"ltx_cite ltx_citemacro_cite\">Bai et al. (<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> also have inherent long visual context processing capability.\nThese training-based methods, however, are often very computationally expensive, especially when fine-tuning large MLLMs, and needs re-training for new foundation models.\nTraining-free long video understanding methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2024b</a>, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2025</a>; Liu et al., <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2025b</a>; Zhang et al., <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2024d</a>)</cite> compress the input visual tokens or the KV cache without the need to fine-tune the model, providing more flexibility for plug-and-play usage in new and more powerful MLLMs.\nStreamMem draws inspirations from the training-free methods for KV cache compression of video tokens, but focuses on the streaming setting where neither the length of the video nor the query is known during memory-constrained video encoding.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">KV cache compression in LLMs.</h4>\n\n<div id=\"S2.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">KV cache compression methods aim to greatly improve both memory and time efficiency of LLMs when operated in long input contexts.\nA number of methods explored leveraging the cross-attention weights between the query and the context to identify the most important entries in the KV cache for LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a href=\"#bib.bib59\" title=\"\" class=\"ltx_ref\">2023</a>; Li et al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2024c</a>; Xu et al., <a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">2024</a>; Fu et al., <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.\nThis strategy is also adopted in MLLMs for efficient visual understanding <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2024b</a>; Zhang et al., <a href=\"#bib.bib57\" title=\"\" class=\"ltx_ref\">2024f</a>)</cite>.\nHowever, the query might not be available when the model processes the long context in many real-world scenarios, limiting the applicability of the query-dependent approach.\nTo eliminate this dependency, some recent works explored query-agnostic KV cache compression mechanisms <cite class=\"ltx_cite ltx_citemacro_citep\">(Ge et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2023</a>; Devoto et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2024</a>; Hooper et al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2024</a>; Park et al., <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2025</a>; Kim et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2025a</a>)</cite>.\nSimilar to this work, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2024c</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Arif et al. (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> explored using the attention weights of the <span class=\"ltx_text ltx_font_typewriter\">[CLS]</span> token for KV cache compression in MLLMs.\nIn between query-dependent and query-agnostic methods, there are also methods which use task instructions or task-specific proxy prompts <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et al., <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2024</a>; Corallo et al., <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>.\nStreamMem belongs to the most flexible category of query-agnostic methods and does not need full-context encoding, making it suitable for streaming encoding of long videos.</p>\n</div>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Preliminaries</h2>\n\n<section id=\"S3.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Offline video understanding with MLLMs.</h4>\n\n<div id=\"S3.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">The standard approach to offline video understanding with MLLMs proceeds as follows.\nGiven a long video, a fixed number of frames <math id=\"S3.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"f_{1},...,f_{T}\" display=\"inline\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">f_{1},...,f_{T}</annotation></semantics></math> are uniformly sampled from the video, where <math id=\"S3.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is determined based on the model’s context length or computational and memory constraints.\nThe frames are then passed through the model’s vision encoder (typically comprising a Vision Transformer (ViT) backbone <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy et al., <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2021</a>; Zhai et al., <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2023</a>; Zhang et al., <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2024e</a>)</cite> and a projection layer) to get <math id=\"S3.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"N\" display=\"inline\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> visual tokens.\nThe visual tokens are concatenated with the text tokens, including system prompts (preceding the visual tokens) and user queries (following the visual tokens), and the entire sequence is fed into the LLM.\nThe LLM then generates a response via autoregressive decoding.\nTo accelerate decoding, key-value (KV) caches are constructed during this process.</p>\n</div>\n</section>\n<section id=\"S3.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">KV cache compression for MLLMs in streaming video.</h4>\n\n<div id=\"S3.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In streaming video processing with MLLMs, the video length is typically unknown in advance, precluding uniform frame sampling strategies used in the offline settings.\nAt each time step <math id=\"S3.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model receives a new video clip <math id=\"S3.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"v_{t}\" display=\"inline\" intent=\":literal\"><semantics><msub><mi>v</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">v_{t}</annotation></semantics></math> (a fixed-length frame segment), encodes it into a sequence of visual tokens, and forwards them through the LLM.\nThe model then generates the corresponding key and value matrices <math id=\"S3.SS0.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"K_{t}^{i}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mi>K</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">K_{t}^{i}</annotation></semantics></math> and <math id=\"S3.SS0.SSS0.Px2.p1.m4\" class=\"ltx_Math\" alttext=\"V_{t}^{i}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mi>V</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">V_{t}^{i}</annotation></semantics></math> at each transformer layer <math id=\"S3.SS0.SSS0.Px2.p1.m5\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, by attending to all accumulated visual tokens from prior clips.</p>\n</div>\n<div id=\"S3.SS0.SSS0.Px2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">However, naively storing all keys and values over time leads to linear growth in memory, which is infeasible for long videos.\nThis motivates the need for KV cache compression mechanisms that maintain a fixed memory footprint.\nWe denote the compressed key and value matrices at time step <math id=\"S3.SS0.SSS0.Px2.p2.m1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and layer <math id=\"S3.SS0.SSS0.Px2.p2.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> as <math id=\"S3.SS0.SSS0.Px2.p2.m3\" class=\"ltx_Math\" alttext=\"K_{t}^{i^{\\prime}}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mi>K</mi><mi>t</mi><msup><mi>i</mi><mo>′</mo></msup></msubsup><annotation encoding=\"application/x-tex\">K_{t}^{i^{\\prime}}</annotation></semantics></math> and <math id=\"S3.SS0.SSS0.Px2.p2.m4\" class=\"ltx_Math\" alttext=\"V_{t}^{i^{\\prime}}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mi>V</mi><mi>t</mi><msup><mi>i</mi><mo>′</mo></msup></msubsup><annotation encoding=\"application/x-tex\">V_{t}^{i^{\\prime}}</annotation></semantics></math>, respectively. The objective is to compute compressed representations:</p>\n<table id=\"S3.Ex1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.Ex1.m1\" class=\"ltx_Math\" alttext=\"K_{t}^{i^{\\prime}},V_{t}^{i^{\\prime}}=\\text{Compress}(K_{t-1}^{i^{\\prime}},K_{t}^{i},V_{t-1}^{i^{\\prime}},V_{t}^{i}),\" display=\"block\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>K</mi><mi>t</mi><msup><mi>i</mi><mo>′</mo></msup></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>t</mi><msup><mi>i</mi><mo>′</mo></msup></msubsup></mrow><mo>=</mo><mrow><mtext>Compress</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>K</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><msup><mi>i</mi><mo>′</mo></msup></msubsup><mo>,</mo><msubsup><mi>K</mi><mi>t</mi><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>V</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><msup><mi>i</mi><mo>′</mo></msup></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>t</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">K_{t}^{i^{\\prime}},V_{t}^{i^{\\prime}}=\\text{Compress}(K_{t-1}^{i^{\\prime}},K_{t}^{i},V_{t-1}^{i^{\\prime}},V_{t}^{i}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">subject to the global memory constraint:</p>\n<table id=\"S3.Ex2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.Ex2.m1\" class=\"ltx_Math\" alttext=\"\\sum_{i=1}^{L}\\|K_{t}^{i^{\\prime}}\\|_{0}\\leq M,\" display=\"block\" intent=\":literal\"><semantics><mrow><mrow><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mrow><mo lspace=\"0em\" stretchy=\"false\">‖</mo><msubsup><mi>K</mi><mi>t</mi><msup><mi>i</mi><mo>′</mo></msup></msubsup><mo stretchy=\"false\">‖</mo></mrow><mn>0</mn></msub></mrow><mo>≤</mo><mi>M</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{L}\\|K_{t}^{i^{\\prime}}\\|_{0}\\leq M,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S3.SS0.SSS0.Px2.p2.m5\" class=\"ltx_Math\" alttext=\"L\" display=\"inline\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the number of transformer layers in the MLLM and <math id=\"S3.SS0.SSS0.Px2.p2.m6\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> is the total memory budget for all layers combined.</p>\n</div>\n<div id=\"S3.SS0.SSS0.Px2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">The design of effective compression strategies that retain essential temporal information while bounding memory usage is a key challenge in streaming long video processing with MLLMs.\nExisting approaches such as ReKV <cite class=\"ltx_cite ltx_citemacro_citep\">(Di et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> and LiveVLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> do not address this constraint effectively, as their KV cache grows linearly over time, resulting in unbounded memory consumption for long videos.</p>\n</div>\n<figure id=\"S3.F3\" class=\"ltx_figure\"><img src=\"./assets/x3.png\" id=\"S3.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"498\" height=\"324\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Visualization of visual tokens attended to by different text queries.<span class=\"ltx_text ltx_font_medium\">\nRed indicates higher attention scores.\nDespite minor variations, different text queries attend to largely overlapping regions of the input images.\nThe “Generic Question” is <span class=\"ltx_text ltx_font_italic\">“What is happening in the video?”</span>, while the “Specific Question” is <span class=\"ltx_text ltx_font_italic\">“What occurs just before reading the magazines?”</span>\nAttention scores are averaged across all layers and heads, and then interpolated from 14×14 to 384×384 to match the image resolution.\nThe MLLM used is LLaVA-OneVision, and the video clip is sourced from the RVS-Ego benchmark (which uses videos from the Ego4D dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Grauman et al., <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>).\n</span></span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Method</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We now describe the key components of StreamMem, our proposed framework for efficient streaming video understanding with MLLMs. At each time step <math id=\"S4.p1.m1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, a new segment of frames is received from the video stream. These frames first undergo an input filtering step to remove temporal redundancy. The filtered frames are then encoded by the vision encoder and processed by the MLLM to produce key-value (KV) representations <math id=\"S4.p1.m2\" class=\"ltx_Math\" alttext=\"\\{K_{t}^{i},V_{t}^{i}\\}_{i=1}^{L}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mi>t</mi><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>t</mi><mi>i</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{t}^{i},V_{t}^{i}\\}_{i=1}^{L}</annotation></semantics></math> at each transformer layer <math id=\"S4.p1.m3\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>.</p>\n</div>\n<div id=\"S4.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">To prevent unbounded memory growth over time, the newly computed KVs are merged with the compressed KV memory from the previous time step, <math id=\"S4.p2.m1\" class=\"ltx_Math\" alttext=\"\\{K_{t-1}^{i\\prime},V_{t-1}^{i\\prime}\\}_{i=1}^{L}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo>,</mo><msubsup><mi>V</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{t-1}^{i\\prime},V_{t-1}^{i\\prime}\\}_{i=1}^{L}</annotation></semantics></math>, and passed through a compression module.\nThis module applies two complementary strategies: (1) a novel attention-based pruning method that leverages cross-attention scores between proxy query tokens and visual tokens, and (2) a frame-wise KV merging mechanism that condenses spatial information into compact prototype representations.\nThe output of the compression module forms the updated memory <math id=\"S4.p2.m2\" class=\"ltx_Math\" alttext=\"\\{K_{t}^{i\\prime},V_{t}^{i\\prime}\\}_{i=1}^{L}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{t}^{i\\prime},V_{t}^{i\\prime}\\}_{i=1}^{L}</annotation></semantics></math>, which is used by the MLLM at the next time step.\nAn overview of the full pipeline is illustrated in Figure <a href=\"#S1.F2\" title=\"Figure 2 ‣ 1 Introduction ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the KV compression procedure is detailed in Algorithm <a href=\"#alg1\" title=\"Algorithm 1 ‣ 4.3 Positional Embedding ‣ 4 Method ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Input Frame Filtering</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Before processing by the MLLM, each incoming video clip (a chunk of consecutive frames) is passed through a lightweight filtering step to reduce temporal redundancy.\nGiven a sequence of frames, we compute their visual embeddings using the vision encoder.\nFor each consecutive pair of frames, we measure the cosine similarity between their embeddings.\nIf the similarity exceeds a predefined threshold <math id=\"S4.SS1.p1.m1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\" intent=\":literal\"><semantics><mi>δ</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, the two frames are deemed redundant and their representations are merged by simple averaging.</p>\n</div>\n<div id=\"S4.SS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">This lightweight filtering step is similar to the temporal compression used in LongVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.\nIn contrast to previous streaming approaches that rely on sliding window attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Di et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>; Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, our method explicitly reduces redundancy in the input space.\nThis ensures that highly similar frames (common in static scenes or high-frame-rate videos) do not overwhelm the KV cache with repetitive information, ultimately preserving the diversity and informativeness of the stored memory.</p>\n</div>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>KV Cache Memory</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">After frame-wise token compression, the retained visual tokens of the current video segment <math id=\"S4.SS2.p1.m1\" class=\"ltx_Math\" alttext=\"v_{t}\" display=\"inline\" intent=\":literal\"><semantics><msub><mi>v</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">v_{t}</annotation></semantics></math> are concatenated with a set of auxiliary query tokens and passed into the MLLM.\nThe model computes key-value pairs <math id=\"S4.SS2.p1.m2\" class=\"ltx_Math\" alttext=\"\\{K_{t}^{i},V_{t}^{i}\\}_{i=1}^{L}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mi>t</mi><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>t</mi><mi>i</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{t}^{i},V_{t}^{i}\\}_{i=1}^{L}</annotation></semantics></math> in each transformer layer <math id=\"S4.SS2.p1.m3\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, attending over both the current tokens, the tokens in the previous time step, and the compressed KV cache from previous time step, <math id=\"S4.SS2.p1.m4\" class=\"ltx_Math\" alttext=\"\\{K_{t-1}^{i\\prime},V_{t-1}^{i\\prime}\\}_{i=1}^{L}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo>,</mo><msubsup><mi>V</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{t-1}^{i\\prime},V_{t-1}^{i\\prime}\\}_{i=1}^{L}</annotation></semantics></math>.</p>\n</div>\n<div id=\"S4.SS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">To guide the KV cache compression process, we rely on the cross-attention scores between the auxiliary query tokens and the visual tokens.\nThis attention-based saliency measure has proven effective in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2024b</a>; Wang et al., <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> for real user queries.\nHowever, unlike those settings, our method operates under a query-agnostic streaming setup, where the user query is unavailable at the time of visual token selection.</p>\n</div>\n<div id=\"S4.SS2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">To approximate a generic query, we leverage the system’s chat template tokens as a proxy.\nSpecifically, we use the tokens: <span class=\"ltx_text ltx_font_typewriter\">&lt;|im_end|&gt;&lt;|im_start|&gt;assistant\\n</span>, which we append after the visual tokens.\nDue to the prevalence of video captioning data during the MLLM pretraining, this implicitly prompts the MLLM to generate a generic video description even in the absence of an explicit question.\nAs a result, we expect the model to implicitly attend to informative visual content in this setup.</p>\n</div>\n<div id=\"S4.SS2.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">Formally, let <math id=\"S4.SS2.p4.m1\" class=\"ltx_Math\" alttext=\"Q\\in\\mathbb{R}^{q\\times d}\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>q</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">Q\\in\\mathbb{R}^{q\\times d}</annotation></semantics></math> be the query representation of the chat template tokens at a given layer, and <math id=\"S4.SS2.p4.m2\" class=\"ltx_Math\" alttext=\"K_{t}^{i}\\in\\mathbb{R}^{n\\times d}\" display=\"inline\" intent=\":literal\"><semantics><mrow><msubsup><mi>K</mi><mi>t</mi><mi>i</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">K_{t}^{i}\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> and <math id=\"S4.SS2.p4.m3\" class=\"ltx_Math\" alttext=\"V_{t}^{i}\\in\\mathbb{R}^{n\\times d}\" display=\"inline\" intent=\":literal\"><semantics><mrow><msubsup><mi>V</mi><mi>t</mi><mi>i</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V_{t}^{i}\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> be the key and value matrices of the <math id=\"S4.SS2.p4.m4\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> visual tokens from the current clip.\nThe cross-attention scores are computed as:</p>\n<table id=\"S4.E1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E1.m1\" class=\"ltx_Math\" alttext=\"A_{t}^{i}=\\text{Softmax}\\left(\\frac{Q(K_{t}^{i})^{\\top}}{\\sqrt{d}}\\right),\" display=\"block\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>A</mi><mi>t</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mtext>Softmax</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>K</mi><mi>t</mi><mi>i</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">A_{t}^{i}=\\text{Softmax}\\left(\\frac{Q(K_{t}^{i})^{\\top}}{\\sqrt{d}}\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS2.p4.m5\" class=\"ltx_Math\" alttext=\"A_{t}^{i}\\in\\mathbb{R}^{q\\times n}\" display=\"inline\" intent=\":literal\"><semantics><mrow><msubsup><mi>A</mi><mi>t</mi><mi>i</mi></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>q</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A_{t}^{i}\\in\\mathbb{R}^{q\\times n}</annotation></semantics></math> denotes the attention weights from chat template tokens to visual tokens.\nWe aggregate these scores (e.g., by averaging over <math id=\"S4.SS2.p4.m6\" class=\"ltx_Math\" alttext=\"q\" display=\"inline\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>) to obtain an importance score for each visual token, which we use to select the top-<math id=\"S4.SS2.p4.m7\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> most salient visual tokens to retain in the compressed cache from each layer. The memory budget is even distributed across all layers.</p>\n</div>\n<div id=\"S4.SS2.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\">In addition to pruning, StreamMem further compresses memory via KV merging.\nInspired by frame-level merging in MLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> and visual token merging <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2024d</a>)</cite>, we compute a <span class=\"ltx_text ltx_font_italic\">prototype</span> key and value representation for each frame.\nThis is done by computing a weighted average of the keys and values based on the normalized attention scores:</p>\n<table id=\"S4.E2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E2.m1\" class=\"ltx_Math\" alttext=\"\\bar{K}_{t}^{i}=\\sum_{j=1}^{n}\\alpha_{j}^{i}\\cdot K_{t,j}^{i},\\quad\\bar{V}_{t}^{i}=\\sum_{j=1}^{n}\\alpha_{j}^{i}\\cdot V_{t,j}^{i},\" display=\"block\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mover accent=\"true\"><mi>K</mi><mo>¯</mo></mover><mi>t</mi><mi>i</mi></msubsup><mo rspace=\"0.111em\">=</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>α</mi><mi>j</mi><mi>i</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msubsup><mi>K</mi><mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow><mi>i</mi></msubsup></mrow></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><msubsup><mover accent=\"true\"><mi>V</mi><mo>¯</mo></mover><mi>t</mi><mi>i</mi></msubsup><mo rspace=\"0.111em\">=</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>α</mi><mi>j</mi><mi>i</mi></msubsup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msubsup><mi>V</mi><mrow><mi>t</mi><mo>,</mo><mi>j</mi></mrow><mi>i</mi></msubsup></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\bar{K}_{t}^{i}=\\sum_{j=1}^{n}\\alpha_{j}^{i}\\cdot K_{t,j}^{i},\\quad\\bar{V}_{t}^{i}=\\sum_{j=1}^{n}\\alpha_{j}^{i}\\cdot V_{t,j}^{i},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS2.p5.m1\" class=\"ltx_Math\" alttext=\"\\alpha_{j}^{i}\" display=\"inline\" intent=\":literal\"><semantics><msubsup><mi>α</mi><mi>j</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\alpha_{j}^{i}</annotation></semantics></math> denotes the normalized importance score of the <math id=\"S4.SS2.p5.m2\" class=\"ltx_Math\" alttext=\"j\" display=\"inline\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th visual token at layer <math id=\"S4.SS2.p5.m3\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>.</p>\n</div>\n<div id=\"S4.SS2.p6\" class=\"ltx_para\">\n<p class=\"ltx_p\">These prototype representations <math id=\"S4.SS2.p6.m1\" class=\"ltx_Math\" alttext=\"\\bar{K}_{t}^{i},\\bar{V}_{t}^{i}\" display=\"inline\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>K</mi><mo>¯</mo></mover><mi>t</mi><mi>i</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>V</mi><mo>¯</mo></mover><mi>t</mi><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\bar{K}_{t}^{i},\\bar{V}_{t}^{i}</annotation></semantics></math> are inserted at the end of the selected token sequence from <math id=\"S4.SS2.p6.m2\" class=\"ltx_Math\" alttext=\"v_{t}\" display=\"inline\" intent=\":literal\"><semantics><msub><mi>v</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">v_{t}</annotation></semantics></math>, preserving frame-wise temporal alignment via position IDs.\nTherefore, the final compressed cache <math id=\"S4.SS2.p6.m3\" class=\"ltx_Math\" alttext=\"\\{K_{t}^{i\\prime},V_{t}^{i\\prime}\\}\" display=\"inline\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\">⁣</mo><mo mathsize=\"1.420em\">′</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{K_{t}^{i\\prime},V_{t}^{i\\prime}\\}</annotation></semantics></math> consists of a mix of salient visual tokens and frame prototypes, enabling both fine-grained and global memory retention.</p>\n</div>\n</section>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Positional Embedding</h3>\n\n<div id=\"S4.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">MLLMs are typically not extensively trained on long video sequences due to the scarcity of high-quality, long-form video-text data.\nAs a result, despite the long context lengths supported by the underlying language models, MLLMs often struggle to generalize effectively in long video understanding scenarios.\nTo address this limitation, we adopt the YaRN context window extension technique <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et al., <a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">2023</a>; Wei and Chen, <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, originally proposed for language models, to extend the visual context capacity of MLLMs for streaming video processing.</p>\n</div>\n<div id=\"S4.SS3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Prior works on streaming processing of long videos with MLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Di et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>; Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>; Kim et al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite> reassign positional IDs to the visual tokens that are retained after KV cache compression.\nHowever, this reassignment discards the original spatial and temporal information associated with these tokens, potentially degrading performance.\nWe demonstrate that applying YaRN with a properly chosen scaling factor (based on the MLLM’s visual context window length) allows us to preserve positional consistency across streaming segments and improves performance compared to naively reassigning position embeddings.</p>\n</div>\n<figure id=\"alg1\" class=\"ltx_float ltx_float_algorithm ltx_framed ltx_framed_top\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 1</span> </span> Streaming Video Encoding and KV Cache Compression</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel\"><span class=\"ltx_text ltx_font_bold\">Require</span>: Total KV Cache size <math id=\"alg1.m1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>, Template tokens <math id=\"alg1.m2\" class=\"ltx_Math\" alttext=\"Q\" display=\"inline\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div class=\"ltx_listing ltx_figure_panel ltx_listing\">\n<div id=\"alg1.l1\" class=\"ltx_listingline\"> Initialize cache <math id=\"alg1.l1.m1\" class=\"ltx_Math\" alttext=\"K,V\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>,</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">K,V</annotation></semantics></math> and score matrix <math id=\"alg1.l1.m2\" class=\"ltx_Math\" alttext=\"s\" display=\"inline\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> (one row for each transformer layer).\n\n</div>\n<div id=\"alg1.l2\" class=\"ltx_listingline\"> <span class=\"ltx_text ltx_font_bold\">while</span> not end of video <span class=\"ltx_text ltx_font_bold\">do</span>\n\n\n\n</div>\n<div id=\"alg1.l3\" class=\"ltx_listingline\">  Fetch a new batch of frames <math id=\"alg1.l3.m1\" class=\"ltx_Math\" alttext=\"v_{i}\" display=\"inline\" intent=\":literal\"><semantics><msub><mi>v</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">v_{i}</annotation></semantics></math> from stream.\n\n</div>\n<div id=\"alg1.l4\" class=\"ltx_listingline\">  <math id=\"alg1.l4.m1\" class=\"ltx_Math\" alttext=\"v_{i}^{\\prime}=\\text{Filter}(v_{i})\" display=\"inline\" intent=\":literal\"><semantics><mrow><msubsup><mi>v</mi><mi>i</mi><mo>′</mo></msubsup><mo>=</mo><mrow><mtext>Filter</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v_{i}^{\\prime}=\\text{Filter}(v_{i})</annotation></semantics></math> based on frame similarity\n</div>\n<div id=\"alg1.l5\" class=\"ltx_listingline\">  <math id=\"alg1.l5.m1\" class=\"ltx_Math\" alttext=\"K_{i},V_{i},s_{i}=\\text{Encode}(v_{i}^{\\prime},Q)\" display=\"inline\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>K</mi><mi>i</mi></msub><mo>,</mo><msub><mi>V</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub></mrow><mo>=</mo><mrow><mtext>Encode</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>v</mi><mi>i</mi><mo>′</mo></msubsup><mo>,</mo><mi>Q</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K_{i},V_{i},s_{i}=\\text{Encode}(v_{i}^{\\prime},Q)</annotation></semantics></math> \n</div>\n<div id=\"alg1.l6\" class=\"ltx_listingline\">  <span class=\"ltx_text ltx_font_bold\">if</span> <math id=\"alg1.l6.m1\" class=\"ltx_Math\" alttext=\"|K|&gt;M\" display=\"inline\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>K</mi><mo stretchy=\"false\">|</mo></mrow><mo>&gt;</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">|K|&gt;M</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">then</span>\n\n\n\n</div>\n<div id=\"alg1.l7\" class=\"ltx_listingline\">   <math id=\"alg1.l7.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{I}=\\text{Topk}(s,k=M)\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">ℐ</mi><mo>=</mo><mrow><mtext>Topk</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>s</mi><mo>,</mo><mi>k</mi></mrow><mo>=</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{I}=\\text{Topk}(s,k=M)</annotation></semantics></math>; <math id=\"alg1.l7.m2\" class=\"ltx_Math\" alttext=\"K,V=K[\\mathcal{I}],V[\\mathcal{I}]\" display=\"inline\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>K</mi><mo>,</mo><mi>V</mi></mrow><mo>=</mo><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">[</mo><mi class=\"ltx_font_mathcaligraphic\">ℐ</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">[</mo><mi class=\"ltx_font_mathcaligraphic\">ℐ</mi><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K,V=K[\\mathcal{I}],V[\\mathcal{I}]</annotation></semantics></math>\n\n</div>\n<div id=\"alg1.l8\" class=\"ltx_listingline\">  <span class=\"ltx_text ltx_font_bold\">end</span> <span class=\"ltx_text ltx_font_bold\">if</span>\n</div>\n<div id=\"alg1.l9\" class=\"ltx_listingline\">  Append <math id=\"alg1.l9.m1\" class=\"ltx_Math\" alttext=\"K_{i},V_{i},s_{i}\" display=\"inline\" intent=\":literal\"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub><mo>,</mo><msub><mi>V</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">K_{i},V_{i},s_{i}</annotation></semantics></math> to <math id=\"alg1.l9.m2\" class=\"ltx_Math\" alttext=\"K,V,s\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>,</mo><mi>V</mi><mo>,</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">K,V,s</annotation></semantics></math>. // Equation <a href=\"#S4.E1\" title=\"Equation 1 ‣ 4.2 KV Cache Memory ‣ 4 Method ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>\n\n</div>\n<div id=\"alg1.l10\" class=\"ltx_listingline\">  Insert <math id=\"alg1.l10.m1\" class=\"ltx_Math\" alttext=\"\\text{Merge}(K_{i}),\\text{Merge}(V_{i})\" display=\"inline\" intent=\":literal\"><semantics><mrow><mrow><mtext>Merge</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>K</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mtext>Merge</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>V</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Merge}(K_{i}),\\text{Merge}(V_{i})</annotation></semantics></math> to <math id=\"alg1.l10.m2\" class=\"ltx_Math\" alttext=\"K,V\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>,</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">K,V</annotation></semantics></math>. // Equation <a href=\"#S4.E2\" title=\"Equation 2 ‣ 4.2 KV Cache Memory ‣ 4 Method ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>\n\n</div>\n<div id=\"alg1.l11\" class=\"ltx_listingline\"> <span class=\"ltx_text ltx_font_bold\">end</span> <span class=\"ltx_text ltx_font_bold\">while</span>\n</div>\n</div>\n</div>\n</div>\n</figure>\n<figure id=\"S4.T1\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Frames/FPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">KV Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MLVU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">EgoSchema</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">VideoMME</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Medium</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Long</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">All</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">70.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MovieChat+</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2048</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Dispider</th>\n<td class=\"ltx_td ltx_align_center\">1 fps</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">61.7</td>\n<td class=\"ltx_td ltx_align_center\">55.6</td>\n<td class=\"ltx_td ltx_align_center\">53.7</td>\n<td class=\"ltx_td ltx_align_center\">49.7</td>\n<td class=\"ltx_td ltx_align_center\">57.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LongVU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1 fps/400</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLaVA-OneVision-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">56.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"color:#808080;\"> +  ReKV<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">†</span></sup></span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"color:#808080;\">0.5 fps</span></td>\n<td class=\"ltx_td ltx_align_center\">353K/h</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"color:#808080;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"color:#808080;\">60.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"color:#808080;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"color:#808080;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"color:#808080;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">   + LiveVLM</th>\n<td class=\"ltx_td ltx_align_center\">0.5/0.2 fps</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">66.3</td>\n<td class=\"ltx_td ltx_align_center\">63.0</td>\n<td class=\"ltx_td ltx_align_center\">56.4</td>\n<td class=\"ltx_td ltx_align_center\">48.8</td>\n<td class=\"ltx_td ltx_align_center\">57.3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   + StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">0.5/0.2 fps</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">66.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">63.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">56.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">50.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">59.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Qwen2-VL-7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">   + InfiniPot-V</th>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">6K</td>\n<td class=\"ltx_td ltx_align_center\">65.8</td>\n<td class=\"ltx_td ltx_align_center\">65.6</td>\n<td class=\"ltx_td ltx_align_center\">60.8</td>\n<td class=\"ltx_td ltx_align_center\">53.4</td>\n<td class=\"ltx_td ltx_align_center\">62.8</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   + StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">4.0/0.5 fps</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">65.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">67.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">62.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">52.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">62.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Qwen2.5-VL-3B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">   + InfiniPot-V</th>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">6K</td>\n<td class=\"ltx_td ltx_align_center\">62.1</td>\n<td class=\"ltx_td ltx_align_center\">61.8</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">59.3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   + StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">4.0/0.5 fps</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">62.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">62.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">60.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">49.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">59.5</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Evaluation results of different MLLMs on offline long video understanding benchmarks. †: ReKV stores the KV cache of all seen frames so it is considered an “upper bound.”</span></figcaption>\n</figure>\n<figure id=\"S4.T2\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">RVS-Ego</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">RVS-Movie</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Acc</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Score</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Acc</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Score</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">ReKV</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">ReKV w/o offloading.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Flash-VStream</th>\n<td class=\"ltx_td ltx_align_center\">57.0</td>\n<td class=\"ltx_td ltx_align_center\">4.0</td>\n<td class=\"ltx_td ltx_align_center\">53.1</td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">InfiniPot-V</th>\n<td class=\"ltx_td ltx_align_center\">57.9</td>\n<td class=\"ltx_td ltx_align_center\">3.5</td>\n<td class=\"ltx_td ltx_align_center\">51.4</td>\n<td class=\"ltx_td ltx_align_center\">3.5</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">57.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">52.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">3.4</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Results of different streaming video question answering methods on RVS-Ego and RVS-Movie benchmarks.</span></figcaption>\n</figure>\n<figure id=\"S4.T3\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">KV Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Holistic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">S.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">All</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Full KV</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">50K</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">76.3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">73.9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">43.3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">65.9</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">InfiniPot-V</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.8</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">77.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">72.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">44.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">65.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">InfiniPot-V</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.0</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">12K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">77.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">73.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">43.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">66.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">InfiniPot-V</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.7</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">24K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">77.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">73.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">44.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">66.3</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Comparison of InfiniPot-V and StreamMem on MLVU with different KV sizes. We use Qwen2VL-7B as the base MLLM.</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n\n<section id=\"S5.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span>Experiment Setup</h3>\n\n<section id=\"S5.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Benchmarks.</h4>\n\n<div id=\"S5.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We evaluate StreamMem on a number of widely used offline long video understanding benchmarks, including MLVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, EgoSchema <cite class=\"ltx_cite ltx_citemacro_citep\">(Mangalam et al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, and VideoMME <cite class=\"ltx_cite ltx_citemacro_citep\">(Fu et al., <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>.\nBy default, we process the video stream at 0.5 frames per second (FPS), in accordance with ReKV <cite class=\"ltx_cite ltx_citemacro_cite\">Di et al. (<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>.\nFor MLVU and EgoSchema, we report the results on the official “dev” set. For VideoMME, we report results without subtitles. Each video clip is set to 8 frames.\nAll experiments can be run with one A100 GPU.</p>\n</div>\n</section>\n<section id=\"S5.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models.</h4>\n\n<div id=\"S5.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We apply our method on three popular open-source MLLMs: LLaVA-OneVision-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite>, Qwen2-VL-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite>, and Qwen2.5-VL-3B <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>.</p>\n</div>\n</section>\n<section id=\"S5.SS1.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines.</h4>\n\n<div id=\"S5.SS1.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We evaluate StreamMem against strong baselines, including:</p>\n<ul id=\"S5.I1\" class=\"ltx_itemize\">\n<li id=\"S5.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Query-agnostic streaming video-language understanding with MLLMs, namely LiveVLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite> and InfiniPot-V <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite>, two recent streaming methods that perform KV cache compression independently of the query.</p>\n</div>\n</li>\n<li id=\"S5.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Online video MLLMs such as MovieChat+ <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et al., <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> and Dispider <cite class=\"ltx_cite ltx_citemacro_cite\">Qian et al. (<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>.</p>\n</div>\n</li>\n<li id=\"S5.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">LongVU <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, an MLLM that utilizes visual token compression for long video understanding.</p>\n</div>\n</li>\n</ul>\n<p class=\"ltx_p\">We also report the performance of ReKV <cite class=\"ltx_cite ltx_citemacro_cite\">Di et al. (<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, which stores the KV cache of all previously seen frames without compression.\nWhile it is not feasible in memory-constrained settings for long videos, ReKV serves as an oracle-style upper bound on performance under unbounded memory.</p>\n</div>\n</section>\n</section>\n<section id=\"S5.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2 </span>Main Results</h3>\n\n<section id=\"S5.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Offline video understanding.</h4>\n\n<div id=\"S5.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We report the main results for offline video understanding benchmarks in Table <a href=\"#S4.T1\" title=\"Table 1 ‣ 4.3 Positional Embedding ‣ 4 Method ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For experiments with LLaVA-OneVision, we sample videos shorter than 30 minutes at 0.5 fps and videos longer than 30 minutes at 0.2 fps and constrain the GPU memory allocation below 24 GB, following the setup of LiveVLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>. For Qwen2-VL and Qwen2.5-VL experiments, we sample video less than 3 minutes at 4.0 fps (to match the uniform sampling of 768 frames in InfiniPot-V <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite> and other videos at 0.5 fps. We keep the KV cache size at 6K per transformer layer in the MLLM.</p>\n</div>\n<div id=\"S5.SS2.SSS0.Px1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">From the results we observe that StreamMem outperforms the baselines on all benchmarks except the “long” subset of VideoMME for Qwen2-VL-7B. On LLaVA-OneVision-7B, StreamMem significantly outperforms the uniform sampling baseline with a comparable KV cache size. This highlights the benefits of streaming processing compared to uniform sampling, where significant information loss can incur in the sampling process. On Qwen2.5-VL-3B, StreamMem significantly narrows the gap between full KV and compressed KV on the challenging MLVU benchmark, showing that StreamMem also works well with smaller MLLMs, which are especially suitable for memory-constrained settings.</p>\n</div>\n<div id=\"S5.SS2.SSS0.Px1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">In addition, we provide the performance of StreamMem on different KV size budgets in Table <a href=\"#S4.T3\" title=\"Table 3 ‣ 4.3 Positional Embedding ‣ 4 Method ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, using Qwen2VL-7B as the base MLLM. Analogous results with Qwen2.5VL-3B are reported in the appendix in Table <a href=\"#S8.T8\" title=\"Table 8 ‣ 8 Additional Experiments ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nWe observe from the results that StreamMem outperforms the InfiniPot-V baseline with a bigger margin when we have a larger KV size. Notably, with a budget of 24K tokens (less than half the size of the full KV cache), StreamMem surpasses the performance of the full KV setting. These findings highlight the effectiveness of streaming-based KV processing and compression and its advantage over uniform frame sampling.</p>\n</div>\n</section>\n<section id=\"S5.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Streaming video understanding.</h4>\n\n<div id=\"S5.SS2.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We evaluate StreamMem on the RVS-Ego and RVS-Movie benchmarks for streaming video understanding using LLaVA-OneVision-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite>. Unlike the offline video understanding benchmarks considered earlier, these two datasets pose open-ended question answering tasks that require models to reason over long visual contexts. Following the evaluation protocol used by prior work, we assess the generated answers using <span class=\"ltx_text ltx_font_typewriter\">GPT-3.5-turbo-0125</span>, which judges both the accuracy and an alignment score from 1 to 5. The results are provided in Table <a href=\"#S4.T2\" title=\"Table 2 ‣ 4.3 Positional Embedding ‣ 4 Method ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Consistent with InfiniPot-V, we constrain GPU memory usage to stay below 28 GB. For ReKV without CPU offloading, it simply discards older KVs and retains only recent context as “short-term memory.” The performance drop between ReKV with and without CPU offloading underscores the importance of maintaining long-range memory for high-quality answers.</p>\n</div>\n<div id=\"S5.SS2.SSS0.Px2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">StreamMem outperforms ReKV without offloading and is competitive with InfiniPot-V and Flash-VStream, demonstrating its effectiveness in open-ended question answering under constrained memory settings. These results highlight the method’s ability to retain and utilize salient long-term information throughout streaming video.</p>\n</div>\n</section>\n</section>\n<section id=\"S5.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3 </span>Ablation Studies</h3>\n\n<figure id=\"S5.T4\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Query Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Holistic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">S.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">All</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">True Query</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">80.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">71.6</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">46.4</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">68.1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Generic Text Query</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">78.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Chat Template Query</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">78.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">71.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.9</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation study on different proxy queries for attention-based KV compression.</span></figcaption>\n</figure>\n<figure id=\"S5.T5\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">KV Merging Strategy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Holistic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">S.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">All</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">No Merging</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Avg. Merging</th>\n<td class=\"ltx_td ltx_align_center\">80.5</td>\n<td class=\"ltx_td ltx_align_center\">70.4</td>\n<td class=\"ltx_td ltx_align_center\">41.3</td>\n<td class=\"ltx_td ltx_align_center\">66.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Weighted Merging</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">78.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">71.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.9</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation study on the effect of different KV merging strategies.</span></figcaption>\n</figure>\n<figure id=\"S5.T6\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Input Filtering</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Holistic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">S.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">All</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">No Filtering</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Filtering (<math id=\"S5.T6.m1\" class=\"ltx_Math\" alttext=\"\\delta=0.90\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.90</mn></mrow><annotation encoding=\"application/x-tex\">\\delta=0.90</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">80.1</td>\n<td class=\"ltx_td ltx_align_center\">69.9</td>\n<td class=\"ltx_td ltx_align_center\">42.2</td>\n<td class=\"ltx_td ltx_align_center\">66.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Filtering (<math id=\"S5.T6.m2\" class=\"ltx_Math\" alttext=\"\\delta=0.95\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\delta=0.95</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">78.8</td>\n<td class=\"ltx_td ltx_align_center\">71.5</td>\n<td class=\"ltx_td ltx_align_center\">43.0</td>\n<td class=\"ltx_td ltx_align_center\">66.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Filtering (<math id=\"S5.T6.m3\" class=\"ltx_Math\" alttext=\"\\delta=0.97\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.97</mn></mrow><annotation encoding=\"application/x-tex\">\\delta=0.97</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">80.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">42.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.6</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation study on the effect of frame filtering with varying thresholds.</span></figcaption>\n</figure>\n<div id=\"S5.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We conduct ablation studies on different components in our method. For the ablation experiments, we use LLaVA-OneVision-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2024a</a>)</cite> on the MLVU benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>. We report the average performance on the three subsets of the MLVU, namely holistic tasks (including Topic Reasoning and Anomaly Recognition), single detail (Needle QA, Ego Reasoning, and Plot QA), and multi-detail (including Action Order, and Action Count).</p>\n</div>\n<section id=\"S5.SS3.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Type of proxy query.</h4>\n\n<div id=\"S5.SS3.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We compare the results for using different queries, including the ground truth query, the chat template query, and a generic text query (“What is happening in the video?”) for the attention-based KV compression module in Table <a href=\"#S5.T4\" title=\"Table 4 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that the generic text query obtains similar performance to the chat template query, suggesting that the chat template query, while not including any real text, is implicitly acting as a generic query of video content. Using the ground truth user query for KV compression still significantly outperforms the query-agnostic methods, especially in “multi-detail” tasks, showing the challenge for query-agnostic methods to retain all the details required to answer the question without knowing the question during video processing.</p>\n</div>\n</section>\n<section id=\"S5.SS3.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Merging strategy.</h4>\n\n<div id=\"S5.SS3.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We compare the results for different KV merging strategies in Table <a href=\"#S5.T5\" title=\"Table 5 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We observe that all frame-wise KV cache merging methods perform better than no KV merging, confirming the results from LiveVLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>. StreamMem improved over LiveVLM which uses simple average merging and inserting to the end of each frame by applying weighted merging based on the attention scores between the chat template tokens and the visual tokens, and inserting to the middle of each frame.</p>\n</div>\n</section>\n<section id=\"S5.SS3.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Input frame filtering.</h4>\n\n<div id=\"S5.SS3.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We compare results for different input frame filtering thresholds in Table <a href=\"#S5.T6\" title=\"Table 6 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The results confirm the benefits of input frame filtering to reduce redundancy in the input frames. In terms of the similarity threshold, we found 0.95 to be a sweet spot. While the LongVU paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> found that a DINO vision encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et al., <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> performs better than SigLIP <cite class=\"ltx_cite ltx_citemacro_cite\">Zhai et al. (<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, it requires an external network which incurs additional memory and compute overhead.</p>\n</div>\n</section>\n</section>\n</section>\n<section id=\"S6\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Enabling continuous video stream processing under a bounded memory constraint is essential for deploying multimodal large language models (MLLMs) in real-world, embodied scenarios. Yet, most prior work in long video-language understanding has focused on static or offline settings, assuming known queries, finite video lengths, and full access to the visusal context in advance. These assumptions limit their applicability in streaming or open-world environments. In this work, we present StreamMem, a training-free and query-agnostic KV cache compression framework tailored for streaming video understanding. By using attention scores between visual tokens and chat template tokens as a proxy for query relevance, StreamMem effectively retains salient visual information without requiring access to future queries. When applied to open-source MLLMs, StreamMem achieves state-of-the-art performance across a diverse set of both offline and streaming long video benchmarks. Beyond demonstrating competitive empirical results, we conduct an in-depth analysis of various components in our framework, including input frame filtering, KV merging strategies, and positional embedding techniques, shedding light on the design considerations for constructing a memory-bounded visual processing pipeline. These insights lay a foundation for future research in scaling MLLMs to continuously process real-world visual streams.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Arif et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nKazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S Nikolopoulos, Hans Vandierendonck, Deepu John, and Bo Ji.\n\n</span>\n<span class=\"ltx_bibblock\">Hired: Attention-guided token dropping for efficient inference of high-resolution vision-language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 39, pages 1773–1781, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bai et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2. 5-vl technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2502.13923</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2024a)</span>\n<span class=\"ltx_bibblock\">\nJoya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou.\n\n</span>\n<span class=\"ltx_bibblock\">Videollm-online: Online video large language model for streaming video.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 18407–18418, 2024a.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2024b)</span>\n<span class=\"ltx_bibblock\">\nLiang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang.\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">European Conference on Computer Vision</em>, pages 19–35. Springer, 2024b.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Comanici et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2507.06261</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Corallo et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nGiulio Corallo, Orion Weller, Fabio Petroni, and Paolo Papotti.\n\n</span>\n<span class=\"ltx_bibblock\">Beyond rag: Task-aware kv cache compression for comprehensive knowledge reasoning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2503.04973</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Devoto et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nAlessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini.\n\n</span>\n<span class=\"ltx_bibblock\">A simple and effective <math id=\"bib.bib7.m1\" class=\"ltx_Math\" alttext=\"l\\_2\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">l\\_2</annotation></semantics></math> norm-based strategy for kv cache compression.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2406.11430</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Di et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nShangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, and Hao Jiang.\n\n</span>\n<span class=\"ltx_bibblock\">Streaming video question-answering with in-context video kv-cache retrieval.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2503.00540</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dosovitskiy et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 16x16 words: Transformers for image recognition at scale.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fu et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nChaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 24108–24118, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao.\n\n</span>\n<span class=\"ltx_bibblock\">Not all heads matter: A head-level kv cache compression method with integrated retrieval and reasoning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2410.19258</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ge et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao.\n\n</span>\n<span class=\"ltx_bibblock\">Model tells you what to discard: Adaptive kv cache compression for llms.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2310.01801</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grauman et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Ego4d: Around the world in 3,000 hours of egocentric video.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 18995–19012, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nBo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim.\n\n</span>\n<span class=\"ltx_bibblock\">Ma-lmm: Memory-augmented large multimodal model for long-term video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 13504–13514, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hooper et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W Mahoney, Kurt Keutzer, and Amir Gholami.\n\n</span>\n<span class=\"ltx_bibblock\">Squeezed attention: Accelerating long context length llm inference.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2411.09688</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nKai Hu, Feng Gao, Xiaohan Nie, Peng Zhou, Son Tran, Tal Neiman, Lingyun Wang, Mubarak Shah, Raffay Hamid, Bing Yin, et al.\n\n</span>\n<span class=\"ltx_bibblock\">M-llm based video frame selection for efficient video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 13702–13712, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hurst et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Gpt-4o system card.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2410.21276</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kim et al. (2025a)</span>\n<span class=\"ltx_bibblock\">\nJang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W Lee, Sangdoo Yun, and Hyun Oh Song.\n\n</span>\n<span class=\"ltx_bibblock\">Kvzip: Query-agnostic kv cache compression with context reconstruction.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2505.23416</em>, 2025a.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kim et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nMinsoo Kim, Kyuhong Shim, Jungwook Choi, and Simyung Chang.\n\n</span>\n<span class=\"ltx_bibblock\">Infinipot: Infinite context processing on memory-constrained llms.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2410.01518</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kim et al. (2025b)</span>\n<span class=\"ltx_bibblock\">\nMinsoo Kim, Kyuhong Shim, Jungwook Choi, and Simyung Chang.\n\n</span>\n<span class=\"ltx_bibblock\">Infinipot-v: Memory-constrained kv cache compression for streaming video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2506.15745</em>, 2025b.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. (2024a)</span>\n<span class=\"ltx_bibblock\">\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Llava-onevision: Easy visual task transfer.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2408.03326</em>, 2024a.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. (2024b)</span>\n<span class=\"ltx_bibblock\">\nYanwei Li, Chengyao Wang, and Jiaya Jia.\n\n</span>\n<span class=\"ltx_bibblock\">Llama-vid: An image is worth 2 tokens in large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">European Conference on Computer Vision</em>, pages 323–340. Springer, 2024b.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. (2024c)</span>\n<span class=\"ltx_bibblock\">\nYuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen.\n\n</span>\n<span class=\"ltx_bibblock\">Snapkv: Llm knows what you are looking for before generation.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 37:22947–22970, 2024c.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJianxin Liang, Xiaojun Meng, Yueqian Wang, Chang Liu, Qun Liu, and Dongyan Zhao.\n\n</span>\n<span class=\"ltx_bibblock\">End-to-end video question answering with frame scoring mechanisms and adaptive sampling.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2407.15047</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\n\n</span>\n<span class=\"ltx_bibblock\">Visual instruction tuning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 36:34892–34916, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n\n</span>\n<span class=\"ltx_bibblock\">Improved baselines with visual instruction tuning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 26296–26306, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. (2025a)</span>\n<span class=\"ltx_bibblock\">\nXiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao.\n\n</span>\n<span class=\"ltx_bibblock\">Video-xl-pro: Reconstructive token compression for extremely long video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2503.18478</em>, 2025a.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. (2025b)</span>\n<span class=\"ltx_bibblock\">\nXuyang Liu, Yiyu Wang, Junpeng Ma, and Linfeng Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Video compression commander: Plug-and-play inference acceleration for video large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2505.14454</em>, 2025b.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mangalam et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nKarttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik.\n\n</span>\n<span class=\"ltx_bibblock\">Egoschema: A diagnostic benchmark for very long-form video language understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 36:46212–46244, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ning et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nZhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, and Jieru Zhao.\n\n</span>\n<span class=\"ltx_bibblock\">Livevlm: Efficient online video understanding via streaming-oriented kv cache and retrieval.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2505.15269</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Oquab et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nMaxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Dinov2: Learning robust visual features without supervision.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2304.07193</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Park et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nJunyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, and Chris Lott.\n\n</span>\n<span class=\"ltx_bibblock\">Keydiff: Key similarity-based kv cache eviction for long-context llm inference in resource-constrained environments.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2504.15364</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Peng et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.\n\n</span>\n<span class=\"ltx_bibblock\">Yarn: Efficient context window extension of large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.00071</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Qian et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nRui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 24045–24055, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shen et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nXiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Longvu: Spatiotemporal adaptive compression for long video-language understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2410.17434</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shu et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nYan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao.\n\n</span>\n<span class=\"ltx_bibblock\">Video-xl: Extra-long vision language model for hour-scale video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 26160–26169, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Song et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nEnxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Moviechat+: Question-aware sparse memory for long video question answering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2404.17176</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Su et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\n\n</span>\n<span class=\"ltx_bibblock\">Roformer: Enhanced transformer with rotary position embedding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Neurocomputing</em>, 568:127063, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tan et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nReuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A Plummer, Bryan Russell, and Kate Saenko.\n\n</span>\n<span class=\"ltx_bibblock\">Koala: Key frame-conditioned long video-llm.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 13581–13591, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tang et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nXi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, and Qixiang Ye.\n\n</span>\n<span class=\"ltx_bibblock\">Adaptive keyframe sampling for long video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 29118–29128, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tao et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nKeda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Dycoke: Dynamic compression of tokens for fast video large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 18992–19001, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2024a)</span>\n<span class=\"ltx_bibblock\">\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2409.12191</em>, 2024a.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2024b)</span>\n<span class=\"ltx_bibblock\">\nXiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie.\n\n</span>\n<span class=\"ltx_bibblock\">Retake: Reducing temporal and knowledge redundancy for long video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2412.20504</em>, 2024b.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nXiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie.\n\n</span>\n<span class=\"ltx_bibblock\">Adaretake: Adaptive redundancy reduction to perceive longer for video-language understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2503.12559</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei and Chen (2024)</span>\n<span class=\"ltx_bibblock\">\nHongchen Wei and Zhenzhong Chen.\n\n</span>\n<span class=\"ltx_bibblock\">Visual context window extension: A new perspective for long video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2409.20018</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xiong et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Effective long-context scaling of foundation models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.16039</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, and Doyen Sahoo.\n\n</span>\n<span class=\"ltx_bibblock\">Think: Thinner key cache by query-driven pruning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2407.21018</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhai et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.\n\n</span>\n<span class=\"ltx_bibblock\">Sigmoid loss for language image pre-training.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF international conference on computer vision</em>, pages 11975–11986, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2025a)</span>\n<span class=\"ltx_bibblock\">\nBoqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Videollama 3: Frontier multimodal foundation models for image and video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2501.13106</em>, 2025a.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024a)</span>\n<span class=\"ltx_bibblock\">\nHaoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin.\n\n</span>\n<span class=\"ltx_bibblock\">Flash-vstream: Memory-based real-time understanding for long video streams.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2406.08085</em>, 2024a.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024b)</span>\n<span class=\"ltx_bibblock\">\nPeiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu.\n\n</span>\n<span class=\"ltx_bibblock\">Long context transfer from language to vision.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2406.16852</em>, 2024b.\n\n</span>\n</li>\n<li id=\"bib.bib52\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024c)</span>\n<span class=\"ltx_bibblock\">\nQizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Beyond text-visual attention: Exploiting visual cues for effective token pruning in vlms.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2412.01818</em>, 2024c.\n\n</span>\n</li>\n<li id=\"bib.bib53\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2025b)</span>\n<span class=\"ltx_bibblock\">\nShaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng.\n\n</span>\n<span class=\"ltx_bibblock\">Llava-mini: Efficient image and video large multimodal models with one vision token.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2501.03895</em>, 2025b.\n\n</span>\n</li>\n<li id=\"bib.bib54\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024d)</span>\n<span class=\"ltx_bibblock\">\nYiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, and Yining Sun.\n\n</span>\n<span class=\"ltx_bibblock\">Beyond training: Dynamic token merging for zero-shot video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2411.14401</em>, 2024d.\n\n</span>\n</li>\n<li id=\"bib.bib55\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024e)</span>\n<span class=\"ltx_bibblock\">\nYiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zhili Feng, Zenghui Ding, and Yining Sun.\n\n</span>\n<span class=\"ltx_bibblock\">Rankclip: Ranking-consistent language-image pretraining.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2404.09387</em>, 2024e.\n\n</span>\n</li>\n<li id=\"bib.bib56\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2025c)</span>\n<span class=\"ltx_bibblock\">\nYiming Zhang, Chengzhang Yu, Zhuokai Zhao, Kun Wang, Qiankun Li, Zihan Chen, Yang Liu, Zenghui Ding, and Yining Sun.\n\n</span>\n<span class=\"ltx_bibblock\">Circuitprobe: Dissecting spatiotemporal visual semantics with circuit tracing.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2507.19420</em>, 2025c.\n\n</span>\n</li>\n<li id=\"bib.bib57\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024f)</span>\n<span class=\"ltx_bibblock\">\nYuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Sparsevlm: Visual token sparsification for efficient vision-language model inference.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2410.04417</em>, 2024f.\n\n</span>\n</li>\n<li id=\"bib.bib58\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024g)</span>\n<span class=\"ltx_bibblock\">\nYuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.\n\n</span>\n<span class=\"ltx_bibblock\">Video instruction tuning with synthetic data.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2410.02713</em>, 2024g.\n\n</span>\n</li>\n<li id=\"bib.bib59\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al.\n\n</span>\n<span class=\"ltx_bibblock\">H2o: Heavy-hitter oracle for efficient generative inference of large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 36:34661–34710, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib60\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nJunjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Mlvu: Benchmarking multi-task long video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pages 13691–13701, 2025.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div id=\"p3\" class=\"ltx_para\">\n<span class=\"ltx_ERROR undefined\" lang=\"en\">\\beginappendix</span>\n</div>\n<section id=\"S7\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Experiment Details</h2>\n\n<section id=\"S7.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Hyper-parameter details.</h4>\n\n<div id=\"S7.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In terms of the YaRN scaling factor <math id=\"S7.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\" intent=\":literal\"><semantics><mi>λ</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, we used <math id=\"S7.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"\\lambda=8\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=8</annotation></semantics></math> for LLaVA-OneVision, <math id=\"S7.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"\\lambda=2\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math> for Qwen2-VL, and <math id=\"S7.SS0.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"\\lambda=1\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math> (no scaling) for Qwen2.5-VL. The difference is due to the different default context length in these MLLMs. We keep the other hyperparameters the same across different models: input frame filtering similarity threshold <math id=\"S7.SS0.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"\\delta=0.95\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\delta=0.95</annotation></semantics></math>; each new frame chunk has a size of 8 frames; weighted frame-wise KV merging based on the attention scores (as described in the main text) and inserted to the middle of each frame.</p>\n</div>\n</section>\n<section id=\"S7.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Video sampling details.</h4>\n\n<div id=\"S7.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Following InfiniPot-V, we use the following hyperparameter values for the Qwen2-VL vision processor: <span class=\"ltx_text ltx_font_typewriter\">FPS_MAX_FRAMES</span> <math id=\"S7.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"=768\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mn>768</mn></mrow><annotation encoding=\"application/x-tex\">=768</annotation></semantics></math> and <span class=\"ltx_text ltx_font_typewriter\">VIDEO_MAX_PIXEL<math id=\"S7.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"=768\\times 28\\times 28\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>768</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>28</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>28</mn></mrow></mrow><annotation encoding=\"application/x-tex\">=768\\times 28\\times 28</annotation></semantics></math></span>. The vision processor resizes each image such that the width and height are both divisible by 28; each frame is encoded into up to 130 tokens for Qwen2-VL and Qwen2.5-VL, and 196 tokens for LLaVA-OneVision.</p>\n</div>\n</section>\n<section id=\"S7.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">MLVU evaluation details.</h4>\n\n<div id=\"S7.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We would like to note that there are two different ways prior papers report results on the MLVU benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>: (1) computing the overall accuracy on the entire benchmark (used by LiveVLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>), and (2) computing the accuracy on each task separately and average the accuracy across tasks (used by InfiniPot-V <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2025b</a>)</cite>). The “overall accuracy” computed with (1) is often a bit higher than that of (2). For a fair comparison against both baselines, we use the first method for experiments using LLaVA-OneVision as the base MLLM and the second method for experiments using Qwen2VL-7B and Qwen2.5VL-3B as the base MLLM.</p>\n</div>\n</section>\n</section>\n<section id=\"S8\" class=\"ltx_section\" lang=\"en\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">8 </span>Additional Experiments</h2>\n\n<figure id=\"S8.T7\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">YaRN Scaling Factor</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Holistic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">S.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">All</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<math id=\"S8.T7.m1\" class=\"ltx_Math\" alttext=\"\\lambda=1\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=1</annotation></semantics></math> (No scaling)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S8.T7.m2\" class=\"ltx_Math\" alttext=\"\\lambda=2\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\">78.6</td>\n<td class=\"ltx_td ltx_align_center\">69.2</td>\n<td class=\"ltx_td ltx_align_center\">41.9</td>\n<td class=\"ltx_td ltx_align_center\">65.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S8.T7.m3\" class=\"ltx_Math\" alttext=\"\\lambda=4\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=4</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\">80.5</td>\n<td class=\"ltx_td ltx_align_center\">70.9</td>\n<td class=\"ltx_td ltx_align_center\">42.4</td>\n<td class=\"ltx_td ltx_align_center\">66.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><math id=\"S8.T7.m4\" class=\"ltx_Math\" alttext=\"\\lambda=8\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=8</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">78.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">71.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.9</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation study on the effect of YaRN visual context window extension with varying scaling factors.</span></figcaption>\n</figure>\n<figure id=\"S8.T8\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">KV Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Holistic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">S.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.D.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">All</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Full KV</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">50K</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">63.3</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">78.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">65.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">41.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">62.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">12K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">77.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">67.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">42.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">63.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">StreamMem (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">24K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">78.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">68.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">44.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">64.3</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 8</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Performance of StreamMem on MLVU with different KV sizes. We use Qwen2.5VL-3B as the base MLLM.</span></figcaption>\n</figure>\n<section id=\"S8.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Ablation on YaRN scaling factor.</h4>\n\n<div id=\"S8.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We report results for LLaVA-OneVision with different YaRN scaling factors in Table <a href=\"#S8.T7\" title=\"Table 7 ‣ 8 Additional Experiments ‣ StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that YaRN visual context window extension significantly improves the video undestanding performance, and the performance can be sensitive to different values of the YaRN scaling factor. While <math id=\"S8.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"\\lambda=4\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=4</annotation></semantics></math> and <math id=\"S8.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"\\lambda=8\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=8</annotation></semantics></math> give decent overall results, <math id=\"S8.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"\\lambda=4\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=4</annotation></semantics></math> performs better on holistic tasks and <math id=\"S8.SS0.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"\\lambda=8\" display=\"inline\" intent=\":literal\"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=8</annotation></semantics></math> performs better on single detail and multi-detail tasks.</p>\n</div>\n</section>\n</section>",
  "css": "",
  "arxiv_id": "2508.15717",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:25.818Z"
}
