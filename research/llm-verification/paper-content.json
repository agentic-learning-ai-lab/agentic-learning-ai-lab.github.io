{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">1â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Introduction</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p1\">\n<p class=\"ltx_p\">Problem-solving with LLMs has shifted from solely querying a model for a solution to a system where models both solve and verify. The paradigm of verifying solutions at test time is broad, spanning both simple strategies, such as generating multiple candidate solutions and using a verifier as a filterÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2025sample</span>)</cite>, and more complex iterative refinement approachesÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">madaan2023selfrefine</span>)</cite>. With generative verification at test time, LLMs can solve more complex problems than they can when used alone <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cobbe2021training</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lightman2024lets</span>)</cite>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p2\">\n<p class=\"ltx_p\">Despite the increasing dominance of this paradigm, studies of solverâ€“verifier interactions have remained limited in scope. Prior work has largely examined how a single model verifies its own solutions (self-verification) and improves itself (self-improvement) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025mind</span>)</cite>, yet self-verification is not guaranteed to be effective: models may be biased toward their own reasoning patterns, their training may reinforce these tendencies, and different tasks may vary in how much they benefit from verification. Additionally, this focus on self-verification offers little insight into how verification behaves when the solver and verifier differ. With open-source model families that have base and post-trained pairs, size variants, reproducible inference pipelines, and explicit reasoning traces, we can systematically study verification across models. We therefore broaden our analysis to include both intra-family and cross-family verification and ask the following central question:</p>\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">When does verification actually pay off, and how does each factor, such as model family, model size, post-training, solverâ€“verifier similarity, or task type, influence how effective verification is in improving the solver?</em></p>\n</blockquote>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p3\">\n<p class=\"ltx_p\">To accomplish this, we evaluate verifiers across a diverse suite of tasks, including synthetic tasks used to test precise logical reasoning or symbolic computation (3-SAT, Sudoku, and matrix multiplication), mathematical reasoning tasks (AIME <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AIME2025</span>)</cite>, GSM8K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cobbe2021training</span>)</cite>), commonsense and factual reasoning (CSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">csqa</span>)</cite>, GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite>), and broad domain knowledge (MMLU in STEM and social sciences <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlu</span>)</cite>) using 37 models from 7 model families.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p4\">\n<p class=\"ltx_p\">Our analysis indicates that self-verification does not always â€œpay offâ€: models often favor solutions resembling their own reasoning (SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS1\" title=\"5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>,Â <a class=\"ltx_ref\" href=\"#S5.SS3\" title=\"5.3 Are Verifiers Biased Toward Solutions That Resemble Their Own? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), post-training can sharpen this bias (SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS4\" title=\"5.4 How Does Post-Training Affect Solver and Verifier Performance? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), and some tasks inherently benefit less from verification than others (SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS5\" title=\"5.5 Which Datasets are Easy to Verify? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>). Therefore, we present the following contributions, which offer actionable and empirically supported guidance for how to use verifiers effectively.</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">â€¢</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">New Metric: Verifier Gain.</span> Verifier accuracy alone provides an incomplete picture of verifier usefulness at test time. To address this, we derive <span class=\"ltx_text ltx_font_italic\">verifier gain</span>, a metric that simulates the improvement obtained from a verifier during test-time rejection sampling. We empirically study rejection sampling with verifiers and show that this theoretical formulation closely reflects empirical performance trends.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">â€¢</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Improvement, Intra-Family Improvement, and Cross-Family Improvement.</span> We extensively compare performance improvements from self-verification, intra-family verification, and cross-family verification, finding that cross-family verification is often the most beneficial, particularly when compared to self-verification. We link these differences to similarities in the solution distributions of the solver and verifier: verifier gain decreases as these distributions become more similar. Our results suggest that as models become stronger, whether through increased scale, post-training, or simply higher solver accuracy, they become less effective as self-verifiers and more effective as cross-family verifiers.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">â€¢</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Verifiability.</span> We study whether tasks that are easy to solve are also easy to verify and whether some tasks are inherently more verifiable than others. We find that verification accuracy generally correlates with solver accuracy, though self-verification yields little verifier gain across all tasks. We also observe that a clear subset of tasks involving mathematical or logical reasoning consistently produces higher verifier gains.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">2â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Related Work</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Verifiers.</span> Broadly, verifiers can operate on the <span class=\"ltx_text ltx_font_italic\">outcome</span> level, by judging only the correctness of the final answer, or the <span class=\"ltx_text ltx_font_italic\">process</span> level, by judging the correctness of the intermediate reasoning steps. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">weng2023-better-reasoners</span></cite>, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu-etal-2024-key-condition-verification</span></cite>, and <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang-etal-2024-forward-backward</span></cite> develop methods for outcome-level self-verification by predicting parts of the question conditioned on the solution. In order to reduce hallucinations, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dhuliawala-etal-2024-chain-of-verification</span></cite> have language models fact-check their own generations by generating fact-check questions. Researchers have also trained general-purpose outcome level verifiersÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hosseini2024vstar</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang-generativeverifiers</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cobbe2021training</span>)</cite> and value modelsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu-etal-2024-ovm</span>)</cite>, either independently or jointly with the solverÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen-etal-2021-generate-rank</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sareen2025putting-value</span>)</cite>. Work on process-level verification has focused on deductively verifying Chain-of-Thought (CoT) reasoningÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ling2023deductive-verification</span>)</cite>, verifying individual proof stepsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang-etal-2022-generating-nl-proofs</span>)</cite>, and training process reward models for mathematical reasoningÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2025improve-automated-process-supervision</span>)</cite>. Finally, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025mind</span></cite> investigate the performance improvement caused by using an outcome-level verifier (the GV-Gap), and how this improvement changes as the solver or verifier increases in capacity. However, they primarily focus on cases where the solver and verifier are the same model. Additionally, they only study base models and do not consider post-trained models in their analysis.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p2\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling test-time compute.</span> Recently, prior work focused on studying scaling test-time compute with verifiers. For example, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2025sample</span></cite> study random sampling with self-verification, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025sets</span></cite> study combining parallel sampling with self-correction, and <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singhi2025solve</span></cite> compare Self-ConsistencyÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023selfconsistency</span>)</cite> to scaling with a generative verifier. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">snell2025scaling-tt-compute</span></cite> investigate compute-optimal approaches to test-time scaling with process-level verification. Finally, verifiers also have their limitations. Imperfect verifiers can produce false positivesÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stroebl2024inference-flaws</span>)</cite>, eliminate valid reasoning pathsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2025scaling-flaws-math</span>)</cite>, and fail to select the right solutionÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brown2024large</span>)</cite>. We present additional related work on test-time verification in AppendixÂ <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Related Work â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p3\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-improvement.</span> Researchers have also studied LLM self-improvement and self-evaluation, with some voicing skepticism <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2024large-cannot-self-correct</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kamoi-etal-2024-when-self-correct</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">olausson2023self</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llm-evaluators</span>)</cite>. On the other hand, recent work has provided a theoretical framework for self-improvement via distribution sharpening, and empirical support alongside <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2024self-sharpening</span>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang-etal-2024-small-large-ver</span></cite> look specifically at self-improvement for small models, arguing that they need to be paired with a stronger verifier. Some practical methods for self-improvement use natural language feedback <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">madaan2023selfrefine</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shinn2023reflexion</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2023language</span>)</cite> or train models for self-correction explicitly <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">welleck2023generating</span>)</cite>. Other methods use tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gou2024critic</span>)</cite>, particularly code interpreters <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2024solving</span>)</cite>, to iteratively improve solutions.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"153\" id=\"S2.F1.g1\" src=\"./assets/x1.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nAverage solver accuracy of each model over all datasets. Base model families are suffixed by <span class=\"ltx_text ltx_font_bold\">-Base</span>. Models within each family are ordered in increasing sizes.\n</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">3â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Preliminaries</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p1\">\n<p class=\"ltx_p\">In this section, we establish the framework used throughout this work. We define datasets, solvers, and verifiers, introduce the metrics used to evaluate solver and verifier behaviors, and specify the verification settings in our empirical analysis.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">3.1â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Dataset, Solvers, and Verifiers</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\">Let <math alttext=\"\\mathcal{D}\\subseteq\\mathcal{X}\\times\\mathcal{Y}^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo>âŠ†</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">ğ’³</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">Ã—</mo><msup><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mo>â‹†</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}\\subseteq\\mathcal{X}\\times\\mathcal{Y}^{\\star}</annotation></semantics></math> be a dataset of pairs <math alttext=\"(x,\\mathcal{Y}_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,\\mathcal{Y}_{x})</annotation></semantics></math>, where <math alttext=\"x\\in\\mathcal{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>âˆˆ</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’³</mi></mrow><annotation encoding=\"application/x-tex\">x\\in\\mathcal{X}</annotation></semantics></math> is a problem and <math alttext=\"\\mathcal{Y}_{x}\\subseteq\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub><mo>âŠ†</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{Y}_{x}\\subseteq\\mathcal{Y}</annotation></semantics></math> is a non-empty set of correct solutions.\nA solver <math alttext=\"S:\\mathcal{X}\\to\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">ğ’³</mi><mo stretchy=\"false\">â†’</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi></mrow></mrow><annotation encoding=\"application/x-tex\">S:\\mathcal{X}\\to\\mathcal{Y}</annotation></semantics></math> is an LLM that produces a solution <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> for a given problem <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, and a verifier <math alttext=\"V:\\mathcal{X}\\times\\mathcal{Y}\\to\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">ğ’³</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">Ã—</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi></mrow><mo stretchy=\"false\">â†’</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">V:\\mathcal{X}\\times\\mathcal{Y}\\to\\{0,1\\}</annotation></semantics></math> is an LLM that evaluates a problemâ€“solution pair and returns a binary judgment. Following <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025mind</span></cite>, who find chain-of-thought (CoT) verification more stable than multiple-choice formats, we instruct both solvers and verifiers to generate CoT reasoning before producing their final solutions and judgments. We define the correctness indicator as <math alttext=\"c(x,y)=\\mathbbm{1}\\{y\\in\\mathcal{Y}_{x}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>ğŸ™</mn><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>y</mi><mo>âˆˆ</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c(x,y)=\\mathbbm{1}\\{y\\in\\mathcal{Y}_{x}\\}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">3.2â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Evaluation Metrics</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\">We define the accuracy of a solver <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> on a dataset <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> as the expected correctness of its outputs over all problems in the dataset: <math alttext=\"\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,c(x,y)\\,\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">ğ”¼</mi><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>âˆ¼</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mi>y</mi><mo>âˆ¼</mo><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" rspace=\"0.170em\">[</mo><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo rspace=\"0.170em\" stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,c(x,y)\\,\\big]</annotation></semantics></math>. Verifier performance has several dimensions. We report common binary classification metrics, including verifier accuracy, false positive rate (FPR), false negative rate (FNR), F1-Score, and precision, with their definitions in AppendixÂ <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Details on Verifier Metrics â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\">Our primary goal is to evaluate whether using a verifier <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> can improve a solver <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> at test time via rejection sampling, in which solver outputs are repeatedly sampled until the verifier accepts one. Assuming the solver has a non-zero probability of sampling a correct solution, and in the limit of infinite resampling, the expected correctness of the accepted solution converges to the verifierâ€™s precision, i.e., the proportion of accepted solutions that are actually correct. To quantify the improvement from combining a solver with a verifier, we define <span class=\"ltx_text ltx_font_bold\">verifier gain</span>:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Gain}(S,V;\\mathcal{D})=\\text{Precision}(S,V;\\mathcal{D})-\\text{SolverAcc}(S;\\mathcal{D}).\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mtext>Gain</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mtext>Precision</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>âˆ’</mo><mrow><mtext>SolverAcc</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Gain}(S,V;\\mathcal{D})=\\text{Precision}(S,V;\\mathcal{D})-\\text{SolverAcc}(S;\\mathcal{D}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">It is worth noting that verifier gain is an asymptotic metric: it reflects the limit of infinite sampling and therefore serves as a bound on the improvement attainable by verifier-based rejection sampling. Throughout this work, we use verifier gain to compare differences in verifier behavior.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">3.3â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Verification Settings</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p1\">\n<p class=\"ltx_p\">We group models into families (e.g., <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span>), where each family contains related models of varying sizes. Because base and post-trained models often exhibit substantially different behaviors, we treat them as distinct families. For example, the base model <span class=\"ltx_text ltx_font_typewriter\">meta-llama/Meta-Llama-3-70B</span> belongs to the <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span> family and the post-trained model <span class=\"ltx_text ltx_font_typewriter\">meta-llama/Meta-Llama-3-8B-Instruct</span> belongs to the <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> family.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p2\">\n<p class=\"ltx_p\">We categorize each solverâ€“verifier pair from our pool of models into one of three verification settings. Each solverâ€“verifier pair is considered an instance of:</p>\n<ol class=\"ltx_enumerate\" id=\"S3.I1\">\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Verification.</span> The solver and verifier are the same model, so the model verifies its own solutions. For example, when a 70B <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> model is used as both the solver and verifier, the verification metric (e.g., accuracy, FPR) is computed on this single pairing.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intra-Family Verification.</span> The verifier evaluates solutions produced by other models within the same family. For example, a 70B <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> verifier may evaluate outputs from 8B or 13B <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> solvers. The reported metric is averaged over all such within-family solvers, excluding the self-verification case.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i3.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Family Verification.</span> The verifier evaluates solutions produced by models from different families. For example, a base <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> verifier may evaluate outputs from <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> or from a post-trained <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>. The reported metric is averaged over all such cross-family solvers.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p3\">\n<p class=\"ltx_p\">Using these three categorizations, we evaluate a verifier by applying it to a set of solver models, computing the corresponding verifier metrics, and then partitioning these metrics according to the three verification settings and averaging within each partition. A formal mathematical description of each verification setting is provided in AppendixÂ <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Additional Details on Verification Settings â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">4â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Experimental Setup</span>\n</h2>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We evaluate the solver and verifier abilities of 21 post-trained models from the <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>Â <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama3</span>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span>Â <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen2.5</span>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span>Â <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3</span>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1</span>Â <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deepseek</span>)</cite> families. For our study of post-training effects in SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS4\" title=\"5.4 How Does Post-Training Affect Solver and Verifier Performance? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>, we additionally evaluate 16 base models from the <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> families. Model sizes range from 0.5B to 72B parameters. The full model list, with sizes, families, and HuggingFace identifiers, is provided in AppendixÂ <a class=\"ltx_ref\" href=\"#A4\" title=\"Appendix D Additional Details on Models â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. FigureÂ <a class=\"ltx_ref\" href=\"#S2.F1\" title=\"Figure 1 â€£ 2 Related Work â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>â€™s legend displays the seven model families and the color scheme assigned to each.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">To comprehensively evaluate each modelâ€™s performance as both a solver and a verifier, we compile a broad suite of real-world and synthetic tasks spanning diverse domains. We include tasks requiring mathematical reasoning (GSM8K, AIME), commonsense knowledge (CSQA), and domain-specific factual knowledge of varying breadth (MMLU STEM, MMLU Social Sciences, GPQA). We also construct synthetic tasks to assess logical reasoning (3SAT), structured puzzle solving (Sudoku), and symbolic computation (Matrix Multiplication). Further dataset details, along with examples of our synthetic tasks, are provided in AppendixÂ <a class=\"ltx_ref\" href=\"#A5\" title=\"Appendix E Additional Details on Datasets â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>. Code for all experiments, including synthetic-data generation, is included in the Supplementary Materials and will be open-sourced upon publication.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Datasets such as Matrix Multiplication and the natural-language benchmarks contain a single ground-truth answer per problem. For these, we extract boxed solver outputs and evaluate them via exact matching. In contrast, datasets like Sudoku and 3SAT may admit multiple valid solutions, so we evaluate solver outputs according to the rules of the respective task. To evaluate verifiers, we prompt each model to generate CoT reasoning from the problem and solver answer before producing a boxed â€œcorrectâ€ or â€œincorrect\", from which we extract the final judgment.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Implementation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">For both solvers and verifiers, we generate with temperatureÂ <math alttext=\"0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>0.7</mn><annotation encoding=\"application/x-tex\">0.7</annotation></semantics></math>, top-pÂ <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>, and a maximum output length of 8192 tokens. We discard outputs that do not contain a boxed answer. All inference experiments are run using vLLM on H200 GPUs. Prompts and additional details on output filtering are provided in AppendixÂ <a class=\"ltx_ref\" href=\"#A6\" title=\"Appendix F Additional Details on Experimental Setup â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">5â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Results</span>\n</h2>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.1â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Do Better Solvers Make Better Verifiers?</span>\n</h3>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Solver performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We first benchmark the performance of all 37 models on each of our 9 datasets, averaging performance across tasks (FigureÂ <a class=\"ltx_ref\" href=\"#S2.F1\" title=\"Figure 1 â€£ 2 Related Work â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) and reporting task-level results in AppendixÂ <a class=\"ltx_ref\" href=\"#A7\" title=\"Appendix G Solver Accuracy by Dataset â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>. Overall, solver accuracy increases with model capacity. Models in the <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> families perform particularly well, whereas <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span> performs poorly due to base models being unfamiliar with the questionâ€“answering instruction format. Within each family, we observe clear performance scaling for <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span>, with the remaining families showing similar upward trends.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Correlating verifier and solver performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\">After establishing solver accuracy, we analyze whether a modelâ€™s solver performance correlates with its performance as a verifier (FigureÂ <a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For each of our 21 post-trained models and each dataset, we evaluate verification on the same set of solver models to obtain verifier accuracy, F1-score, precision, FPR, FNR, and gain for every solverâ€“verifier pair. For each verifier, we then divide the verifier metrics into three verification settings and average within each setting over solvers and datasets.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p2\">\n<p class=\"ltx_p\">Verifier accuracy tends to improve with the verifierâ€™s own solver accuracy, but the relationship becomes more nuanced when examining other metrics. The FPR increases during self-verification and intra-family verification but decreases slightly during cross-family verification. This indicates that verifiers with stronger solver abilities are more likely to incorrectly label solutions as correct when verifying their own outputs or outputs from models within their family. We provide additional visualizations for F1-score and precision in AppendixÂ <a class=\"ltx_ref\" href=\"#A8\" title=\"Appendix H F1-Score and Precision Visualization â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_figure\" id=\"S5.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1036\" id=\"S5.F2.g1\" src=\"./assets/x2.png\" width=\"789\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nCorrelation between each verifierâ€™s metrics (rows) and its own solver accuracy for all 21 post-trained models, averaged over all datasets. Each verifier metric is computed over our three verification settings (columns).\n</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_figure\" id=\"S5.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"323\" id=\"S5.F3.g1\" src=\"./assets/x3.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nCorrelation between each verifierâ€™s metrics (rows) and model size for all 21 post-trained models, averaged over all datasets. In each plot, models are separated by family and ordered by increasing size. Each verifier metric is computed over our three verification settings (columns).\n</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p3\">\n<p class=\"ltx_p\">To better interpret the trends suggested by the accuracy and FPR results, we examine verifier gain in the final row. Verifier gain quantifies the expected benefit of using the verifier during rejection sampling, i.e., repeatedly sampling until the verifier accepts a solution (a common solverâ€“verifier interaction setting <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025mind</span>)</cite>). This visualization offers a clearer view of verification quality: self-verification yields the smallest gains, and more accurate solvers do not exhibit greater self-improvement. Gains increase slightly in intra-family verification, while cross-family verification provides the greatest potential benefits.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Examining verifier performance at different model families and sizes.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\">In FigureÂ <a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we repeat the experiments from FigureÂ <a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> but plot each verifier metric as a function of model size within each model family. We observe that verification accuracy and FNR consistently improve as models become larger, whereas FPR behaves more inconsistently, often increasing with model size (e.g., intra-family verification for <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span>).</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p2\">\n<p class=\"ltx_p\">For state-of-the-art post-trained models such as <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span>, we find that verifier gains are largest in the cross-family setting, smaller in the intra-family setting, and minimal during self-verification. At first glance, this appears to contradict <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025mind</span></cite>, who report that self-verification GV-Gaps increase with more pretraining FLOPs. However, their analysis focuses on older model families, and FigureÂ <a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> likewise shows larger verifier gains for older models such as <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> and <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p3\">\n<p class=\"ltx_p\">We hypothesize that stronger post-trained models like <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> show negligible gains in self-verification and limited gains in intra-family verification for two reasons: (a) they may already engage in <span class=\"ltx_text ltx_font_italic\">spontaneous</span> self-verification when used as solvers, reducing the benefit of an additional <span class=\"ltx_text ltx_font_italic\">forced</span> verification round, and (b) their distributions are significantly sharpened by post-training <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2024self-sharpening</span>)</cite>, which limits the improvement obtained from rejection sampling.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p4\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span></p>\n<ul class=\"ltx_itemize\" id=\"S5.I1\">\n<li class=\"ltx_item\" id=\"S5.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">â€¢</span>\n<div class=\"ltx_para\" id=\"S5.I1.i1.p1\">\n<p class=\"ltx_p\">Verifier models are biased toward accepting incorrect solutions when performing self-verification or intra-family verification.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">â€¢</span>\n<div class=\"ltx_para\" id=\"S5.I1.i2.p1\">\n<p class=\"ltx_p\">Verification accuracy alone is not a reliable predictor of how much a verifier can improve a solver at test time. Instead, computing verifier gain using solver accuracy and verifier precision provides a more reliable metric.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">â€¢</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.I1.i3.p1\">\n<p class=\"ltx_p\">While model families like <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> show some ability to self-improve based on their verifier gains, stronger model families like <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> do not.</p>\n</div>\n</li>\n</ul>\n</blockquote>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"323\" id=\"S5.F4.g1\" src=\"./assets/x4.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nComparison between theoretical and empirical verifier gains (rows) for each verification setting (columns). Row 1 shows verifier gains computed from EquationÂ <a class=\"ltx_ref\" href=\"#S3.E1\" title=\"Equation 1 â€£ 3.2 Evaluation Metrics â€£ 3 Preliminaries â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Rows 2 and 3 each show the gains from rejection sampling, computed from rejection sampling using verifiers for up to 5 and 9 solver attempts, respectively.\n</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.2â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Is Verifier Gain a Good Predictor for Improvements from Resampling?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p1\">\n<p class=\"ltx_p\">Our verifier gain metric estimates the expected improvement in a solverâ€™s accuracy when using a verifier for rejection sampling. To assess how well this metric predicts real performance, we conduct rejection sampling experiments across all solverâ€“verifier pairs from a 12-model subset of our post-trained models, consisting of the three smallest models from each of the four post-training families. For each problem in each dataset, the solver generates solutions until the verifier labels one as correct, for up to nine attempts; if no such solution is found, we retain the final attempt. The empirical results, along with the corresponding theoretical verifier gains, are shown in FigureÂ <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 â€£ Examining verifier performance at different model families and sizes. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p2\">\n<p class=\"ltx_p\">Although the measured improvements can be noisy when verifier gains are small and the nine-attempt cap limits the rejection sampling procedure, the overall trends align closely with theoretical predictions. In particular, we observe smaller gains from self-verification than from both intra-family and cross-family verification, as well as stronger scaling behavior for cross-family verification relative to the other settings.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p3\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>Â \nThe verifier gain given by EquationÂ <a class=\"ltx_ref\" href=\"#S3.E1\" title=\"Equation 1 â€£ 3.2 Evaluation Metrics â€£ 3 Preliminaries â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> is a reliable predictor of performance improvements under rejection sampling. Crucially, it can be estimated from one round of verification without requiring computationally expensive rejection sampling experiments.</p>\n</blockquote>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.3â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Are Verifiers Biased Toward Solutions That Resemble Their Own?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\">Humans tend to judge solutions that resemble their own reasoning as more likely to be correct. This mirrors the well-documented self-enhancement biasÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">self-enhancement-bias</span>)</cite>, in which individuals evaluate themselves more favorably than objective evidence would suggest. Our results suggest that an analogous effect may arise in solverâ€“verifier interactions. As shown earlier in FiguresÂ <a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> andÂ <a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, strong reasoning models benefit the least from self-verification and the most from cross-family verification, hinting at a similar bias.</p>\n</div>\n<figure class=\"ltx_figure ltx_align_floatleft\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1253\" id=\"S5.F5.g1\" src=\"./assets/x5.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nCorrelation between verifier metrics with similarity scores between solver-verifier pairs. Each marker is colored based on the verifier model family.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p2\">\n<p class=\"ltx_p\">To directly investigate this behavior, we conduct cross-verification experiments using 12 post-trained models (the three smallest models from each of the four families) and compute all verifier metrics for each pair. For intra-family verification, each solver has 2 verifiers from the same family (excluding itself), resulting in <math alttext=\"12\\times 2=24\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>12</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">Ã—</mo><mn>2</mn></mrow><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">12\\times 2=24</annotation></semantics></math> solverâ€“verifier pairs. For cross-family verification, each solver has 9 verifiers from other families, giving <math alttext=\"12\\times 9=108\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>12</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">Ã—</mo><mn>9</mn></mrow><mo>=</mo><mn>108</mn></mrow><annotation encoding=\"application/x-tex\">12\\times 9=108</annotation></semantics></math> cross-family pairs. For each pair, we plot the verifier metric against the <span class=\"ltx_text ltx_font_bold\">solverâ€“verifier similarity score</span>, defined as the average cosine similarity between the two modelsâ€™ solution embeddings across all dataset problems. Solutions are embedded using <span class=\"ltx_text ltx_font_typewriter\">sentence-transformers</span> <span class=\"ltx_text ltx_font_typewriter\">/all-mpnet-base-v2</span>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p3\">\n<p class=\"ltx_p\">FigureÂ <a class=\"ltx_ref\" href=\"#S5.F5\" title=\"Figure 5 â€£ 5.3 Are Verifiers Biased Toward Solutions That Resemble Their Own? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows that, for both intra-family and cross-family settings, FPR exhibits a clear positive trend: the more similar the solver and verifier are in their solution distributions, the more likely the verifier is to accept the solverâ€™s incorrect answers. While intra-family verifier gains are too small to yield a strong correlation, cross-family verifier gains decrease significantly with increasing similarity. This indicates that, when selecting a solverâ€“verifier pair, choosing a verifier whose solution distribution differs from that of the solver leads to more reliable verification.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p4\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>Â \nHigher similarity between solver and verifier solution distributions increases the verifierâ€™s tendency to accept incorrect solver outputs, reducing verifier gain. Using a verifier with a meaningfully different solution distribution mitigates this bias.</p>\n</blockquote>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.4â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">How Does Post-Training Affect Solver and Verifier Performance?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.p1\">\n<p class=\"ltx_p\">We examine how post-training influences verifier behavior. Our analysis focuses on the <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span>/<span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span>/<span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> model pairs. We exclude <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span> due to its weak solver and verifier performance and omit <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> because matching base models are unavailable. Verification metrics are computed across all 37 base and post-trained models.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S5.SS4.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Post-training effect on solver performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We begin by evaluating how solver accuracy changes after post-training. For each model pair, we compute solver accuracy and average results across model families and datasets. As expected, post-training yields substantial improvements: <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> solvers improve by an average of <math alttext=\"8.2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>8.2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">8.2\\%</annotation></semantics></math>, while <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> shows a striking <math alttext=\"35.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>35.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">35.4\\%</annotation></semantics></math> gain. Full results are provided in AppendixÂ <a class=\"ltx_ref\" href=\"#A9\" title=\"Appendix I Effect of Post-Training on Solver Performance â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS4.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Post-training effect on verifier performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We next analyze how post-training affects verifier behavior (FigureÂ <a class=\"ltx_ref\" href=\"#S5.F6\" title=\"Figure 6 â€£ Post-training effect on verifier performance. â€£ 5.4 How Does Post-Training Affect Solver and Verifier Performance? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). For each model, we compute verifier metrics against all solvers and datasets, partition results by verification setting, and average within families. For both <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span>, post-training increases FPR and reduces verifier gain in self-verification, despite improvements in FNR. Although <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> benefits more than <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> in solver accuracy (FigureÂ <a class=\"ltx_ref\" href=\"#A9.F11\" title=\"Figure 11 â€£ Appendix I Effect of Post-Training on Solver Performance â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>), its post-trained verifiers show higher FPRs and lower gains in both self- and intra-family verification. In contrast, both families, and especially <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span>, show substantial improvements in cross-family verification.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.SSS0.Px2.p2\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>Â \nPost-training significantly enhances a base modelâ€™s problem-solving ability but can reduce its self- or intra-family improvement potential. In contrast, it boosts modelsâ€™ performance in cross-family verification.</p>\n</blockquote>\n</div>\n<figure class=\"ltx_figure ltx_align_floatleft\" id=\"S5.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"550\" id=\"S5.F6.g1\" src=\"./assets/x6.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nChanges in verifier metrics of the <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> models from post-training.</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS5\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.5â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Which Datasets are Easy to Verify?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p1\">\n<p class=\"ltx_p\">Thus far, we have examined verifier performance and its contribution to solver accuracy through rejection sampling. We now shift to a task-level perspective and ask: <span class=\"ltx_text ltx_font_italic\">are tasks that are easy to solve also easy to verify?</span> In FigureÂ <a class=\"ltx_ref\" href=\"#S5.F7\" title=\"Figure 7 â€£ 5.5 Which Datasets are Easy to Verify? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we recompute the verifier metrics from SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS1\" title=\"5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, average them across all verifier models, and plot them against solver accuracies.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p2\">\n<p class=\"ltx_p\">We find that verification accuracy correlates strongly with solver accuracy. In contrast, verifier gains from self-verification do not correlate with problem difficulty, whereas both intra-family and cross-family verifier gains exhibit clear positive correlations. Notably, AIME appears as an outlier in the final plot, potentially because some models have encountered similar problems during post-training.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p3\">\n<p class=\"ltx_p\">The best-fit lines for verifier accuracy further reveal two distinct clusters of tasks (colored red and blue), indicating that verification difficulty cannot be explained solely by solver accuracy. This leads to our next question: <span class=\"ltx_text ltx_font_italic\">are some tasks inherently easier to verify than others?</span> Relatedly, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025mind</span></cite> find that models cannot self-improve on tasks requiring factual recall, but can self-improve on Sudoku with sufficient pretraining scale. We observe a similar separation in FigureÂ <a class=\"ltx_ref\" href=\"#S5.F7\" title=\"Figure 7 â€£ 5.5 Which Datasets are Easy to Verify? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>: AIME, GSM8K, 3SAT, and Sudoku exhibit a higher ratio of verifier accuracy to solver accuracy and deliver higher gains across all verification settings.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p4\">\n<p class=\"ltx_p\">To explain this, we notice that among our synthetic datasets, Sudoku and 3SAT are classic examples of problems that require exponential solving time but allow polynomial-time verification. By contrast, there is no clear shortcut for verifying the product of two matrices without effectively recomputing it for Matrix Multiplication. Among the real-world datasets, GSM8K and AIME involve problems solvable with high-school-level mathematics, whereas MMLU (Social Sciences) requires domain-specific knowledge, CommonsenseQA relies on implicit world knowledge, and GPQA and MMLU (STEM) draw on specialized natural science content. For these latter tasks, verifying an answer requires essentially the same knowledge as solving the problem.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p5\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>Â \nAlthough tasks that are easy to solve are typically easier to verify, some tasks are inherently easier to verify. These include synthetic problems with logical or structured reasoning (e.g., 3SAT, Sudoku) and real-world tasks relying primarily on mathematical reasoning rather than extensive factual recall (e.g., GSM8K, AIME). Such tasks also yield larger gains from test-time rejection sampling with verifiers.</p>\n</blockquote>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"369\" id=\"S5.F7.g1\" src=\"./assets/x7.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Correlation of verifier metrics (rows) with solver accuracies, averaged over solver-verifier pairs that belong to each verification setting (columns).</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">6â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Conclusion</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p1\">\n<p class=\"ltx_p\">This work presents a comprehensive study of LLM-based verification for problem solving. We show that verification accuracy alone provides an incomplete picture of the expected improvement obtained by using a verifier for test-time rejection sampling, motivating the introduction of <span class=\"ltx_text ltx_font_italic\">verifier gain</span>, a more informative measure that captures this expected improvement. Using this metric, our analysis shows that verifier gain is often lower for self-verification and intra-family verification than for cross-family verification, particularly as model size increases or post-training is applied. Further analysis reveals that decreases in verifier gain correlate with greater similarity between the solverâ€™s and verifierâ€™s solution distributions. Finally, we show that some tasks are inherently easier for LLMs to verify than others: more difficult tasks generally require domain-specific or implicit world knowledge, whereas easier tasks tend to involve logical reasoning, mathematical reasoning, or structured puzzle solving.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S6.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations and future work.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS5\" title=\"5.5 Which Datasets are Easy to Verify? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> shows that some tasks are inherently more verifiable than others, motivating future work on developing a predictive model for the verifiability of individual tasks or questions. SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS3\" title=\"5.3 Are Verifiers Biased Toward Solutions That Resemble Their Own? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> shows that LLMs are biased toward accepting incorrect solutions that resemble their own reasoning, indicating that it will be worthwhile to examine the origins of this bias in pre-training and/or post-training.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#2C2C2C;\">Acknowledgement</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">We thank members of the NYU Agentic Learning AI Lab for their helpful discussions. JL is supported by the NSERC PGS-D Scholarship.\nThe work is supported in part by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) under grant RS-2024-00469482, funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research.\nThe compute is supported by the NYU High Performance Computing\nresources, services, and staff expertise. We also thank Modal for providing additional compute resources.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"Ax1\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n</section>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Related Work</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling test-time compute.</span> A simple method for scaling test-time compute involves sampling several candidates and selecting one <span class=\"ltx_text ltx_font_italic\">post hoc</span>, for example via Best-of-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>. This can take the form of sample-and-rank approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nichols2020collaborative</span>)</cite>, majority vote <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023selfconsistency</span>)</cite>, model-based aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024universal</span>)</cite>, or sampling then filtering <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">weng2023-better-reasoners</span>)</cite>. LLMs can also be finetuned to explicitly optimize Best-of-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> performance <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chow2025inferenceaware-bon</span>)</cite>. Instead of <span class=\"ltx_text ltx_font_italic\">post hoc</span> selection, we can guide the model towards good samples via constraints <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">roy-roth-2015-solving</span>)</cite>, a scoring function <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang-etal-2022-generating-nl-proofs</span>)</cite>, or sequential construction <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kang2024mindstar</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">khalifa-etal-2023-grace</span>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Details on Verifier Metrics</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.p1\">\n<p class=\"ltx_p\">We show the mathematical definitions of relevant verifier metrics below. For clarity, we include dependencies (e.g., <math alttext=\"(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,V;\\mathcal{D})</annotation></semantics></math>) in the definitions, but sometimes omit them for brevity when the context is clear.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.p2\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A9.EGx1\">\n<tbody id=\"A2.Ex1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{VerifierAcc}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex1.m1\" intent=\":literal\"><semantics><mrow><mtext>VerifierAcc</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{VerifierAcc}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,\\mathbbm{1}\\{\\,V(x,y)=c(x,y)\\,\\}\\,\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex1.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">ğ”¼</mi><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>âˆ¼</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mi>y</mi><mo>âˆ¼</mo><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo><mrow><mn class=\"ltx_mathvariant_double-struck\" mathvariant=\"double-struck\">â€‰1</mn><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo rspace=\"0.170em\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,\\mathbbm{1}\\{\\,V(x,y)=c(x,y)\\,\\}\\,\\big]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{TPR}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex2.m1\" intent=\":literal\"><semantics><mrow><mtext>TPR</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{TPR}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex2.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">ğ”¼</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mrow><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>âˆ£</mo><mi>y</mi></mrow><mo>âˆˆ</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{FPR}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex3.m1\" intent=\":literal\"><semantics><mrow><mtext>FPR</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{FPR}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\notin\\mathcal{Y}_{x}\\,]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex3.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">ğ”¼</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mrow><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>âˆ£</mo><mi>y</mi></mrow><mo>âˆ‰</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\notin\\mathcal{Y}_{x}\\,]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{FNR}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex4.m1\" intent=\":literal\"><semantics><mrow><mtext>FNR</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{FNR}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,1-V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]=1-\\text{TPR}(S,V;\\mathcal{D}).\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex4.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">ğ”¼</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mn>â€‰1</mn><mo>âˆ’</mo><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>âˆ£</mo><mi>y</mi></mrow></mrow><mo>âˆˆ</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ğ’´</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mtext>TPR</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,1-V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]=1-\\text{TPR}(S,V;\\mathcal{D}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Precision}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex5.m1\" intent=\":literal\"><semantics><mrow><mtext>Precision</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Precision}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,c(x,y)\\mid V(x,y)=1\\,]=\\frac{\\text{SolverAcc}\\cdot\\text{TPR}}{\\text{SolverAcc}\\cdot\\text{TPR}+(1-\\text{SolverAcc})\\cdot\\text{FPR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex5.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">ğ”¼</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mrow><mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>âˆ£</mo><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo lspace=\"0.170em\" stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mtext>SolverAcc</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">â‹…</mo><mtext>TPR</mtext></mrow><mrow><mrow><mtext>SolverAcc</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">â‹…</mo><mtext>TPR</mtext></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mtext>SolverAcc</mtext></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">â‹…</mo><mtext>FPR</mtext></mrow></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,c(x,y)\\mid V(x,y)=1\\,]=\\frac{\\text{SolverAcc}\\cdot\\text{TPR}}{\\text{SolverAcc}\\cdot\\text{TPR}+(1-\\text{SolverAcc})\\cdot\\text{FPR}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex6\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Recall}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex6.m1\" intent=\":literal\"><semantics><mrow><mtext>Recall</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Recall}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\text{TPR}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex6.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mtext>TPR</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\text{TPR}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex7\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{F1}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex7.m1\" intent=\":literal\"><semantics><mrow><mtext>F1</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{F1}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{2\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex7.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">â‹…</mo><mtext>Precision</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">â‹…</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{2\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Details on Verification Settings</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.p1\">\n<p class=\"ltx_p\">We show the mathematical definitions of our three verification settings. Let <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">â„³</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> denote the space of models and <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">â„±</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math> the space of model families. We define a function</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"A3.Ex8\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Family}:\\mathcal{M}\\to\\mathcal{F},\" class=\"ltx_Math\" display=\"block\" id=\"A3.Ex8.m1\" intent=\":literal\"><semantics><mrow><mrow><mtext>Family</mtext><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">â„³</mi><mo stretchy=\"false\">â†’</mo><mi class=\"ltx_font_mathcaligraphic\">â„±</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Family}:\\mathcal{M}\\to\\mathcal{F},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">that maps each model (e.g., <span class=\"ltx_text ltx_font_typewriter\">meta-llama/Meta-Llama-3-70B</span>) to its corresponding family (e.g., <span class=\"ltx_text ltx_font_typewriter\">Llama-3-Base</span>).\nNote that <math alttext=\"S,V\\in\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo>,</mo><mi>V</mi></mrow><mo>âˆˆ</mo><mi class=\"ltx_font_mathcaligraphic\">â„³</mi></mrow><annotation encoding=\"application/x-tex\">S,V\\in\\mathcal{M}</annotation></semantics></math>. For any verifier metric <math alttext=\"M(\\cdot,\\cdot;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">â‹…</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">â‹…</mo><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot,\\cdot;\\mathcal{D})</annotation></semantics></math> such as\nVerifierAcc, TPR, or <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m5\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math>, we define:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.p2\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A9.EGx2\">\n<tbody id=\"A3.Ex9\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Self-Verif}(V;\\mathcal{D},M)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex9.m1\" intent=\":literal\"><semantics><mrow><mtext>Self-Verif</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Self-Verif}(V;\\mathcal{D},M)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=M(V,V;\\mathcal{D}),\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex9.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=M(V,V;\\mathcal{D}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A3.Ex10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Intra-Verif}(V,\\mathcal{S};\\mathcal{D},M)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex10.m1\" intent=\":literal\"><semantics><mrow><mtext>Intra-Verif</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’®</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Intra-Verif}(V,\\mathcal{S};\\mathcal{D},M)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\nS\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:S\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\}|},\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex10.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mo>âˆ‘</mo><mtable rowspacing=\"0pt\"><mtr><mtd><mrow><mi>S</mi><mo>âˆˆ</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’®</mi></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>S</mi><mo>â‰ </mo><mi>V</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable></msub><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>S</mi><mo>âˆˆ</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’®</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi>S</mi><mo>â‰ </mo><mi>V</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\nS\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:S\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\}|},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A3.Ex11\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Cross-Verif}(V,\\mathcal{S};\\mathcal{D},M)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex11.m1\" intent=\":literal\"><semantics><mrow><mtext>Cross-Verif</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’®</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Cross-Verif}(V,\\mathcal{S};\\mathcal{D},M)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\n\\text{Family}(S)\\neq\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:\\text{Family}(S)\\neq\\text{Family}(V)\\}|}.\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex11.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mo>âˆ‘</mo><mtable rowspacing=\"0pt\"><mtr><mtd><mrow><mi>S</mi><mo>âˆˆ</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’®</mi></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>â‰ </mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></msub><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’Ÿ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>S</mi><mo>âˆˆ</mo><mi class=\"ltx_font_mathcaligraphic\">ğ’®</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>â‰ </mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">â€‹</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\n\\text{Family}(S)\\neq\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:\\text{Family}(S)\\neq\\text{Family}(V)\\}|}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Additional Details on Models</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A4.p1\">\n<p class=\"ltx_p\">We show the information for each of our 37 evaluated models in TableÂ <a class=\"ltx_ref\" href=\"#A9.T1\" title=\"Table 1 â€£ Appendix I Effect of Post-Training on Solver Performance â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A5\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Additional Details on Datasets</h2>\n<section class=\"ltx_subsection\" id=\"A5.SS1\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">E.1â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Real-World Datasets</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS1.p1\">\n<p class=\"ltx_p\">Note that for MMLU (STEM) and MMLU (Social Sciences), we concatenate questions from all subjects that belong to the STEM and Social Sciences supercategories inÂ <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlu</span></cite>, respectively.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A5.SS2\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">E.2â€ƒâ€Š<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Synthetic Datasets</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p1\">\n<p class=\"ltx_p\">We generate three synthetic datasets, named 3SAT, Matrix Multiplication, and Sudoku, with 1000 samples each. We submit the data generation code in the Supplementary Materials, but briefly explain each synthetic datasetâ€™s generation parameters below.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p2\">\n<p class=\"ltx_p\">Each 3SAT CNF contains uniformly sampled numbers of variables and clauses from 2 to 8 (inclusive). Each Sudoku puzzle is a 9x9 grid with 12 randomly missing cells. Each Matrix Multiplication problem is about multiplying 2 4x4 integer matrices with values uniformly sampled from <math alttext=\"[-5,5]\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mo>âˆ’</mo><mn>5</mn></mrow><mo>,</mo><mn>5</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-5,5]</annotation></semantics></math>. All data are generated in a way that ensures the existence of a valid solution. Note that while Matrix Multiplication has a singular correct answer for each problem, Sudoku and 3SAT are allowed multiple correct answers as long as the solverâ€™s answer is correct by their rules.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p3\">\n<p class=\"ltx_p\">The generation code files for all synthetic datasets are seeded for reproducibility.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p4\">\n<p class=\"ltx_p\">An example of a generated 3SAT problem:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\n## Problem Definition\n\n**SAT (Boolean Satisfiability Problem)** is a fundamental problem in computer science\nwhere we need to determine if there exists an assignment of Boolean values (True/False)\nto variables that makes a given Boolean formula evaluate to True.\n**Variables**: In this problem, variables are named as single letters. Each variable can\nbe assigned either True (T) or False (F).\n**Literals**: A literal is either a variable (like a) or its negation (like Ëœa, meaning\n\"not a\"). If a is True, then Ëœa is False, and vice versa.\n**Clauses**: A clause is a disjunction (OR operation) of literals. A clause is satisfied\n(True) if at least one of its literals is True. For example, the clause (a or Ëœb) is True if\neither a is True OR b is False (or both).\n**CNF (Conjunctive Normal Form)**: The Boolean formula is given in CNF, which is a\nconjunction (AND operation) of multiple clauses. The entire formula is satisfied only if\nALL clauses are satisfied simultaneously.\n**3SAT**: This is a special case of SAT where every clause contains exactly 3 literals.\n\n## The Problem\n\nFind a satisfying assignment for the following CNF formula: (Ëœc or Ëœb or d) and\n(d or Ëœb or Ëœc) and (d or a or c) and (Ëœc or d or a) and (b or Ëœa or d) and (c or d or Ëœb)\n\n## Instructions\n\nProvide your answer as a list of variable assignments, one per line, in the format\n\"variable_name T\" or \"variable_name F.\" For example:\n\\boxed{\na T\nb F\n}\nThis means a=True, b=False.\n\nAnother example answer is\n\\boxed{\na F\nb T\n}\nThis means a=False, b=True.\n\nOutput and only output the T/F values for the variables that appear in the provided\nCNF formula.\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p5\">\n<p class=\"ltx_p\">An example of a generated Sudoku problem:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\n## Sudoku Problem\n\n**Sudoku** is a logic-based number-placement puzzle. The objective is to fill a 9x9 grid\nwith numbers so that each column, each row, and each of the 3x3 sub-grids contains all\nof the numbers from 1 to 9.\n\n## The Puzzle\n\nComplete the following 9x9 Sudoku grid (empty cells are marked with â€™_â€™):\n\n7 4 2 1 _ 5 8 9 6\n1 6 9 2 4 8 3 5 7\n8 5 3 _ _ 7 2 1 4\n2 _ 8 9 7 1 4 6 5\n5 7 6 4 8 2 9 3 _\n4 9 1 3 _ 6 _ 8 _\n3 1 5 8 2 4 6 7 9\n6 8 _ 7 1 _ 5 2 3\n_ 2 7 5 6 _ 1 4 8\n\n## Instructions\n\nProvide your answer as a completed 9x9 grid with all numbers filled in, formatted exactly\nlike the puzzle above but with numbers instead of underscores.\n\nFor example, a completed 4x4 grid should look like:\n\\boxed{\n1 2 3 4\n3 4 1 2\n2 3 4 1\n4 1 2 3\n}\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p6\">\n<p class=\"ltx_p\">An example of a generated Matrix Multiplication problem:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\n## Matrix Multiplication Problem\n\n**Matrix Multiplication** is a fundamental operation in linear algebra where we compute\nthe product of two matrices. For two square matrices A and B of size 4x4, the product\nC = A x B is computed as:\n\nC[i][j] = Sum(k=0 to 3) A[i][k] x B[k][j]\n\n## The Problem\n\nCompute the product of the following two 4x4 matrices:\n\n**Matrix A:**\n0 1 1 4\n-1 3 4 4\n-2 -5 -5 0\n-4 4 5 0\n\n**Matrix B:**\n1 2 0 5\n1 -2 0 0\n3 -1 -3 -3\n2 5 -4 2\n\n## Instructions\n\nProvide your answer as the resulting 4x4 matrix C = A x B, formatted with each row\non a separate line and numbers separated by spaces.\n\nFor example, a 2x2 result matrix is formatted like:\n\\boxed{\n1 2\n3 4\n}\n</pre>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A6\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix F </span>Additional Details on Experimental Setup</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p1\">\n<p class=\"ltx_p\">We use the following solver prompt for all models:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\nPlease reason step by step, and put your final answer within \\boxed{{}}.\n\n{question}\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p2\">\n<p class=\"ltx_p\">We use the following verifier prompt for all models:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\nYou are a teacher that is evaluating a studentâ€™s answer to a question.\nYour task is to determine whether the answer is correct or incorrect.\n\nQuestion: {question}\n\nStudentâ€™s Answer: {response}\n\nPlease evaluate the studentâ€™s answer carefully. Consider:\n- Is the answer factually accurate?\n- Is the reasoning sound and logical?\n- Does it fully address the question asked?\n\nAfter your evaluation, provide your judgment in the\nfollowing format:\n- If the answer is correct, write: \\boxed{{correct}}.\n- If the answer is incorrect, write: \\boxed{{incorrect}}.\n\nFirst explain your analysis over the studentâ€™s answer, then provide your final judgment in\nthe boxed format. Make sure the final judgment is either \"correct\" or \"incorrect\" inside\nthe \\boxed{{}}. Do not put anything else in \\boxed{{}}. Do not repeat the studentâ€™s answer\nin \\boxed{{}}.\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p3\">\n<p class=\"ltx_p\">FigureÂ <a class=\"ltx_ref\" href=\"#A6.F8\" title=\"Figure 8 â€£ Appendix F Additional Details on Experimental Setup â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> displays the ratio of filtered solver outputs due to not containing a box for answer extraction, averaged across all datasets.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A6.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"217\" id=\"A6.F8.g1\" src=\"./assets/x8.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Average ratio of filtered solver outputs for each model over all datasets. Base model families are suffixed by <span class=\"ltx_text ltx_font_bold\">-Base</span>. Models within each family are ordered in increasing size.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_appendix\" id=\"A7\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix G </span>Solver Accuracy by Dataset</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A7.p1\">\n<p class=\"ltx_p\">FigureÂ <a class=\"ltx_ref\" href=\"#A7.F9\" title=\"Figure 9 â€£ Appendix G Solver Accuracy by Dataset â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the solver accuracies of all 37 models on each of our 9 datasets.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A7.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"816\" id=\"A7.F9.g1\" src=\"./assets/x9.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">The solver accuracies of 37 models on each dataset.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A8\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix H </span>F1-Score and Precision Visualization</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A8.p1\">\n<p class=\"ltx_p\">FigureÂ <a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the correlation between each modelâ€™s verification ability and its own solver accuracy for all 21 post-trained models. We additionally display verifier F1-Score and precision in FigureÂ <a class=\"ltx_ref\" href=\"#A8.F10\" title=\"Figure 10 â€£ Appendix H F1-Score and Precision Visualization â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A8.p2\">\n<p class=\"ltx_p\">In comparison to verifier accuracy, while F1-Score also positively correlates with verifierâ€™s own solver accuracy for all verification settings, the slopes decrease from self-verification to intra-family verification, and further decrease for cross-family verification, showing that the increase in false positive rate in FigureÂ <a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 â€£ Correlating verifier and solver performance. â€£ 5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> has a stronger negative impact on lowering F1-score than accuracy.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A8.p3\">\n<p class=\"ltx_p\">While SectionÂ <a class=\"ltx_ref\" href=\"#S5.SS1\" title=\"5.1 Do Better Solvers Make Better Verifiers? â€£ 5 Results â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> explains the low verifier gains for self- and intra-family verification through close examination of FPR, we additionally plot verifier precision in FigureÂ <a class=\"ltx_ref\" href=\"#A8.F10\" title=\"Figure 10 â€£ Appendix H F1-Score and Precision Visualization â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. However, since precision is the expected performance of verifier-based rejection sampling in the limit of infinite sampling and our main metric â€œverifier gainâ€ is defined in terms of it (EquationÂ <a class=\"ltx_ref\" href=\"#S3.E1\" title=\"Equation 1 â€£ 3.2 Evaluation Metrics â€£ 3 Preliminaries â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), precision does not help explain the differences in verifier gains across verification settings itself.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A8.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1014\" id=\"A8.F10.g1\" src=\"./assets/x10.png\" width=\"768\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Correlation between each modelâ€™s verifier metrics (rows) and its own solver accuracy for all 21 post-trained models, averaged over all datasets. Each verifier metric is computed over three settings (columns): self-verification, intra-family verification, and cross-family verification. We use the same set of post-trained models as the set of solver models.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_appendix\" id=\"A9\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix I </span>Effect of Post-Training on Solver Performance</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A9.p1\">\n<p class=\"ltx_p\">FigureÂ <a class=\"ltx_ref\" href=\"#A9.F11\" title=\"Figure 11 â€£ Appendix I Effect of Post-Training on Solver Performance â€£ When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average improvement in solver accuracies of <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> families of models from their respective post-training procedures.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A9.T1\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Complete list of each evaluated modelâ€™s HuggingFace identifier, family, and size.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">HuggingFace Identifier</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Family</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-0.6B-Base</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">0.6B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-1.7B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">1.7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-4B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">4B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-8B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-14B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-0.6B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">0.6B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-1.7B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">1.7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-4B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">4B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-8B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-14B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-32B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">32B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-0.5B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">0.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-1.5B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">1.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-3B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-7B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-14B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-32B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">32B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-72B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">72B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-0.5B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">0.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-1.5B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">1.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-3B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-7B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-14B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-32B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">32B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-72B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">72B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.2-1B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">1B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.2-3B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.1-8B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.1-70B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">70B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.2-1B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">1B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.2-3B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.1-8B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.1-70B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">70B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">1.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-14B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">32B</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_table\" id=\"A9.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">HuggingFace information and sizes of real-world datasets.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset Name</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">HuggingFace Identifier</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">HuggingFace Split</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">GSM8K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">openai/gsm8k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1319</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">AIME</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">TianHongZXY/aime-1983-2025</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">963</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MMLU (STEM)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">cais/mmlu</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">316</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MMLU (Social Sciences)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">cais/mmlu</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">308</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CSQA</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">tau/commonsense_qa</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">validation</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2442</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">GPQA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Idavidrein/gpqa</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">train</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">198</td>\n</tr>\n</tbody>\n</table>\n</figure>\n<figure class=\"ltx_figure\" id=\"A9.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"331\" id=\"A9.F11.g1\" src=\"./assets/x11.png\" width=\"331\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nImprovements in solver accuracies of <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> models from post-training.\n</span></figcaption>\n</figure>\n</section>",
  "css": "",
  "arxiv_id": "2512.02304",
  "source": "arxiv-experimental",
  "generated": "2025-12-17T15:26:03.470Z"
}
