{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">1\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Introduction</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p1\">\n<p class=\"ltx_p\">Problem-solving with LLMs has shifted from solely querying a model for a solution to a system where models both solve and verify. The paradigm of verifying solutions at test time is broad, spanning both simple strategies, such as generating multiple candidate solutions and using a verifier as a filter\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"#bib.zhao2025sample\" title=\"\">2025</a>)</cite>, and more complex iterative refinement approaches\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Madaan et al., <a class=\"ltx_ref\" href=\"#bib.madaan2023selfrefine\" title=\"\">2023</a>)</cite>. With generative verification at test time, LLMs can solve more complex problems than they can when used alone <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"#bib.cobbe2021training\" title=\"\">2021</a>; Lightman et al., <a class=\"ltx_ref\" href=\"#bib.lightman2024lets\" title=\"\">2024</a>)</cite>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p2\">\n<p class=\"ltx_p\">Despite the increasing dominance of this paradigm, studies of solver\u2013verifier interactions have remained limited in scope. Prior work has largely examined how a single model verifies its own solutions (self-verification) and improves itself (self-improvement) <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et al., <a class=\"ltx_ref\" href=\"#bib.song2025mind\" title=\"\">2025</a>)</cite>, yet self-verification is not guaranteed to be effective: models may be biased toward their own reasoning patterns, their training may reinforce these tendencies, and different tasks may vary in how much they benefit from verification. Additionally, this focus on self-verification offers little insight into how verification behaves when the solver and verifier differ. With open-source model families that have base and post-trained pairs, size variants, reproducible inference pipelines, and explicit reasoning traces, we can systematically study verification across models. We therefore broaden our analysis to include both intra-family and cross-family verification and ask the following central question:</p>\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">When does verification actually pay off, and how does each factor, such as model family, model size, post-training, solver\u2013verifier similarity, or task type, influence how effective verification is in improving the solver?</em></p>\n</blockquote>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p3\">\n<p class=\"ltx_p\">To accomplish this, we evaluate verifiers across a diverse suite of tasks, including synthetic tasks used to test precise logical reasoning or symbolic computation (3-SAT, Sudoku, and matrix multiplication), mathematical reasoning tasks (AIME <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathematical Association of America, <a class=\"ltx_ref\" href=\"#bib.AIME2025\" title=\"\">2025</a>)</cite>, GSM8K <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"#bib.cobbe2021training\" title=\"\">2021</a>)</cite>), commonsense and factual reasoning (CSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Talmor et al., <a class=\"ltx_ref\" href=\"#bib.csqa\" title=\"\">2019</a>)</cite>, GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Rein et al., <a class=\"ltx_ref\" href=\"#bib.gpqa\" title=\"\">2024</a>)</cite>), and broad domain knowledge (MMLU in STEM and social sciences <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"#bib.mmlu\" title=\"\">2021</a>)</cite>) using 37 models from 7 model families.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p4\">\n<p class=\"ltx_p\">Our analysis indicates that self-verification does not always \u201cpay off\u201d: models often favor solutions resembling their own reasoning (Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS1\" title=\"5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>,\u00a0<a class=\"ltx_ref\" href=\"#S5.SS3\" title=\"5.3 Are Verifiers Biased Toward Solutions That Resemble Their Own? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>), post-training can sharpen this bias (Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS4\" title=\"5.4 How Does Post-Training Affect Solver and Verifier Performance? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>), and some tasks inherently benefit less from verification than others (Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS5\" title=\"5.5 Which Datasets are Easy to Verify? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>). Therefore, we present the following contributions, which offer actionable and empirically supported guidance for how to use verifiers effectively.</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">New Metric: Verifier Gain.</span> Verifier accuracy alone provides an incomplete picture of verifier usefulness at test time. To address this, we derive <span class=\"ltx_text ltx_font_italic\">verifier gain</span>, a metric that simulates the improvement obtained from a verifier during test-time rejection sampling. We empirically study rejection sampling with verifiers and show that this theoretical formulation closely reflects empirical performance trends.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Improvement, Intra-Family Improvement, and Cross-Family Improvement.</span> We extensively compare performance improvements from self-verification, intra-family verification, and cross-family verification, finding that cross-family verification is often the most beneficial, particularly when compared to self-verification. We link these differences to similarities in the solution distributions of the solver and verifier: verifier gain decreases as these distributions become more similar. Our results suggest that as models become stronger, whether through increased scale, post-training, or simply higher solver accuracy, they become less effective as self-verifiers and more effective as cross-family verifiers.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset Verifiability.</span> We study whether tasks that are easy to solve are also easy to verify and whether some tasks are inherently more verifiable than others. We find that verification accuracy generally correlates with solver accuracy, though self-verification yields little verifier gain across all tasks. We also observe that a clear subset of tasks involving mathematical or logical reasoning consistently produces higher verifier gains.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">2\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Related Work</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Verifiers.</span> Broadly, verifiers can operate on the <span class=\"ltx_text ltx_font_italic\">outcome</span> level, by judging only the correctness of the final answer, or the <span class=\"ltx_text ltx_font_italic\">process</span> level, by judging the correctness of the intermediate reasoning steps. <cite class=\"ltx_cite ltx_citemacro_citet\">Weng et al. (<a class=\"ltx_ref\" href=\"#bib.weng2023-better-reasoners\" title=\"\">2023</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_citet\">Wu et al. (<a class=\"ltx_ref\" href=\"#bib.wu-etal-2024-key-condition-verification\" title=\"\">2024</a>)</cite>, and <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et al. (<a class=\"ltx_ref\" href=\"#bib.jiang-etal-2024-forward-backward\" title=\"\">2024</a>)</cite> develop methods for outcome-level self-verification by predicting parts of the question conditioned on the solution. In order to reduce hallucinations, <cite class=\"ltx_cite ltx_citemacro_citet\">Dhuliawala et al. (<a class=\"ltx_ref\" href=\"#bib.dhuliawala-etal-2024-chain-of-verification\" title=\"\">2024</a>)</cite> have language models fact-check their own generations by generating fact-check questions. Researchers have also trained general-purpose outcome level verifiers\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Hosseini et al., <a class=\"ltx_ref\" href=\"#bib.hosseini2024vstar\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.zhang-generativeverifiers\" title=\"\">2025</a>; Cobbe et al., <a class=\"ltx_ref\" href=\"#bib.cobbe2021training\" title=\"\">2021</a>)</cite> and value models\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a class=\"ltx_ref\" href=\"#bib.yu-etal-2024-ovm\" title=\"\">2024</a>)</cite>, either independently or jointly with the solver\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et al., <a class=\"ltx_ref\" href=\"#bib.shen-etal-2021-generate-rank\" title=\"\">2021</a>; Sareen et al., <a class=\"ltx_ref\" href=\"#bib.sareen2025putting-value\" title=\"\">2025</a>)</cite>. Work on process-level verification has focused on deductively verifying Chain-of-Thought (CoT) reasoning\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et al., <a class=\"ltx_ref\" href=\"#bib.ling2023deductive-verification\" title=\"\">2023</a>)</cite>, verifying individual proof steps\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"#bib.yang-etal-2022-generating-nl-proofs\" title=\"\">2022</a>)</cite>, and training process reward models for mathematical reasoning\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et al., <a class=\"ltx_ref\" href=\"#bib.luo2025improve-automated-process-supervision\" title=\"\">2025</a>)</cite>. Finally, <cite class=\"ltx_cite ltx_citemacro_citet\">Song et al. (<a class=\"ltx_ref\" href=\"#bib.song2025mind\" title=\"\">2025</a>)</cite> investigate the performance improvement caused by using an outcome-level verifier (the GV-Gap), and how this improvement changes as the solver or verifier increases in capacity. However, they primarily focus on cases where the solver and verifier are the same model. Additionally, they only study base models and do not consider post-trained models in their analysis.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p2\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling test-time compute.</span> Recently, prior work focused on studying scaling test-time compute with verifiers. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et al. (<a class=\"ltx_ref\" href=\"#bib.zhao2025sample\" title=\"\">2025</a>)</cite> study random sampling with self-verification, <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et al. (<a class=\"ltx_ref\" href=\"#bib.chen2025sets\" title=\"\">2025</a>)</cite> study combining parallel sampling with self-correction, and <cite class=\"ltx_cite ltx_citemacro_citet\">Singhi et al. (<a class=\"ltx_ref\" href=\"#bib.singhi2025solve\" title=\"\">2025</a>)</cite> compare Self-Consistency\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.wang2023selfconsistency\" title=\"\">2023</a>)</cite> to scaling with a generative verifier. <cite class=\"ltx_cite ltx_citemacro_citet\">Snell et al. (<a class=\"ltx_ref\" href=\"#bib.snell2025scaling-tt-compute\" title=\"\">2025</a>)</cite> investigate compute-optimal approaches to test-time scaling with process-level verification. Finally, verifiers also have their limitations. Imperfect verifiers can produce false positives\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Stroebl et al., <a class=\"ltx_ref\" href=\"#bib.stroebl2024inference-flaws\" title=\"\">2024</a>)</cite>, eliminate valid reasoning paths\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a class=\"ltx_ref\" href=\"#bib.yu2025scaling-flaws-math\" title=\"\">2025</a>)</cite>, and fail to select the right solution\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"#bib.brown2024large\" title=\"\">2024</a>)</cite>. We present additional related work on test-time verification in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Related Work \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p3\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-improvement.</span> Researchers have also studied LLM self-improvement and self-evaluation, with some voicing skepticism <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"#bib.huang2024large-cannot-self-correct\" title=\"\">2024b</a>; Kamoi et al., <a class=\"ltx_ref\" href=\"#bib.kamoi-etal-2024-when-self-correct\" title=\"\">2024</a>; Olausson et al., <a class=\"ltx_ref\" href=\"#bib.olausson2023self\" title=\"\">2023</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"#bib.llm-evaluators\" title=\"\">2024</a>)</cite>. On the other hand, recent work has provided a theoretical framework for self-improvement via distribution sharpening, and empirical support alongside <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"#bib.huang2024self-sharpening\" title=\"\">2024a</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.zhang-etal-2024-small-large-ver\" title=\"\">2024</a>)</cite> look specifically at self-improvement for small models, arguing that they need to be paired with a stronger verifier. Some practical methods for self-improvement use natural language feedback <cite class=\"ltx_cite ltx_citemacro_citep\">(Madaan et al., <a class=\"ltx_ref\" href=\"#bib.madaan2023selfrefine\" title=\"\">2023</a>; Shinn et al., <a class=\"ltx_ref\" href=\"#bib.shinn2023reflexion\" title=\"\">2023</a>; Kim et al., <a class=\"ltx_ref\" href=\"#bib.kim2023language\" title=\"\">2023</a>)</cite> or train models for self-correction explicitly <cite class=\"ltx_cite ltx_citemacro_citep\">(Welleck et al., <a class=\"ltx_ref\" href=\"#bib.welleck2023generating\" title=\"\">2023</a>)</cite>. Other methods use tools <cite class=\"ltx_cite ltx_citemacro_citep\">(Gou et al., <a class=\"ltx_ref\" href=\"#bib.gou2024critic\" title=\"\">2024</a>)</cite>, particularly code interpreters <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"#bib.zhou2024solving\" title=\"\">2024</a>)</cite>, to iteratively improve solutions.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"153\" id=\"S2.F1.g1\" src=\"./assets/x1.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nAverage solver accuracy of each model over all datasets. Base model families are suffixed by <span class=\"ltx_text ltx_font_bold\">-Base</span>. Models within each family are ordered in increasing sizes.\n</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">3\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Preliminaries</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p1\">\n<p class=\"ltx_p\">In this section, we establish the framework used throughout this work. We define datasets, solvers, and verifiers, introduce the metrics used to evaluate solver and verifier behaviors, and specify the verification settings in our empirical analysis.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">3.1\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Dataset, Solvers, and Verifiers</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\">Let <math alttext=\"\\mathcal{D}\\subseteq\\mathcal{X}\\times\\mathcal{Y}^{\\star}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo>\u2286</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><msup><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mo>\u22c6</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}\\subseteq\\mathcal{X}\\times\\mathcal{Y}^{\\star}</annotation></semantics></math> be a dataset of pairs <math alttext=\"(x,\\mathcal{Y}_{x})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,\\mathcal{Y}_{x})</annotation></semantics></math>, where <math alttext=\"x\\in\\mathcal{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi></mrow><annotation encoding=\"application/x-tex\">x\\in\\mathcal{X}</annotation></semantics></math> is a problem and <math alttext=\"\\mathcal{Y}_{x}\\subseteq\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub><mo>\u2286</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{Y}_{x}\\subseteq\\mathcal{Y}</annotation></semantics></math> is a non-empty set of correct solutions.\nA solver <math alttext=\"S:\\mathcal{X}\\to\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo stretchy=\"false\">\u2192</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow></mrow><annotation encoding=\"application/x-tex\">S:\\mathcal{X}\\to\\mathcal{Y}</annotation></semantics></math> is an LLM that produces a solution <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> for a given problem <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, and a verifier <math alttext=\"V:\\mathcal{X}\\times\\mathcal{Y}\\to\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb3</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi></mrow><mo stretchy=\"false\">\u2192</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">V:\\mathcal{X}\\times\\mathcal{Y}\\to\\{0,1\\}</annotation></semantics></math> is an LLM that evaluates a problem\u2013solution pair and returns a binary judgment. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song et al. (<a class=\"ltx_ref\" href=\"#bib.song2025mind\" title=\"\">2025</a>)</cite>, who find chain-of-thought (CoT) verification more stable than multiple-choice formats, we instruct both solvers and verifiers to generate CoT reasoning before producing their final solutions and judgments. We define the correctness indicator as <math alttext=\"c(x,y)=\\mathbbm{1}\\{y\\in\\mathcal{Y}_{x}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mn>\ud835\udfd9</mn><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>y</mi><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">c(x,y)=\\mathbbm{1}\\{y\\in\\mathcal{Y}_{x}\\}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">3.2\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Evaluation Metrics</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\">We define the accuracy of a solver <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> on a dataset <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> as the expected correctness of its outputs over all problems in the dataset: <math alttext=\"\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,c(x,y)\\,\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">\ud835\udd3c</mi><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mi>y</mi><mo>\u223c</mo><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" rspace=\"0.170em\">[</mo><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo rspace=\"0.170em\" stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,c(x,y)\\,\\big]</annotation></semantics></math>. Verifier performance has several dimensions. We report common binary classification metrics, including verifier accuracy, false positive rate (FPR), false negative rate (FNR), F1-Score, and precision, with their definitions in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Details on Verifier Metrics \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\">Our primary goal is to evaluate whether using a verifier <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> can improve a solver <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> at test time via rejection sampling, in which solver outputs are repeatedly sampled until the verifier accepts one. Assuming the solver has a non-zero probability of sampling a correct solution, and in the limit of infinite resampling, the expected correctness of the accepted solution converges to the verifier\u2019s precision, i.e., the proportion of accepted solutions that are actually correct. To quantify the improvement from combining a solver with a verifier, we define <span class=\"ltx_text ltx_font_bold\">verifier gain</span>:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Gain}(S,V;\\mathcal{D})=\\text{Precision}(S,V;\\mathcal{D})-\\text{SolverAcc}(S;\\mathcal{D}).\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mtext>Gain</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mtext>Precision</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2212</mo><mrow><mtext>SolverAcc</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Gain}(S,V;\\mathcal{D})=\\text{Precision}(S,V;\\mathcal{D})-\\text{SolverAcc}(S;\\mathcal{D}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">It is worth noting that verifier gain is an asymptotic metric: it reflects the limit of infinite sampling and therefore serves as a bound on the improvement attainable by verifier-based rejection sampling. Throughout this work, we use verifier gain to compare differences in verifier behavior.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">3.3\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Verification Settings</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p1\">\n<p class=\"ltx_p\">We group models into families (e.g., <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span>), where each family contains related models of varying sizes. Because base and post-trained models often exhibit substantially different behaviors, we treat them as distinct families. For example, the base model <span class=\"ltx_text ltx_font_typewriter\">meta-llama/Meta-Llama-3-70B</span> belongs to the <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span> family and the post-trained model <span class=\"ltx_text ltx_font_typewriter\">meta-llama/Meta-Llama-3-8B-Instruct</span> belongs to the <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> family.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p2\">\n<p class=\"ltx_p\">We categorize each solver\u2013verifier pair from our pool of models into one of three verification settings. Each solver\u2013verifier pair is considered an instance of:</p>\n<ol class=\"ltx_enumerate\" id=\"S3.I1\">\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Verification.</span> The solver and verifier are the same model, so the model verifies its own solutions. For example, when a 70B <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> model is used as both the solver and verifier, the verification metric (e.g., accuracy, FPR) is computed on this single pairing.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Intra-Family Verification.</span> The verifier evaluates solutions produced by other models within the same family. For example, a 70B <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> verifier may evaluate outputs from 8B or 13B <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> solvers. The reported metric is averaged over all such within-family solvers, excluding the self-verification case.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i3.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Family Verification.</span> The verifier evaluates solutions produced by models from different families. For example, a base <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> verifier may evaluate outputs from <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> or from a post-trained <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>. The reported metric is averaged over all such cross-family solvers.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p3\">\n<p class=\"ltx_p\">Using these three categorizations, we evaluate a verifier by applying it to a set of solver models, computing the corresponding verifier metrics, and then partitioning these metrics according to the three verification settings and averaging within each partition. A formal mathematical description of each verification setting is provided in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Additional Details on Verification Settings \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">4\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Experimental Setup</span>\n</h2>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We evaluate the solver and verifier abilities of 21 post-trained models from the <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et al., <a class=\"ltx_ref\" href=\"#bib.llama3\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et al., <a class=\"ltx_ref\" href=\"#bib.qwen2.5\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"#bib.qwen3\" title=\"\">2025</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek-R1</span>\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(DeepSeek-AI et al., <a class=\"ltx_ref\" href=\"#bib.deepseek\" title=\"\">2025</a>)</cite> families. For our study of post-training effects in Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS4\" title=\"5.4 How Does Post-Training Affect Solver and Verifier Performance? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>, we additionally evaluate 16 base models from the <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> families. Model sizes range from 0.5B to 72B parameters. The full model list, with sizes, families, and HuggingFace identifiers, is provided in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A4\" title=\"Appendix D Additional Details on Models \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Figure\u00a0<a class=\"ltx_ref\" href=\"#S2.F1\" title=\"Figure 1 \u2023 2 Related Work \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>\u2019s legend displays the seven model families and the color scheme assigned to each.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">To comprehensively evaluate each model\u2019s performance as both a solver and a verifier, we compile a broad suite of real-world and synthetic tasks spanning diverse domains. We include tasks requiring mathematical reasoning (GSM8K, AIME), commonsense knowledge (CSQA), and domain-specific factual knowledge of varying breadth (MMLU STEM, MMLU Social Sciences, GPQA). We also construct synthetic tasks to assess logical reasoning (3SAT), structured puzzle solving (Sudoku), and symbolic computation (Matrix Multiplication). Further dataset details, along with examples of our synthetic tasks, are provided in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A5\" title=\"Appendix E Additional Details on Datasets \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>. Code for all experiments, including synthetic-data generation, is included in the Supplementary Materials and will be open-sourced upon publication.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Datasets such as Matrix Multiplication and the natural-language benchmarks contain a single ground-truth answer per problem. For these, we extract boxed solver outputs and evaluate them via exact matching. In contrast, datasets like Sudoku and 3SAT may admit multiple valid solutions, so we evaluate solver outputs according to the rules of the respective task. To evaluate verifiers, we prompt each model to generate CoT reasoning from the problem and solver answer before producing a boxed \u201ccorrect\u201d or \u201cincorrect\", from which we extract the final judgment.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Implementation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">For both solvers and verifiers, we generate with temperature\u00a0<math alttext=\"0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>0.7</mn><annotation encoding=\"application/x-tex\">0.7</annotation></semantics></math>, top-p\u00a0<math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>, and a maximum output length of 8192 tokens. We discard outputs that do not contain a boxed answer. All inference experiments are run using vLLM on H200 GPUs. Prompts and additional details on output filtering are provided in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A6\" title=\"Appendix F Additional Details on Experimental Setup \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">5\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Results</span>\n</h2>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.1\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Do Better Solvers Make Better Verifiers?</span>\n</h3>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Solver performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We first benchmark the performance of all 37 models on each of our 9 datasets, averaging performance across tasks (Figure\u00a0<a class=\"ltx_ref\" href=\"#S2.F1\" title=\"Figure 1 \u2023 2 Related Work \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) and reporting task-level results in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A7\" title=\"Appendix G Solver Accuracy by Dataset \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>. Overall, solver accuracy increases with model capacity. Models in the <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> families perform particularly well, whereas <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span> performs poorly due to base models being unfamiliar with the question\u2013answering instruction format. Within each family, we observe clear performance scaling for <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span>, with the remaining families showing similar upward trends.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Correlating verifier and solver performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\">After establishing solver accuracy, we analyze whether a model\u2019s solver performance correlates with its performance as a verifier (Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). For each of our 21 post-trained models and each dataset, we evaluate verification on the same set of solver models to obtain verifier accuracy, F1-score, precision, FPR, FNR, and gain for every solver\u2013verifier pair. For each verifier, we then divide the verifier metrics into three verification settings and average within each setting over solvers and datasets.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p2\">\n<p class=\"ltx_p\">Verifier accuracy tends to improve with the verifier\u2019s own solver accuracy, but the relationship becomes more nuanced when examining other metrics. The FPR increases during self-verification and intra-family verification but decreases slightly during cross-family verification. This indicates that verifiers with stronger solver abilities are more likely to incorrectly label solutions as correct when verifying their own outputs or outputs from models within their family. We provide additional visualizations for F1-score and precision in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A8\" title=\"Appendix H F1-Score and Precision Visualization \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_figure\" id=\"S5.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1036\" id=\"S5.F2.g1\" src=\"./assets/x2.png\" width=\"789\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nCorrelation between each verifier\u2019s metrics (rows) and its own solver accuracy for all 21 post-trained models, averaged over all datasets. Each verifier metric is computed over our three verification settings (columns).\n</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_figure\" id=\"S5.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"323\" id=\"S5.F3.g1\" src=\"./assets/x3.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nCorrelation between each verifier\u2019s metrics (rows) and model size for all 21 post-trained models, averaged over all datasets. In each plot, models are separated by family and ordered by increasing size. Each verifier metric is computed over our three verification settings (columns).\n</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p3\">\n<p class=\"ltx_p\">To better interpret the trends suggested by the accuracy and FPR results, we examine verifier gain in the final row. Verifier gain quantifies the expected benefit of using the verifier during rejection sampling, i.e., repeatedly sampling until the verifier accepts a solution (a common solver\u2013verifier interaction setting <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et al., <a class=\"ltx_ref\" href=\"#bib.song2025mind\" title=\"\">2025</a>)</cite>). This visualization offers a clearer view of verification quality: self-verification yields the smallest gains, and more accurate solvers do not exhibit greater self-improvement. Gains increase slightly in intra-family verification, while cross-family verification provides the greatest potential benefits.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Examining verifier performance at different model families and sizes.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\">In Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we repeat the experiments from Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> but plot each verifier metric as a function of model size within each model family. We observe that verification accuracy and FNR consistently improve as models become larger, whereas FPR behaves more inconsistently, often increasing with model size (e.g., intra-family verification for <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span>).</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p2\">\n<p class=\"ltx_p\">For state-of-the-art post-trained models such as <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> and <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span>, we find that verifier gains are largest in the cross-family setting, smaller in the intra-family setting, and minimal during self-verification. At first glance, this appears to contradict <cite class=\"ltx_cite ltx_citemacro_citet\">Song et al. (<a class=\"ltx_ref\" href=\"#bib.song2025mind\" title=\"\">2025</a>)</cite>, who report that self-verification GV-Gaps increase with more pretraining FLOPs. However, their analysis focuses on older model families, and Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> likewise shows larger verifier gains for older models such as <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> and <span class=\"ltx_text ltx_font_typewriter\">Llama3</span>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p3\">\n<p class=\"ltx_p\">We hypothesize that stronger post-trained models like <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> show negligible gains in self-verification and limited gains in intra-family verification for two reasons: (a) they may already engage in <span class=\"ltx_text ltx_font_italic\">spontaneous</span> self-verification when used as solvers, reducing the benefit of an additional <span class=\"ltx_text ltx_font_italic\">forced</span> verification round, and (b) their distributions are significantly sharpened by post-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"#bib.huang2024self-sharpening\" title=\"\">2024a</a>)</cite>, which limits the improvement obtained from rejection sampling.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p4\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span></p>\n<ul class=\"ltx_itemize\" id=\"S5.I1\">\n<li class=\"ltx_item\" id=\"S5.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S5.I1.i1.p1\">\n<p class=\"ltx_p\">Verifier models are biased toward accepting incorrect solutions when performing self-verification or intra-family verification.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S5.I1.i2.p1\">\n<p class=\"ltx_p\">Verification accuracy alone is not a reliable predictor of how much a verifier can improve a solver at test time. Instead, computing verifier gain using solver accuracy and verifier precision provides a more reliable metric.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.I1.i3.p1\">\n<p class=\"ltx_p\">While model families like <span class=\"ltx_text ltx_font_typewriter\">Llama3</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> show some ability to self-improve based on their verifier gains, stronger model families like <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> do not.</p>\n</div>\n</li>\n</ul>\n</blockquote>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"323\" id=\"S5.F4.g1\" src=\"./assets/x4.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nComparison between theoretical and empirical verifier gains (rows) for each verification setting (columns). Row 1 shows verifier gains computed from Equation\u00a0<a class=\"ltx_ref\" href=\"#S3.E1\" title=\"Equation 1 \u2023 3.2 Evaluation Metrics \u2023 3 Preliminaries \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Rows 2 and 3 each show the gains from rejection sampling, computed from rejection sampling using verifiers for up to 5 and 9 solver attempts, respectively.\n</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.2\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Is Verifier Gain a Good Predictor for Improvements from Resampling?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p1\">\n<p class=\"ltx_p\">Our verifier gain metric estimates the expected improvement in a solver\u2019s accuracy when using a verifier for rejection sampling. To assess how well this metric predicts real performance, we conduct rejection sampling experiments across all solver\u2013verifier pairs from a 12-model subset of our post-trained models, consisting of the three smallest models from each of the four post-training families. For each problem in each dataset, the solver generates solutions until the verifier labels one as correct, for up to nine attempts; if no such solution is found, we retain the final attempt. The empirical results, along with the corresponding theoretical verifier gains, are shown in Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 \u2023 Examining verifier performance at different model families and sizes. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p2\">\n<p class=\"ltx_p\">Although the measured improvements can be noisy when verifier gains are small and the nine-attempt cap limits the rejection sampling procedure, the overall trends align closely with theoretical predictions. In particular, we observe smaller gains from self-verification than from both intra-family and cross-family verification, as well as stronger scaling behavior for cross-family verification relative to the other settings.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p3\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>\u00a0\nThe verifier gain given by Equation\u00a0<a class=\"ltx_ref\" href=\"#S3.E1\" title=\"Equation 1 \u2023 3.2 Evaluation Metrics \u2023 3 Preliminaries \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> is a reliable predictor of performance improvements under rejection sampling. Crucially, it can be estimated from one round of verification without requiring computationally expensive rejection sampling experiments.</p>\n</blockquote>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.3\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Are Verifiers Biased Toward Solutions That Resemble Their Own?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\">Humans tend to judge solutions that resemble their own reasoning as more likely to be correct. This mirrors the well-documented self-enhancement bias\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Krueger, <a class=\"ltx_ref\" href=\"#bib.self-enhancement-bias\" title=\"\">1998</a>)</cite>, in which individuals evaluate themselves more favorably than objective evidence would suggest. Our results suggest that an analogous effect may arise in solver\u2013verifier interactions. As shown earlier in Figures\u00a0<a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and\u00a0<a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, strong reasoning models benefit the least from self-verification and the most from cross-family verification, hinting at a similar bias.</p>\n</div>\n<figure class=\"ltx_figure ltx_align_floatleft\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1253\" id=\"S5.F5.g1\" src=\"./assets/x5.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nCorrelation between verifier metrics with similarity scores between solver-verifier pairs. Each marker is colored based on the verifier model family.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p2\">\n<p class=\"ltx_p\">To directly investigate this behavior, we conduct cross-verification experiments using 12 post-trained models (the three smallest models from each of the four families) and compute all verifier metrics for each pair. For intra-family verification, each solver has 2 verifiers from the same family (excluding itself), resulting in <math alttext=\"12\\times 2=24\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>12</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>2</mn></mrow><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">12\\times 2=24</annotation></semantics></math> solver\u2013verifier pairs. For cross-family verification, each solver has 9 verifiers from other families, giving <math alttext=\"12\\times 9=108\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>12</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>9</mn></mrow><mo>=</mo><mn>108</mn></mrow><annotation encoding=\"application/x-tex\">12\\times 9=108</annotation></semantics></math> cross-family pairs. For each pair, we plot the verifier metric against the <span class=\"ltx_text ltx_font_bold\">solver\u2013verifier similarity score</span>, defined as the average cosine similarity between the two models\u2019 solution embeddings across all dataset problems. Solutions are embedded using <span class=\"ltx_text ltx_font_typewriter\">sentence-transformers</span> <span class=\"ltx_text ltx_font_typewriter\">/all-mpnet-base-v2</span>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p3\">\n<p class=\"ltx_p\">Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F5\" title=\"Figure 5 \u2023 5.3 Are Verifiers Biased Toward Solutions That Resemble Their Own? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows that, for both intra-family and cross-family settings, FPR exhibits a clear positive trend: the more similar the solver and verifier are in their solution distributions, the more likely the verifier is to accept the solver\u2019s incorrect answers. While intra-family verifier gains are too small to yield a strong correlation, cross-family verifier gains decrease significantly with increasing similarity. This indicates that, when selecting a solver\u2013verifier pair, choosing a verifier whose solution distribution differs from that of the solver leads to more reliable verification.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p4\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>\u00a0\nHigher similarity between solver and verifier solution distributions increases the verifier\u2019s tendency to accept incorrect solver outputs, reducing verifier gain. Using a verifier with a meaningfully different solution distribution mitigates this bias.</p>\n</blockquote>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.4\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">How Does Post-Training Affect Solver and Verifier Performance?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.p1\">\n<p class=\"ltx_p\">We examine how post-training influences verifier behavior. Our analysis focuses on the <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span>/<span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span>/<span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> model pairs. We exclude <span class=\"ltx_text ltx_font_typewriter\">Llama3-Base</span> due to its weak solver and verifier performance and omit <span class=\"ltx_text ltx_font_typewriter\">DeepSeek</span> because matching base models are unavailable. Verification metrics are computed across all 37 base and post-trained models.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S5.SS4.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Post-training effect on solver performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We begin by evaluating how solver accuracy changes after post-training. For each model pair, we compute solver accuracy and average results across model families and datasets. As expected, post-training yields substantial improvements: <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> solvers improve by an average of <math alttext=\"8.2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>8.2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">8.2\\%</annotation></semantics></math>, while <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> shows a striking <math alttext=\"35.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>35.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">35.4\\%</annotation></semantics></math> gain. Full results are provided in Appendix\u00a0<a class=\"ltx_ref\" href=\"#A9\" title=\"Appendix I Effect of Post-Training on Solver Performance \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS4.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Post-training effect on verifier performance.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We next analyze how post-training affects verifier behavior (Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F6\" title=\"Figure 6 \u2023 Post-training effect on verifier performance. \u2023 5.4 How Does Post-Training Affect Solver and Verifier Performance? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). For each model, we compute verifier metrics against all solvers and datasets, partition results by verification setting, and average within families. For both <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span>, post-training increases FPR and reduces verifier gain in self-verification, despite improvements in FNR. Although <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span> benefits more than <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5</span> in solver accuracy (Figure\u00a0<a class=\"ltx_ref\" href=\"#A9.F11\" title=\"Figure 11 \u2023 Appendix I Effect of Post-Training on Solver Performance \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>), its post-trained verifiers show higher FPRs and lower gains in both self- and intra-family verification. In contrast, both families, and especially <span class=\"ltx_text ltx_font_typewriter\">Qwen3</span>, show substantial improvements in cross-family verification.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.SSS0.Px2.p2\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>\u00a0\nPost-training significantly enhances a base model\u2019s problem-solving ability but can reduce its self- or intra-family improvement potential. In contrast, it boosts models\u2019 performance in cross-family verification.</p>\n</blockquote>\n</div>\n<figure class=\"ltx_figure ltx_align_floatleft\" id=\"S5.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"550\" id=\"S5.F6.g1\" src=\"./assets/x6.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nChanges in verifier metrics of the <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> models from post-training.</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS5\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">5.5\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Which Datasets are Easy to Verify?</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p1\">\n<p class=\"ltx_p\">Thus far, we have examined verifier performance and its contribution to solver accuracy through rejection sampling. We now shift to a task-level perspective and ask: <span class=\"ltx_text ltx_font_italic\">are tasks that are easy to solve also easy to verify?</span> In Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F7\" title=\"Figure 7 \u2023 5.5 Which Datasets are Easy to Verify? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we recompute the verifier metrics from Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS1\" title=\"5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, average them across all verifier models, and plot them against solver accuracies.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p2\">\n<p class=\"ltx_p\">We find that verification accuracy correlates strongly with solver accuracy. In contrast, verifier gains from self-verification do not correlate with problem difficulty, whereas both intra-family and cross-family verifier gains exhibit clear positive correlations. Notably, AIME appears as an outlier in the final plot, potentially because some models have encountered similar problems during post-training.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p3\">\n<p class=\"ltx_p\">The best-fit lines for verifier accuracy further reveal two distinct clusters of tasks (colored red and blue), indicating that verification difficulty cannot be explained solely by solver accuracy. This leads to our next question: <span class=\"ltx_text ltx_font_italic\">are some tasks inherently easier to verify than others?</span> Relatedly, <cite class=\"ltx_cite ltx_citemacro_citet\">Song et al. (<a class=\"ltx_ref\" href=\"#bib.song2025mind\" title=\"\">2025</a>)</cite> find that models cannot self-improve on tasks requiring factual recall, but can self-improve on Sudoku with sufficient pretraining scale. We observe a similar separation in Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F7\" title=\"Figure 7 \u2023 5.5 Which Datasets are Easy to Verify? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>: AIME, GSM8K, 3SAT, and Sudoku exhibit a higher ratio of verifier accuracy to solver accuracy and deliver higher gains across all verification settings.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p4\">\n<p class=\"ltx_p\">To explain this, we notice that among our synthetic datasets, Sudoku and 3SAT are classic examples of problems that require exponential solving time but allow polynomial-time verification. By contrast, there is no clear shortcut for verifying the product of two matrices without effectively recomputing it for Matrix Multiplication. Among the real-world datasets, GSM8K and AIME involve problems solvable with high-school-level mathematics, whereas MMLU (Social Sciences) requires domain-specific knowledge, CommonsenseQA relies on implicit world knowledge, and GPQA and MMLU (STEM) draw on specialized natural science content. For these latter tasks, verifying an answer requires essentially the same knowledge as solving the problem.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS5.p5\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif ltx_font_bold\" style=\"--ltx-fg-color:#2C2C2C;\">Takeaways:</span>\u00a0\nAlthough tasks that are easy to solve are typically easier to verify, some tasks are inherently easier to verify. These include synthetic problems with logical or structured reasoning (e.g., 3SAT, Sudoku) and real-world tasks relying primarily on mathematical reasoning rather than extensive factual recall (e.g., GSM8K, AIME). Such tasks also yield larger gains from test-time rejection sampling with verifiers.</p>\n</blockquote>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"369\" id=\"S5.F7.g1\" src=\"./assets/x7.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Correlation of verifier metrics (rows) with solver accuracies, averaged over solver-verifier pairs that belong to each verification setting (columns).</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">6\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Conclusion</span>\n</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p1\">\n<p class=\"ltx_p\">This work presents a comprehensive study of LLM-based verification for problem solving. We show that verification accuracy alone provides an incomplete picture of the expected improvement obtained by using a verifier for test-time rejection sampling, motivating the introduction of <span class=\"ltx_text ltx_font_italic\">verifier gain</span>, a more informative measure that captures this expected improvement. Using this metric, our analysis shows that verifier gain is often lower for self-verification and intra-family verification than for cross-family verification, particularly as model size increases or post-training is applied. Further analysis reveals that decreases in verifier gain correlate with greater similarity between the solver\u2019s and verifier\u2019s solution distributions. Finally, we show that some tasks are inherently easier for LLMs to verify than others: more difficult tasks generally require domain-specific or implicit world knowledge, whereas easier tasks tend to involve logical reasoning, mathematical reasoning, or structured puzzle solving.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S6.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations and future work.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS5\" title=\"5.5 Which Datasets are Easy to Verify? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> shows that some tasks are inherently more verifiable than others, motivating future work on developing a predictive model for the verifiability of individual tasks or questions. Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS3\" title=\"5.3 Are Verifiers Biased Toward Solutions That Resemble Their Own? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> shows that LLMs are biased toward accepting incorrect solutions that resemble their own reasoning, indicating that it will be worthwhile to examine the origins of this bias in pre-training and/or post-training.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_section\" style=\"font-size:144%;--ltx-fg-color:#2C2C2C;\">Acknowledgement</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">We thank members of the NYU Agentic Learning AI Lab for their helpful discussions. JL is supported by the NSERC PGS-D Scholarship.\nThe work is supported in part by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) under grant RS-2024-00469482, funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research.\nThe compute is supported by the NYU High Performance Computing\nresources, services, and staff expertise. We also thank Modal for providing additional compute resources.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_bibliography\" style=\"font-size:144%;--ltx-fg-color:#6B7280;\">\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">References</span>\n</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.brown2024large\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brown et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R\u00e9, C., and Mirhoseini, A. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nLarge language monkeys: Scaling inference compute with repeated sampling.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2407.21787</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.chen2025sets\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nChen, J., Ren, J., Chen, X., Yang, C., Sun, R., Yoon, J., and Ar\u0131k, S. \u00d6. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nSets: Leveraging self-verification and self-correction for improved test-time scaling.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2501.19306</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.chen2024universal\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nChen, X., Aksitov, R., Alon, U., Ren, J., Xiao, K., Yin, P., Prakash, S., Sutton, C., Wang, X., and Zhou, D. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nUniversal self-consistency for large language models.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICML 2024 Workshop on In-Context Learning</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.chow2025inferenceaware-bon\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chow et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nChow, Y., Tennenholtz, G., Gur, I., Zhuang, V., Dai, B., Kumar, A., Agarwal, R., Thiagarajan, S., Boutilier, C., and Faust, A. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nInference-aware fine-tuning for best-of-n sampling in large language models.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.cobbe2021training\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cobbe et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021).\n</span>\n<span class=\"ltx_bibblock\">\nTraining verifiers to solve math word problems.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2110.14168</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.deepseek\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">DeepSeek-AI et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2501.12948</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.dhuliawala-etal-2024-chain-of-verification\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dhuliawala et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nDhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., and Weston, J. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nChain-of-verification reduces hallucination in large language models.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ACL Findings</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.gou2024critic\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gou et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nGou, Z., Shao, Z., Gong, Y., yelong shen, Yang, Y., Duan, N., and Chen, W. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nCRITIC: Large language models can self-correct with tool-interactive critiquing.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.llama3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grattafiori et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzm\u00e1n, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., \u00c7elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nThe llama 3 herd of models.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2407.21783</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.mmlu\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hendrycks et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021).\n</span>\n<span class=\"ltx_bibblock\">\nMeasuring massive multitask language understanding.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.hosseini2024vstar\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hosseini et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nHosseini, A., Yuan, X., Malkin, N., Courville, A., Sordoni, A., and Agarwal, R. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nV-STar: Training verifiers for self-taught reasoners.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>COLM</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.huang2024self-sharpening\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Huang et al. [2024a]</span>\n<span class=\"ltx_bibblock\">\nHuang, A., Block, A., Foster, D. J., Rohatgi, D., Zhang, C., Simchowitz, M., Ash, J. T., and Krishnamurthy, A. (2024a).\n</span>\n<span class=\"ltx_bibblock\">\nSelf-improvement in language models: The sharpening mechanism.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2412.01951</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.huang2024large-cannot-self-correct\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Huang et al. [2024b]</span>\n<span class=\"ltx_bibblock\">\nHuang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. (2024b).\n</span>\n<span class=\"ltx_bibblock\">\nLarge language models cannot self-correct reasoning yet.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.jiang-etal-2024-forward-backward\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nJiang, W., Shi, H., Yu, L., Liu, Z., Zhang, Y., Li, Z., and Kwok, J. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nForward-backward reasoning in large language models for mathematical verification.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ACL Findings</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.kamoi-etal-2024-when-self-correct\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kamoi et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nKamoi, R., Zhang, Y., Zhang, N., Han, J., and Zhang, R. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nWhen can LLMs actually correct their own mistakes? a critical survey of self-correction of LLMs.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.kang2024mindstar\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nKang, J., Li, X. Z., Chen, X., Kazemi, A., Sun, Q., Chen, B., Li, D., He, X., He, Q., Wen, F., et al. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nMindstar: Enhancing math reasoning in pre-trained llms at inference time.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2405.16265</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.khalifa-etal-2023-grace\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Khalifa et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nKhalifa, M., Logeswaran, L., Lee, M., Lee, H., and Wang, L. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nGRACE: Discriminator-guided chain-of-thought reasoning.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>EMNLP Findings</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.kim2023language\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kim et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nKim, G., Baldi, P., and McAleer, S. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nLanguage models can solve computer tasks.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>NeurIPS</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.self-enhancement-bias\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Krueger [1998]</span>\n<span class=\"ltx_bibblock\">\nKrueger, J. (1998).\n</span>\n<span class=\"ltx_bibblock\">\nEnhancement bias in descriptions of self and others.\n</span>\n<span class=\"ltx_bibblock\">\n<em>Personality and Social Psychology Bulletin</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.lightman2024lets\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lightman et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nLet's verify step by step.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.ling2023deductive-verification\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ling et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nLing, Z., Fang, Y., Li, X., Huang, Z., Lee, M., Memisevic, R., and Su, H. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nDeductive verification of chain-of-thought reasoning.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>NeurIPS</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.luo2025improve-automated-process-supervision\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Luo et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nLuo, L., Liu, Y., Liu, R., Phatale, S., Guo, M., Lara, H., Li, Y., Shu, L., Meng, L., Sun, J., and Rastogi, A. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nImprove mathematical reasoning in language models with automated process supervision.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2406.06592</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.madaan2023selfrefine\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Madaan et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nSelf-refine: Iterative refinement with self-feedback.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.AIME2025\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mathematical Association of America [2025]</span>\n<span class=\"ltx_bibblock\">\nMathematical Association of America (2025).\n</span>\n<span class=\"ltx_bibblock\">\n1983-2025 American Invitational Mathematics Examination (AIME) I: Problems and Solutions.\n</span>\n<span class=\"ltx_bibblock\">\nArt of Problem Solving Wiki entry.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.nichols2020collaborative\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nichols et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nNichols, E., Gao, L., and Gomez, R. (2020).\n</span>\n<span class=\"ltx_bibblock\">\nCollaborative storytelling with large-scale neural language models.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ACM SIGGRAPH Conference on Motion, Interaction and Games</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.olausson2023self\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Olausson et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nOlausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-Lezama, A. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nIs self-repair a silver bullet for code generation?\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2306.09896</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.llm-evaluators\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Panickssery et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nPanickssery, A., Bowman, S. R., and Feng, S. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nLlm evaluators recognize and favor their own generations.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.qwen2.5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Qwen et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nQwen2.5 technical report.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2412.15115</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.gpqa\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rein et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nGPQA: A graduate-level google-proof q&amp;a benchmark.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>COLM</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.roy-roth-2015-solving\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Roy and Roth [2015]</span>\n<span class=\"ltx_bibblock\">\nRoy, S. and Roth, D. (2015).\n</span>\n<span class=\"ltx_bibblock\">\nSolving general arithmetic word problems.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>EMNLP</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.sareen2025putting-value\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sareen et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nSareen, K., Moss, M. M., Sordoni, A., Agarwal, R., and Hosseini, A. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nPutting the value back in rl: Better test-time scaling by unifying llm reasoners with verifiers.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2505.04842</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.shen-etal-2021-generate-rank\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shen et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nShen, J., Yin, Y., Li, L., Shang, L., Jiang, X., Zhang, M., and Liu, Q. (2021).\n</span>\n<span class=\"ltx_bibblock\">\nGenerate &amp; rank: A multi-task framework for math word problems.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>EMNLP</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.shinn2023reflexion\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shinn et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nReflexion: language agents with verbal reinforcement learning.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>NeurIPS</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.singhi2025solve\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Singhi et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nSinghi, N., Bansal, H., Hosseini, A., Grover, A., Chang, K.-W., Rohrbach, M., and Rohrbach, A. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nWhen to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2504.01005</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.snell2025scaling-tt-compute\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Snell et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nSnell, C. V., Lee, J., Xu, K., and Kumar, A. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nScaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.song2025mind\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Song et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nSong, Y., Zhang, H., Eisenach, C., Kakade, S. M., Foster, D., and Ghai, U. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nMind the gap: Examining the self-improvement capabilities of large language models.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.stroebl2024inference-flaws\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Stroebl et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nStroebl, B., Kapoor, S., and Narayanan, A. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nInference scaling flaws: The limits of llm resampling with imperfect verifiers.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2411.17501</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.csqa\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Talmor et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nTalmor, A., Herzig, J., Lourie, N., and Berant, J. (2019).\n</span>\n<span class=\"ltx_bibblock\">\nCommonsenseqa: A question answering challenge targeting commonsense knowledge.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>NAACL 2019</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.wang2023selfconsistency\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nWang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nSelf-consistency improves chain of thought reasoning in language models.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.welleck2023generating\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Welleck et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nWelleck, S., Lu, X., West, P., Brahman, F., Shen, T., Khashabi, D., and Choi, Y. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nGenerating sequences by learning to self-correct.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.weng2023-better-reasoners\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Weng et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nWeng, Y., Zhu, M., Xia, F., Li, B., He, S., Liu, S., Sun, B., Liu, K., and Zhao, J. (2023).\n</span>\n<span class=\"ltx_bibblock\">\nLarge language models are better reasoners with self-verification.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>EMNLP Findings</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.wu-etal-2024-key-condition-verification\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nWu, Z., Zeng, Q., Zhang, Z., Tan, Z., Shen, C., and Jiang, M. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nLarge language models can self-correct with key condition verification.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>EMNLP</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.qwen3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nYang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nQwen3 technical report.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2505.09388</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.yang-etal-2022-generating-nl-proofs\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nYang, K., Deng, J., and Chen, D. (2022).\n</span>\n<span class=\"ltx_bibblock\">\nGenerating natural language proofs with verifier-guided search.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>EMNLP</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.yu-etal-2024-ovm\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nYu, F., Gao, A., and Wang, B. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nOVM, outcome-supervised value models for planning in mathematical reasoning.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>NAACL Findings</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.yu2025scaling-flaws-math\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nYu, F., Li, Y., and Wang, B. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nScaling flaws of verifier-guided search in mathematical reasoning.\n</span>\n<span class=\"ltx_bibblock\">\n<em>arXiv preprint arXiv:2502.00271</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.zhang-generativeverifiers\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nZhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nGenerative verifiers: Reward modeling as next-token prediction.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.zhang-etal-2024-small-large-ver\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nZhang, Y., Khalifa, M., Logeswaran, L., Kim, J., Lee, M., Lee, H., and Wang, L. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nSmall language models need strong verifiers to self-correct reasoning.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ACL</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.zhao2025sample\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nZhao, E., Awasthi, P., and Gollapudi, S. (2025).\n</span>\n<span class=\"ltx_bibblock\">\nSample, scrutinize and scale: Effective inference-time search by scaling verification.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICML</em>.\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.zhou2024solving\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nZhou, A., Wang, K., Lu, Z., Shi, W., Luo, S., Qin, Z., Lu, S., Jia, A., Song, L., Zhan, M., and Li, H. (2024).\n</span>\n<span class=\"ltx_bibblock\">\nSolving challenging math word problems using GPT-4 code interpreter with code-based self-verification.\n</span>\n<span class=\"ltx_bibblock\">\nIn <em>ICLR</em>.\n</span>\n</li>\n</ul>\n</section>\n<section class=\"ltx_appendix\" id=\"Ax1\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n</section>\n\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Related Work</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scaling test-time compute.</span> A simple method for scaling test-time compute involves sampling several candidates and selecting one <span class=\"ltx_text ltx_font_italic\">post hoc</span>, for example via Best-of-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>. This can take the form of sample-and-rank approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Nichols et al., <a class=\"ltx_ref\" href=\"#bib.nichols2020collaborative\" title=\"\">2020</a>)</cite>, majority vote <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.wang2023selfconsistency\" title=\"\">2023</a>)</cite>, model-based aggregation <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.chen2024universal\" title=\"\">2024</a>)</cite>, or sampling then filtering <cite class=\"ltx_cite ltx_citemacro_citep\">(Weng et al., <a class=\"ltx_ref\" href=\"#bib.weng2023-better-reasoners\" title=\"\">2023</a>)</cite>. LLMs can also be finetuned to explicitly optimize Best-of-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Chow et al., <a class=\"ltx_ref\" href=\"#bib.chow2025inferenceaware-bon\" title=\"\">2025</a>)</cite>. Instead of <span class=\"ltx_text ltx_font_italic\">post hoc</span> selection, we can guide the model towards good samples via constraints <cite class=\"ltx_cite ltx_citemacro_citep\">(Roy and Roth, <a class=\"ltx_ref\" href=\"#bib.roy-roth-2015-solving\" title=\"\">2015</a>)</cite>, a scoring function <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"#bib.yang-etal-2022-generating-nl-proofs\" title=\"\">2022</a>)</cite>, or sequential construction <cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et al., <a class=\"ltx_ref\" href=\"#bib.kang2024mindstar\" title=\"\">2024</a>; Khalifa et al., <a class=\"ltx_ref\" href=\"#bib.khalifa-etal-2023-grace\" title=\"\">2023</a>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Details on Verifier Metrics</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.p1\">\n<p class=\"ltx_p\">We show the mathematical definitions of relevant verifier metrics below. For clarity, we include dependencies (e.g., <math alttext=\"(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,V;\\mathcal{D})</annotation></semantics></math>) in the definitions, but sometimes omit them for brevity when the context is clear.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.p2\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A9.EGx1\">\n<tbody id=\"A2.Ex1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{VerifierAcc}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex1.m1\" intent=\":literal\"><semantics><mrow><mtext>VerifierAcc</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{VerifierAcc}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,\\mathbbm{1}\\{\\,V(x,y)=c(x,y)\\,\\}\\,\\big]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex1.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><msub><mi mathvariant=\"normal\">\ud835\udd3c</mi><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mi>y</mi><mo>\u223c</mo><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo><mrow><mn class=\"ltx_mathvariant_double-struck\" mathvariant=\"double-struck\">\u20091</mn><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo rspace=\"0.170em\" stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}_{(x,\\mathcal{Y}_{x})\\sim\\mathcal{D},\\,y\\sim S(x)}\\big[\\,\\mathbbm{1}\\{\\,V(x,y)=c(x,y)\\,\\}\\,\\big]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{TPR}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex2.m1\" intent=\":literal\"><semantics><mrow><mtext>TPR</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{TPR}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex2.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\ud835\udd3c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mrow><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2223</mo><mi>y</mi></mrow><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{FPR}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex3.m1\" intent=\":literal\"><semantics><mrow><mtext>FPR</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{FPR}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\notin\\mathcal{Y}_{x}\\,]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex3.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\ud835\udd3c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mrow><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2223</mo><mi>y</mi></mrow><mo>\u2209</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,V(x,y)\\mid y\\notin\\mathcal{Y}_{x}\\,]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{FNR}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex4.m1\" intent=\":literal\"><semantics><mrow><mtext>FNR</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{FNR}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,1-V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]=1-\\text{TPR}(S,V;\\mathcal{D}).\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex4.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\ud835\udd3c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mn>\u20091</mn><mo>\u2212</mo><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2223</mo><mi>y</mi></mrow></mrow><mo>\u2208</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcb4</mi><mi>x</mi></msub></mrow><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>\u2212</mo><mrow><mtext>TPR</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,1-V(x,y)\\mid y\\in\\mathcal{Y}_{x}\\,]=1-\\text{TPR}(S,V;\\mathcal{D}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Precision}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex5.m1\" intent=\":literal\"><semantics><mrow><mtext>Precision</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Precision}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\mathbb{E}[\\,c(x,y)\\mid V(x,y)=1\\,]=\\frac{\\text{SolverAcc}\\cdot\\text{TPR}}{\\text{SolverAcc}\\cdot\\text{TPR}+(1-\\text{SolverAcc})\\cdot\\text{FPR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex5.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi mathvariant=\"normal\">\ud835\udd3c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mrow><mrow><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2223</mo><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo lspace=\"0.170em\" stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mtext>SolverAcc</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mtext>TPR</mtext></mrow><mrow><mrow><mtext>SolverAcc</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mtext>TPR</mtext></mrow><mo>+</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>\u2212</mo><mtext>SolverAcc</mtext></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">\u22c5</mo><mtext>FPR</mtext></mrow></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathbb{E}[\\,c(x,y)\\mid V(x,y)=1\\,]=\\frac{\\text{SolverAcc}\\cdot\\text{TPR}}{\\text{SolverAcc}\\cdot\\text{TPR}+(1-\\text{SolverAcc})\\cdot\\text{FPR}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex6\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Recall}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex6.m1\" intent=\":literal\"><semantics><mrow><mtext>Recall</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Recall}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\text{TPR}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex6.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mtext>TPR</mtext></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\text{TPR}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A2.Ex7\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{F1}(S,V;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex7.m1\" intent=\":literal\"><semantics><mrow><mtext>F1</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{F1}(S,V;\\mathcal{D})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{2\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.Ex7.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mtext>Precision</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mtext>Recall</mtext></mrow><mrow><mtext>Precision</mtext><mo>+</mo><mtext>Recall</mtext></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{2\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Details on Verification Settings</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.p1\">\n<p class=\"ltx_p\">We show the mathematical definitions of our three verification settings. Let <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> denote the space of models and <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math> the space of model families. We define a function</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"A3.Ex8\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\text{Family}:\\mathcal{M}\\to\\mathcal{F},\" class=\"ltx_Math\" display=\"block\" id=\"A3.Ex8.m1\" intent=\":literal\"><semantics><mrow><mrow><mtext>Family</mtext><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi><mo stretchy=\"false\">\u2192</mo><mi class=\"ltx_font_mathcaligraphic\">\u2131</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Family}:\\mathcal{M}\\to\\mathcal{F},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">that maps each model (e.g., <span class=\"ltx_text ltx_font_typewriter\">meta-llama/Meta-Llama-3-70B</span>) to its corresponding family (e.g., <span class=\"ltx_text ltx_font_typewriter\">Llama-3-Base</span>).\nNote that <math alttext=\"S,V\\in\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo>,</mo><mi>V</mi></mrow><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\u2133</mi></mrow><annotation encoding=\"application/x-tex\">S,V\\in\\mathcal{M}</annotation></semantics></math>. For any verifier metric <math alttext=\"M(\\cdot,\\cdot;\\mathcal{D})\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">\u22c5</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">\u22c5</mo><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot,\\cdot;\\mathcal{D})</annotation></semantics></math> such as\nVerifierAcc, TPR, or <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m5\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math>, we define:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.p2\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A9.EGx2\">\n<tbody id=\"A3.Ex9\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Self-Verif}(V;\\mathcal{D},M)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex9.m1\" intent=\":literal\"><semantics><mrow><mtext>Self-Verif</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Self-Verif}(V;\\mathcal{D},M)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=M(V,V;\\mathcal{D}),\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex9.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=M(V,V;\\mathcal{D}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A3.Ex10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Intra-Verif}(V,\\mathcal{S};\\mathcal{D},M)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex10.m1\" intent=\":literal\"><semantics><mrow><mtext>Intra-Verif</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Intra-Verif}(V,\\mathcal{S};\\mathcal{D},M)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\nS\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:S\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\}|},\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex10.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mo>\u2211</mo><mtable rowspacing=\"0pt\"><mtr><mtd><mrow><mi>S</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>S</mi><mo>\u2260</mo><mi>V</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable></msub><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>S</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi>S</mi><mo>\u2260</mo><mi>V</mi></mrow><mo rspace=\"0.337em\">,</mo><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\nS\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:S\\neq V,\\,\\text{Family}(S)=\\text{Family}(V)\\}|},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"A3.Ex11\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Cross-Verif}(V,\\mathcal{S};\\mathcal{D},M)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex11.m1\" intent=\":literal\"><semantics><mrow><mtext>Cross-Verif</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo>,</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Cross-Verif}(V,\\mathcal{S};\\mathcal{D},M)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\n\\text{Family}(S)\\neq\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:\\text{Family}(S)\\neq\\text{Family}(V)\\}|}.\" class=\"ltx_Math\" display=\"inline\" id=\"A3.Ex11.m2\" intent=\":literal\"><semantics><mrow><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mo>\u2211</mo><mtable rowspacing=\"0pt\"><mtr><mtd><mrow><mi>S</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2260</mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></msub><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>V</mi><mo>;</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>S</mi><mo>\u2208</mo><mi class=\"ltx_font_mathcaligraphic\">\ud835\udcae</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2260</mo><mrow><mtext>Family</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac></mstyle></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{\\sum_{\\begin{subarray}{c}S\\in\\mathcal{S}\\\\\n\\text{Family}(S)\\neq\\text{Family}(V)\\end{subarray}}M(S,V;\\mathcal{D})}{|\\{S\\in\\mathcal{S}:\\text{Family}(S)\\neq\\text{Family}(V)\\}|}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Additional Details on Models</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A4.p1\">\n<p class=\"ltx_p\">We show the information for each of our 37 evaluated models in Table\u00a0<a class=\"ltx_ref\" href=\"#A9.T1\" title=\"Table 1 \u2023 Appendix I Effect of Post-Training on Solver Performance \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A5\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Additional Details on Datasets</h2>\n<section class=\"ltx_subsection\" id=\"A5.SS1\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">E.1\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Real-World Datasets</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS1.p1\">\n<p class=\"ltx_p\">Note that for MMLU (STEM) and MMLU (Social Sciences), we concatenate questions from all subjects that belong to the STEM and Social Sciences supercategories in\u00a0<cite class=\"ltx_cite ltx_citemacro_citet\">Hendrycks et al. (<a class=\"ltx_ref\" href=\"#bib.mmlu\" title=\"\">2021</a>)</cite>, respectively.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A5.SS2\">\n<h3 class=\"ltx_title ltx_font_sansserif ltx_font_bold ltx_title_subsection\" style=\"font-size:120%;--ltx-fg-color:#6B7280;\">E.2\u2003\u200a<span class=\"ltx_text\" style=\"--ltx-fg-color:#2C2C2C;\">Synthetic Datasets</span>\n</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p1\">\n<p class=\"ltx_p\">We generate three synthetic datasets, named 3SAT, Matrix Multiplication, and Sudoku, with 1000 samples each. We submit the data generation code in the Supplementary Materials, but briefly explain each synthetic dataset\u2019s generation parameters below.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p2\">\n<p class=\"ltx_p\">Each 3SAT CNF contains uniformly sampled numbers of variables and clauses from 2 to 8 (inclusive). Each Sudoku puzzle is a 9x9 grid with 12 randomly missing cells. Each Matrix Multiplication problem is about multiplying 2 4x4 integer matrices with values uniformly sampled from <math alttext=\"[-5,5]\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mo>\u2212</mo><mn>5</mn></mrow><mo>,</mo><mn>5</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-5,5]</annotation></semantics></math>. All data are generated in a way that ensures the existence of a valid solution. Note that while Matrix Multiplication has a singular correct answer for each problem, Sudoku and 3SAT are allowed multiple correct answers as long as the solver\u2019s answer is correct by their rules.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p3\">\n<p class=\"ltx_p\">The generation code files for all synthetic datasets are seeded for reproducibility.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p4\">\n<p class=\"ltx_p\">An example of a generated 3SAT problem:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\n## Problem Definition\n\n**SAT (Boolean Satisfiability Problem)** is a fundamental problem in computer science\nwhere we need to determine if there exists an assignment of Boolean values (True/False)\nto variables that makes a given Boolean formula evaluate to True.\n**Variables**: In this problem, variables are named as single letters. Each variable can\nbe assigned either True (T) or False (F).\n**Literals**: A literal is either a variable (like a) or its negation (like \u02dca, meaning\n\"not a\"). If a is True, then \u02dca is False, and vice versa.\n**Clauses**: A clause is a disjunction (OR operation) of literals. A clause is satisfied\n(True) if at least one of its literals is True. For example, the clause (a or \u02dcb) is True if\neither a is True OR b is False (or both).\n**CNF (Conjunctive Normal Form)**: The Boolean formula is given in CNF, which is a\nconjunction (AND operation) of multiple clauses. The entire formula is satisfied only if\nALL clauses are satisfied simultaneously.\n**3SAT**: This is a special case of SAT where every clause contains exactly 3 literals.\n\n## The Problem\n\nFind a satisfying assignment for the following CNF formula: (\u02dcc or \u02dcb or d) and\n(d or \u02dcb or \u02dcc) and (d or a or c) and (\u02dcc or d or a) and (b or \u02dca or d) and (c or d or \u02dcb)\n\n## Instructions\n\nProvide your answer as a list of variable assignments, one per line, in the format\n\"variable_name T\" or \"variable_name F.\" For example:\n\\boxed{\na T\nb F\n}\nThis means a=True, b=False.\n\nAnother example answer is\n\\boxed{\na F\nb T\n}\nThis means a=False, b=True.\n\nOutput and only output the T/F values for the variables that appear in the provided\nCNF formula.\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p5\">\n<p class=\"ltx_p\">An example of a generated Sudoku problem:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\n## Sudoku Problem\n\n**Sudoku** is a logic-based number-placement puzzle. The objective is to fill a 9x9 grid\nwith numbers so that each column, each row, and each of the 3x3 sub-grids contains all\nof the numbers from 1 to 9.\n\n## The Puzzle\n\nComplete the following 9x9 Sudoku grid (empty cells are marked with \u2019_\u2019):\n\n7 4 2 1 _ 5 8 9 6\n1 6 9 2 4 8 3 5 7\n8 5 3 _ _ 7 2 1 4\n2 _ 8 9 7 1 4 6 5\n5 7 6 4 8 2 9 3 _\n4 9 1 3 _ 6 _ 8 _\n3 1 5 8 2 4 6 7 9\n6 8 _ 7 1 _ 5 2 3\n_ 2 7 5 6 _ 1 4 8\n\n## Instructions\n\nProvide your answer as a completed 9x9 grid with all numbers filled in, formatted exactly\nlike the puzzle above but with numbers instead of underscores.\n\nFor example, a completed 4x4 grid should look like:\n\\boxed{\n1 2 3 4\n3 4 1 2\n2 3 4 1\n4 1 2 3\n}\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p6\">\n<p class=\"ltx_p\">An example of a generated Matrix Multiplication problem:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\n## Matrix Multiplication Problem\n\n**Matrix Multiplication** is a fundamental operation in linear algebra where we compute\nthe product of two matrices. For two square matrices A and B of size 4x4, the product\nC = A x B is computed as:\n\nC[i][j] = Sum(k=0 to 3) A[i][k] x B[k][j]\n\n## The Problem\n\nCompute the product of the following two 4x4 matrices:\n\n**Matrix A:**\n0 1 1 4\n-1 3 4 4\n-2 -5 -5 0\n-4 4 5 0\n\n**Matrix B:**\n1 2 0 5\n1 -2 0 0\n3 -1 -3 -3\n2 5 -4 2\n\n## Instructions\n\nProvide your answer as the resulting 4x4 matrix C = A x B, formatted with each row\non a separate line and numbers separated by spaces.\n\nFor example, a 2x2 result matrix is formatted like:\n\\boxed{\n1 2\n3 4\n}\n</pre>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A6\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix F </span>Additional Details on Experimental Setup</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p1\">\n<p class=\"ltx_p\">We use the following solver prompt for all models:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\nPlease reason step by step, and put your final answer within \\boxed{{}}.\n\n{question}\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p2\">\n<p class=\"ltx_p\">We use the following verifier prompt for all models:</p>\n<pre class=\"ltx_verbatim ltx_font_typewriter\">\nYou are a teacher that is evaluating a student\u2019s answer to a question.\nYour task is to determine whether the answer is correct or incorrect.\n\nQuestion: {question}\n\nStudent\u2019s Answer: {response}\n\nPlease evaluate the student\u2019s answer carefully. Consider:\n- Is the answer factually accurate?\n- Is the reasoning sound and logical?\n- Does it fully address the question asked?\n\nAfter your evaluation, provide your judgment in the\nfollowing format:\n- If the answer is correct, write: \\boxed{{correct}}.\n- If the answer is incorrect, write: \\boxed{{incorrect}}.\n\nFirst explain your analysis over the student\u2019s answer, then provide your final judgment in\nthe boxed format. Make sure the final judgment is either \"correct\" or \"incorrect\" inside\nthe \\boxed{{}}. Do not put anything else in \\boxed{{}}. Do not repeat the student\u2019s answer\nin \\boxed{{}}.\n</pre>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p3\">\n<p class=\"ltx_p\">Figure\u00a0<a class=\"ltx_ref\" href=\"#A6.F8\" title=\"Figure 8 \u2023 Appendix F Additional Details on Experimental Setup \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> displays the ratio of filtered solver outputs due to not containing a box for answer extraction, averaged across all datasets.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A6.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"217\" id=\"A6.F8.g1\" src=\"./assets/x8.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Average ratio of filtered solver outputs for each model over all datasets. Base model families are suffixed by <span class=\"ltx_text ltx_font_bold\">-Base</span>. Models within each family are ordered in increasing size.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_appendix\" id=\"A7\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix G </span>Solver Accuracy by Dataset</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A7.p1\">\n<p class=\"ltx_p\">Figure\u00a0<a class=\"ltx_ref\" href=\"#A7.F9\" title=\"Figure 9 \u2023 Appendix G Solver Accuracy by Dataset \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the solver accuracies of all 37 models on each of our 9 datasets.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A7.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"816\" id=\"A7.F9.g1\" src=\"./assets/x9.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">The solver accuracies of 37 models on each dataset.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A8\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix H </span>F1-Score and Precision Visualization</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A8.p1\">\n<p class=\"ltx_p\">Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the correlation between each model\u2019s verification ability and its own solver accuracy for all 21 post-trained models. We additionally display verifier F1-Score and precision in Figure\u00a0<a class=\"ltx_ref\" href=\"#A8.F10\" title=\"Figure 10 \u2023 Appendix H F1-Score and Precision Visualization \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A8.p2\">\n<p class=\"ltx_p\">In comparison to verifier accuracy, while F1-Score also positively correlates with verifier\u2019s own solver accuracy for all verification settings, the slopes decrease from self-verification to intra-family verification, and further decrease for cross-family verification, showing that the increase in false positive rate in Figure\u00a0<a class=\"ltx_ref\" href=\"#S5.F2\" title=\"Figure 2 \u2023 Correlating verifier and solver performance. \u2023 5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> has a stronger negative impact on lowering F1-score than accuracy.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A8.p3\">\n<p class=\"ltx_p\">While Section\u00a0<a class=\"ltx_ref\" href=\"#S5.SS1\" title=\"5.1 Do Better Solvers Make Better Verifiers? \u2023 5 Results \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> explains the low verifier gains for self- and intra-family verification through close examination of FPR, we additionally plot verifier precision in Figure\u00a0<a class=\"ltx_ref\" href=\"#A8.F10\" title=\"Figure 10 \u2023 Appendix H F1-Score and Precision Visualization \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. However, since precision is the expected performance of verifier-based rejection sampling in the limit of infinite sampling and our main metric \u201cverifier gain\u201d is defined in terms of it (Equation\u00a0<a class=\"ltx_ref\" href=\"#S3.E1\" title=\"Equation 1 \u2023 3.2 Evaluation Metrics \u2023 3 Preliminaries \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), precision does not help explain the differences in verifier gains across verification settings itself.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A8.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1014\" id=\"A8.F10.g1\" src=\"./assets/x10.png\" width=\"768\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Correlation between each model\u2019s verifier metrics (rows) and its own solver accuracy for all 21 post-trained models, averaged over all datasets. Each verifier metric is computed over three settings (columns): self-verification, intra-family verification, and cross-family verification. We use the same set of post-trained models as the set of solver models.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_appendix\" id=\"A9\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix I </span>Effect of Post-Training on Solver Performance</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A9.p1\">\n<p class=\"ltx_p\">Figure\u00a0<a class=\"ltx_ref\" href=\"#A9.F11\" title=\"Figure 11 \u2023 Appendix I Effect of Post-Training on Solver Performance \u2023 When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average improvement in solver accuracies of <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> families of models from their respective post-training procedures.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A9.T1\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Complete list of each evaluated model\u2019s HuggingFace identifier, family, and size.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">HuggingFace Identifier</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Family</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-0.6B-Base</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">0.6B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-1.7B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">1.7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-4B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">4B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-8B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen/Qwen3-14B-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">Qwen3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#87CEEB;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-0.6B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">0.6B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-1.7B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">1.7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-4B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">4B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-8B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-14B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen/Qwen3-32B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">Qwen3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#1F77B4;\">32B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-0.5B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">0.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-1.5B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">1.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-3B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-7B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-14B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-32B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">32B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen/Qwen2.5-72B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">Qwen2.5-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FFB347;\">72B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-0.5B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">0.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-1.5B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">1.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-3B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-7B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-14B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-32B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">32B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen/Qwen2.5-72B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">Qwen2.5</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#D62728;\">72B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.2-1B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">1B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.2-3B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.1-8B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">meta-llama/Llama-3.1-70B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">Llama3-Base</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#A5D6A7;\">70B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.2-1B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">1B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.2-3B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">3B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.1-8B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">8B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">meta-llama/Llama-3.1-70B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">Llama3</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#2CA02C;\">70B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">1.5B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">7B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-14B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">14B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">DeepSeek</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9467BD;\">32B</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_table\" id=\"A9.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">HuggingFace information and sizes of real-world datasets.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset Name</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">HuggingFace Identifier</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">HuggingFace Split</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">GSM8K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">openai/gsm8k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1319</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">AIME</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">TianHongZXY/aime-1983-2025</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">963</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MMLU (STEM)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">cais/mmlu</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">316</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MMLU (Social Sciences)</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">cais/mmlu</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">308</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">CSQA</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">tau/commonsense_qa</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">validation</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2442</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">GPQA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Idavidrein/gpqa</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">train</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">198</td>\n</tr>\n</tbody>\n</table>\n</figure>\n<figure class=\"ltx_figure\" id=\"A9.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"331\" id=\"A9.F11.g1\" src=\"./assets/x11.png\" width=\"331\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nImprovements in solver accuracies of <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5-Base</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Base</span> models from post-training.\n</span></figcaption>\n</figure>\n</section>",
  "css": "",
  "arxiv_id": "2512.02304",
  "source": "arxiv-experimental",
  "generated": "2025-12-17T15:26:03.470Z"
}