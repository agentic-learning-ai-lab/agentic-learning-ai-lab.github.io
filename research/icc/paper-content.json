{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\">Central to any clustering procedure is a similarity measure that makes it possible to separate data into meaningful groups. Classical methods often rely on predefined measures, such as k-means with Euclidean distance, and therefore impose strong assumptions on the underlying data distributions. As a result, these approaches often struggle with high-dimensional and semantically complex data such as text <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2003</a>; Shah and Mahajan, <a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">2012</a>)</cite>, images <cite class=\"ltx_cite ltx_citemacro_citep\">(Wazarkar and Keshavamurthy, <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2018</a>; Chang et al., <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">2017</a>; Guérin and Boots, <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">2018</a>)</cite>, and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Meinedo and Neto, <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">2003</a>; Alwassel et al., <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">2020</a>)</cite>, where similarity is context-dependent and cannot be easily captured by a rigid predefined function.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\">Recent advances in Large Language Models (LLMs) offer a promising alternative through in-context learning (ICL) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2017</a>; Brown et al., <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2020</a>)</cite>, which has been proven effective across a variety of data distributions <cite class=\"ltx_cite ltx_citemacro_citep\">(Tsimpoukelli et al., <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">2021</a>; Garg et al., <a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">2022</a>; Gruver et al., <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2023</a>; Vacareanu et al., <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">2024</a>)</cite>. Instead of using a predefined similarity function, LLMs capture context-dependent relations through an attention mechanism with query and key projections learned from large-scale pretraining. The ability to recognize contextual relationships among in-context examples provides a foundation for flexible clustering that can adapt to diverse data and different criteria. This LLM-based approach particularly excels in <em class=\"ltx_emph ltx_font_italic\">few-shot scenarios involving semantically rich, naturalistic data</em>, complementing classical methods optimized for structured large-scale datasets.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\">In this work, we propose <em class=\"ltx_emph ltx_font_italic\">In-Context Clustering</em> (ICC), extending in-context learning to an unsupervised setting (<a class=\"ltx_ref\" href=\"#S1.F1\" title=\"In 1 Introduction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>). Different from previous in-context supervised learning that requires multiple input-output pairs in the prompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2020</a>)</cite>, ICC utilizes only unlabeled input data in the context. Given a natural language instruction specifying the clustering objective and a sequence of inputs, the LLM generates cluster labels autoregressively. When the clustering condition changes (e.g., grouping by color instead of class as shown in <a class=\"ltx_ref\" href=\"#S5.F5\" title=\"In Data. ‣ 5 Text-Conditioned Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>), one can simply modify the prompt without updating model weights or features. We evaluate ICC on numerical data and image data using a variety of synthetic and real-world datasets to demonstrate the effectiveness and flexibility of ICC.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\">Our paper is structured as follows:</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\">We demonstrate that LLMs can provide surprisingly strong zero-shot in-context clustering capabilities (<a class=\"ltx_ref\" href=\"#S3.SS1\" title=\"3.1 Zero-shot In-Context Clustering ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.1</span></a>).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\">We find attention matrices in intermediate layers show salient cluster structures. Moreover, spectral clustering using these attention matrices yields impressive performance (<a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.2</span></a>).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\">With lightweight LoRA fine-tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2021</a>)</cite> using NTP loss on generated clustering data, we find ICC significantly improves on numeric (<a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and image data (<a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), especially under heavy-tailed distributions and for images with rich semantics.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i4.p1\">\n<p class=\"ltx_p\">We show that ICC has the relatively distinct ability to do text-conditional image clustering, demonstrating flexibility beyond classical methods. For example, “cluster based on color”, or “cluster based on foreground”.\nWe believe that this ability to change the way clustering is done based on different prompts makes ICC, and this research direction, particularly compelling. Finally, we show ICC outperforms recent caption-based LLM clustering <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2024</a>)</cite> (<a class=\"ltx_ref\" href=\"#S5\" title=\"5 Text-Conditioned Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"189\" id=\"S1.F1.g1\" src=\"./assets/x1.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">In-Context Clustering</em><span class=\"ltx_text\" style=\"font-size:90%;\"> (ICC). LLMs can flexibly handle diverse modalities and perform text-conditioned clustering. We show the zero-shot clustering capability in pretrained LLMs and further strengthen it through finetuning.\n</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Classical Clustering Algorithms.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">Classical clustering methods can be classified into hierarchical, partitional, and density-based methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain et al., <a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">1999</a>; Wazarkar and Keshavamurthy, <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2018</a>)</cite>. Hierarchical methods continuously merge data points into clusters based on their similarity with others, resulting in a dendrogram of the data <cite class=\"ltx_cite ltx_citemacro_citep\">(Ward Jr, <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">1963</a>; Murtagh and Contreras, <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">2012</a>)</cite>. By contrast, partitional clustering algorithms output a single partition of the data instead of a clustering hierarchy <cite class=\"ltx_cite ltx_citemacro_citep\">(Ikotun et al., <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2023</a>)</cite>. K-means is one of the most widely used partitional clustering methods based on Euclidean distance and works well for spherical Gaussian clusters. Density-based methods can find arbitrarily shaped clusters by detecting the dense regions in the given dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Ester et al., <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">1996</a>)</cite>. Although widely used, classical methods lack the ability to do representation learning, instead relying on predefined similarity measures that make strong or often unrealistic assumptions about the data. These drawbacks motivate a more flexible clustering algorithm effective for diverse distributions.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLMs for Text Clustering.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">LLMs have demonstrated their excellent ability to understand and reason with natural language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bubeck et al., <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">2023</a>; Huang and Chang, <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">2024</a>)</cite>. Recent studies have demonstrated the effectiveness of LLMs in text clustering <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2023</a>; Viswanathan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2024</a>; Nakshatri et al., <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2023</a>; Tipirneni et al., <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">2024</a>)</cite>. Various strategies have been explored to enhance clustering performance, including LLM-generated embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2023</a>)</cite> and few-shot prompting <cite class=\"ltx_cite ltx_citemacro_citep\">(Viswanathan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2024</a>)</cite>. However, these practices are limited to text, where the success is somewhat expected, given that the input aligns closely with the pre-training data of the LLMs. In this paper, we extend LLM clustering to non-textual modalities. We find that language pretaining provides a strong foundation for clustering numeric and imagery data.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Multimodal Clustering.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Multimodal data introduces challenges in aligning heterogeneous information across modalities. Clustering can be performed jointly across modalities using a shared embedding space, or conditionally where one modality guides the clustering of another. As an example for joint multimodal clustering, <cite class=\"ltx_cite ltx_citemacro_citet\">Su et al. (<a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">2024</a>)</cite> propose Multimodal Generalized Category Discovery (Multimodal GCD) that focuses on partitioning a shared multimodal embedding space into known and novel categories. As for conditional multimodal clustering, <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2024</a>)</cite> and SSD-LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et al., <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">2025</a>)</cite> both leverage LLMs for text-conditioned image clustering by converting images to captions. <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> distills image captions into one-word labels using an LLM, which are clustered according to the given textual criteria, and the final assignment is made by prompting the LLM to match image captions to the cluster labels. SSD-LLM uses LLMs iteratively to refine and produce subpopulation structures based on image captions, and then utilizes the subpopulation structures for clustering. While the task of text-conditioned image clustering is similar to ours in <a class=\"ltx_ref\" href=\"#S5\" title=\"5 Text-Conditioned Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>, these caption-based approaches are highly constrained by the caption quality, failing to generalize when the data has complicated or nuanced relationships that the captioner is unable to capture.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"211\" id=\"S2.F2.g1\" src=\"./assets/x2.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot Clustering Accuracy on <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.F2.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-Distribution with Different Degrees of Freedom. When <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"S2.F2.m4\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math> is small, the data distribution has a heavy tail, which violates the Gaussian assumption of k-means. LLMs show impressive zero-shot clustering capabilities on heavy-tailed data.\n</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Zero-shot Clustering</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\">In this section, we show that LLMs pre-trained on large text corpus are capable of zero-shot clustering. LLMs outperform k-means on non-Gaussian data, demonstrating their potential to perform in-context clustering. We also observe that a cluster-like pattern emerges in the self-attention of pretrained LLMs and using the attention matrices for spectral clustering results in competitive performance.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Zero-shot In-Context Clustering</h3>\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Experimental Setup.</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">To understand the zero-shot clustering capabilities of different model families and model sizes, we test pre-trained Llama 3.1&amp;3.2 <cite class=\"ltx_cite ltx_citemacro_citep\">(AI@Meta, <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2024</a>)</cite>, Qwen 2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2023</a>)</cite> with different sizes, and various closed-source GPT models <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et al., <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">2023</a>)</cite> including <span class=\"ltx_text ltx_font_smallcaps\">GPT-4o</span> and <span class=\"ltx_text ltx_font_smallcaps\">GPT-4.1</span> series. We round all numbers to two decimal places and use text to represent the input numeric data as a double list where the inner list represents one data point. Our prompt is as follows:</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px1.p2\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Cluster the following data into <span class=\"ltx_text ltx_font_upright\" style=\"--ltx-fg-color:#0000FF;\">{<span class=\"ltx_text ltx_font_italic\">#clusters</span>}</span> clusters. Only output the cluster labels for each point as a list of integers. Data: <span class=\"ltx_text ltx_font_upright\" style=\"--ltx-fg-color:#0000FF;\">{<span class=\"ltx_text ltx_font_italic\">input data</span>}</span> Labels:</em></p>\n</blockquote>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data.</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We sample data from a <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-distribution to evaluate ICC under diverse conditions: When <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math> are large, it approximates the Gaussian distribution; when <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math> are small, it exhibits a heavy tail. We first sample the cluster centroids by drawing each dimension uniformly from <math alttext=\"[-10,10]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mo>−</mo><mn>10</mn></mrow><mo>,</mo><mn>10</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-10,10]</annotation></semantics></math>, and then generate data points within each cluster by sampling from a <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-distribution with the specified <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math>. For each combination of the number of clusters <math alttext=\"c\\in\\{2,3,4,5\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{2,3,4,5\\}</annotation></semantics></math>, dimensions <math alttext=\"d\\in\\{1,2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d\\in\\{1,2,3,4\\}</annotation></semantics></math>, and different degrees of freedom <math alttext=\"df\\in\\{1,1.25,1.5,1.75,2,5,100\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>1.25</mn><mo>,</mo><mn>1.5</mn><mo>,</mo><mn>1.75</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1,1.25,1.5,1.75,2,5,100\\}</annotation></semantics></math>, we generate 100 samples with length randomly drawn from <math alttext=\"[10,50]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>10</mn><mo>,</mo><mn>50</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[10,50]</annotation></semantics></math>. The size of each cluster is also random but forced to be nonempty.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results.</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\">We report zero-shot accuracy<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Since clustering is invariant to label permutation, we adopt the Hungarian Algorithm to find the optimal assignment before computing the accuracy.</span></span></span> in <a class=\"ltx_ref\" href=\"#S2.F2\" title=\"In Multimodal Clustering. ‣ 2 Related Work ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a> and include more results of different numbers of clusters and dimensions in <a class=\"ltx_ref\" href=\"#A1.F6\" title=\"In Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">6</span></a> of <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">A</span></a>. LLMs show impressive zero-shot clustering capabilities, outperforming k-means when the data has heavy tails. When <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math> is small, the Gaussian assumption of k-means is violated, leading to a significant drop in performance. <span class=\"ltx_text ltx_font_smallcaps\">gpt-4</span> and <span class=\"ltx_text ltx_font_smallcaps\">gpt-4.1</span> outperform k-means when data is heavy-tailed and high-dimensional, demonstrating the potential of applying LLMs for clustering high-dimensional non-Gaussian data.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px3.p2\">\n<p class=\"ltx_p\">The performance of LLMs is correlated with the model size and training choices. Small LLMs with 3B or 8B parameters can produce non-trivial answers when the clustering data is simple (with lower dimensions and fewer clusters, shown in <a class=\"ltx_ref\" href=\"#A1.F6\" title=\"In Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">6</span></a>). When the data becomes more complicated, these small LLMs are either unable to follow the instruction of generating the correct number of clusters or produce answers that are close to random guesses. We also observe that instruction tuning improves the overall accuracy, without which the model is unable to follow the instructions of the clustering task (<a class=\"ltx_ref\" href=\"#A1.F7\" title=\"In Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>). There is still a gap between the performance of small open-source models and GPT models, probably due to the difference in the model size and pretraining. In <a class=\"ltx_ref\" href=\"#S4\" title=\"4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>, we show that finetuning Llama models on synthetic clustering data helps close the gap.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Emergence of Clusters in Attention</h3>\n<figure class=\"ltx_figure\" id=\"S3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"327\" id=\"S3.F3.g1\" src=\"./assets/x3.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of Attention Allocation of Input Data and Generated Cluster Labels at an Intermediate Layer. The x-axis and y-axis are the ground-truth cluster labels. The left figure is for the pretrained <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8b-Instruct</span>, and the right is after fine-tuning(details in <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>). The top right curves are the average accuracy of spectral clustering using the input-input attention score matrices (top-left) across different layers, compared with the average accuracy of LLM generation.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\">To better understand the inner mechanism of ICC, we visualize the attention scores across different transformer layers. All LLMs considered here are causal transformers with multi-head self-attention. Given a textual prompt as described in <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>, the model autoregressively generates cluster labels conditioned on the input data and previous generation. At each layer, we extract the self-attention matrix <math alttext=\"A\\in\\mathbb{R}^{n\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A\\in\\mathbb{R}^{n\\times n}</annotation></semantics></math>, a lower-triangular matrix due to causality, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the total number of tokens. For multi-head attention, we use average attention scores across heads in this section.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\">To focus on input data and output cluster label tokens, we discard instruction and system prompt tokens. Since each input data point may span multiple tokens, we aggregate token-level attention scores to obtain data-level attention scores. Let <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> denote the number of input data points. From the full matrix <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>, we construct an aggregated attention matrix with the following block structure:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"A=\\begin{bmatrix}A^{II}&amp;0\\\\\nA^{OI}&amp;A^{OO}\\end{bmatrix}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd><msup><mi>A</mi><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><msup><mi>A</mi><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup></mtd><mtd><msup><mi>A</mi><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>O</mi></mrow></msup></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">A=\\begin{bmatrix}A^{II}&amp;0\\\\\nA^{OI}&amp;A^{OO}\\end{bmatrix}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Here, <math alttext=\"A^{II}\\in\\mathbb{R}^{m\\times m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>A</mi><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A^{II}\\in\\mathbb{R}^{m\\times m}</annotation></semantics></math> represents the input-input matrix capturing attention scores among input data points, <math alttext=\"A^{OI}\\in\\mathbb{R}^{m\\times m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>A</mi><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A^{OI}\\in\\mathbb{R}^{m\\times m}</annotation></semantics></math> represents the output-input matrix that reflects how generated cluster labels attend to input data, and <math alttext=\"A^{OO}\\in\\mathbb{R}^{m\\times m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mrow><msup><mi>A</mi><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>O</mi></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">A^{OO}\\in\\mathbb{R}^{m\\times m}</annotation></semantics></math> represents the output-output matrix containing attention scores among output tokens. Each input data point <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> may span multiple tokens, indexed from <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m7\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math> to <math alttext=\"e_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m8\" intent=\":literal\"><semantics><msub><mi>e</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">e_{i}</annotation></semantics></math>. We compute <math alttext=\"A^{II}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m9\" intent=\":literal\"><semantics><msup><mi>A</mi><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><annotation encoding=\"application/x-tex\">A^{II}</annotation></semantics></math> by averaging attention scores across all token pairs between <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m10\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> and <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m11\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"A^{II}_{ij}:=\\frac{1}{(e_{i}-s_{i}+1)(e_{j}-s_{j}+1)}\\sum_{p=s_{i}}^{e_{i}}\\sum_{q=s_{j}}^{e_{j}}A_{pq}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>A</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msubsup><mo>:=</mo><mrow><mfrac><mn>1</mn><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>−</mo><msub><mi>s</mi><mi>i</mi></msub></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>e</mi><mi>j</mi></msub><mo>−</mo><msub><mi>s</mi><mi>j</mi></msub></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>p</mi><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub></mrow><msub><mi>e</mi><mi>i</mi></msub></munderover><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>q</mi><mo>=</mo><msub><mi>s</mi><mi>j</mi></msub></mrow><msub><mi>e</mi><mi>j</mi></msub></munderover><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>q</mi></mrow></msub></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">A^{II}_{ij}:=\\frac{1}{(e_{i}-s_{i}+1)(e_{j}-s_{j}+1)}\\sum_{p=s_{i}}^{e_{i}}\\sum_{q=s_{j}}^{e_{j}}A_{pq}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Each output cluster label is represented by a single token, indexed as <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m12\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> for the label of <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m13\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math>. The remaining attention blocks are defined as:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E3\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"A^{OI}_{ij}:=\\frac{1}{e_{j}-s_{j}+1}\\sum_{p=s_{j}}^{e_{j}}A_{t_{i}p},\\;\\;\\;\\;A^{OO}_{ij}:=A_{t_{i}t_{j}}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>A</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msubsup><mo>:=</mo><mrow><mfrac><mn>1</mn><mrow><mrow><msub><mi>e</mi><mi>j</mi></msub><mo>−</mo><msub><mi>s</mi><mi>j</mi></msub></mrow><mo>+</mo><mn>1</mn></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>p</mi><mo>=</mo><msub><mi>s</mi><mi>j</mi></msub></mrow><msub><mi>e</mi><mi>j</mi></msub></munderover><msub><mi>A</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>p</mi></mrow></msub></mrow></mrow></mrow><mo rspace=\"1.277em\">,</mo><mrow><msubsup><mi>A</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>O</mi></mrow></msubsup><mo>:=</mo><msub><mi>A</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>t</mi><mi>j</mi></msub></mrow></msub></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">A^{OI}_{ij}:=\\frac{1}{e_{j}-s_{j}+1}\\sum_{p=s_{j}}^{e_{j}}A_{t_{i}p},\\;\\;\\;\\;A^{OO}_{ij}:=A_{t_{i}t_{j}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"#S3.F3\" title=\"In 3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> visualizes this block matrix , with <math alttext=\"A^{II}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m14\" intent=\":literal\"><semantics><msup><mi>A</mi><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><annotation encoding=\"application/x-tex\">A^{II}</annotation></semantics></math> in the top-left, <math alttext=\"A^{OI}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m15\" intent=\":literal\"><semantics><msup><mi>A</mi><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><annotation encoding=\"application/x-tex\">A^{OI}</annotation></semantics></math> in the bottom-left, and <math alttext=\"A^{OO}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m16\" intent=\":literal\"><semantics><msup><mi>A</mi><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>O</mi></mrow></msup><annotation encoding=\"application/x-tex\">A^{OO}</annotation></semantics></math> in the bottom-right. Here, we take one clustering example generated from Gaussian distribution with two clusters. We observe that <em class=\"ltx_emph ltx_font_italic\">attention matrices in intermediate layers show block structures that align with cluster identities</em>. The transformer assigns higher attention scores to similar data within the same cluster that has been seen in the past. We provide more examples across different layers in <a class=\"ltx_ref\" href=\"#A2.SS1\" title=\"B.1 Attention of Different Layers and Attention Heads ‣ Appendix B Emergence of Clusters in Attention ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. This cluster pattern is consistent and salient in most middle layers. In contrast, the final layer typically shows a vertical-slash pattern, as also observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et al. (<a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">2024</a>)</cite>. We also observe that most attention heads show similar cluster patterns in <a class=\"ltx_ref\" href=\"#A2.F10\" title=\"In B.1 Attention of Different Layers and Attention Heads ‣ Appendix B Emergence of Clusters in Attention ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p3\">\n<p class=\"ltx_p\">Although the pretrained model (left in <a class=\"ltx_ref\" href=\"#S3.F3\" title=\"In 3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>) has a clear cluster pattern in the input-input matrix, clusters are not observed in attention related to outputs. This suggests that the model learns similarity among input data during pretraining, but is not optimized for generating cluster labels as explicit clustering tasks are very likely rare in pretraining.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Llama 3 models are claimed to be trained on ”15T tokens that were all collected from publicly available sources”<cite class=\"ltx_cite ltx_citemacro_citep\">(AI@Meta, <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2024</a>)</cite>, but details are not disclosed.</span></span></span> After fine-tuning on ICC data, the cluster structure in the input-input matrix becomes stronger, and similar clusters also emerge in output-input and output-output matrices.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p4\">\n<p class=\"ltx_p\">To quantify how well the attention captures the similarity among the input data, we use these input-input attention score matrices for spectral clustering <cite class=\"ltx_cite ltx_citemacro_citep\">(Ng et al., <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">2001</a>; von Luxburg, <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">2007</a>)</cite> (more details and results are in <a class=\"ltx_ref\" href=\"#A2.SS2\" title=\"B.2 Spectral Clustering ‣ Appendix B Emergence of Clusters in Attention ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). Although the zero-shot accuracy of prompting pretrained <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8b-Instruct</span> to cluster is 74%, the spectral clustering using attention with the optimal choice of layers achieves 85% before fine-tuning. This surprising result suggests that attention of LLMs already encodes rich structural information beyond what is directly generated. In addition to prompting the LLM for generation, directly using attention can be an alternative to leverage pretrained LLM for in-context clustering in zero shot.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Learning Clustering with Next Token Prediction</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\">While pretrained LLMs show promising zero-shot clustering capabilities, small open-source models lag behind classical methods and proprietary LLMs. In this section, we show that the clusterng capabilities of pretrained LLMs can be further enhanced through LoRA fine-tuning using NTP loss. Inspired by the meta learning literature <cite class=\"ltx_cite ltx_citemacro_citep\">(Ravi and Larochelle, <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2017</a>; Min et al., <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">2022</a>; Najdenkoska et al., <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2023</a>)</cite>, we construct various clustering episodes to make pretrained (multimodal) LLM learn to cluster in context and then test it on unseen classes. We experiment on both numeric and image data.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Numeric Data Clustering</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Experiment Setup.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We follow the standard Supervised Fine-Tuning (SFT) procedure to fine-tune pre-trained Llama models with different sizes (<span class=\"ltx_text ltx_font_smallcaps\">Llama-3.2-1B-Instruct</span>, <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.2-3B-Instruct</span>, <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8B-Instruct</span>) using NTP loss. Similarly to how we construct the clustering data in <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>, we construct the data by randomly sampling data from a <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-distribution with different degrees of freedom <math alttext=\"df\\in\\{1,2,5,100\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1,2,5,100\\}</annotation></semantics></math>, the number of clusters <math alttext=\"c\\in\\{2,3,4,5\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{2,3,4,5\\}</annotation></semantics></math>, and dimensions of each point <math alttext=\"d\\in\\{1,2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d\\in\\{1,2,3,4\\}</annotation></semantics></math>. We generate around 100k input-label pairs, where each sample has a length randomly drawn from <math alttext=\"[10,50]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>10</mn><mo>,</mo><mn>50</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[10,50]</annotation></semantics></math>. We use LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2021</a>)</cite> to fine-tune the pre-trained Llama model for one epoch with an effective batch size of 32 and a learning rate of 5e-4.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Effect of Finetuning on <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-Distributed Data with Different Degrees of Freedom. Input <math alttext=\"dim=3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">dim=3</annotation></semantics></math> and number of clusters <math alttext=\"c=3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">c=3</annotation></semantics></math>. We report average accuracy (%) and one standard error.\n</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:94.5pt;vertical-align:-45.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-33.5pt,7.3pt) scale(0.866190321305276,0.866190321305276) ;\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"--ltx-bg-color:#666666;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#666666;\">df=1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"--ltx-bg-color:#666666;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#666666;\">df=2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"--ltx-bg-color:#666666;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#666666;\">df=5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"--ltx-bg-color:#666666;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#666666;\">df=100</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">kmeans</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">67.95</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.46</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">75.43</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.52</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">85.57</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.20</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">87.55</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.32</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">89.05</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.27</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">95.29</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">97.08</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.82</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">gpt-4o</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">77.75</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">80.60</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.20</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">86.99</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">87.08</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.26</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">89.56</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">93.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">96.25</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.86</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">(a) Llama-3.2-1B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">45.40</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.64</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">47.09</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.71</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.77</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.63</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.54</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">45.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.64</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">47.36</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m27\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.77</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">(a) + finetune</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">82.66<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m28\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.30</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">86.45<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m29\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.23</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">91.10<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m30\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.90</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">89.46<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m31\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.18</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">88.76<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m32\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.20</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">95.09<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m33\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.93</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">96.28<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m34\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.88</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">(b) Llama-3.2-3B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m35\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.67</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.09</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.72</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.35</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m37\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.62</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.85</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m38\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.05</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m39\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.82</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m40\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.72</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">46.35</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m41\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.86</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">(b) + finetune</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">88.54<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m42\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.03</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">91.05<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m43\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.00</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">94.31<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m44\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.77</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">93.33<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m45\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.90</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">94.51<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m46\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.90</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">98.08<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m47\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.49</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">97.64<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m48\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.78</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">(c) Llama-3.1-8B-Instruct</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">55.29</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m49\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.34</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">55.38</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m50\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">59.80</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m51\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.57</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">61.09</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m52\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">61.21</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m53\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.47</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">64.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m54\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">64.42</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m55\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.73</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">(c) + finetune</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">90.66<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m56\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">92.20<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m57\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">95.25<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m58\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.54</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">94.57<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m59\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.86</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">95.44<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m60\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.71</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">98.90<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m61\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">97.85<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m62\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.76</span>\n</td>\n</tr>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We use the test data in <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> (<math alttext=\"df\\in\\{1,1.25,1.5,1.75,2,5,100\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>1.25</mn><mo>,</mo><mn>1.5</mn><mo>,</mo><mn>1.75</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1,1.25,1.5,1.75,2,5,100\\}</annotation></semantics></math>) with <math alttext=\"df\\in\\{1.25,1.5,1.75\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1.25</mn><mo>,</mo><mn>1.5</mn><mo>,</mo><mn>1.75</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1.25,1.5,1.75\\}</annotation></semantics></math> to test the robustness of the fine-tuned model. During fine-tuning, the LLM exhibits a two-phase learning pattern where it first learns the correct format and then gradually develops a clustering mechanism. Initially, the LLM (especially smaller models with 1B or 3B parameters) struggles with instruction following and produces repetitive outputs. These poorly formatted predictions are heavily penalized by the NTP loss. As training progresses, the model learns to effectively differentiate among cluster labels based on the input data and achieves a high accuracy.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px2.p2\">\n<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"#S4.T1\" title=\"In Experiment Setup. ‣ 4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>, all fine-tuned models show superior performance compared to k-means and <span class=\"ltx_text ltx_font_smallcaps\">gpt-4o</span> (the complete results are in <a class=\"ltx_ref\" href=\"#A1.F7\" title=\"In Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a> of <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">A</span></a>). Although these LLMs are fine-tuned on <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-distributed data with <math alttext=\"df\\in\\{1,2,5,100\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1,2,5,100\\}</annotation></semantics></math>, they show generalization capability to more <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math> and different distributions. All fine-tuned models perform consistently well on <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-distributed data with new <math alttext=\"df\\in\\{1.25,1.5,1.75\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1.25</mn><mo>,</mo><mn>1.5</mn><mo>,</mo><mn>1.75</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1.25,1.5,1.75\\}</annotation></semantics></math>. While these models are fine-tuned on a symmetric distribution, they also significantly outperform k-means and <span class=\"ltx_text ltx_font_smallcaps\">gpt-4o</span> on a skewed distribution (lognormal) as shown in <a class=\"ltx_ref\" href=\"#A1.T4\" title=\"In Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a> in <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">A</span></a>. We also observe that models with higher accuracy tend to be more invariant to\npermutation in input data, and data augmentation is effective in improving consistency, as shown in <a class=\"ltx_ref\" href=\"#A1.T5\" title=\"In Appendix A Additional Results of Numeric Data Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px2.p3\">\n<p class=\"ltx_p\">We study the effect of fine-tuning by analyzing the attention pattern as visualized in <a class=\"ltx_ref\" href=\"#S3.F3\" title=\"In 3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>. The cluster pattern in the attention score matrix of the input data is significantly more salient after fine-tuning, indicating that the model learns a better similarity function among the data through its attention mechanism during fine-tuning. The accuracy of spectral clustering using attention scores increases as well. More visualization and results are in <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Emergence of Clusters in Attention ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Image Clustering</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\">Here, we extend ICC to multimodal LLMs and present results of image clustering. Given a set of images, the goal is to cluster based on their semantic meanings. By projecting image embeddings obtained from a pretrained visual encoder, LLMs can learn to produce meaningful groupings that outperform an LLM-based method that relies on image captions.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Model.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We use <span class=\"ltx_text ltx_font_typewriter\">llava-interleave-qwen-7b-hf</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2024a</a>)</cite>, a multimodal LLM pretrained with multi-image inputs, as our base model. In the LLaVA framework, each image is segmented into 729 patches encoded by a pre-trained ViT, namely the SigLIP’s visual encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhai et al., <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">2023</a>)</cite>, then projected through an MLP layer into the embedding space of the base LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et al., <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2023</a>)</cite>. While such a high-granularity representation may benefit downstream tasks like object detection, we argue that it is not optimal for clustering tasks. Clustering typically involves a large number of images; thus, using hundreds of tokens per image can quickly exceed context length limitations and significantly increase computational costs during fine-tuning. Additionally, high granularity might be unnecessary for some clustering tasks that only rely on global features.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"282\" id=\"S4.F4.g1\" src=\"./assets/x4.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Left: Multimodal LLM Architecture with Average Pooling for Image Features. Right: Qualitative Comparison of Models on Image Clustering — ICC outperforms k-means when the data has rich semantic information.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p2\">\n<p class=\"ltx_p\">To address these efficiency concerns, we implement average pooling after the projection layer to reduce per-image token lengths, as illustrated in <a class=\"ltx_ref\" href=\"#S4.F4\" title=\"In Model. ‣ 4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a> (left). Each input image is divided into patches, which are preprocessed and flattened (omitted from the figure for clarity), and then encoded by a vision transformer. We reshape the flattened image features back to 2D and then apply average pooling to reduce dimensionality. The pooled features are then flattened, projected into the LLM’s embedding space, and concatenated with text token embeddings. We experiment with various pooling kernel sizes in <a class=\"ltx_ref\" href=\"#A3.SS1\" title=\"C.1 Pooling ‣ Appendix C Additional Experiment Details and Results of Image Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a>. No padding is applied and the stride is the same as the kernel width.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We collect images from ImageNet21k <cite class=\"ltx_cite ltx_citemacro_citep\">(Ridnik et al., <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">2021</a>)</cite> where images sharing the same label are considered part of the same cluster. We reserve the 384 image classes covered in ImageNet-with-Attributes <cite class=\"ltx_cite ltx_citemacro_citep\">(Russakovsky and Fei-Fei, <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">2010</a>)</cite> for testing and the remaining 18K classes for training. For training, we construct 192K image clustering episodes of various numbers of clusters <math alttext=\"c\\in\\{2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{2,3,4\\}</annotation></semantics></math>, with random length <math alttext=\"l\\in[10,30]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in[10,30]</annotation></semantics></math> and random cluster proportion. For testing, we use the reserved test classes to construct 100 clustering episodes for each number of clusters. To test generalization on out-of-domain data, we include Plant Disease and EuroSAT datasets from the Cross-Domain Few-Shot Learning (CD-FSL) Benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2020</a>)</cite> with details in <a class=\"ltx_ref\" href=\"#A3.SS2\" title=\"C.2 Out-of-Domain Image datasets ‣ Appendix C Additional Experiment Details and Results of Image Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.2</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Experiment Setup.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Similarly to previous numerical experiments, we use LoRA to fine-tune the LLM with NTP loss. The visual encoder and projection layer are frozen during training. We fine-tune for one epoch with an effective batch size of 32 and a learning rate of 5e-4.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px4.p1\">\n<p class=\"ltx_p\">To ensure a fair comparison, we use average-pooled image features from the vision encoder of the base model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2024a</a>)</cite> as the inputs to k-means. We also compare ICC against <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2024</a>)</cite>, a recent LLM-based image clustering method. We use the same model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2024a</a>)</cite> to generate image captions for <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> then use <span class=\"ltx_text ltx_font_smallcaps\">gpt-3.5-turbo</span> to distill and cluster the captions according to the given number of clusters and the clustering condition. Although converting images to short captions facilitates clustering via LLMs, <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> experiences information loss during the captioning and summarization stage, limiting its performance on challenging data.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Image Clustering Accuracy (%) with Standard Error. <span class=\"ltx_text ltx_font_smallcaps\">ICC(gpt-4o)</span> is zero-shot ICC using gpt-4o and the shaded rows represent models finetuned on ImageNet data with numbers of clusters <math alttext=\"c\\in\\{2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{2,3,4\\}</annotation></semantics></math>, where <span class=\"ltx_text ltx_font_smallcaps\">Small, Medium, Large</span> refer to the per-image token length in <a class=\"ltx_ref\" href=\"#A3.SS1\" title=\"C.1 Pooling ‣ Appendix C Additional Experiment Details and Results of Image Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a>. Our finetuned models can generalize to unseen <math alttext=\"c=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">c=5</annotation></semantics></math> and other datasets that deviate from ImageNet. </span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:293.3pt;height:47.4pt;vertical-align:-22.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-154.2pt,24.9pt) scale(0.487372066724932,0.487372066724932) ;\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ImageNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Plant</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">EuroSAT</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">number of clusters</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">c=2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">c=3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">c=4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">c=5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">c=2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">c=2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">k-means</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">89.43</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.57</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">82.09</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">79.07</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">77.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.70<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.40</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">85.52<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.43</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">IC|TC</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">(</span>Kwon et��al.<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2024</a><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">90.20</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.54</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">78.86</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.41</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">76.49</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">73.99</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.58</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">67.40</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.23</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">72.97</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.42</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ICC (gpt-4o)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">82.46</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.40</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">80.25</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.73</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">75.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.73</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">78.08</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">84.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.25</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">79.08</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.41</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Small)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">96.81<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m23\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.83</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">91.94<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m24\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.03</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">89.83<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m25\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.19</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">82.08<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m26\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.01</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">73.03<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m27\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.58</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">78.17<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m28\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.53</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Medium)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">98.26</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m29\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.71</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">95.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m30\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.90</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">91.62</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m31\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.16</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">84.92</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m32\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">82.28<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m33\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.85</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">78.64<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m34\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.61</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Large)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">99.12<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m35\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.41</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">91.95</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m36\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.96</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">92.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m37\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">84.96<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m38\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">85.09</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m39\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.80</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">77.35<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m40\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.70</span></span></td>\n</tr>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px5.p1\">\n<p class=\"ltx_p\">The performance of different models is summarized in <a class=\"ltx_ref\" href=\"#S4.T2\" title=\"In Baselines. ‣ 4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a>. While zero-shot ICC using <span class=\"ltx_text ltx_font_smallcaps\">gpt-4o</span> achieves competitive performance, it is less effective than on text-encoded data. This is likely due to the current limitations of multimodal LLMs on long sequences of complex images. Our proposed finetuning method significantly closes this gap, achieving strong performance across all datasets. Despite being only fine-tuned on ImageNet data with the number of clusters less than five, our model can generalize to within-domain data of five clusters and out-of-domain data including plant leaves and satellite images.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px5.p2\">\n<p class=\"ltx_p\">With good image features, k-means is effective on datasets with limited semantic complexity, such as Plant Disease and EuroSAT. However, it loses its competence on ImageNet, where images often depict complex scenes involving multiple objects. The caption-based method, <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span>, performs poorly on Plant Disease or EuroSAT, as its captioning model lacks domain-specific knowledge. This observation highlights a key weakness of caption-based clustering: its dependence on accurate and relevant captions limits its applicability to novel domains. Our model avoids these pitfalls, demonstrating superior flexibility and performance across both general and specialized domains.</p>\n</div>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Text-Conditioned Clustering</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\">While the experiments in the previous section assume a single, fixed clustering objective, real-world data admits multiple plausible clusterings depending on the objective. For example, the same set of animal images can be clustered by visual properties like colors (orange vs. white) or semantic categories like species (dog vs. cat), as shown in <a class=\"ltx_ref\" href=\"#S5.F5\" title=\"In Data. ‣ 5 Text-Conditioned Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>. When the clustering condition changes, classical methods typically require retraining or re-engineering features. In contrast, LLMs can easily adapt to new conditions through prompting thanks to their powerful contextual understanding capability. In this section, we perform text-conditioned image clustering by fine-tuning multimodal LLMs with the NTP loss.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We construct conditional clustering using ImageNet-with-Attributes <cite class=\"ltx_cite ltx_citemacro_citep\">(Russakovsky and Fei-Fei, <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">2010</a>)</cite>, which includes 384 classes with 4 categories of attributes (<span class=\"ltx_text ltx_font_smallcaps\">color</span>, <span class=\"ltx_text ltx_font_smallcaps\">shape</span>, <span class=\"ltx_text ltx_font_smallcaps\">pattern</span>, <span class=\"ltx_text ltx_font_smallcaps\">texture</span>). We split the data into 80% training classes and 20% testing classes. We treat the category name as the clustering condition that will be specified in the prompt and use the attribute value as cluster labels. In addition, we include an <span class=\"ltx_text ltx_font_smallcaps\">object</span> category that is similar to <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, where we use the class name of the images as cluster labels. Images with ambiguous annotations are filtered out. For training, we construct around 280K image conditional clustering episodes of various numbers of clusters <math alttext=\"c\\in\\{2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{2,3,4\\}</annotation></semantics></math>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The pattern category only has two available values, so we don’t have <math alttext=\"c\\in\\{2,3\\}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{2,3\\}</annotation></semantics></math> for this category.</span></span></span> with random length <math alttext=\"l\\in[10,30]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in[10,30]</annotation></semantics></math> and random cluster proportion.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"282\" id=\"S5.F5.g1\" src=\"./assets/x5.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">LLMs are able to produce different clusterings according to the condition in the prompt.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px1.p2\">\n<p class=\"ltx_p\">To test the performance of the model on different conditions, we use the reserved test classes of ImageNet-with-Attributes and also include the Stanford 40 Action dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2011</a>)</cite> with annotations on the <span class=\"ltx_text ltx_font_smallcaps\">location</span> of the scene, the <span class=\"ltx_text ltx_font_smallcaps\">action</span> and <span class=\"ltx_text ltx_font_smallcaps\">mood</span> of the people in the image provided by <cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et al., <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2024</a>)</cite>. For each dataset and clustering condition, we sample 100 clustering data from two random classes of each attribute category, with random size <math alttext=\"l\\in[10,30]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">l\\in[10,30]</annotation></semantics></math> and random cluster proportion.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Experiment Setup.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Following the SFT procedure in <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, we use LoRA to fine-tune \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">llava-interleave-qwen-7b-hf</span> with different pooling ratios. We keep the visual encoder and projection layer frozen during training. We use NTP loss to fine-tune for one epoch with an effective batch size of 32 and a learning rate of 5e-4.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">We test both unconditional and conditional clustering methods. K-means is a unconditional baseline as it does not allow injecting clustering criteria. For conditional clustering methods, we test <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> explicitly specifying conditions in the prompts for all the summarization and clustering stages, with <span class=\"ltx_text ltx_font_smallcaps\">gpt-3.5-turbo</span> as the LLM to save costs.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Conditional Image Clustering Accuracy (%) with Standard Error. Here, <span class=\"ltx_text ltx_font_smallcaps\">ICC (Medium:<a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>)</span> represents the model finetuned on unconditional image clustering data in <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, while others use conditional image clustering data in <a class=\"ltx_ref\" href=\"#S5\" title=\"5 Text-Conditioned Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>. Our method outperforms all baselines on ImageNet and Stanford 40 Action. <span class=\"ltx_text ltx_font_smallcaps\">Small, Median, Large</span> refer to the per-image token length in <a class=\"ltx_ref\" href=\"#A3.SS1\" title=\"C.1 Pooling ‣ Appendix C Additional Experiment Details and Results of Image Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.\n</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:345.0pt;height:65pt;vertical-align:-31.4pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-182.2pt,34.3pt) scale(0.486261316422649,0.486261316422649) ;\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ImageNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Stanford 40 Action</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">object</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">color</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">pattern</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">shape</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">texture</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">action</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">mood</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">location</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Unconditional Methods</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">k-means</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">89.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">66.40</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.16</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">62.36</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.98</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">75.76</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.78</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">78.53</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.65</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">79.90</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">70.93</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.43</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">78.11</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.50</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Conditional Methods</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">IC|TC</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">(</span>Kwon et al.<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2024</a><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">91.93</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.38</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">69.70</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">76.12</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.53</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">70.15</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.34</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">68.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.34</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">93.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.25</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">75.65</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">75.49</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.64</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ICC(gpt-4o)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">67.58</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.30</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">66.36</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">65.61</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">70.15</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.72</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">73.54</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.54</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">80.59</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.28</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">68.61</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.61</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">67.75</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">1.33</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Small)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">98.25<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m25\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.71</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">76.31<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m26\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.38</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">85.50<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m27\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.78</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">81.75<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m28\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.69</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">82.82<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m29\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.62</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">89.60<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m30\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.52</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">67.89<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m31\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.27</span></span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">83.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m32\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.53</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Medium)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">98.64<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m33\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">0.58</span></span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">81.02</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m34\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.31</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">93.28</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m35\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.56</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">83.02</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m36\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.69</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">86.04</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m37\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.52</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">95.98</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m38\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">76.77</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m39\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">77.18<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m40\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.67</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Medium:<a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">98.88</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m41\" intent=\":literal\" style=\"--ltx-bg-color:#E6E6E6;\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.500em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.55</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">71.39<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m42\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.31</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">65.04<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m43\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.01</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">72.72<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m44\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.37</span></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">83.04<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m45\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.55</span></span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">96.47<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m46\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.95</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">78.46<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m47\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.46</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">86.19<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m48\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.53</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">ICC (Large)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">99.52<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m49\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.22</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">84.29<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m50\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.26</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">94.43<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m51\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">0.40</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">83.72<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m52\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.71</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">87.27<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m53\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;--ltx-bg-color:#E6E6E6;\">1.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">94.14<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m54\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.26</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">73.42<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m55\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.47</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-bg-color:#E6E6E6;\">81.72<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m56\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" mathsize=\"0.560em\" style=\"--ltx-bg-color:#E6E6E6;\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:56%;\">1.62</span></span></td>\n</tr>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">The quantitative evaluation of different models is summarized in <a class=\"ltx_ref\" href=\"#S5.T3\" title=\"In Baselines. ‣ 5 Text-Conditioned Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> and qualitative examples are shown in <a class=\"ltx_ref\" href=\"#A4\" title=\"Appendix D Additional Results for Conditional Image Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">D</span></a>. Similar to results in <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, zero-shot performance of <span class=\"ltx_text ltx_font_smallcaps\">gpt-4o</span> is promising but ultimately falls short of our finetuned approach. Our finetuned models outperform all baselines on ImageNet and Stanford 40 Action. In general, our method with higher per-image token lengths performs better in this conditional clustering task. Unlike experiments in <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a> where the difference between different granularity is small, this task requires more fine-grained information and thus using more tokens to represent images is preferred. K-means and caption-based <span class=\"ltx_text ltx_font_smallcaps\">IC|TC</span> often fail to capture such details, particularly for attributes like <span class=\"ltx_text ltx_font_smallcaps\">color</span>, <span class=\"ltx_text ltx_font_smallcaps\">shape</span>, and <span class=\"ltx_text ltx_font_smallcaps\">pattern</span>, where our method is more than 10% higher than all baselines.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px4.p2\">\n<p class=\"ltx_p\">Our method generalizes to unseen data and conditions from the Stanford 40 Action dataset. Surprisingly, our model trained solely on clustering objects in ImageNet, achieves the highest accuracy. This suggests that the inductive bias from image-based clustering and the visual-language pretraining enables the model to infer clustering objectives implicitly. We notice that the finetuned models are less competitive on <span class=\"ltx_text ltx_font_smallcaps\">mood</span> and <span class=\"ltx_text ltx_font_smallcaps\">location</span>. We attribute this to the training data (ImageNet-with-Attributes), which emphasizes prominent foreground objects (typically non-human), causing the model to overlook cues from human facial expressions or the background. Scaling our approach to more diverse datasets and clustering conditions could mitigate this bias and further strengthen the model’s generalization capabilities.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n<p class=\"ltx_p\">In-Context Clustering (ICC) generalizes in-context learning to the unsupervised setting. ICC does not make restrictive similarity assumptions on the input data and enables flexible, text-conditioned clustering objectives through prompting. We find that large LLMs provide strong zero-shot performance on text-encoded numeric data, and further show that this capability can be significantly strengthened for smaller and multimodal models through simple fine-tuning using the NTP loss. Multimodal LLMs enhanced by our proposed finetuning achieve impressive performance on image clustering and text-conditioned image clustering. These findings highlight that LLMs can be effectively used to solve clustering tasks that involve complex semantics and contextual understanding.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.p2\">\n<p class=\"ltx_p\">While we demonstrate ICC’s effectiveness and flexibility, ICC is complementary to classical clustering methods, and has certain limitations that would be exciting to address in future work. For application to larger datasets, it would be particularly promising to scale ICC to longer contexts, which can be computationally expensive for LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2024b</a>; Liu et al., <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2024</a>)</cite>. Our experiments with average pooling for image features show promise in reducing token usage, and recent advances such as dynamic context selection <cite class=\"ltx_cite ltx_citemacro_citep\">(Hao et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2025</a>)</cite> and token pruning <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2024</a>; Jianjian et al., <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2024</a>)</cite> can further address the long-context challenge in future work. Moreover, while visualizing attention provides some insights into the way ICC performs clustering, a theoretical understanding of ICC would be particularly valuable. Emergence of clusters in self-attention have been theoretically studied by <cite class=\"ltx_cite ltx_citemacro_citet\">Geshkovski et al. (<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2023</a>)</cite>, but under a simplified setting (without multi-head attention, feed-forward layers, and layer normalization). Developing theoretical frameworks to explain and exploit these attention structures remains an important open direction.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgments</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">We thank Shikai Qiu, Nate Gruver, Zhe Zeng, Lily Li, and Bayan Bruss for helpful discussions. We are grateful for support from the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) with a grant funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research. (No. RS-2024-00469482 &amp; RS-2024-00509279),\nNSF CAREER IIS-2145492, NSF CDS&amp;E-MSS 2134216,\nNSF HDR-2118310, BigHat Biosciences, and Capital One.\nWe are also thankful for NYU IT High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Achiam et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nOpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, et al.\n\n</span>\n<span class=\"ltx_bibblock\">GPT-4 Technical Report.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2303.08774</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">AI@Meta [2024]</span>\n<span class=\"ltx_bibblock\">\nAI@Meta.\n\n</span>\n<span class=\"ltx_bibblock\">Llama 3 Model Card.\n\n</span>\n<span class=\"ltx_bibblock\">2024.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md\" title=\"\">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Alwassel et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nHumam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran.\n\n</span>\n<span class=\"ltx_bibblock\">Self-Supervised Learning by Cross-Modal Audio-Video Clustering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bai et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.16609</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brown et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\n\n</span>\n<span class=\"ltx_bibblock\">Language Models are Few-Shot Learners.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bubeck et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Sparks of artificial general intelligence: Early experiments with gpt-4.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2303.12712</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chang et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nJianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan.\n\n</span>\n<span class=\"ltx_bibblock\">Deep Adaptive Image Clustering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Computer Vision (ICCV)</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nLiang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang.\n\n</span>\n<span class=\"ltx_bibblock\">An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">European Conference on Computer Vision (ECCV)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ester et al. [1996]</span>\n<span class=\"ltx_bibblock\">\nMartin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu.\n\n</span>\n<span class=\"ltx_bibblock\">A density-based algorithm for discovering clusters in large spatial databases with noise.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Knowledge Discovery and Data Mining</em>, 1996.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Garg et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant.\n\n</span>\n<span class=\"ltx_bibblock\">What Can Transformers Learn In-Context? A Case Study of Simple Function Classes.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Geshkovski et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nBorjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet.\n\n</span>\n<span class=\"ltx_bibblock\">The emergence of clusters in self-attention dynamics.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gruver et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nNate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson.\n\n</span>\n<span class=\"ltx_bibblock\">Large Language Models Are Zero Shot Time Series Forecasters.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guérin and Boots [2018]</span>\n<span class=\"ltx_bibblock\">\nJoris Guérin and Byron Boots.\n\n</span>\n<span class=\"ltx_bibblock\">Improving image clustering with multiple pretrained cnn feature extractors.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1807.07760</em>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guo et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nYunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana Rosing, and Rogerio Feris.\n\n</span>\n<span class=\"ltx_bibblock\">A broader study of cross-domain few-shot learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">European Conference on Computer Vision (ECCV)</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hao et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nJitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, and Sheng Guo.\n\n</span>\n<span class=\"ltx_bibblock\">OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations (ICLR)</em>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Helber et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.\n\n</span>\n<span class=\"ltx_bibblock\">EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\n\n</span>\n<span class=\"ltx_bibblock\">Lora: Low-rank adaptation of large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2106.09685</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Huang and Chang [2023]</span>\n<span class=\"ltx_bibblock\">\nJie Huang and Kevin Chen-Chuan Chang.\n\n</span>\n<span class=\"ltx_bibblock\">Towards reasoning in large language models: A survey.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Findings of the Association for Computational Linguistics</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ikotun et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nAbiodun M. Ikotun, Absalom E. Ezugwu, Laith Abualigah, Belal Abuhaija, and Jia Heming.\n\n</span>\n<span class=\"ltx_bibblock\">K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Information Sciences</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jain et al. [1999]</span>\n<span class=\"ltx_bibblock\">\nA. K. Jain, M. N. Murty, and P. J. Flynn.\n\n</span>\n<span class=\"ltx_bibblock\">Data clustering: a review.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">ACM Comput. Surv.</em>, 1999.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nHuiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\n\n</span>\n<span class=\"ltx_bibblock\">MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jianjian et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nCao Jianjian, Ye Peng, Li Shengze, Yu Chong, Tang Yansong, Lu Jiwen, and Chen Tao.\n\n</span>\n<span class=\"ltx_bibblock\">MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kwon et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nSehyun Kwon, Jaeseung Park, Minkyu Kim, Jaewoong Cho, Ernest K. Ryu, and Kangwook Lee.\n\n</span>\n<span class=\"ltx_bibblock\">Image clustering conditioned on text criteria.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations (ICLR)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2024a]</span>\n<span class=\"ltx_bibblock\">\nFeng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li.\n\n</span>\n<span class=\"ltx_bibblock\">Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2407.07895</em>, 2024a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2024b]</span>\n<span class=\"ltx_bibblock\">\nTianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen.\n\n</span>\n<span class=\"ltx_bibblock\">Long-context llms struggle with long in-context learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2404.02060</em>, 2024b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n\n</span>\n<span class=\"ltx_bibblock\">Lost in the middle: How language models use long contexts.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Transactions of the Association for Computational Linguistics (ACL)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2003]</span>\n<span class=\"ltx_bibblock\">\nTao Liu, Shengping Liu, Zheng Chen, and Wei-Ying Ma.\n\n</span>\n<span class=\"ltx_bibblock\">An evaluation on feature selection for text clustering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning (ICML)</em>, 2003.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Luo et al. [2025]</span>\n<span class=\"ltx_bibblock\">\nYulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, Jiaming Liu, and Shanghang Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Llm as dataset analyst: Subpopulation structure discovery with large language model.\n\n</span>\n<span class=\"ltx_bibblock\">2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Meinedo and Neto [2003]</span>\n<span class=\"ltx_bibblock\">\nH. Meinedo and J. Neto.\n\n</span>\n<span class=\"ltx_bibblock\">Audio segmentation, classification and clustering in a broadcast news task.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2003.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Min et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.\n\n</span>\n<span class=\"ltx_bibblock\">MetaICL: Learning to learn in context.\n\n</span>\n<span class=\"ltx_bibblock\">2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mohanty et al. [2016]</span>\n<span class=\"ltx_bibblock\">\nSharada P. Mohanty, David P. Hughes, and Marcel Salathé.\n\n</span>\n<span class=\"ltx_bibblock\">Using Deep Learning for Image-Based Plant Disease Detection.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Frontiers in Plant Science</em>, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Murtagh and Contreras [2012]</span>\n<span class=\"ltx_bibblock\">\nFionn Murtagh and Pedro Contreras.\n\n</span>\n<span class=\"ltx_bibblock\">Algorithms for hierarchical clustering: an overview.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 2012.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Najdenkoska et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nIvona Najdenkoska, Xiantong Zhen, and Marcel Worring.\n\n</span>\n<span class=\"ltx_bibblock\">Meta learning to bridge vision and language models for multimodal few-shot learning.\n\n</span>\n<span class=\"ltx_bibblock\">2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nakshatri et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nNishanth Nakshatri, Siyi Liu, Sihao Chen, Dan Roth, Dan Goldwasser, and Daniel Hopkins.\n\n</span>\n<span class=\"ltx_bibblock\">Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Findings of the Association for Computational Linguistics: EMNLP</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ng et al. [2001]</span>\n<span class=\"ltx_bibblock\">\nAndrew Ng, Michael Jordan, and Yair Weiss.\n\n</span>\n<span class=\"ltx_bibblock\">On Spectral Clustering: Analysis and an algorithm.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2001.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ravi and Larochelle [2017]</span>\n<span class=\"ltx_bibblock\">\nSachin Ravi and Hugo Larochelle.\n\n</span>\n<span class=\"ltx_bibblock\">Optimization as a model for few-shot learning.\n\n</span>\n<span class=\"ltx_bibblock\">2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ridnik et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nTal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.\n\n</span>\n<span class=\"ltx_bibblock\">ImageNet-21K Pretraining for the Masses.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2104.10972</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Russakovsky and Fei-Fei [2010]</span>\n<span class=\"ltx_bibblock\">\nOlga Russakovsky and Li Fei-Fei.\n\n</span>\n<span class=\"ltx_bibblock\">Attribute Learning in Large-scale Datasets.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">ECCV, International Workshop on Parts and Attributes</em>, 2010.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shah and Mahajan [2012]</span>\n<span class=\"ltx_bibblock\">\nNeepa Shah and Sunita Mahajan.\n\n</span>\n<span class=\"ltx_bibblock\">Document clustering: a detailed review.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Journal of Applied Information Systems</em>, 2012.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Su et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nYuchang Su, Renping Zhou, Siyu Huang, Xingjian Li, Tianyang Wang, Ziyue Wang, and Min Xu.\n\n</span>\n<span class=\"ltx_bibblock\">Multimodal Generalized Category Discovery.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2409.11624</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tipirneni et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nSindhu Tipirneni, Ravinarayana Adkathimar, Nurendra Choudhary, Gaurush Hiranandani, Rana Ali Amjad, Vassilis N. Ioannidis, Changhe Yuan, and Chandan K. Reddy.\n\n</span>\n<span class=\"ltx_bibblock\">Context-Aware Clustering using Large Language Models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2405.00988</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tsimpoukelli et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill.\n\n</span>\n<span class=\"ltx_bibblock\">Multimodal Few-Shot Learning with Frozen Language Models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vacareanu et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nRobert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu.\n\n</span>\n<span class=\"ltx_bibblock\">From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Conference on Language Modeling (COLM)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vaswani et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.\n\n</span>\n<span class=\"ltx_bibblock\">Attention is all you need.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Viswanathan et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nVijay Viswanathan, Kiril Gashteovski, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, and Graham Neubig.\n\n</span>\n<span class=\"ltx_bibblock\">Large Language Models Enable Few-Shot Clustering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Transactions of the Association for Computational Linguistics (ACL)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">von Luxburg [2007]</span>\n<span class=\"ltx_bibblock\">\nUlrike von Luxburg.\n\n</span>\n<span class=\"ltx_bibblock\">A Tutorial on Spectral Clustering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:0711.0189</em>, 2007.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ward Jr [1963]</span>\n<span class=\"ltx_bibblock\">\nJoe H Ward Jr.\n\n</span>\n<span class=\"ltx_bibblock\">Hierarchical grouping to optimize an objective function.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of the American Statistical Association</em>, 1963.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wazarkar and Keshavamurthy [2018]</span>\n<span class=\"ltx_bibblock\">\nSeema Wazarkar and Bettahally N. Keshavamurthy.\n\n</span>\n<span class=\"ltx_bibblock\">A survey on image data analysis through clustering techniques for real world applications.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Visual Communication and Image Representation</em>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yao et al. [2011]</span>\n<span class=\"ltx_bibblock\">\nBangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas Guibas, and Li Fei-Fei.\n\n</span>\n<span class=\"ltx_bibblock\">Human action recognition by learning bases of action attributes and parts.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Computer Vision (ICCV)</em>, 2011.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhai et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.\n\n</span>\n<span class=\"ltx_bibblock\">Sigmoid loss for language image pre-training.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Computer Vision (ICCV)</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nYadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei.\n\n</span>\n<span class=\"ltx_bibblock\">LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2404.01230</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nYuwei Zhang, Zihan Wang, and Jingbo Shang.\n\n</span>\n<span class=\"ltx_bibblock\">ClusterLLM: Large Language Models as a Guide for Text Clustering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_logical-block ltx_align_bottom\">\n<span class=\"ltx_rule\" style=\"width:100%;height:4.0pt;--ltx-bg-color:black;display:inline-block;\"> </span>\n<div class=\"ltx_para\" id=\"p1\">\n<p class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:173%;\">Appendix</span></p>\n<span class=\"ltx_rule ltx_align_center\" style=\"width:100%;height:1.0pt;--ltx-bg-color:black;display:inline-block;\"> </span>\n</div>\n</div>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Results of Numeric Data Clustering</h2>\n<figure class=\"ltx_figure\" id=\"A1.F6\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"211\" id=\"A1.F6.g1\" src=\"./assets/x6.png\" width=\"660\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"211\" id=\"A1.F6.g2\" src=\"./assets/x7.png\" width=\"660\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"211\" id=\"A1.F6.g3\" src=\"./assets/x8.png\" width=\"660\"/></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot Clustering Accuracy. Test data is t-distributed with different degrees of freedom, number of clusters and dimensions. Note that “Ins” represents “Instruct” in the legend.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A1.F7\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"266\" id=\"A1.F7.g1\" src=\"./assets/x9.png\" width=\"660\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"266\" id=\"A1.F7.g2\" src=\"./assets/x10.png\" width=\"660\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"266\" id=\"A1.F7.g3\" src=\"./assets/x11.png\" width=\"660\"/></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Impact of Instruction Tuning and Clustering-Specific Fine-tuning on Clustering Accuracy. Test data is t-distributed with different degrees of freedom, number of clusters and dimensions. Note that “Ins” represents “Instruct”, and “finetune” refers to the fine-tuning on t-distributed clustering data with <math alttext=\"df\\in\\{1,2,5,100\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.F7.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1,2,5,100\\}</annotation></semantics></math> as in <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<figure class=\"ltx_table\" id=\"A1.T4\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Average Clustering Accuracy with One Standard Error on Lognormal Data. <span class=\"ltx_text ltx_font_smallcaps\">finetuned</span> represents the fine-tuned <span class=\"ltx_text ltx_font_smallcaps\">llama-3.1-8b</span> model on t-distributed clustering data with <math alttext=\"df\\in\\{1,2,5,100\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>∈</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">df\\in\\{1,2,5,100\\}</annotation></semantics></math> as in <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. Although the model is not fine-tuned on lognormal data, it still outperforms other models in almost all settings. </span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><math alttext=\"c=2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m3\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">c</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">2</mn></mrow><annotation encoding=\"application/x-tex\">c=2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><math alttext=\"c=3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m4\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">c</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">3</mn></mrow><annotation encoding=\"application/x-tex\">c=3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><math alttext=\"c=4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m5\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">c</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">4</mn></mrow><annotation encoding=\"application/x-tex\">c=4</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\"><math alttext=\"dim=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">dim=1</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">kmeans</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.86</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.77</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">gpt-4o</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.75</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.89<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.79<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.76<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\"><math alttext=\"dim=2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m16\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">dim=2</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">kmeans</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.82</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">gpt-4o</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.80</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.94<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.91<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.86<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\"><math alttext=\"dim=3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m26\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">dim=3</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">kmeans</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.98<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m27\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.92</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m28\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m29\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">gpt-4o</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.94</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.86</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m31\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.88</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m32\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.94</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m33\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.94<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m34\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m35\" intent=\":literal\"><semantics><mo mathsize=\"0.560em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math></span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure class=\"ltx_table\" id=\"A1.T5\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Sensitivity to Input Order. The reported values are average accuracy on t-distributed (c=2, dim=3) data, with average standard deviation over five runs of permuted input data in parentheses. We use the standard deviation to reflect the consistency of clustering methods given permutations of input data. <span class=\"ltx_text ltx_font_smallcaps\">finetuned</span> denotes the <span class=\"ltx_text ltx_font_smallcaps\">llama-3.1-8b</span> model finetuned on t-distributed clustering data in <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, and <span class=\"ltx_text ltx_font_smallcaps\">finetuned-aug</span> denotes finetuning on augmented data with 3 times of permutation. We notice that the model with higher clustering accuracy tends to be more invariant to permutation in input data. Data augmentation is also effective in improving the consistency.</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=100</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">k-means</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.75(0.04)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.95(0.03)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.99(0.00)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.99(0.00)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">gpt-4o</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.83(0.08)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.95(0.03)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.97(0.02)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.98(0.01)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.92(0.04)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.97(0.02)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.98(0.01)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.99(0.01)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned-aug</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.93(0.03)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.98(0.01)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.98(0.01)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.99(0.00)</span></td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Emergence of Clusters in Attention</h2>\n<section class=\"ltx_subsection\" id=\"A2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Attention of Different Layers and Attention Heads</h3>\n<figure class=\"ltx_figure ltx_align_floatright\" id=\"A2.SS1.fig1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"132\" id=\"A2.SS1.g1\" src=\"./assets/x12.png\" width=\"132\"/>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"740\" id=\"A2.F8.g1\" src=\"./assets/x13.png\" width=\"528\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Attention Allocation of <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8b-Instruct</span> across Layers. The attention scores are logarithmized for better visualization. Each cluster is generated from a Gaussian distribution, as shown in top right. <a class=\"ltx_ref\" href=\"#S3.F3\" title=\"In 3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> is a zoom-in view of layer 15 here.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"862\" id=\"A2.F9.g1\" src=\"./assets/x14.png\" width=\"595\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Attention Allocation of <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8b-Instruct</span> on <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.F9.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-Distributed Data with Different <math alttext=\"df\" class=\"ltx_Math\" display=\"inline\" id=\"A2.F9.m6\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">df</annotation></semantics></math>, before and after Finetuning. Note that <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.F9.m7\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-distribution with <math alttext=\"df=inf\" class=\"ltx_Math\" display=\"inline\" id=\"A2.F9.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow><mo>=</mo><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi></mrow></mrow><annotation encoding=\"application/x-tex\">df=inf</annotation></semantics></math> is Gaussian. The attention scores are logarithmized for better visualization.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1004\" id=\"A2.F10.g1\" src=\"./assets/x15.png\" width=\"627\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Attention Allocation of <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8b-Instruct</span> across attention heads at layer 15. The attention scores are logarithmized for better visualization. Each cluster is generated from a Gaussian distribution, as shown in top left.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Spectral Clustering</h3>\n<div class=\"ltx_para\" id=\"A2.SS2.p1\">\n<p class=\"ltx_p\">As described in <a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we perform spectral clustering using the input-input attention score matrix <math alttext=\"A^{II}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>A</mi><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><annotation encoding=\"application/x-tex\">A^{II}</annotation></semantics></math>. We first standardize <math alttext=\"A^{II}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>A</mi><mrow><mi>I</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>I</mi></mrow></msup><annotation encoding=\"application/x-tex\">A^{II}</annotation></semantics></math> so that each row sums to one. Due to causality, early tokens cannot attend to later tokens, making the attention scores scale uneven across rows. For example, the second data point always allocates very high attention to the first one regardless of its semantic similarity. To mitigate this imbalance, we further rescale each row by the number of non-zero entries in the row. Finally, we symmetrize the matrix and the resulting matrix is used as the precomputed affinity matrix for spectral clustering. The complete preprocessing procedure is visualized in <a class=\"ltx_ref\" href=\"#A2.F11\" title=\"In B.2 Spectral Clustering ‣ Appendix B Emergence of Clusters in Attention ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">11</span></a>. We use the <span class=\"ltx_text ltx_font_typewriter\"> sklearn.cluster.SpectralClustering</span> implementation.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"149\" id=\"A2.F11.g1\" src=\"./assets/x16.png\" width=\"660\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Preprocessing Attention Matrix for Spectral Clustering.</span></figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"A2.T6\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Spectral Clustering using Attention Scores. Reported values are average accuracy on t-distributed test data as in <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>, with one standard error. Models used here are pretrained <span class=\"ltx_text ltx_font_smallcaps\">Llama-3.1-8b-Instruct</span> and its fine-tuned checkpoint as in <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Numeric Data Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. <span class=\"ltx_text ltx_font_smallcaps\">SC</span> represents spectral clustering using attention scores with <span class=\"ltx_text ltx_font_smallcaps\">opt</span> denoting the highest accuracy across all layers and <span class=\"ltx_text ltx_font_smallcaps\">l23</span> denoting the accuracy using a fixed layer 23 (indexing from 0). <span class=\"ltx_text ltx_font_smallcaps\">Gen</span> represents generation using direct LLM prompting. Spectral clustering using attention achieves surprisingly competitive performance that outperforms the raw generation before finetuning.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:310.5pt;height:181.4pt;vertical-align:-89.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-73.7pt,43.1pt) scale(0.677990458057933,0.677990458057933) ;\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=1.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">df=100</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">num of clusters = 2, dim = 1</em></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">pretrained</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(opt)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.68</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.70</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(l23)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.68</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.68</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.69</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.69</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.70</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.77</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(opt)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.70</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m27\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m28\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(l23)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.67</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m29\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.70</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m31\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m32\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.72</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m33\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.76</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m34\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.75</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m35\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.85</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.86</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m37\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m38\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.89</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m39\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.90</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m40\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m41\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.94</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m42\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">num of clusters = 2, dim = 2</em></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">pretrained</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(opt)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.75</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m43\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.76</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m44\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m45\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.78</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m46\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.81</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m47\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.82</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m48\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.88</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m49\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(l23)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m50\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m51\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m52\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.76</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m53\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.78</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m54\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.80</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m55\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m56\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.69</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m57\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.68</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m58\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.69</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m59\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m60\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.69</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m61\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m62\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.75</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m63\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(opt)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m64\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m65\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.85</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m66\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m67\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m68\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.89</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m69\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m70\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(l23)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.77</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m71\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.81</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m72\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.80</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m73\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.82</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m74\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.83</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m75\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m76\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.94</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m77\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.92</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m78\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.94</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m79\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.93</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m80\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.95</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m81\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.94</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m82\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m83\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.98</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m84\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">num of clusters = 2, dim = 3</em></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">pretrained</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(opt)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.77</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m85\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.79</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m86\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.78</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m87\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.80</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m88\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.83</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m89\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.85</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m90\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.88</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m91\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(l23)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.68</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m92\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m93\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.73</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m94\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.74</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m95\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.76</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m96\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.81</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m97\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.85</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m98\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.64</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m99\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.65</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m100\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.66</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m101\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.67</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m102\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.69</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m103\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.70</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m104\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.71</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m105\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">finetuned</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(opt)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.90</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m106\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m107\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.93</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m108\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m109\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.93</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m110\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m111\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.99</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m112\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.00</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">SC(l23)</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.83</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m113\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.86</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m114\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.89</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m115\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.87</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m116\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.91</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m117\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.95</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m118\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.97</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m119\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m120\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.97</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m121\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.98</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m122\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.96</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m123\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.98</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m124\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">0.99</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m125\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.00</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">1.00</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T6.m126\" intent=\":literal\"><semantics><mo mathsize=\"0.500em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">0.00</span>\n</td>\n</tr>\n</table>\n</span></div>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Experiment Details and Results of Image Clustering</h2>\n<section class=\"ltx_subsection\" id=\"A3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.1 </span>Pooling</h3>\n<figure class=\"ltx_table\" id=\"A3.T7\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Pooling kernel size and corresponding per-image token length. The original pixel size is 384x384 with a patch size of 14, resulting in 27x27(729) image tokens. </span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">pooling kernel</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">token length</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Default</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">1x1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">27 x 27 (729)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Large</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">2x2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">13 x 13 (169)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Medium</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">3x3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">9 x 9 (81)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">9x9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">3 x 3 (9)</span></td>\n</tr>\n</table>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.2 </span>Out-of-Domain Image datasets</h3>\n<div class=\"ltx_para\" id=\"A3.SS2.p1\">\n<p class=\"ltx_p\">To test the generalization capability of the model, we include two more image datasets from Cross-Domain Few-Shot Learning (CD-FSL) Benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et al. [<a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2020</a>]</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"A3.SS2.p2\">\n<ul class=\"ltx_itemize\" id=\"A3.I1\">\n<li class=\"ltx_item\" id=\"A3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A3.I1.i1.p1\">\n<p class=\"ltx_p\">Plant Disease <cite class=\"ltx_cite ltx_citemacro_cite\">Mohanty et al. [<a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2016</a>]</cite>: Leaves of different trees that are healthy or have different crop diseases. We construct 100 clustering samples based on the plant names, where each sample contains 10-30 images from 3 random classes.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A3.I1.i2.p1\">\n<p class=\"ltx_p\">EuroSAT <cite class=\"ltx_cite ltx_citemacro_cite\">Helber et al. [<a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">2019</a>]</cite>: Satellite images of different land use and land cover classes. We construct 100 clustering samples where each sample contains 10-30 images from 3 random classes.</p>\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F12\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"225\" id=\"A3.F12.g1\" src=\"./assets/x17.png\" width=\"528\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 12</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Example of Plant Disease and EuroSAT datasets. The color of frame represents different clusters predicted by our model. Our model can generalize to these images that are quite different from ImageNet.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.3 </span>Attention</h3>\n<div class=\"ltx_para\" id=\"A3.SS3.p1\">\n<p class=\"ltx_p\">Similar as the numeric experiments in <a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Emergence of Clusters in Attention ‣ 3 Zero-shot Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we visualize the attention allocation for image clustering below (<a class=\"ltx_ref\" href=\"#A3.F13\" title=\"In C.3 Attention ‣ Appendix C Additional Experiment Details and Results of Image Clustering ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">13</span></a>). The model used here is fine-tuned model (medium) as in <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Image Clustering ‣ 4 Learning Clustering with Next Token Prediction ‣ In-Context Clustering with Large Language Models\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. The attention scores have block structures that roughly align with the ground-truth identities in intermediate layers. We notice that the allocation of attention weights can be uneven within one cluster, where representative samples are assigned with higher weights. The attention patterns for images are generally more complicated than those for synthetic low-dimensional data due to the semantically rich information in images.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F13\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"897\" id=\"A3.F13.g1\" src=\"./assets/x18.png\" width=\"594\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 13</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Attention Allocation of Image Clustering. Different colors represent different clusters.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Additional Results for Conditional Image Clustering</h2>\n<figure class=\"ltx_figure\" id=\"A4.F14\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"312\" id=\"A4.F14.g1\" src=\"./assets/x19.png\" width=\"595\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"312\" id=\"A4.F14.g2\" src=\"./assets/x20.png\" width=\"595\"/></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 14</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Examples of ICC on ImageNet-with-Attributes. The color of the frame indicates different clusters predicted by our model. Most of the images contain multiple objects, making the task more challenging.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2510.08466",
  "source": "arxiv-experimental",
  "generated": "2025-10-27T23:03:00.964Z"
}
