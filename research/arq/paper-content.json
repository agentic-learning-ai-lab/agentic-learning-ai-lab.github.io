{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\">The success of deep learning has relied on <em class=\"ltx_emph ltx_font_italic\">backpropagation</em> <cite class=\"ltx_cite ltx_citemacro_citep\">(Rumelhart et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">1986</a>)</cite>, a procedure that has significant limitations in terms of biological plausibility as it requires synchronous computations and weight symmetry. Many works have provided backprop-free alternatives for training deep neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Lillicrap et al., <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2016a</a>; Nøkland, <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">2016</a>; Nøkland and Eidnes, <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">2019</a>)</cite>. Notably, <cite class=\"ltx_cite ltx_citemacro_citet\">Hinton, (<a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2022</a>)</cite> proposed the Forward-Forward algorithm (FF), a new approach that performs layerwise contrastive learning between positive and negative samples. This algorithm is lightweight and entirely eliminates the need for backpropagation, thereby addressing some of the biological plausibility concerns.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\">However, most studies on backprop-free methods are focused on the search for a biologically plausible mechanism for performing gradient updates on supervised tasks. Could a biologically plausible source of learning signals be equally meaningful?\nReward-centric environments and temporal-difference (TD) methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Sutton, <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">1988</a>)</cite> serve as natural candidates for filling this gap. Biological brains have evolved through a series of reward-guided evolution, while ample evidence has shown that our brains could be implementing TD <cite class=\"ltx_cite ltx_citemacro_citep\">(Schultz et al., <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">1997a</a>; O’Doherty et al., <a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">2003</a>; Watabe-Uchida et al., <a class=\"ltx_ref\" href=\"#bib.bib68\" title=\"\">2017</a>; Amo et al., <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2022</a>)</cite>. Since the goodness score in FF models the “compatibility” between the inputs and labels, this local learning paradigm can be readily adapted to a reinforcement learning (RL) setting where we model the value of an input state and an action from each layer’s activities. See Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for a comparison between the supervised learning and RL setups of the forward-forward learning paradigm.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\">Towards integrating local methods and RL, <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> recently proposed Artificial Dopamine (AD) that incorporates top-down and temporal connections in a Q-learning framework. Since the local Q-function estimation needs to be explicitly predicted, <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> uses a dot-product between two sets of mappings from the inputs that produces the value estimate for each action. This design, while backprop-free, makes the architecture more flexible modeling complex inputs. However, AD still relies on the output of the dot-product to be the same dimension as the action space, limiting the flexibility of the method.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"208\" id=\"S1.F1.g1\" src=\"./assets/x1.png\" width=\"665\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nLocal learning paradigms inspired by the Forward-Forward (FF) algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">(Hinton, <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2022</a>)</cite>. a) The original FF is designed for supervised learning, where each layer models the “goodness” between image <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S1.F1.m5\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and label <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S1.F1.m6\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math>. Information is carried forward only through bottom-up and optionally top-down connections without backpropagation. b) We extend FF local learning for reinforcement learning—each layer takes a state observation <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S1.F1.m7\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and an action candidate <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S1.F1.m8\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> as input, and estimates the Q value by taking the root mean squared function of the hidden vector.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\">Inspired by FF’s local goodness function from using layer statistics, we propose Action-conditioned Root mean squared Q-Function (ARQ), a simple vector-based alternative to traditional scalar-based Q-value predictors designed for local RL. ARQ is composed of two key ingredients: a goodness function that extracts value predictions from a vector of arbitrary size, and action conditioning by inserting an action candidate at the model input. ARQ significantly improves the expressivity of a local cell by allowing more neurons at the output layer without sacrificing the backprop-free property. By applying action conditioning, we further unleash the capacity of the network to produce representation specific to each state-action pair. Moreover, ARQ can be readily implemented on AD and take full advantage of their non-linearity and attention-like mechanisms.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\">We evaluate our method on the MinAtar benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Young and Tian, <a class=\"ltx_ref\" href=\"#bib.bib72\" title=\"\">2019</a>)</cite> and the DeepMind Control Suite, challenging suites designed to test RL algorithms in low-dimensional settings where local methods remain viable. Our results show that our method consistently outperforms current local RL methods and surpasses conventional backprop-based value-learning methods in most games, demonstrating strong decision-making capabilities without relying on backpropagation.\nThrough this contribution, we seek to encourage further exploration of the intersection between RL and biologically plausible learning methods.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Works</h2>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Backprop-free learning methods &amp; FF:</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">In recent years, several backprop-free training algorithms have been proposed to address the limitations of traditional backpropagation in neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Lillicrap et al., <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2016a</a>; Nøkland, <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">2016</a>; Nøkland and Eidnes, <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">2019</a>; Belilovsky et al., <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">2019</a>; Baydin et al., <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2022</a>; Ren et al., <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">2023</a>; Fournier et al., <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2023</a>; Singhal et al., <a class=\"ltx_ref\" href=\"#bib.bib57\" title=\"\">2023</a>; Innocenti et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2025</a>)</cite>. One notable method is the Forward-Forward Algorithm (FF) <cite class=\"ltx_cite ltx_citemacro_citep\">(Hinton, <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2022</a>)</cite>, which offers a biologically plausible alternative to backpropagation. To extend the capabilities of FF, <cite class=\"ltx_cite ltx_citemacro_citet\">Ororbia and Mali, (<a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">2023</a>)</cite> proposed the Predictive Forward-Forward Algorithm, showing that a top-down generative circuit can be trained jointly with FF. <cite class=\"ltx_cite ltx_citemacro_citet\">Tosato et al., (<a class=\"ltx_ref\" href=\"#bib.bib64\" title=\"\">2023</a>)</cite> found that models trained with FF objectives generate highly sparse representations. This pattern closely resembles the observations of neuronal ensembles in cortical sensory areas, suggesting FF may be a suitable candidate for modeling biological learning. Recently, <cite class=\"ltx_cite ltx_citemacro_citet\">Sun et al., (<a class=\"ltx_ref\" href=\"#bib.bib60\" title=\"\">2025</a>)</cite> proposed DeeperForward, integrating residual connections <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2016</a>)</cite>, the mean goodness function, and a channel-wise cross-entropy based objective function <cite class=\"ltx_cite ltx_citemacro_citep\">(Papachristodoulou et al., <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">2024</a>)</cite> into FF. However, these works have mostly focused on supervised image classification rather than RL tasks.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Value Estimation in Deep Neural Networks:</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">TD methods for value estimation have been particularly useful in the recent decade as the rise of deep neural networks offers a powerful function approximator. <cite class=\"ltx_cite ltx_citemacro_citet\">Mnih et al., (<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2015</a>)</cite> introduced DQN, where a deep neural network is applied to approximate the Q-Function. They showed that this method significantly outperformed earlier methods on the Atari 2600 games, initiating a family of methods built upon this architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Van Hasselt et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2016</a>; Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib67\" title=\"\">2016</a>; Dabney et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2018b</a>; Hessel et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2018</a>; Fortunato et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2018</a>; Dabney et al., <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">2018a</a>; Hausknecht and Stone, <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2015</a>)</cite>. In actor-critic architectures, it is also common to use a deep neural network for value and advantage estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Schulman et al., <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">2017</a>; Schulman et al., <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2015a</a>; Schulman et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2015b</a>; Lillicrap et al., <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2016b</a>; Mnih et al., <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">2016</a>; Haarnoja et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2018b</a>; Haarnoja et al., <a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">2018a</a>; Fujimoto et al., <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">2018</a>; Gruslys et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2018</a>; Abdolmaleki et al., <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">2018</a>; Yarats et al., <a class=\"ltx_ref\" href=\"#bib.bib70\" title=\"\">2021b</a>; Yarats et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2021a</a>)</cite>. For planning-based methods using either Monte Carlo tree search (MCTS) or a learned model, value estimation is also significant in driving the planning process <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrittwieser et al., <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">2020</a>; Silver et al., <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2016</a>, <a class=\"ltx_ref\" href=\"#bib.bib56\" title=\"\">2018</a>; Hansen et al., <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2024</a>; Sacks et al., <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">2024</a>; Ye et al., <a class=\"ltx_ref\" href=\"#bib.bib71\" title=\"\">2021</a>; Hafner et al., <a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">2020</a>, <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2019</a>)</cite> Yet, few works have investigated the capability of local learning on value estimation.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Action Conditioning of Value Estimators:</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">An important design choice in value estimation is whether the network is conditioned on the action. Early neural value estimation methods <cite class=\"ltx_cite ltx_citemacro_citet\">Riedmiller, (<a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2005</a>)</cite> incorporated action conditioning by incorporating both state and action as model inputs. With the advent of deep neural network approaches such as DQN, practices began to diverge. Purely value-based methods like DQN are typically only state-conditioned, with action-specific predictions produced at the output layer by indexing over action values. This design is computationally efficient and well-suited for discrete tasks with low-dimensional action spaces. In contrast, actor–critic methods developed for high-dimensional continuous control tasks <cite class=\"ltx_cite ltx_citemacro_citet\">Lillicrap et al., <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2016b</a> ; Haarnoja et al., <a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">2018a</a> </cite> condition on both state and action at the input of their critic networks. Although this distinction is largely arbitrary in backpropagation-based architectures and can be adapted to the task, we show that action conditioning at model inputs is strictly preferable for local RL.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Local and Decentralized Reinforcement Learning:</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">The concept of decentralized RL can be dated back to the dawn of RL. <cite class=\"ltx_cite ltx_citemacro_citet\">Klopf, (<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">1982</a>)</cite> introduced the idea of the hedonistic neuron, which hypothesized that each of our neurons may be guided by their independent rewards. Instead of being a miniscule part of a large operating neural system, each neuron may be an RL agent itself. In modern RL literature, the localized formulation of RL methods can be related to the multi-agent RL (MARL) setup, where multiple independent agents can be designed to cooperate well toward maximizing their joint rewards <cite class=\"ltx_cite ltx_citemacro_citep\">(Tan, <a class=\"ltx_ref\" href=\"#bib.bib62\" title=\"\">1993</a>; Foerster et al., <a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">2017</a>; Palmer et al., <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">2018</a>; Su et al., <a class=\"ltx_ref\" href=\"#bib.bib59\" title=\"\">2022</a>; Lauer and Riedmiller, <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">2000</a>; Jiang and Lu, <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">2023</a>; De Witt et al., <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">2020</a>; Su and Lu, <a class=\"ltx_ref\" href=\"#bib.bib58\" title=\"\">2022</a>; Arslan and Yüksel, <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">2016</a>; Jin et al., <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">2022</a>)</cite>. Conveniently, we can frame the problem of training RL using local objectives as a MARL problem where each agent represents different modules within a network. Recently, <cite class=\"ltx_cite ltx_citemacro_citet\">Seyde et al., (<a class=\"ltx_ref\" href=\"#bib.bib54\" title=\"\">2023</a>)</cite> has explored a similar approach for the continuous control problem, showing that using a separate critics network for each fixed action after action discretization works surprisingly well. <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> shows that a network with nonlinear local operations, decentralized objectives, and top-down connections across the temporal dimension can exceed state-of-the-art methods trained end-to-end. We extend upon this literature of decentralized methods for value estimation.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Background</h2>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Forward-Forward (FF):</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">The FF Algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">(Hinton, <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2022</a>)</cite>, as its name denotes, uses two forward passes instead of one forward pass and one backward pass used in backpropagation. The first forward pass carries the positive data, or real data, while the second pass carries the negative data, or fake data either manually defined or synthetically generated by the network. The network is then trained by maximizing the goodness of each layer in the positive pass, while minimizing the goodness of each layer in the negative pass. The definition of goodness based on a hidden vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is as follows:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"G_{z}=\\sum_{z_{i}\\in z}\\;z_{i}^{2}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>G</mi><mi>z</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>∈</mo><mi>z</mi></mrow></munder><msubsup><mi>z</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">G_{z}=\\sum_{z_{i}\\in z}\\;z_{i}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">In layman’s terms, this equation represents the sum of squares of all activations over <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, a measure of the magnitude and orientation of the activation vector. By training its layers greedily, FF is biologically plausible and could serve as a model for our future discovery of the inner mechanisms of the human brain.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Value Estimation in Deep RL:</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Estimation of the value function is core to RL.\nIn layman’s terms, the value function measures the expected sum of future rewards after discounting given a current state. A similar formulation can be constructed when we are interested in the goodness of a state-action pair, which is usually termed the q-function. Formally,</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx1\">\n<tbody id=\"S3.E2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle Q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\bigg|S_{t}=s,A_{t}=a\\right].\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>Q</mi><mi>π</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mi>π</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mrow><mrow><mrow><mrow><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant=\"normal\">∞</mi></munderover></mstyle><mrow><msup><mi>γ</mi><mi>k</mi></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo fence=\"false\" mathsize=\"2.100em\">|</mo><msub><mi>S</mi><mi>t</mi></msub></mrow><mo>=</mo><mi>s</mi></mrow><mo>,</mo><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle Q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\ \\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}\\bigg|S_{t}=s,A_{t}=a\\right].</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"298\" id=\"S3.F2.g1\" src=\"./assets/x2.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nHigh-level computation diagram between <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> and ARQ. Key implementations of ARQ are highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>. AD cells take activations (highlighted in <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>, darker color means earlier layer) and the state observation as input and produces a vector of size <math alttext=\"n_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m2\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">n_{a}</annotation></semantics></math>, each indicating the value prediction of an action candidate. Our ARQ takes activations, the state observation, and the action candidate as input, and produces a hidden vector of arbitrary size, before passing it through a root mean squared function to yield a scalar prediction.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px2.p2\">\n<p class=\"ltx_p\">A widely used class of methods for value estimation is temporal difference (TD) learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Sutton, <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">1988</a>)</cite>, which bootstraps value estimates by blending immediate rewards with future predictions, allowing for online, incremental updates. This method paved the way for the development of many subsequent approaches, particularly Q-learning. Take a q-function <math alttext=\"Q(s,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(s,a)</annotation></semantics></math>. To update the function given an experience <math alttext=\"(S_{t},a,r,S_{t+1)}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><mi>a</mi><mo>,</mo><mi>r</mi><mo>,</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">(S_{t},a,r,S_{t+1)}</annotation></semantics></math>, Q-learning makes the following iterative update</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx2\">\n<tbody id=\"S3.E3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle Q^{(i+1)}(S_{t},A_{t})=\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle Q^{(i+1)}(S_{t},A_{t})=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;Q^{(i)}(S_{t},A_{t})+\\alpha(R_{t}+\\gamma\\max_{a’}Q^{(i)}(S_{t+1},a’)-Q^{(i)}(S_{t},A_{t})),\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E3.m2\" intent=\":literal\"><semantics><mrow><mrow><mrow><msup><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>α</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>γ</mi><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><munder><mi>max</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow></munder><mo lspace=\"0.167em\">⁡</mo><msup><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><msup><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;Q^{(i)}(S_{t},A_{t})+\\alpha(R_{t}+\\gamma\\max_{a’}Q^{(i)}(S_{t+1},a’)-Q^{(i)}(S_{t},A_{t})),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mi>γ</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> is a discounting factor, <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mi>α</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> is a pre-determined learning rate, and <math alttext=\"a’\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow><annotation encoding=\"application/x-tex\">a’</annotation></semantics></math> represents any possible actions in the next step.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px2.p3\">\n<p class=\"ltx_p\">Recently, the rise of neural networks pushed q-learning to new heights. <cite class=\"ltx_cite ltx_citemacro_citet\">Mnih et al., (<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2015</a>)</cite> proposed DQN, approximating q-function values using a deep neural network. Based on the Bellman equation, DQN constructs a mean squared error function as the objective, namely</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx3\">\n<tbody id=\"S3.E4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle L_{\\theta}=\\left(R_{t}+\\gamma\\max_{a’}Q_{\\theta}(S_{t+1},a’)-Q_{\\theta}(S_{t},A_{t})\\right)^{2}.\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E4.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>L</mi><mi>θ</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>γ</mi><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><munder><mi>max</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow></munder><mo lspace=\"0.167em\">⁡</mo><msub><mi>Q</mi><mi>θ</mi></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle L_{\\theta}=\\left(R_{t}+\\gamma\\max_{a’}Q_{\\theta}(S_{t+1},a’)-Q_{\\theta}(S_{t},A_{t})\\right)^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Mnih et al., (<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2015</a>)</cite> tested their agents on the Atari 2600 environment, and show that a convolutional neural network trained in this fashion is able to achieve near-human performance level from raw pixel inputs, a feat previously considered far-fetched.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Artificial Dopamine (AD):</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">AD <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> trains a local RL agent using Q-learning. An AD network is consisted of multiple AD cells, each of which makes an independent estimation of <math alttext=\"Q(S_{t},A_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(S_{t},A_{t})</annotation></semantics></math>.\nTo yield a scalar estimation, each AD cell adopts an attention-like mechanism\nto compute a weighted sum of its hidden activations using weights from a separate linear projection, effectively incorporating nonlinearity while maintaining backprop-free. Additionally, each AD cell takes inputs from the layer below, the layer above, and also the raw state observation, enabling skip connections, top-down connections, and information flow throughout the temporal dimension in an RL environment. Mathematically, an AD cell at depth <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> conducts the following operations,</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx4\">\n<tbody id=\"S3.E5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle X=\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E5.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle X=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;\\mathrm{concat}(s_{t},h_{t}^{l-1},h_{t-1}^{l+1}),\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>concat</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;\\mathrm{concat}(s_{t},h_{t}^{l-1},h_{t-1}^{l+1}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n<tbody id=\"S3.E6\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle h_{t}^{l}=\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E6.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle h_{t}^{l}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;\\mathrm{ReLU}(W_{h}X),\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E6.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>ReLU</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>h</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;\\mathrm{ReLU}(W_{h}X),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n<tbody id=\"S3.E7\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle Q(s_{t},a_{t})=\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle Q(s_{t},a_{t})=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;\\mathrm{tanh}(W_{att}X)^{T}h_{t}^{l},\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E7.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>tanh</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;\\mathrm{tanh}(W_{att}X)^{T}h_{t}^{l},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(7)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math alttext=\"h_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">h_{t}^{l}</annotation></semantics></math> represents the activation of the AD cell at time <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and depth <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>. While this attention-like mechanism brings exciting nonlinearity to a single AD cell without the need for backpropagation, the scalar nature of <math alttext=\"Q(s_{t},a_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(s_{t},a_{t})</annotation></semantics></math> implies that the dimensionality of <math alttext=\"W_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">W_{att}</annotation></semantics></math> must be limited by the size of the action space. We aim to remove this constraint.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>ARQ: Action-conditioned Root mean squared Q-function</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\">In the context of FF, the goodness function measures the likelihood of the observation to come from the postive distribution. In the context of RL, the concept of value measures the expected sum of future rewards for the trajectories starting from a given state. We observe a connection—both denote a measure of the current input’s desirability to an agent. Could the association between goodness and value be exploited to unleash the capacity of local RL networks? In this section, we introduce a novel vector-based training mechanism for local value estimation that can be used out-of-the-box. We term it the Action-conditioned Root mean squared Q-function (ARQ).</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>ARQ</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n<p class=\"ltx_p\">Take a state <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and an action <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math>. Based on the Bellman equation, we are interested in finding</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx5\">\n<tbody id=\"S4.E8\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle Q_{*}(s,a)=\\mathbb{E}_{\\pi}\\left[R_{t}+\\gamma\\max_{a^{\\prime}}Q_{*}(S_{t+1},a^{\\prime})\\bigl|S_{t}=s,A_{t}=a\\right].\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E8.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>Q</mi><mo>∗</mo></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mi>π</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mrow><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mrow><mrow><mi>γ</mi><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><munder><mi>max</mi><msup><mi>a</mi><mo>′</mo></msup></munder><mo lspace=\"0.167em\">⁡</mo><msub><mi>Q</mi><mo>∗</mo></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msup><mi>a</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo fence=\"false\" mathsize=\"1.200em\">|</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></mrow><mo>=</mo><mi>s</mi></mrow><mo>,</mo><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle Q_{*}(s,a)=\\mathbb{E}_{\\pi}\\left[R_{t}+\\gamma\\max_{a^{\\prime}}Q_{*}(S_{t+1},a^{\\prime})\\bigl|S_{t}=s,A_{t}=a\\right].</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(8)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Inspired by the association between the concept of goodness from FF and the concept of value in RL, we directly approximate <math alttext=\"Q(s,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(s,a)</annotation></semantics></math> using the goodness function. Given a hidden vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>, which can be either an intermediate action or an output embedding from a neural network.\nInstead of taking the sum of each vector unit squared, we make a small modification and take the root mean squared (RMS) function of the vector after mean subtraction to prevent its goodness values from exploding as we scale up the number of units. This is equivalent to the standard deviation of the hidden vector. In mathematical terms, we compute the estimated value of applying action <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> on state <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> using</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx6\">\n<tbody id=\"S4.E9\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mu_{y}=\\mathop{\\mathbb{E}}_{y_{i}\\in y}y_{i},\\;\\;Q_{\\theta}(s,a)=\\sqrt{\\mathop{\\mathbb{E}}_{y_{i}\\in y}\\;(y_{i}-\\mu_{y})^{2}},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E9.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>μ</mi><mi>y</mi></msub><mo rspace=\"0.1389em\">=</mo><mrow><munder><mo lspace=\"0.1389em\" movablelimits=\"false\" rspace=\"0.167em\">𝔼</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mi>y</mi></mrow></munder><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow><mo rspace=\"0.727em\">,</mo><mrow><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msqrt><mrow><munder><mo movablelimits=\"false\">𝔼</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mi>y</mi></mrow></munder><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mu_{y}=\\mathop{\\mathbb{E}}_{y_{i}\\in y}y_{i},\\;\\;Q_{\\theta}(s,a)=\\sqrt{\\mathop{\\mathbb{E}}_{y_{i}\\in y}\\;(y_{i}-\\mu_{y})^{2}},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(9)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mi>θ</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> denotes the parameters of the network\n, and <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> denotes a hidden vector produced by the network.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p2\">\n<p class=\"ltx_p\">To train this network, we update our weights using the same mean squared objective function as Deep Q-Learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Mnih et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2015</a>)</cite>. Namely,</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx7\">\n<tbody id=\"S4.E10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle L_{\\theta}=\\left(R_{t}+\\gamma\\max_{a’}Q_{\\theta}(S_{t+1},a’)-Q_{\\theta}(S_{t},A_{t})\\right)^{2}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E10.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>L</mi><mi>θ</mi></msub><mo>=</mo><msup><mrow><mo>(</mo><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mrow><mi>γ</mi><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><munder><mi>max</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow></munder><mo lspace=\"0.167em\">⁡</mo><msub><mi>Q</mi><mi>θ</mi></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">’</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle L_{\\theta}=\\left(R_{t}+\\gamma\\max_{a’}Q_{\\theta}(S_{t+1},a’)-Q_{\\theta}(S_{t},A_{t})\\right)^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(10)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Note that it is possible to sample positive and negative data in order to train in the same contrastive fashion as the original FF algorithm, particularly when our method is used with a training mechanism that maintains a replay buffer. We leave this for future investigations to keep our method versatile.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p3\">\n<p class=\"ltx_p\">ARQ can be implemented out-of-the-box in place of the standard Q-learning formulation. Given any intermediate vector produced by an arbitrary neural network architecture, ARQ can extract scalar statistics that serve as a prediction for the estimated Q-value without any parameters. This property allows architectures designed for local RL to enjoy greater flexibility.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Action Conditioning:</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">Due to the nature of goodness functions producing scalar values, it is natural to implement ARQ with action conditioning at the model input. Concretely, to estimate <math alttext=\"Q_{\\theta}(s,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q_{\\theta}(s,a)</annotation></semantics></math>, the neural network <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>θ</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> takes both the state vector <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and the action vector <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> as inputs and outputs a single scalar prediction. This contrasts with implementations such as <cite class=\"ltx_cite ltx_citemacro_citet\">Mnih et al., (<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2015</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>, where the model receives only the state vector <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and produces an output of dimension <math alttext=\"n_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">n_{a}</annotation></semantics></math>, with each entry corresponding to the value of a discrete action. We demonstrate in Section <a class=\"ltx_ref\" href=\"#S5\" title=\"5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that this minor design decision is critical to the performance of local RL methods. For tasks with discrete action spaces, we use a one-hot vector to represent an action candidate. For tasks with continuous action spaces, we apply bang-bang discretization on the action space following <cite class=\"ltx_cite ltx_citemacro_citet\">Seyde et al., (<a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">2021</a>)</cite> and condition the network on the binary action vector.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Implementation</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\">To evaluate our method against state-of-the-art local RL architectures, we implement AR on top of AD <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\n<p class=\"ltx_p\">Our implementation is consisted of multiple cells stacked together, each of which takes inputs from the layer below, the layer above, the input observation, and an action candidate <math alttext=\"a_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">a_{t}</annotation></semantics></math> to make an estimation of <math alttext=\"Q(s_{t},a_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(s_{t},a_{t})</annotation></semantics></math>. Each cell adopts a similar attention-like mechanism as <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>. After the attention mechanism, we apply the goodness function on the intermediate vector after the attention computation. Specifically, a cell at depth <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> conducts the following operations,</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p3\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Sx1.EGx8\">\n<tbody id=\"S4.E11\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle X=\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E11.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle X=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;\\mathrm{concat}(s_{t},h_{t}^{l-1},h_{t-1}^{l+1},a_{t}),\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E11.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>concat</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;\\mathrm{concat}(s_{t},h_{t}^{l-1},h_{t-1}^{l+1},a_{t}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(11)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E12\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle h_{t}^{l}=\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E12.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle h_{t}^{l}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;\\mathrm{ReLU}(W_{h}X),\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E12.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>ReLU</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>h</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;\\mathrm{ReLU}(W_{h}X),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(12)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E13\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle y_{t}^{l}=\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E13.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>y</mi><mi>t</mi><mi>l</mi></msubsup><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle y_{t}^{l}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\;\\mathrm{tanh}(W_{att}X)^{T}h_{t}^{l},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E13.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>tanh</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mi>T</mi></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;\\mathrm{tanh}(W_{att}X)^{T}h_{t}^{l},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(13)</span></td>\n</tr></tbody>\n<tbody id=\"S4.E14\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mu_{y}=\\mathop{\\mathbb{E}}_{y_{i}\\in y}y_{i},\\;\\;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E14.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>μ</mi><mi>y</mi></msub><mo rspace=\"0.1389em\">=</mo><mrow><munder><mo lspace=\"0.1389em\" movablelimits=\"false\" rspace=\"0.167em\">𝔼</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mi>y</mi></mrow></munder><msub><mi>y</mi><mi>i</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mu_{y}=\\mathop{\\mathbb{E}}_{y_{i}\\in y}y_{i},\\;\\;</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle Q(s_{t},a_{t})=\\sqrt{\\mathop{\\mathbb{E}}_{y_{i}\\in y_{t}^{l}}(y_{i}-\\mu_{y})^{2}},\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E14.m2\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msqrt><mrow><munder><mo movablelimits=\"false\">𝔼</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><msubsup><mi>y</mi><mi>t</mi><mi>l</mi></msubsup></mrow></munder><msup><mrow><mo lspace=\"0em\" stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>y</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle Q(s_{t},a_{t})=\\sqrt{\\mathop{\\mathbb{E}}_{y_{i}\\in y_{t}^{l}}(y_{i}-\\mu_{y})^{2}},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(14)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Gradients are passed only within each cell to ensure the architecture is backprop-free. Pseudocode comparising AD and ARQ can be found in Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4.2 Implementation ‣ 4 ARQ: Action-conditioned Root mean squared Q-function ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Most training choices are inherited from <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F3\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_float ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top ltx_framed ltx_framed_top\" id=\"alg1\" style=\"width:165.6pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 1</span> </span> AD <cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite></figcaption>\n<div class=\"ltx_listing ltx_listing\">\n<div class=\"ltx_listingline\" id=\"alg1.l1\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">1:</span></span><math alttext=\"X\\leftarrow[s_{t},\\ h_{t}^{l-1},\\ h_{t-1}^{l+1}]\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">←</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>s</mi><mi>t</mi></msub><mo rspace=\"0.667em\">,</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo rspace=\"0.667em\">,</mo><msubsup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X\\leftarrow[s_{t},\\ h_{t}^{l-1},\\ h_{t-1}^{l+1}]</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l2\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">2:</span></span><math alttext=\"h_{t}^{l}\\leftarrow\\text{LayerNorm}(\\mathrm{ReLU}(W_{h}X))\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup><mo stretchy=\"false\">←</mo><mrow><mtext>LayerNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>ReLU</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>h</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h_{t}^{l}\\leftarrow\\text{LayerNorm}(\\mathrm{ReLU}(W_{h}X))</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l3\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">3:</span></span><math alttext=\"Z_{1}\\leftarrow W_{att_{1}}X\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>1</mn></msub><mo stretchy=\"false\">←</mo><mrow><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>t</mi><mn>1</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{1}\\leftarrow W_{att_{1}}X</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l4\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">4:</span></span><math alttext=\"Z_{2}\\leftarrow W_{att_{2}}X\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>2</mn></msub><mo stretchy=\"false\">←</mo><mrow><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>t</mi><mn>2</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{2}\\leftarrow W_{att_{2}}X</annotation></semantics></math> <span class=\"ltx_text\" style=\"float:right;\"><math alttext=\"\\triangleright\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m2\" intent=\":literal\"><semantics><mo>⊳</mo><annotation encoding=\"application/x-tex\">\\triangleright</annotation></semantics></math> <math alttext=\"Z_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m3\" intent=\":literal\"><semantics><msub><mi mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">Z</mi><mn mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">2</mn></msub><annotation encoding=\"application/x-tex\">Z_{2}</annotation></semantics></math><span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\"> has dimension <math alttext=\"n_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m4\" intent=\":literal\"><semantics><msub><mi mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">n</mi><mi mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">a</mi></msub><annotation encoding=\"application/x-tex\">n_{a}</annotation></semantics></math></span>\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l5\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">5:</span></span><math alttext=\"W\\leftarrow Z_{2}^{\\top}Z_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l5.m1\" intent=\":literal\"><semantics><mrow><mi>W</mi><mo stretchy=\"false\">←</mo><mrow><msubsup><mi>Z</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>Z</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">W\\leftarrow Z_{2}^{\\top}Z_{1}</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l6\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">6:</span></span><math alttext=\"W\\leftarrow\\text{LayerNorm}(\\tanh(W))\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l6.m1\" intent=\":literal\"><semantics><mrow><mi>W</mi><mo stretchy=\"false\">←</mo><mrow><mtext>LayerNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>tanh</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">W\\leftarrow\\text{LayerNorm}(\\tanh(W))</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l7\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">7:</span></span><math alttext=\"Q\\leftarrow Wh_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m1\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo stretchy=\"false\">←</mo><mrow><mi>W</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">Q\\leftarrow Wh_{t}^{l}</annotation></semantics></math> <span class=\"ltx_text\" style=\"float:right;\"><math alttext=\"\\triangleright\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m2\" intent=\":literal\"><semantics><mo>⊳</mo><annotation encoding=\"application/x-tex\">\\triangleright</annotation></semantics></math> <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math><span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\"> has dimension <math alttext=\"n_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m4\" intent=\":literal\"><semantics><msub><mi mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">n</mi><mi mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">a</mi></msub><annotation encoding=\"application/x-tex\">n_{a}</annotation></semantics></math></span>\n</span>\n</div>\n</div>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_float ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top ltx_framed ltx_framed_top\" id=\"alg2\" style=\"width:165.6pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 2</span> </span> ARQ (Ours)</figcaption>\n<div class=\"ltx_listing ltx_listing\">\n<div class=\"ltx_listingline\" id=\"alg2.l1\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">1:</span></span><math alttext=\"X\\leftarrow[s_{t},\\ h_{t}^{l-1},\\ h_{t-1}^{l+1}]\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">←</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>s</mi><mi>t</mi></msub><mo rspace=\"0.667em\">,</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo rspace=\"0.667em\">,</mo><msubsup><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X\\leftarrow[s_{t},\\ h_{t}^{l-1},\\ h_{t-1}^{l+1}]</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l2\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">2:</span></span><math alttext=\"h_{t}^{l}\\leftarrow\\text{LayerNorm}(\\mathrm{ReLU}(W_{h}X))\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l2.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup><mo stretchy=\"false\">←</mo><mrow><mtext>LayerNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>ReLU</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>W</mi><mi>h</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h_{t}^{l}\\leftarrow\\text{LayerNorm}(\\mathrm{ReLU}(W_{h}X))</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l3\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">3:</span></span>Repeat <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l3.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> along batch dim <math alttext=\"n_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l3.m2\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">n_{a}</annotation></semantics></math> times\n\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l4\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">4:</span></span><math alttext=\"X\\leftarrow[X,\\ a_{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l4.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">←</mo><mrow><mo stretchy=\"false\">[</mo><mi>X</mi><mo rspace=\"0.667em\">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X\\leftarrow[X,\\ a_{t}]</annotation></semantics></math> <span class=\"ltx_text\" style=\"float:right;\"><math alttext=\"\\triangleright\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l4.m2\" intent=\":literal\"><semantics><mo>⊳</mo><annotation encoding=\"application/x-tex\">\\triangleright</annotation></semantics></math> Action conditioning\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l5\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">5:</span></span><math alttext=\"Z_{1}\\leftarrow W_{att_{1}}X\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l5.m1\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>1</mn></msub><mo stretchy=\"false\">←</mo><mrow><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>t</mi><mn>1</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{1}\\leftarrow W_{att_{1}}X</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l6\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">6:</span></span><math alttext=\"Z_{2}\\leftarrow W_{att_{2}}X\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>2</mn></msub><mo stretchy=\"false\">←</mo><mrow><msub><mi>W</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>t</mi><mn>2</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>X</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{2}\\leftarrow W_{att_{2}}X</annotation></semantics></math> <span class=\"ltx_text\" style=\"float:right;\"><math alttext=\"\\triangleright\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l6.m2\" intent=\":literal\"><semantics><mo>⊳</mo><annotation encoding=\"application/x-tex\">\\triangleright</annotation></semantics></math> <math alttext=\"Z_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l6.m3\" intent=\":literal\"><semantics><msub><mi mathcolor=\"#FF0000\" style=\"--ltx-fg-color:#FF0000;\">Z</mi><mn mathcolor=\"#FF0000\" style=\"--ltx-fg-color:#FF0000;\">2</mn></msub><annotation encoding=\"application/x-tex\">Z_{2}</annotation></semantics></math><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\"> has dimension <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l6.m4\" intent=\":literal\"><semantics><mi mathcolor=\"#FF0000\" style=\"--ltx-fg-color:#FF0000;\">d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span>\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l7\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">7:</span></span><math alttext=\"W\\leftarrow Z_{2}^{\\top}Z_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l7.m1\" intent=\":literal\"><semantics><mrow><mi>W</mi><mo stretchy=\"false\">←</mo><mrow><msubsup><mi>Z</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>Z</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">W\\leftarrow Z_{2}^{\\top}Z_{1}</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l8\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">8:</span></span><math alttext=\"W\\leftarrow\\text{LayerNorm}(\\tanh(W))\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l8.m1\" intent=\":literal\"><semantics><mrow><mi>W</mi><mo stretchy=\"false\">←</mo><mrow><mtext>LayerNorm</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>tanh</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">W\\leftarrow\\text{LayerNorm}(\\tanh(W))</annotation></semantics></math>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l9\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">9:</span></span><math alttext=\"y\\leftarrow Wh_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l9.m1\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo stretchy=\"false\">←</mo><mrow><mi>W</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>h</mi><mi>t</mi><mi>l</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">y\\leftarrow Wh_{t}^{l}</annotation></semantics></math> <span class=\"ltx_text\" style=\"float:right;\"><math alttext=\"\\triangleright\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l9.m2\" intent=\":literal\"><semantics><mo>⊳</mo><annotation encoding=\"application/x-tex\">\\triangleright</annotation></semantics></math> <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l9.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#FF0000\" style=\"--ltx-fg-color:#FF0000;\">y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\"> has dimension <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l9.m4\" intent=\":literal\"><semantics><mi mathcolor=\"#FF0000\" style=\"--ltx-fg-color:#FF0000;\">d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span>\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg2.l10\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">10:</span></span><math alttext=\"Q\\leftarrow\\mathrm{RMSQ}(y)\" class=\"ltx_Math\" display=\"inline\" id=\"alg2.l10.m1\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo stretchy=\"false\">←</mo><mrow><mi>RMSQ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Q\\leftarrow\\mathrm{RMSQ}(y)</annotation></semantics></math>\n</div>\n</div>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Comparison of AD and ARQ implemented on top of AD. For ARQ, action conditioning is applied as part of the input (Line 4,5, Algorithm 2). Note that ARQ allows <math alttext=\"Z_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F3.m5\" intent=\":literal\"><semantics><msub><mi>Z</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">Z_{2}</annotation></semantics></math> and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F3.m6\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> to have dimension <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F3.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>, which can be arbitrary, while AD fixes it at <math alttext=\"n_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F3.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">n_{a}</annotation></semantics></math>, one for each action output.\n</span></figcaption>\n</figure>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Why ARQ benefits local Q-learning?</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\">As demonstrated in Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4.2 Implementation ‣ 4 ARQ: Action-conditioned Root mean squared Q-function ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ARQ allows the hidden output to have arbitrary dimensions. We hypothesize that ARQ’s flexibility to account for arbitrary hidden dimensions allows it to take full advantage of non-linearity within each AD cell. Furthermore, ARQ applies action conditioning at the model input, rather than using vector indices at the output layer as conditioning. We conjecture that this allows the entire module to produce representation specific to each state-action pair, rather than action-agnostic information based on only the observation. Combining these two properties, ARQ exploits the full capacity of the attention-like mechanism that modern local RL methods operates on, allowing greater expressivity of each state-action pair.</p>\n</div>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Benchmarks:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We test ARQ on the MinAtar benchmark <cite class=\"ltx_cite ltx_citemacro_citep\">(Young and Tian, <a class=\"ltx_ref\" href=\"#bib.bib72\" title=\"\">2019</a>)</cite> and the DeepMind Control (DMC) Suite <cite class=\"ltx_cite ltx_citemacro_citep\">(Tassa et al., <a class=\"ltx_ref\" href=\"#bib.bib63\" title=\"\">2018</a>)</cite> following <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>. MinAtar is a miniaturized version of the Atari 2600 games, using 10x10 grids instead of 210x160 frames as inputs. The DMC Suite is a benchmark for continuous control tasks featuring low-level observations and actions, designed to evaluate the performance of RL methods in physics-based environments. Both benchmarks involve low-dimensional inputs and outputs instead of high-dimensional raw sensory inputs, making them appropriate testbeds for evaluating the decision-making ability of local methods in simple environments.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">For comparisons with cutting-edge local RL methods, we compare our results with AD for both benchmarks. To evaluate our methods against backprop-based algorithms, we also compare our method against DQN <cite class=\"ltx_cite ltx_citemacro_citep\">(Mnih et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2015</a>)</cite> for MinAtar. DQN is a widely used baseline that trains deep neural networks to directly compute scalar Q-values through backpropagation. We follow the DQN implementation used by <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"157\" id=\"S5.F4.g1\" src=\"./assets/x3.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nTraining performance on the MinAtar games, compared between DQN (blue), AD (orange), and ARQ(green). The x-axis denotes the number of training steps (in millions), and the y-axis indicates average episodic returns. Shaded regions represent standard deviations across 3 seeds. We find that ARQ consistently outperforms AD in all MinAtar games, while outperforming DQN in most games.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Implementation Details:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Guan et al., (<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>, we use a three-layer fully-connected network, with hidden dimensions being 400, 200, and 200 for MinAtar. We use a three-layer network with hidden dimensions 128, 96, and 96 for DMC tasks. We use a replay buffer and a target network for stability. We incorporate skip connections from the input and top-down connections from the layer above. For all experiments, we use an epsilon-greedy policy with linear decay from 1 to 0.01 using an exploration fraction of 0.1. We run our experiments for 4 million steps, where the model starts learning from step 50,000. Learning rate is set fixed at 1e-4. A batch size of 512 is used. For MinAtar, we condition on action candidates by passing them as one-hot vectors into the network. For DMC tasks, we discretize our action space and condition action vectors as model inputs.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Main Results:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">We present our results in Table <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Game Analysis: ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We run each experiment with three different random seeds and plot their average returns over 100-episode windows along with their standard deviations in shadows. We also calculated the average returns of the last 100 episodes of each training run to obtain a quantitative measure of the final performance of our method, which can be found in Table <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Game Analysis: ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. As demonstrated, ARQ consistently outperforms AD in all MinAtar games. Surprisingly, ARQ also outperforms DQN in all games. In DMC Suite tasks, ARQ achieves superior returns compared to AD, while also exceeding back-prop based methods in most games. We present our numbers in Table <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Game Analysis: ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Game Analysis:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px5.p1\">\n<p class=\"ltx_p\">We note that ARQ outperforms DQN by a wide margin on Breakout and SpaceInvaders. Both of these games operate on similar mechanisms: players aim to remove targets by controlling projectile interactions of objects. To yield higher scores, players need to perform combos of actions to yield higher scores, for instance moving to a sweet spot then waiting for the target to arrive before firing a bullet. We argue that top-down connections in AD provide temporal coherence, which allows our agents to perform sequences of actions smoothly. Additionally, we note that while AD fails to match DQN on Seaquest, ARQ surpasses DQN. Seaquest is a game involving firing bullets to remove enemies, with an additional rule that players need to manage an oxygen tank by surfacing above water to refill their tank. This represents that the policy distribution can be bi-modal such that attacking enemies and refilling tanks are both locally optimal policies. We hypothesize that by applying action conditioning, ARQ can capture these policy structures more effectively than AD, which is only state-conditioned.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T1\">\n<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Previous methods and ARQ compared in MinAtar and DeepMind Control (DMC) tasks. </figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MinAtar</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Freeway</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Breakout</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpaceInvaders</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Seaquest</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Asterix</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/ back-prop</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">DQN</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">55.86 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.32</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">27.09 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 5.74</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">188.03 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 15.81</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">37.96 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 9.28</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">13.60 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">w/o back-prop</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">AD</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">57.64 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.49</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">67.40 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 4.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">369.96 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 23.46</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">30.32 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 4.79</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">24.05 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 5.44</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">lightblue!20\nARQ (Ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">60.77 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m11\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">88.93 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m12\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 5.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">555.29 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m13\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 56.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.81 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m14\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 6.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">37.89 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m15\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DMC</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Walker Walk</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Walker Run</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Hopper Hop</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Cheetah Run</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Reacher Hard</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/ back-prop</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">TD-MPC2</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">958.80 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.29</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">834.07 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 10.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">348.55 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 26.65</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">808.46 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 92.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">934.84 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 6.86</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">SAC</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">980.43 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.63</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">895.02 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 46.35</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">319.46 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 31.21</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">917.40 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 2.45</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">980.01 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.19</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">w/o back-prop</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">AD</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">975.25 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.87</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">761.11 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m27\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.17</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">485.75 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m28\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 57.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">831.57 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m29\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 15.90</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">954.34 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 7.30</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">lightblue!20\nARQ (Ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">976.26 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m31\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 2.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">771.63 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m32\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">515.45 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m33\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 21.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">881.30 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m34\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 17.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">972.45 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m35\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 6.04</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n<figure class=\"ltx_figure\" id=\"S5.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"202\" id=\"S5.F5.g1\" src=\"./assets/x4.png\" width=\"580\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation on action conditioning for AD and ARQ. Action conditioning substantially improves performance. Note that this improvement is particularly significant for ARQ, with average returns of <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.m4\" intent=\":literal\"><semantics><mo>∼</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>85 vs. <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.m5\" intent=\":literal\"><semantics><mo>∼</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>55, a 50<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F5.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> improvement. This indicates that the combination of the RMS function and action conditioning makes ARQ effective.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S5.T2\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Our method Using Different Nonlinearities Compared in MinAtar Breakout. ‘MS’ is short for the mean squared function and ‘Var’ is short for variance. Default ARQ uses the root mean squared (RMS) function.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Nonlinearity</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Breakout</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SpaceInvaders</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours-ARQ</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">88.83<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>8.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">555.29<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>56.97</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours-Mean</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">79.84</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">13.23</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">500.13</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">47.78</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours-MS</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">82.10</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3.28</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">434.88</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">14.37</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ours-Var</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">81.34</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">416.46</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 66.60</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">AD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">67.40 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 4.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">369.96 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 23.46</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px6\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Effect of Action Conditioning at Input:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px6.p1\">\n<p class=\"ltx_p\">How does action conditioning affect the performance of local RL methods? To investigate, we conduct ablation experiments on two games from MinAtar, Breakout and SpaceInvaders, using both AD and ARQ. The results can be found in Figure <a class=\"ltx_ref\" href=\"#S5.F5\" title=\"Figure 5 ‣ Game Analysis: ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We find a significant improvement when actions are conditioned at the input instead of at the output. Interestingly, this design choice provides only a slight improvement for AD, while yielding a significant increase in performance for ARQ. We conjecture this is due to the increase in the capacity of each cell to capture the granularity within each specific state-action pair, while AD saturates with action-agnostic information.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px7\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Effect of Goodness Nonlinearities:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px7.p1\">\n<p class=\"ltx_p\">One question that naturally arises is the choice of the goodness function. Does the RMS function perform superiorly compared to other functions? We ablate on this design choice and conduct experiments on two games from MinAtar, Breakout and SpaceInvaders. As shown in Table <a class=\"ltx_ref\" href=\"#S5.T2\" title=\"Table 2 ‣ Game Analysis: ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we find that using the RMS goodness functions yields superior performance, followed by the mean and the mean squared function. We conjecture that a smaller magnitude in the goodness can enhance stability of training. However, we note that all functions perform superiorly compared with AD, which demonstrates the versatility of our method. We leave it for future work to study the intricate effect each function has on training.</p>\n</div>\n<figure class=\"ltx_table ltx_align_floatright\" id=\"S5.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">AD vs. ARQ Across Multiple Scales for MinAtar Breakout.\n</span></figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Scale Ratio</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARQ</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.5</span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">66.34 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 5.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">68.12 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 5.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1</span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">64.20 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.90</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">86.26 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">56.63 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 5.39</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">70.40 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 3.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2</span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">59.79 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 4.77</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">83.26 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mo>±</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 2.32</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px8\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Is it because ARQ has more hidden units?</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px8.p1\">\n<p class=\"ltx_p\">Compared with AD, ARQ employs a larger number of parameters since ARQ allows an arbitrary dimension for its hidden vectors. Could ARQ, however, simply achieve the same improvement with mere scaling? We conduct experiments on AD and ARQ with the same number of total parameters to answer this question. Across different ratios of total parameters (compared with the original AD as a baseline), we run both AD and ARQ on the MinAtar Breakout game with two different random seeds. As shown in Table <a class=\"ltx_ref\" href=\"#S5.T3\" title=\"Table 3 ‣ Effect of Goodness Nonlinearities: ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, ARQ consistently outperforms AD across all scales. This verifies the effectiveness of our method beyond scale.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"331\" id=\"S5.F6.g1\" src=\"./assets/x5.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nVisualization of Neurons in Layer 0 under Different Scenarios in Breakout Game. 20 neurons w/ highest average activities are visualized. Top Left: When the ball is approaching towards the left side of the brick, neurons show larger magnitude when the action candidate is to “move left”, prompting the agent to move towards the ball. Bottom Left: When the ball approaches the right side of the brick, neurons show larger magnitude when the action candidate is to “move right”. Right: The average root mean squared (RMS) activations of 20 top neurons\nacross 100 states is collected. Note that top neurons exhibit significantly larger RMS activations than the average RMS activation, implying that these neurons are “dominant” neurons. While most neurons demonstrate similar magnitude between both actions, some neurons appear to be more specialized. </span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px9\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Neurons Are Sensitive to Different Scenarios:</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px9.p1\">\n<p class=\"ltx_p\">How does our method learn through a goodness function? We investigate its inner mechanism by visualizing the activations at each layer under different states. As illustrated in Figure <a class=\"ltx_ref\" href=\"#S5.F6\" title=\"Figure 6 ‣ Is it because ARQ has more hidden units? ‣ 5 Experiments ‣ Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we find that the hidden activations tend to show larger magnitudes under “correct” state-action pairs. For instance, in scenarios where the agent should move right to accurately catch the incoming ball, neurons in the hidden activations show the largest magnitude when the action input matches correspondingly. Interestingly, we observe that\ndifferent neurons are, in general, activated to different degrees for various action candidates. This implies our objective function could be encouraging specialized neurons, each of which is responsible for recognizing certain categories of positive signals.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Discussion</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n<p class=\"ltx_p\">Previous studies on biologically plausible learning have largely focused on the search for a biologically plausible mechanism for performing gradient updates. As we approach the era of AIs with agentic learning and experience, we argue that a biologically plausible learning environment can be equally meaningful in guiding us towards understanding the mystery behind how biological brains learn. Reward-centric environments provide a biologically grounded environment that aligns with the evolutionary role of survival signals and behavioral shaping through positive or negative reinforcement. The structure of such environments mirrors the ecological settings in which animals adaptively refine behavior through trial-and-error interactions, suggesting that learning systems shaped by rewards may naturally emerge in both artificial and biological agents. Additionally, temporal difference methods are ideal candidates as there exist evidence showing that biological neurons learn through temporal difference, with hormones conveying the prediction error as a source of the learning signal to independent neurons <cite class=\"ltx_cite ltx_citemacro_citep\">(Schultz et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">1997b</a>; Bayer and Glimcher, <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2005</a>)</cite>. On the other hand, reinforcement learning has largely focused on learning through interactions with an agent’s surrounding environment and maximizing its rewards through centralized value estimation. Yet, increasing neuroscientific evidence has shown that neurons make decentralized, independent value estimations <cite class=\"ltx_cite ltx_citemacro_citep\">(Tsutsui et al., <a class=\"ltx_ref\" href=\"#bib.bib65\" title=\"\">2016</a>; Knutson et al., <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2005</a>)</cite>. Few studies in the RL community have investigated whether this biological phenomenon has practical implications. ARQ is an effort towards this direction, as each cell in our network can be seen as a decentralized value estimator.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S7\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Conclusion</h2>\n<div class=\"ltx_para\" id=\"S7.p1\">\n<p class=\"ltx_p\">This work proposes Action-conditioned Root mean squared Q-function (ARQ), a vector-based alternative to scalar Q-learning for backprop-free local learning. ARQ enables arbitrary hidden dimensions and improved expressivity by extracting value predictions from hidden activations and applying action conditioning at the model input.\nWe show that, when applied on RL environments, ARQ performs superiorly compared to current local methods, while also outperforming backprop-based methods on most games. Whereas current biologically plausible algorithms are mostly based on the supervised setting, our study suggests that exploring local learning within reinforcement learning may provide a promising avenue for future research in both domains.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgement</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">We thank Jonas Guan for his help in reproducing AD. MR is supported by the Institute of Information &amp; Communications Technology Planning Evaluation (IITP) under grant RS-2024-00469482, funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research.\nThe compute is supported by the NYU High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Abdolmaleki et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nAbdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Maximum a posteriori policy optimisation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Amo et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nAmo, R., Matias, S., Yamanaka, A., Tanaka, K. F., Uchida, N., and Watabe-Uchida, M. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">A gradual temporal shift of dopamine responses mirrors the progression of temporal difference error in machine learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature neuroscience</span>, 25(8):1082–1092.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Arslan and Yüksel,  (2016)</span>\n<span class=\"ltx_bibblock\">\nArslan, G. and Yüksel, S. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Decentralized q-learning for stochastic teams and games.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE Transactions on Automatic Control</span>, 62(4):1545–1558.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baydin et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nBaydin, A. G., Pearlmutter, B. A., Syme, D., Wood, F., and Torr, P. H. S. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Gradients without backpropagation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">CoRR</span>, abs/2202.08587.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bayer and Glimcher,  (2005)</span>\n<span class=\"ltx_bibblock\">\nBayer, H. M. and Glimcher, P. W. (2005).\n\n</span>\n<span class=\"ltx_bibblock\">Midbrain dopamine neurons encode a quantitative reward prediction error signal.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neuron</span>, 47(1):129–141.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Belilovsky et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nBelilovsky, E., Eickenberg, M., and Oyallon, E. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Greedy layerwise learning can scale to imagenet.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the 36th International Conference on Machine Learning, ICML</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(7)</span>\n<span class=\"ltx_bibblock\">\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. (2018a).\n\n</span>\n<span class=\"ltx_bibblock\">Implicit quantile networks for distributional reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1096–1105. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(8)</span>\n<span class=\"ltx_bibblock\">\nDabney, W., Rowland, M., Bellemare, M., and Munos, R. (2018b).\n\n</span>\n<span class=\"ltx_bibblock\">Distributional reinforcement learning with quantile regression.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the AAAI conference on artificial intelligence</span>, volume 32.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">De Witt et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nDe Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M., and Whiteson, S. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Is independent learning all you need in the starcraft multi-agent challenge?\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2011.09533</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Foerster et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nFoerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H., Kohli, P., and Whiteson, S. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Stabilising experience replay for deep multi-agent reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1146–1155. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fortunato et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Hessel, M., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Noisy networks for exploration.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fournier et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nFournier, L., Rivaud, S., Belilovsky, E., Eickenberg, M., and Oyallon, E. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Can forward gradient match backpropagation?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Machine Learning</span>, pages 10249–10264. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fujimoto et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nFujimoto, S., Hoof, H., and Meger, D. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Addressing function approximation error in actor-critic methods.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1587–1596. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gruslys et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nGruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare, M., and Munos, R. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guan et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nGuan, J., Verch, S., Voelcker, C., Jackson, E., Papernot, N., and Cunningham, W. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Temporal-difference learning using distributed error signals.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, 37:108710–108734.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(16)</span>\n<span class=\"ltx_bibblock\">\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018a).\n\n</span>\n<span class=\"ltx_bibblock\">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1861–1870. Pmlr.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(17)</span>\n<span class=\"ltx_bibblock\">\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., and Levine, S. (2018b).\n\n</span>\n<span class=\"ltx_bibblock\">Soft actor-critic algorithms and applications.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">CoRR</span>, abs/1812.05905.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hafner et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Dream to control: Learning behaviors by latent imagination.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hafner et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Learning latent dynamics for planning from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 2555–2565. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hafner et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nHafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Mastering atari with discrete world models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hafner et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nHafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">Mastering diverse control tasks through world models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature</span>, 640:647–653.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hansen et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nHansen, N., Su, H., and Wang, X. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">TD-MPC2: Scalable, robust world models for continuous control.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">The Twelfth International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hausknecht and Stone,  (2015)</span>\n<span class=\"ltx_bibblock\">\nHausknecht, M. J. and Stone, P. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Deep recurrent q-learning for partially observable mdps.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">AAAI fall symposia</span>, volume 45, page 141.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Deep residual learning for image recognition.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hessel et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nHessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Rainbow: Combining improvements in deep reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the AAAI conference on artificial intelligence</span>, volume 32.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hinton,  (2022)</span>\n<span class=\"ltx_bibblock\">\nHinton, G. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">The forward-forward algorithm: Some preliminary investigations.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2212.13345</span>, 2(3):5.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Innocenti et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nInnocenti, F., Achour, E. M., and Buckley, C. L. (2025).\n\n</span>\n<span class=\"ltx_bibblock\"><math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib27.m1\" intent=\":literal\"><semantics><mi>μ</mi><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math>pc: Scaling predictive coding to 100+ layer networks.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2505.13124</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang and Lu,  (2023)</span>\n<span class=\"ltx_bibblock\">\nJiang, J. and Lu, Z. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Best possible q-learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2302.01188</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jin et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nJin, C., Liu, Q., Wang, Y., and Yu, T. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">V-learning – a simple, efficient, decentralized algorithm for multiagent RL.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR 2022 Workshop on Gamification and Multiagent Solutions</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Klopf,  (1982)</span>\n<span class=\"ltx_bibblock\">\nKlopf, A. H. (1982).\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">The Hedonistic Neuron: A Theory of Memory, Learning and Intelligence</span>.\n\n</span>\n<span class=\"ltx_bibblock\">Washington : Hemisphere Pub. Corp.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Knutson et al.,  (2005)</span>\n<span class=\"ltx_bibblock\">\nKnutson, B., Taylor, J., Kaufman, M., Peterson, R., and Glover, G. (2005).\n\n</span>\n<span class=\"ltx_bibblock\">Distributed neural representation of expected value.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Neuroscience</span>, 25(19):4806–4812.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lauer and Riedmiller,  (2000)</span>\n<span class=\"ltx_bibblock\">\nLauer, M. and Riedmiller, M. A. (2000).\n\n</span>\n<span class=\"ltx_bibblock\">An algorithm for distributed reinforcement learning in cooperative multi-agent systems.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the seventeenth international conference on machine learning</span>, pages 535–542.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(33)</span>\n<span class=\"ltx_bibblock\">\nLillicrap, T. P., Cownden, D., Tweed, D. B., and Akerman, C. J. (2016a).\n\n</span>\n<span class=\"ltx_bibblock\">Random synaptic feedback weights support error backpropagation for deep learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature communications</span>, 7(1):13276.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(34)</span>\n<span class=\"ltx_bibblock\">\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2016b).\n\n</span>\n<span class=\"ltx_bibblock\">Continuous control with deep reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR (Poster)</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mnih et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Asynchronous methods for deep reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1928–1937. PmLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mnih et al.,  (2015)</span>\n<span class=\"ltx_bibblock\">\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Playing atari with deep reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature</span>, 518:529–533.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nøkland,  (2016)</span>\n<span class=\"ltx_bibblock\">\nNøkland, A. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Direct feedback alignment provides learning in deep neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 29.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nøkland and Eidnes,  (2019)</span>\n<span class=\"ltx_bibblock\">\nNøkland, A. and Eidnes, L. H. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Training neural networks with local error signals.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 4839–4850. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">O’Doherty et al.,  (2003)</span>\n<span class=\"ltx_bibblock\">\nO’Doherty, J. P., Dayan, P., Friston, K., Critchley, H., and Dolan, R. J. (2003).\n\n</span>\n<span class=\"ltx_bibblock\">Temporal difference models and reward-related learning in the human brain.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neuron</span>, 38(2):329–337.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ororbia and Mali,  (2023)</span>\n<span class=\"ltx_bibblock\">\nOrorbia, A. and Mali, A. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">The predictive forward-forward algorithm.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2301.01452</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Palmer et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nPalmer, G., Tuyls, K., Bloembergen, D., and Savani, R. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Lenient multi-agent deep reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems</span>, AAMAS ’18, page 443–451, Richland, SC. International Foundation for Autonomous Agents and Multiagent Systems.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Papachristodoulou et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nPapachristodoulou, A., Kyrkou, C., Timotheou, S., and Theocharides, T. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Convolutional channel-wise competitive learning for the forward-forward algorithm.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 38, pages 14536–14544.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ren et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nRen, M., Kornblith, S., Liao, R., and Hinton, G. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Scaling forward gradient with local losses.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Riedmiller,  (2005)</span>\n<span class=\"ltx_bibblock\">\nRiedmiller, M. (2005).\n\n</span>\n<span class=\"ltx_bibblock\">Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European conference on machine learning</span>, pages 317–328. Springer.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rumelhart et al.,  (1986)</span>\n<span class=\"ltx_bibblock\">\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986).\n\n</span>\n<span class=\"ltx_bibblock\">Learning representations by back-propagating errors.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">nature</span>, 323(6088):533–536.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sacks et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nSacks, J., Rana, R., Huang, K., Spitzer, A., Shi, G., and Boots, B. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Deep model predictive optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">2024 IEEE International Conference on Robotics and Automation (ICRA)</span>, pages 16945–16953. IEEE.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Schrittwieser et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Mastering atari, go, chess and shogi by planning with a learned model.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature</span>, 588(7839):604–609.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(48)</span>\n<span class=\"ltx_bibblock\">\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015a).\n\n</span>\n<span class=\"ltx_bibblock\">Trust region policy optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1889–1897. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(49)</span>\n<span class=\"ltx_bibblock\">\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2015b).\n\n</span>\n<span class=\"ltx_bibblock\">High-dimensional continuous control using generalized advantage estimation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1506.02438</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Schulman et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Proximal policy optimization algorithms.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1707.06347</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(51)</span>\n<span class=\"ltx_bibblock\">\nSchultz, W., Dayan, P., and Montague, P. R. (1997a).\n\n</span>\n<span class=\"ltx_bibblock\">A neural substrate of prediction and reward.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Science</span>, 275(5306):1593–1599.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(52)</span>\n<span class=\"ltx_bibblock\">\nSchultz, W., Dayan, P., and Montague, P. R. (1997b).\n\n</span>\n<span class=\"ltx_bibblock\">A neural substrate of prediction and reward.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Science</span>, 275(5306):1593–1599.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Seyde et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nSeyde, T., Gilitschenski, I., Schwarting, W., Stellato, B., Riedmiller, M., Wulfmeier, M., and Rus, D. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Is bang-bang control all you need? solving continuous control with bernoulli policies.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, 34:27209–27221.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Seyde et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nSeyde, T., Werner, P., Schwarting, W., Gilitschenski, I., Riedmiller, M., Rus, D., and Wulfmeier, M. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Solving continuous control via q-learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">The Eleventh International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Silver et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Mastering the game of go with deep neural networks and tree search.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">nature</span>, 529(7587):484–489.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Silver et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Science</span>, 362(6419):1140–1144.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Singhal et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nSinghal, U., Cheung, B., Chandra, K., Ragan-Kelley, J., Tenenbaum, J. B., Poggio, T. A., and Yu, S. X. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">How to guess a gradient.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2312.04709</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Su and Lu,  (2022)</span>\n<span class=\"ltx_bibblock\">\nSu, K. and Lu, Z. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Decentralized policy optimization.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2211.03032</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Su et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nSu, K., Zhou, S., Jiang, J., Gan, C., Wang, X., and Lu, Z. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Ma2ql: A minimalist approach to fully decentralized multi-agent reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2209.08244</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sun et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nSun, L., Zhang, Y., He, W., Wen, J., Shen, L., and Xie, W. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">Deeperforward: Enhanced forward-forward training for deeper and better performance.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">The Thirteenth International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sutton,  (1988)</span>\n<span class=\"ltx_bibblock\">\nSutton, R. S. (1988).\n\n</span>\n<span class=\"ltx_bibblock\">Learning to predict by the methods of temporal differences.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Machine Learning</span>, 3:9–44.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tan,  (1993)</span>\n<span class=\"ltx_bibblock\">\nTan, M. (1993).\n\n</span>\n<span class=\"ltx_bibblock\">Multi-agent reinforcement learning: independent versus cooperative agents.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML’93: Proceedings of the Tenth International Conference on International Conference on Machine Learning</span>, pages 330–337.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tassa et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Deepmind control suite.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1801.00690</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tosato et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nTosato, N., Basile, L., Ballarin, E., de Alteriis, G., Cazzaniga, A., and Ansuini, A. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Emergent representations in networks trained with the forward-forward algorithm.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tsutsui et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nTsutsui, K.-I., Grabenhorst, F., Kobayashi, S., and Schultz, W. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">A dynamic code for economic object valuation in prefrontal cortex neurons.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature communications</span>, 7(1):12554.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Van Hasselt et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nVan Hasselt, H., Guez, A., and Silver, D. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Deep reinforcement learning with double q-learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the AAAI conference on artificial intelligence</span>, volume 30.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Dueling network architectures for deep reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1995–2003. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Watabe-Uchida et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nWatabe-Uchida, M., Eshel, N., and Uchida, N. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Neural circuitry of reward prediction error.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Annu. Rev. Neurosci.</span>, 40:373–394.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(69)</span>\n<span class=\"ltx_bibblock\">\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2021a).\n\n</span>\n<span class=\"ltx_bibblock\">Mastering visual continuous control: Improved data-augmented reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Deep RL Workshop NeurIPS 2021</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(70)</span>\n<span class=\"ltx_bibblock\">\nYarats, D., Kostrikov, I., and Fergus, R. (2021b).\n\n</span>\n<span class=\"ltx_bibblock\">Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nYe, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Mastering atari games with limited data.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 34:25476–25488.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Young and Tian,  (2019)</span>\n<span class=\"ltx_bibblock\">\nYoung, K. and Tian, T. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1903.03176</span>.\n\n</span>\n</li>\n</ul>\n</section>",
  "css": "",
  "arxiv_id": "2510.06649",
  "source": "arxiv-experimental",
  "generated": "2025-10-18T01:15:13.213Z"
}
