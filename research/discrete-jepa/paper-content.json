{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">The ability to extract meaningful patterns from visual observations and systematically predict future outcomes represents a cornerstone of cognitive intelligence, forming the foundation for what cognitive scientists term System 2 reasoning—deliberate, systematic thinking that enables complex planning and problem-solving <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahneman, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2011</a>; Evans &amp; Stanovich, <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2013</a>; Bengio et al., <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>. In artificial intelligence, this capability translates to the fundamental challenge of developing world models that can perform symbolic abstraction and logical reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(Goyal et al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2021</a>; Sehgal et al., <a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">2023</a>; Tang et al., <a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">2024</a>; Baek et al., <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, enabling agents to plan effectively over extended temporal horizons.</p>\n</div>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Recent advances in image tokenization <cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den Oord et al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2017</a>; Esser et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2021</a>; Ramesh et al., <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2021</a>; Razavi et al., <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2019</a>; Yu et al., <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> and autoregressive modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2021</a>; Chang et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>; Yu et al., <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2023</a>; Yan et al., <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> have demonstrated remarkable progress in visual understanding and generation tasks. However, these approaches primarily focus on patch-level local feature tokenization, which, while effective for reconstruction and generation, exhibit significant limitations when applied to tasks requiring symbolic reasoning and logical planning capabilities. The granular nature of patch-based representations introduces computational overhead and, more critically, fails to capture the high-level semantic abstractions necessary for systematic inference and long-horizon planning.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">Contemporary efforts to address these limitations have explored semantic-level tokenization approaches, such as <cite class=\"ltx_cite ltx_citemacro_citet\">Yu et al. (<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2024</a>); Wu et al. (<a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">2024</a>); Kim et al. (<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2025</a>); Bachmann et al. (<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, which attempt to move beyond patch-level representations toward more meaningful 1D tokenization. However, these methods remain constrained by their reliance on pixel-level reconstruction objectives, resulting in tokens that encode unnecessary visual details rather than the abstract semantic concepts crucial for symbolic reasoning. This fundamental mismatch between the granularity of representation and the requirements of symbolic planning tasks represents a significant barrier to developing truly intelligent visual reasoning systems.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">The Joint-Embedding Predictive Architecture (JEPA) framework <cite class=\"ltx_cite ltx_citemacro_citep\">(LeCun, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2022</a>; Assran et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>; Bardes et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2023b</a>; Sobal et al., <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> offers a promising alternative by learning representations through latent-space prediction rather than pixel-level reconstruction. By predicting masked representations in latent space, <cite class=\"ltx_cite ltx_citemacro_citet\">Assran et al. (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> demonstrates the potential for learning more semantically meaningful features. However, the continuous nature of its representations limits their applicability to autoregressive modeling paradigms, where discrete tokens are essential for effective sequence modeling and long-horizon prediction with reduced accumulated error.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\">To bridge this gap, we propose <span class=\"ltx_text ltx_font_italic\">Discrete-JEPA</span>, a novel extension of the JEPA framework that learns discrete semantic tokens capturing high-level semantic abstractions while preserving the benefits of latent-space predictive learning. Our approach introduces semantic-level vector quantization to the JEPA architecture while maintaining the framework’s core advantage of latent-space predictive learning. Through a carefully designed unified predictive framework, Discrete-JEPA learns to encode global semantic information into discrete tokens while preserving fine-grained spatial details through complementary continuous representations.</p>\n</div>\n<div id=\"S1.p6\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our contributions are threefold: (1) We introduce the Discrete-JEPA architecture, which extends the JEPA framework with semantic tokenization and novel complementary objectives (Semantic-to-Patch, Patch-to-Semantic, and Patch-to-Patch prediction) to learn robust discrete semantic tokens for enhanced representation learning. (2) We demonstrate that Discrete-JEPA significantly outperforms existing baselines across challenging visual symbolic prediction tasks, validating the effectiveness of our semantic tokenization approach. (3) We provide compelling visual evidence of systematic patterns that emerge within the learned semantic token space, offering insights into the model’s representation capabilities and potential for more complex reasoning tasks.</p>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Works</h2>\n\n<div id=\"S2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-supervised Visual Representation Learning.</span> Self-supervised learning has evolved through contrastive learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2020</a>; He et al., <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2020</a>; Caron et al., <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, variance-based regularization <cite class=\"ltx_cite ltx_citemacro_citep\">(Bardes et al., <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>, bootstrap methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Grill et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, self-distillation <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2021</a>; Oquab et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, and masked reconstruction approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2022</a>; Bao et al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2021</a>; Zhou et al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. Recent work has also explored unified multi-modal frameworks <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et al., <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. While these methods achieve strong performance on recognition tasks, they predominantly learn patch-level embeddings optimized for local features rather than the global semantic abstractions required for symbolic reasoning. Our approach addresses this limitation by learning semantic-level discrete tokens that capture high-level conceptual information.</p>\n</div>\n<div id=\"S2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discrete Image Tokenization.</span> Discrete visual representations emerged with VQ-VAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den Oord et al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2017</a>)</cite> and subsequent vector quantization methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Esser et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2021</a>; Yu et al., <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>, enabling token-based autoregressive generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Ramesh et al., <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2021</a>; Chang et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. Building upon these foundations, researchers have developed alternative quantization schemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et al., <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2022</a>; Van Balen &amp; Levy, <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2019</a>; Takida et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2022</a>; Mentzer et al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> and extended tokenization to video domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. More recently, semantic-level approaches have explored 1D tokenization <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2024</a>; Chen et al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2025b</a>; Wang et al., <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2025</a>; Bachmann et al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>. However, reliance on pixel-level reconstruction objectives biases representations toward fine-grained details rather than semantic concepts essential for symbolic reasoning. We overcome this limitation through latent predictive learning that avoids reconstruction bias.</p>\n</div>\n<div id=\"S2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Joint-Embedding Predictive Architectures.</span> JEPA <cite class=\"ltx_cite ltx_citemacro_citep\">(LeCun, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> introduced latent-space prediction as an alternative to pixel reconstruction. I-JEPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Assran et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> demonstrated superior sample efficiency through masked representation prediction, inspiring extensions to audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Fei et al., <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, video <cite class=\"ltx_cite ltx_citemacro_citep\">(Bardes et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2023a</a>)</cite>, multi-modal motion-content learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Bardes et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2023b</a>)</cite>, and diffusion applications <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2025a</a>)</cite>. Despite these advances, continuous representations suffer from accumulated errors in sequential prediction and lack discrete structure necessary for robust symbolic reasoning. Our work extends JEPA with discrete semantic tokenization and complementary predictive objectives to enable stable long-horizon prediction.</p>\n</div>\n<figure id=\"S2.F2\" class=\"ltx_figure\">\n<p class=\"ltx_p ltx_align_center ltx_align_center\"><img src=\"./assets/x2.png\" id=\"S2.F2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"564\" height=\"451\" alt=\"Refer to caption\"></p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span><span class=\"ltx_text ltx_font_bold\">Discrete-JEPA Architecture Overview.</span>\nThe context encoder <math id=\"S2.F2.m9\" class=\"ltx_Math\" alttext=\"f_{\\theta}^{c}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><annotation encoding=\"application/x-tex\">f_{\\theta}^{c}</annotation></semantics></math> takes masked inputs with learnable tokens <math id=\"S2.F2.m10\" class=\"ltx_Math\" alttext=\"z_{s}^{0}\" display=\"inline\"><semantics><msubsup><mi>z</mi><mi>s</mi><mn>0</mn></msubsup><annotation encoding=\"application/x-tex\">z_{s}^{0}</annotation></semantics></math> and generates semantic (<math id=\"S2.F2.m11\" class=\"ltx_Math\" alttext=\"z_{s}\" display=\"inline\"><semantics><msub><mi>z</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">z_{s}</annotation></semantics></math>) and patch (<math id=\"S2.F2.m12\" class=\"ltx_Math\" alttext=\"z_{p}\" display=\"inline\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math>) representations, while the target encoder <math id=\"S2.F2.m13\" class=\"ltx_Math\" alttext=\"f_{\\bar{\\theta}}^{t}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">f_{\\bar{\\theta}}^{t}</annotation></semantics></math> processes the complete image to produce target representations <math id=\"S2.F2.m14\" class=\"ltx_Math\" alttext=\"\\bar{z_{s}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{z_{s}}</annotation></semantics></math> and <math id=\"S2.F2.m15\" class=\"ltx_Math\" alttext=\"\\bar{z_{p}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>z</mi><mi>p</mi></msub><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{z_{p}}</annotation></semantics></math>. Vector quantization (VQ) is applied only to semantic representations to create discrete tokens <math id=\"S2.F2.m16\" class=\"ltx_Math\" alttext=\"z^{\\text{discrete}}_{s}\" display=\"inline\"><semantics><msubsup><mi>z</mi><mi>s</mi><mtext>discrete</mtext></msubsup><annotation encoding=\"application/x-tex\">z^{\\text{discrete}}_{s}</annotation></semantics></math>. Using these discrete semantic tokens and continuous patch tokens, the model performs three complementary prediction tasks (<span class=\"ltx_text ltx_font_typewriter\">S2P</span>, <span class=\"ltx_text ltx_font_typewriter\">P2S</span>, <span class=\"ltx_text ltx_font_typewriter\">P2P</span>) and compares predictions against the target encoder outputs, whose parameters are updated via EMA.</figcaption>\n</figure>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Preliminaries</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Joint-Embedding Predictive Architecture.</span>\nJoint-Embedding Predictive Architecture (JEPA) <cite class=\"ltx_cite ltx_citemacro_citep\">(LeCun, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2022</a>; Assran et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> learns representations by predicting masked portions of the input in representation space rather than pixel space. Specifically, <cite class=\"ltx_cite ltx_citemacro_citet\">Assran et al. (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> employs three key components: a context encoder <math id=\"S3.p1.m1\" class=\"ltx_Math\" alttext=\"f_{\\theta}^{c}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><annotation encoding=\"application/x-tex\">f_{\\theta}^{c}</annotation></semantics></math>, a target encoder <math id=\"S3.p1.m2\" class=\"ltx_Math\" alttext=\"f^{t}_{\\bar{\\theta}}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">f^{t}_{\\bar{\\theta}}</annotation></semantics></math>, and a predictor <math id=\"S3.p1.m3\" class=\"ltx_Math\" alttext=\"g_{\\phi}\" display=\"inline\"><semantics><msub><mi>g</mi><mi>ϕ</mi></msub><annotation encoding=\"application/x-tex\">g_{\\phi}</annotation></semantics></math>.</p>\n</div>\n<div id=\"S3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Given an input image <math id=\"S3.p2.m1\" class=\"ltx_Math\" alttext=\"x\\in\\mathbb{R}^{H\\times W\\times C}\" display=\"inline\"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x\\in\\mathbb{R}^{H\\times W\\times C}</annotation></semantics></math>, the image is divided into patches and processed as follows:</p>\n<ol id=\"S3.I1\" class=\"ltx_enumerate\">\n<li id=\"S3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span> \n<div id=\"S3.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context Processing</span>: Visible patches (context block) <math id=\"S3.I1.i1.p1.m1\" class=\"ltx_Math\" alttext=\"x_{\\mathcal{V}}\" display=\"inline\"><semantics><msub><mi>x</mi><mi class=\"ltx_font_mathcaligraphic\">𝒱</mi></msub><annotation encoding=\"application/x-tex\">x_{\\mathcal{V}}</annotation></semantics></math> are encoded by the context encoder to obtain context representations <math id=\"S3.I1.i1.p1.m2\" class=\"ltx_Math\" alttext=\"z_{c}=f_{\\theta}^{c}(x_{\\mathcal{V}})\" display=\"inline\"><semantics><mrow><msub><mi>z</mi><mi>c</mi></msub><mo>=</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi class=\"ltx_font_mathcaligraphic\">𝒱</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{c}=f_{\\theta}^{c}(x_{\\mathcal{V}})</annotation></semantics></math>.</p>\n</div>\n</li>\n<li id=\"S3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span> \n<div id=\"S3.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Target Processing</span>: The entire image is processed by the target encoder to obtain actual patch representations at target locations <math id=\"S3.I1.i2.p1.m1\" class=\"ltx_Math\" alttext=\"z_{t}=f_{\\bar{\\theta}}^{t}(x_{\\mathcal{M}})\" display=\"inline\"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mrow><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{t}=f_{\\bar{\\theta}}^{t}(x_{\\mathcal{M}})</annotation></semantics></math>.</p>\n</div>\n</li>\n<li id=\"S3.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span> \n<div id=\"S3.I1.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prediction</span>: The predictor takes context representations and target position indices to predict what representations should exist at those target locations: <math id=\"S3.I1.i3.p1.m1\" class=\"ltx_Math\" alttext=\"\\hat{z}_{t}=g_{\\phi}(z_{c},\\mathcal{M})\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>g</mi><mi>ϕ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>c</mi></msub><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{z}_{t}=g_{\\phi}(z_{c},\\mathcal{M})</annotation></semantics></math>, where <math id=\"S3.I1.i3.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathcal{M}\" display=\"inline\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> contains the positional indices of target patches.</p>\n</div>\n</li>\n</ol>\n<p class=\"ltx_p\">The training objective minimizes the L2 distance between predicted and target representations:</p>\n<table id=\"S3.E1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{\\text{I-JEPA}}=\\sum_{i\\in\\mathcal{M}}||f_{\\bar{\\theta}}^{t}(x_{i})-g_{\\phi}(f_{\\theta}^{c}(x_{\\mathcal{V}}),i)||_{2}^{2}\" display=\"block\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext>I-JEPA</mtext></msub><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi></mrow></munder><msubsup><mrow><mo stretchy=\"false\">‖</mo><mrow><mrow><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>g</mi><mi>ϕ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi class=\"ltx_font_mathcaligraphic\">𝒱</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{I-JEPA}}=\\sum_{i\\in\\mathcal{M}}||f_{\\bar{\\theta}}^{t}(x_{i})-g_{\\phi}(f_{\\theta}^{c}(x_{\\mathcal{V}}),i)||_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S3.p2.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> represents the positional index of target patches, and the predictor <math id=\"S3.p2.m3\" class=\"ltx_Math\" alttext=\"g_{\\phi}\" display=\"inline\"><semantics><msub><mi>g</mi><mi>ϕ</mi></msub><annotation encoding=\"application/x-tex\">g_{\\phi}</annotation></semantics></math> takes both the context representations and the target position index <math id=\"S3.p2.m4\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> to predict what should be at that location. The target encoder <math id=\"S3.p2.m5\" class=\"ltx_Math\" alttext=\"f_{\\bar{\\theta}}^{t}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">f_{\\bar{\\theta}}^{t}</annotation></semantics></math> processes the actual patches to provide the ground truth representations for comparison. Crucially, the target encoder parameters are updated via exponential moving average (EMA) of the context encoder, as shown in <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2020</a>; Caron et al., <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</p>\n</div>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Discrete JEPA Tokenization</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We propose Discrete JEPA, which extends the Joint-Embedding Predictive Architecture to learn discrete semantic tokens for symbolic reasoning and long-horizon planning. Our approach discretizes only semantic representations while maintaining continuous patch representations as intermediate features during training.</p>\n</div>\n<div id=\"S4.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">The method comprises three key components: an extended JEPA framework (Section <a href=\"#S4.SS1\" title=\"4.1 Architecture ‣ 4 Discrete JEPA Tokenization ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>), a semantic and patch tokenization strategy (Section <a href=\"#S4.SS2\" title=\"4.2 Semantic and Patch Tokenization ‣ 4 Discrete JEPA Tokenization ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), and complementary predictive objectives (Section <a href=\"#S4.SS3\" title=\"4.3 Complementary Predictive Objectives ‣ 4 Discrete JEPA Tokenization ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>).</p>\n</div>\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Architecture</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our approach builds upon the JEPA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Assran et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, which employs three key components: a context encoder <math id=\"S4.SS1.p1.m1\" class=\"ltx_Math\" alttext=\"f_{\\theta}^{c}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><annotation encoding=\"application/x-tex\">f_{\\theta}^{c}</annotation></semantics></math>, a target encoder <math id=\"S4.SS1.p1.m2\" class=\"ltx_Math\" alttext=\"f^{t}_{\\bar{\\theta}}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">f^{t}_{\\bar{\\theta}}</annotation></semantics></math>, and predictors <math id=\"S4.SS1.p1.m3\" class=\"ltx_Math\" alttext=\"g_{\\phi}\" display=\"inline\"><semantics><msub><mi>g</mi><mi>ϕ</mi></msub><annotation encoding=\"application/x-tex\">g_{\\phi}</annotation></semantics></math>. We extend this architecture to support <span class=\"ltx_text ltx_font_italic\">semantic-level discrete tokenization</span> while preserving the original spatial prediction capabilities.</p>\n</div>\n<div id=\"S4.SS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Given an input image <math id=\"S4.SS1.p2.m1\" class=\"ltx_Math\" alttext=\"x\\in\\mathbb{R}^{H\\times W\\times C}\" display=\"inline\"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">x\\in\\mathbb{R}^{H\\times W\\times C}</annotation></semantics></math>, our Discrete JEPA processes the image with the following components:</p>\n</div>\n<div id=\"S4.SS1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context Encoder</span> <math id=\"S4.SS1.p3.m1\" class=\"ltx_Math\" alttext=\"f_{\\theta}^{c}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><annotation encoding=\"application/x-tex\">f_{\\theta}^{c}</annotation></semantics></math>: Processes visible image patches <math id=\"S4.SS1.p3.m2\" class=\"ltx_Math\" alttext=\"x_{\\mathcal{V}}\" display=\"inline\"><semantics><msub><mi>x</mi><mi class=\"ltx_font_mathcaligraphic\">𝒱</mi></msub><annotation encoding=\"application/x-tex\">x_{\\mathcal{V}}</annotation></semantics></math>, sampled from patched inputs <math id=\"S4.SS1.p3.m3\" class=\"ltx_Math\" alttext=\"\\{x_{i}\\}_{i=0}^{N_{p}}\" display=\"inline\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>N</mi><mi>p</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{x_{i}\\}_{i=0}^{N_{p}}</annotation></semantics></math> according to masking strategies, to obtain semantic and patch-level representations <math id=\"S4.SS1.p3.m4\" class=\"ltx_Math\" alttext=\"z_{s},z_{p}\" display=\"inline\"><semantics><mrow><msub><mi>z</mi><mi>s</mi></msub><mo>,</mo><msub><mi>z</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">z_{s},z_{p}</annotation></semantics></math>:</p>\n<table id=\"S4.E2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E2.m1\" class=\"ltx_Math\" alttext=\"z_{s},z_{p}=f_{\\theta}^{c}(z_{s}^{0},x_{\\mathcal{V}})\" display=\"block\"><semantics><mrow><mrow><msub><mi>z</mi><mi>s</mi></msub><mo>,</mo><msub><mi>z</mi><mi>p</mi></msub></mrow><mo>=</mo><mrow><msubsup><mi>f</mi><mi>θ</mi><mi>c</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>z</mi><mi>s</mi><mn>0</mn></msubsup><mo>,</mo><msub><mi>x</mi><mi class=\"ltx_font_mathcaligraphic\">𝒱</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{s},z_{p}=f_{\\theta}^{c}(z_{s}^{0},x_{\\mathcal{V}})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS1.p3.m5\" class=\"ltx_Math\" alttext=\"z_{s}^{0}\" display=\"inline\"><semantics><msubsup><mi>z</mi><mi>s</mi><mn>0</mn></msubsup><annotation encoding=\"application/x-tex\">z_{s}^{0}</annotation></semantics></math> consists of <math id=\"S4.SS1.p3.m6\" class=\"ltx_Math\" alttext=\"L\" display=\"inline\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> learnable tokens.</p>\n</div>\n<div id=\"S4.SS1.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Target Encoder</span> <math id=\"S4.SS1.p4.m1\" class=\"ltx_Math\" alttext=\"f^{t}_{\\bar{\\theta}}\" display=\"inline\"><semantics><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">f^{t}_{\\bar{\\theta}}</annotation></semantics></math>: Processes the entire image <math id=\"S4.SS1.p4.m2\" class=\"ltx_Math\" alttext=\"x\" display=\"inline\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> along with learnable tokens <math id=\"S4.SS1.p4.m3\" class=\"ltx_Math\" alttext=\"z_{s}^{0}\" display=\"inline\"><semantics><msubsup><mi>z</mi><mi>s</mi><mn>0</mn></msubsup><annotation encoding=\"application/x-tex\">z_{s}^{0}</annotation></semantics></math> to generate target semantic and patch representations <math id=\"S4.SS1.p4.m4\" class=\"ltx_Math\" alttext=\"{\\bar{z}_{s}},{\\bar{z}_{p}}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mi>s</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\bar{z}_{s}},{\\bar{z}_{p}}</annotation></semantics></math>:</p>\n<table id=\"S4.E3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E3.m1\" class=\"ltx_Math\" alttext=\"{\\bar{z}_{s}},{\\bar{z}_{p}}=f_{\\bar{\\theta}}^{t}(z_{s}^{0},x)\" display=\"block\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mi>s</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mi>p</mi></msub></mrow><mo>=</mo><mrow><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>z</mi><mi>s</mi><mn>0</mn></msubsup><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bar{z}_{s}},{\\bar{z}_{p}}=f_{\\bar{\\theta}}^{t}(z_{s}^{0},x)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS1.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vector Quantization</span>: Applies vector quantization to semantic representations from both encoders to obtain discrete semantic tokens using a shared semantic codebook <math id=\"S4.SS1.p5.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{C}_{s}\\in\\mathbb{R}^{K_{s}\\times D_{s}}\" display=\"inline\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mi>s</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>K</mi><mi>s</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>D</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}_{s}\\in\\mathbb{R}^{K_{s}\\times D_{s}}</annotation></semantics></math>:</p>\n<table id=\"S4.E4\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E4.m1\" class=\"ltx_Math\" alttext=\"z_{s}^{\\text{discrete}}=\\text{VQ}(z_{s}),\\quad{\\bar{z}_{s}}^{\\text{discrete}}=\\text{VQ}(\\bar{z_{s}})\" display=\"block\"><semantics><mrow><mrow><msubsup><mi>z</mi><mi>s</mi><mtext>discrete</mtext></msubsup><mo>=</mo><mrow><mtext>VQ</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>s</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mmultiscripts><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mi>s</mi><mrow></mrow><mrow></mrow><mtext>discrete</mtext></mmultiscripts><mo>=</mo><mrow><mtext>VQ</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{s}^{\\text{discrete}}=\\text{VQ}(z_{s}),\\quad{\\bar{z}_{s}}^{\\text{discrete}}=\\text{VQ}(\\bar{z_{s}})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS1.p6\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Predictors</span> <math id=\"S4.SS1.p6.m1\" class=\"ltx_Math\" alttext=\"g_{\\phi}\" display=\"inline\"><semantics><msub><mi>g</mi><mi>ϕ</mi></msub><annotation encoding=\"application/x-tex\">g_{\\phi}</annotation></semantics></math>: Process semantic and patch tokens <math id=\"S4.SS1.p6.m2\" class=\"ltx_Math\" alttext=\"z_{s}^{\\text{discrete}},z_{p}\" display=\"inline\"><semantics><mrow><msubsup><mi>z</mi><mi>s</mi><mtext>discrete</mtext></msubsup><mo>,</mo><msub><mi>z</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">z_{s}^{\\text{discrete}},z_{p}</annotation></semantics></math> with target masks <math id=\"S4.SS1.p6.m3\" class=\"ltx_Math\" alttext=\"\\mathcal{M}\" display=\"inline\"><semantics><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> to generate predictions for their respective objectives:</p>\n<table id=\"S4.E5\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E5.m1\" class=\"ltx_math_unparsed\" alttext=\"\\footnotesize{\\hat{z_{p}}=g_{\\phi}^{\\texttt{S2P}}(z_{s}^{\\text{discrete}},\\mathcal{M}),\\hat{z_{s}}=g_{\\phi}^{\\texttt{P2S}}(z_{p}),\\hat{z_{p}}=g_{\\phi}^{\\texttt{P2P}}(z_{p},\\mathcal{M}))}\" display=\"block\"><semantics><mrow><mover accent=\"true\"><msub><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">p</mi></msub><mo mathsize=\"0.800em\">^</mo></mover><mo mathsize=\"0.800em\">=</mo><msubsup><mi mathsize=\"0.800em\">g</mi><mi mathsize=\"0.800em\">ϕ</mi><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.800em\">S2P</mtext></msubsup><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msubsup><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">s</mi><mtext mathsize=\"0.800em\">discrete</mtext></msubsup><mo mathsize=\"0.800em\">,</mo><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">ℳ</mi><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow><mo mathsize=\"0.800em\">,</mo><mover accent=\"true\"><msub><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">s</mi></msub><mo mathsize=\"0.800em\">^</mo></mover><mo mathsize=\"0.800em\">=</mo><msubsup><mi mathsize=\"0.800em\">g</mi><mi mathsize=\"0.800em\">ϕ</mi><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.800em\">P2S</mtext></msubsup><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msub><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">p</mi></msub><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow><mo mathsize=\"0.800em\">,</mo><mover accent=\"true\"><msub><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">p</mi></msub><mo mathsize=\"0.800em\">^</mo></mover><mo mathsize=\"0.800em\">=</mo><msubsup><mi mathsize=\"0.800em\">g</mi><mi mathsize=\"0.800em\">ϕ</mi><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.800em\">P2P</mtext></msubsup><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msub><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">p</mi></msub><mo mathsize=\"0.800em\">,</mo><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">ℳ</mi><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\footnotesize{\\hat{z_{p}}=g_{\\phi}^{\\texttt{S2P}}(z_{s}^{\\text{discrete}},\\mathcal{M}),\\hat{z_{s}}=g_{\\phi}^{\\texttt{P2S}}(z_{p}),\\hat{z_{p}}=g_{\\phi}^{\\texttt{P2P}}(z_{p},\\mathcal{M}))}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS1.p7\" class=\"ltx_para\">\n<p class=\"ltx_p\">Figure <a href=\"#S2.F2\" title=\"Figure 2 ‣ 2 Related Works ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the complete architecture and information flow of our Discrete JEPA framework.</p>\n</div>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Semantic and Patch Tokenization</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our approach employs two distinct types of tokens, each serving specific functional roles within the learning framework:</p>\n</div>\n<div id=\"S4.SS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Tokens (Discrete).</span> The semantic representation <math id=\"S4.SS2.p2.m1\" class=\"ltx_Math\" alttext=\"\\bar{z_{s}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{z_{s}}</annotation></semantics></math> captures global image context and is discretized through vector quantization to produce discrete semantic tokens. Given the continuous representation <math id=\"S4.SS2.p2.m2\" class=\"ltx_Math\" alttext=\"\\bar{z_{s}}\\in\\mathbb{R}^{D_{s}}\" display=\"inline\"><semantics><mrow><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mi>s</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\bar{z_{s}}\\in\\mathbb{R}^{D_{s}}</annotation></semantics></math> and a learnable codebook <math id=\"S4.SS2.p2.m3\" class=\"ltx_Math\" alttext=\"\\mathcal{C}_{s}={c_{1},c_{2},...,c_{K_{s}}}\\subset\\mathbb{R}^{D_{s}}\" display=\"inline\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mi>s</mi></msub><mo>=</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi></mrow></mrow><mo>,</mo><mrow><msub><mi>c</mi><msub><mi>K</mi><mi>s</mi></msub></msub><mo>⊂</mo><msup><mi>ℝ</mi><msub><mi>D</mi><mi>s</mi></msub></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}_{s}={c_{1},c_{2},...,c_{K_{s}}}\\subset\\mathbb{R}^{D_{s}}</annotation></semantics></math> with <math id=\"S4.SS2.p2.m4\" class=\"ltx_Math\" alttext=\"K_{s}\" display=\"inline\"><semantics><msub><mi>K</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">K_{s}</annotation></semantics></math> prototypes, we find the nearest entry:</p>\n<table id=\"S4.E6\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E6.m1\" class=\"ltx_Math\" alttext=\"\\bar{k}^{*}={\\arg\\min}_{k\\in{1,...,K_{s}}}||\\bar{z_{s}}-c_{k}||_{2}\" display=\"block\"><semantics><mrow><msup><mover accent=\"true\"><mi>k</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace=\"0.167em\">⁡</mo><munder><mi>min</mi><mrow><mi>k</mi><mo>∈</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>K</mi><mi>s</mi></msub></mrow></mrow></munder></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mrow><mo stretchy=\"false\">‖</mo><mrow><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><mo>−</mo><msub><mi>c</mi><mi>k</mi></msub></mrow><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{k}^{*}={\\arg\\min}_{k\\in{1,...,K_{s}}}||\\bar{z_{s}}-c_{k}||_{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">The discrete semantic token is then:</p>\n<table id=\"S4.E7\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E7.m1\" class=\"ltx_Math\" alttext=\"{\\bar{z}}_{s}^{\\text{discrete}}=C({\\bar{k}}^{*})=c_{{\\bar{k}}^{*}}.\" display=\"block\"><semantics><mrow><mrow><msubsup><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mi>s</mi><mtext>discrete</mtext></msubsup><mo>=</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>k</mi><mo>¯</mo></mover><mo>∗</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>c</mi><msup><mover accent=\"true\"><mi>k</mi><mo>¯</mo></mover><mo>∗</mo></msup></msub></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">{\\bar{z}}_{s}^{\\text{discrete}}=C({\\bar{k}}^{*})=c_{{\\bar{k}}^{*}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(7)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">These discrete tokens serve as the primary output for downstream symbolic reasoning and long-horizon planning tasks. For training, we follow standard vector quantization procedures with commitment loss and exponential moving average updates, following <cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den Oord et al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2017</a>; Esser et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</p>\n</div>\n<div id=\"S4.SS2.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Patch Tokens (Continuous).</span> We maintain continuous patch tokens <math id=\"S4.SS2.p4.m1\" class=\"ltx_Math\" alttext=\"\\bar{z_{p}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>z</mi><mi>p</mi></msub><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{z_{p}}</annotation></semantics></math> that capture fine-grained spatial details from the encoder <math id=\"S4.SS2.p4.m2\" class=\"ltx_Math\" alttext=\"f_{\\bar{\\theta}}^{t}(z_{s}^{0},x)\" display=\"inline\"><semantics><mrow><msubsup><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>¯</mo></mover><mi>t</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>z</mi><mi>s</mi><mn>0</mn></msubsup><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{\\bar{\\theta}}^{t}(z_{s}^{0},x)</annotation></semantics></math>.\nUnlike discrete semantic tokens, patch tokens remain continuous and serve exclusively as intermediate representations during training. These continuous tokens facilitate effective information flow between semantic and spatial levels through our unified predictive framework, but are not used in the final tokenized output.\n</p>\n</div>\n<div id=\"S4.SS2.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic-Patch Interaction. </span> The interaction between discrete semantic tokens (<math id=\"S4.SS2.p5.m1\" class=\"ltx_Math\" alttext=\"\\bar{z_{s}}^{\\text{discrete}}\" display=\"inline\"><semantics><msup><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><mtext>discrete</mtext></msup><annotation encoding=\"application/x-tex\">\\bar{z_{s}}^{\\text{discrete}}</annotation></semantics></math>) and continuous patch tokens (<math id=\"S4.SS2.p5.m2\" class=\"ltx_Math\" alttext=\"\\bar{z_{p}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>z</mi><mi>p</mi></msub><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{z_{p}}</annotation></semantics></math>) from encoders forms the foundation for our unified predictive training framework. Discrete semantic tokens provide global context that guides spatial prediction, while continuous patch tokens contribute local details that enhance semantic understanding. This bidirectional relationship enables effective learning between global and local representations, setting the stage for the complementary predictive objectives detailed in the following section.</p>\n</div>\n<figure id=\"S4.F3\" class=\"ltx_figure\">\n<p class=\"ltx_p ltx_align_center\"><img src=\"./assets/x3.png\" id=\"S4.F3.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"830\" height=\"213\" alt=\"Refer to caption\"></p>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span><span class=\"ltx_text ltx_font_bold\">Long-horizon prediction performance on Dancing-Sprites-Pattern dataset.</span> Performance comparison across color (left), shape (center), and position (right) prediction tasks over 200 rollout steps. Discrete-JEPA maintains stable performance while I-JEPA variants degrade over time due to accumulated errors in continuous space. D-JEPA achieves perfect color prediction stability, highlighting the benefits of discrete semantic tokenization for symbolic reasoning tasks.\n</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Complementary Predictive Objectives</h3>\n\n<div id=\"S4.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We introduce three predictive objectives that operate between discrete semantic tokens and continuous patch tokens, each serving a distinct role in learning meaningful discrete semantic tokens:</p>\n</div>\n<div id=\"S4.SS3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic-to-Patch (<span class=\"ltx_text ltx_font_typewriter\">S2P</span>) Prediction.</span>\nThe <span class=\"ltx_text ltx_font_typewriter\">S2P</span> objective encourages discrete semantic tokens to encode sufficient global context by predicting continuous patch tokens at target locations:</p>\n<table id=\"S4.E8\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E8.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{\\texttt{S2P}}=\\sum_{i\\in\\mathcal{M}}||\\bar{z_{p}}^{(i)}-g_{\\phi}^{\\texttt{S2P}}(z_{s}^{\\text{discrete}},i)||_{2}^{2}.\" display=\"block\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext class=\"ltx_mathvariant_monospace\">S2P</mtext></msub><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi></mrow></munder><msubsup><mrow><mo stretchy=\"false\">‖</mo><mrow><msup><mover accent=\"true\"><msub><mi>z</mi><mi>p</mi></msub><mo>¯</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>−</mo><mrow><msubsup><mi>g</mi><mi>ϕ</mi><mtext class=\"ltx_mathvariant_monospace\">S2P</mtext></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>z</mi><mi>s</mi><mtext>discrete</mtext></msubsup><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\texttt{S2P}}=\\sum_{i\\in\\mathcal{M}}||\\bar{z_{p}}^{(i)}-g_{\\phi}^{\\texttt{S2P}}(z_{s}^{\\text{discrete}},i)||_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(8)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS3.p2.m1\" class=\"ltx_Math\" alttext=\"z_{s}^{\\text{discrete}}\" display=\"inline\"><semantics><msubsup><mi>z</mi><mi>s</mi><mtext>discrete</mtext></msubsup><annotation encoding=\"application/x-tex\">z_{s}^{\\text{discrete}}</annotation></semantics></math> is the discrete semantic token and <math id=\"S4.SS3.p2.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> encodes the spatial position. This objective enables the model to learn how global semantic information relates to local spatial details.</p>\n</div>\n<div id=\"S4.SS3.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Patch-to-Semantic (<span class=\"ltx_text ltx_font_typewriter\">P2S</span>) Prediction.</span>\nThe <span class=\"ltx_text ltx_font_typewriter\">P2S</span> objective learns to extract semantic abstractions from continuous patch tokens:</p>\n<table id=\"S4.E9\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E9.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{\\texttt{P2S}}=||\\bar{z_{s}}-g_{\\phi}^{\\texttt{P2S}}(z_{p})||_{2}^{2}.\" display=\"block\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext class=\"ltx_mathvariant_monospace\">P2S</mtext></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">‖</mo><mrow><mover accent=\"true\"><msub><mi>z</mi><mi>s</mi></msub><mo>¯</mo></mover><mo>−</mo><mrow><msubsup><mi>g</mi><mi>ϕ</mi><mtext class=\"ltx_mathvariant_monospace\">P2S</mtext></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\texttt{P2S}}=||\\bar{z_{s}}-g_{\\phi}^{\\texttt{P2S}}(z_{p})||_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(9)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">This objective encourages continuous patch tokens to contribute meaningfully to global semantic understanding, ensuring consistency between continuous and discrete token representations.</p>\n</div>\n<div id=\"S4.SS3.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Patch-to-Patch (<span class=\"ltx_text ltx_font_typewriter\">P2P</span>) Prediction.</span>\nThe P2P objective maintains spatial coherence by predicting continuous patch tokens from other continuous patch tokens, following the original JEPA framework:</p>\n<table id=\"S4.E10\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E10.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{\\texttt{P2P}}=\\sum_{i\\in\\mathcal{M}}||\\bar{z_{p}}^{(i)}-g_{\\phi}^{\\texttt{P2P}}(z_{p},i)||_{2}^{2}.\" display=\"block\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext class=\"ltx_mathvariant_monospace\">P2P</mtext></msub><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>∈</mo><mi class=\"ltx_font_mathcaligraphic\">ℳ</mi></mrow></munder><msubsup><mrow><mo stretchy=\"false\">‖</mo><mrow><msup><mover accent=\"true\"><msub><mi>z</mi><mi>p</mi></msub><mo>¯</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>−</mo><mrow><msubsup><mi>g</mi><mi>ϕ</mi><mtext class=\"ltx_mathvariant_monospace\">P2P</mtext></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>p</mi></msub><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\texttt{P2P}}=\\sum_{i\\in\\mathcal{M}}||\\bar{z_{p}}^{(i)}-g_{\\phi}^{\\texttt{P2P}}(z_{p},i)||_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(10)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">This objective ensures that our extension preserves the spatial prediction capabilities of the original JEPA framework.</p>\n</div>\n<div id=\"S4.SS3.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Training Objective.</span> The complete training objective combines all predictive losses with the vector quantization commitment loss:</p>\n<table id=\"S4.E11\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E11.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{\\text{total}}=\\lambda_{1}\\mathcal{L}_{\\texttt{S2P}}+\\lambda_{2}\\mathcal{L}_{\\texttt{P2S}}+\\lambda_{3}\\mathcal{L}_{\\texttt{P2P}}+\\mathcal{L}_{\\text{VQ}}.\" display=\"block\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext>total</mtext></msub><mo>=</mo><mrow><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext class=\"ltx_mathvariant_monospace\">S2P</mtext></msub></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext class=\"ltx_mathvariant_monospace\">P2S</mtext></msub></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext class=\"ltx_mathvariant_monospace\">P2P</mtext></msub></mrow><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext>VQ</mtext></msub></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{total}}=\\lambda_{1}\\mathcal{L}_{\\texttt{S2P}}+\\lambda_{2}\\mathcal{L}_{\\texttt{P2S}}+\\lambda_{3}\\mathcal{L}_{\\texttt{P2P}}+\\mathcal{L}_{\\text{VQ}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(11)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS3.p5.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{\\text{VQ}}\" display=\"inline\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mtext>VQ</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{VQ}}</annotation></semantics></math> includes the standard VQ commitment loss for the discrete semantic tokens. This unified predictive framework enables the learning of discrete semantic tokens that effectively capture global context, while continuous patch tokens provide detailed local information for complex reasoning tasks.</p>\n</div>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets &amp; Evaluation Protocol.</span> We evaluate Discrete-JEPA on two challenging visual sequence prediction tasks designed to assess symbolic reasoning and long-horizon planning capabilities. (1) <span class=\"ltx_text ltx_font_italic\">Dancing-Sprites-Pattern</span> consists of image sequences featuring a single object that follows various color transition patterns (<span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Linear</span>, <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Repeat-2</span>, <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Zigzag-3</span>, <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Repeat-3</span>). Given 4 conditioning frames, we evaluate long-horizon prediction performance over approximately 200 time steps, measuring accuracy on color, shape, and position property classification tasks. (2) <span class=\"ltx_text ltx_font_italic\">Blinking-Ball</span> features sequences with four balls exhibiting interacting position and color patterns, requiring simultaneous tracking of spatial and chromatic dependencies. We assess prediction capabilities over approximately 1,000 rollout steps, measuring performance through pixel-wise reconstruction accuracy.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Both datasets provide controlled environments for evaluating symbolic reasoning capabilities while maintaining sufficient complexity to effectively distinguish between different tokenization approaches. Detailed dataset specifications and evaluation protocols are provided in Appendix <a href=\"#A1\" title=\"Appendix A Additional Details of Dataset Design ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<div id=\"S5.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span> We compare Discrete-JEPA against I-JEPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Assran et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> as our primary baseline. I-JEPA represents the most direct comparison as it shares the same underlying architectural framework but operates with continuous representations rather than discrete tokens. For fair comparison, we adapt I-JEPA to the sequential prediction setting by training autoregressive world models on the continuous representations learned by I-JEPA. This baseline allows us to isolate the specific contribution of discrete semantic tokenization while controlling for architectural differences.</p>\n</div>\n<div id=\"S5.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details.</span> Our implementation extends the I-JEPA framework with semantic tokenization and complementary prediction objectives. We train autoregressive world models using standard Vision Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy et al., <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> for long-horizon sequence prediction tasks. Complete implementation details, hyperparameters, and training configurations are provided in Appendix <a href=\"#A2\" title=\"Appendix B Additional Implementation Details ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n<section id=\"S5.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span>Main Results</h3>\n\n<section id=\"S5.SS1.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.1.1 </span>Long-Horizon Symbolic Prediction Tasks</h4>\n\n<div id=\"S5.SS1.SSS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discrete Tokenization Mitigates Accumulated Prediction Errors.</span> A fundamental advantage of Discrete-JEPA emerges in its ability to prevent error accumulation over extended prediction horizons. By operating in a constrained discrete index space rather than continuous representations, Discrete-JEPA eliminates the compounding errors that plague continuous prediction approaches. This is demonstrated in Dancing-Sprites-Pattern color prediction, where Discrete-JEPA maintains perfect accuracy (1.0) across 200 timesteps while I-JEPA variants show substantial degradation (Figure <a href=\"#S4.F3\" title=\"Figure 3 ‣ 4.2 Semantic and Patch Tokenization ‣ 4 Discrete JEPA Tokenization ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), and in Blinking-Ball, where Discrete-JEPA stabilizes while I-JEPA exhibits continuous decline (Figure <a href=\"#S5.F4\" title=\"Figure 4 ‣ 5.1.1 Long-Horizon Symbolic Prediction Tasks ‣ 5.1 Main Results ‣ 5 Experiments ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.1.1 Long-Horizon Symbolic Prediction Tasks ‣ 5.1 Main Results ‣ 5 Experiments ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n</div>\n<div id=\"S5.SS1.SSS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Abstraction Enables Robust Pattern Recognition.</span> Discrete-JEPA’s semantic tokens, which integrate information across spatial patches, demonstrate superior capability for tasks requiring holistic understanding. This advantage is particularly evident in shape prediction tasks within Dancing-Sprites-Pattern, where semantic abstraction enables robust recognition of object-level properties. The approach effectively balances the need for high-level abstraction with sufficient detail retention for symbolic pattern modeling.</p>\n</div>\n<div id=\"S5.SS1.SSS1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Trade-off Between Abstraction and Spatial Precision.</span> While discrete semantic tokenization provides substantial benefits for symbolic reasoning, it involves a deliberate trade-off with fine-grained spatial information. This trade-off manifests in position prediction tasks, where I-JEPA (Concat) initially outperforms Discrete-JEPA due to explicit patch-level spatial encoding. However, the superior long-horizon stability of discrete approaches ultimately proves more valuable for extended sequence modeling. The multi-object complexity in Blinking-Ball further illustrates this trade-off, where Discrete-JEPA shows initial performance adjustment before achieving stable prediction, reflecting the increased demands of detailed positional reasoning in complex scenes.</p>\n</div>\n<figure id=\"S5.F4\" class=\"ltx_figure\"><img src=\"./assets/x4.png\" id=\"S5.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"706\" height=\"561\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span><span class=\"ltx_text ltx_font_bold\">Long-horizon prediction on Blinking-Ball task.</span> Discrete-JEPA maintains stable performance while I-JEPA degrades due to accumulated prediction errors, illustrating the benefits of discrete semantic tokenization for long-horizon sequence modeling.</figcaption>\n</figure>\n<figure id=\"S5.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span><span class=\"ltx_text ltx_font_bold\">Blinking-Ball long-horizon prediction metrics.</span> I-JEPA shows better initial performance but continuous degradation, while Discrete-JEPA stabilizes after step 50 with superior long-horizon results (6× better LPIPS, 5× better MSE at 1000 steps).</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"8\">Rollout Steps</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\">Metric</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">10</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">20</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">50</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">100</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">200</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">400</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">800</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t\">1000</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LPIPS(<math id=\"S5.T1.m1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics><mo stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Ours</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.0028</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.0052</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0099</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0134</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0189</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0216</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0245</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0242</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">I-JEPA</td>\n</tr>\n</table>\n</th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">0.0024</span></td>\n<td class=\"ltx_td ltx_align_left\">0.0114</td>\n<td class=\"ltx_td ltx_align_left\">0.0290</td>\n<td class=\"ltx_td ltx_align_left\">0.0578</td>\n<td class=\"ltx_td ltx_align_left\">0.1205</td>\n<td class=\"ltx_td ltx_align_left\">0.1538</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">0.1554</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">MSE(<math id=\"S5.T1.m2\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics><mo stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Ours</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.046</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.079</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.138</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.174</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.235</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.263</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.293</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.289</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><math id=\"S5.T1.m3\" class=\"ltx_math_unparsed\" alttext=\"(\\times 10^{-2})\" display=\"inline\"><semantics><mrow><mo maxsize=\"0.500em\" minsize=\"0.500em\">(</mo><mo lspace=\"0em\" mathsize=\"0.500em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.500em\">10</mn><mrow><mo mathsize=\"0.500em\">−</mo><mn mathsize=\"0.500em\">2</mn></mrow></msup><mo maxsize=\"0.500em\" minsize=\"0.500em\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\times 10^{-2})</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">I-JEPA</td>\n</tr>\n</table>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.003</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.031</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.131</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">0.337</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">0.654</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">1.263</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">1.449</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\">1.461</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section id=\"S5.SS1.SSS2\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.1.2 </span>Visualization of Planning on Semantic Space</h4>\n\n<div id=\"S5.SS1.SSS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Systematic Pattern Maintenance vs. Reactive Prediction.</span> Beyond quantitative performance metrics, Discrete-JEPA exhibits qualitatively distinct prediction behavior that suggests systematic planning capabilities rather than myopic next-step prediction. Figure <a href=\"#S5.F5\" title=\"Figure 5 ‣ 5.1.2 Visualization of Planning on Semantic Space ‣ 5.1 Main Results ‣ 5 Experiments ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reveals this through extended sequence visualization on the Blinking-Ball task, where Discrete-JEPA maintains coherent pattern integrity throughout 1,000 timesteps while I-JEPA breaks systematic consistency around t=600 despite initially accurate predictions. This divergence indicates that Discrete-JEPA operates through deliberate pattern-based reasoning rather than reactive prediction.</p>\n</div>\n<div id=\"S5.SS1.SSS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evidence of Deliberate Reasoning in Semantic Token Space.</span> The preserved pattern consistency in Discrete-JEPA’s predictions provides compelling evidence of Symbolic reasoning within the learned semantic token space. While I-JEPA’s early accuracy suggests local prediction competence, its eventual pattern breakdown reveals the limitations of continuous representation for maintaining global symbolic consistency. In contrast, Discrete-JEPA’s sustained adherence to underlying symbolic rules demonstrates that semantic tokenization enables the model to internalize and execute systematic reasoning processes, moving beyond immediate sensory-motor responses toward planned, rule-based behavior characteristic of deliberate cognitive processes.</p>\n</div>\n<figure id=\"S5.F5\" class=\"ltx_figure\">\n<p class=\"ltx_p ltx_align_center ltx_align_center\"><img src=\"./assets/x5.png\" id=\"S5.F5.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"846\" height=\"353\" alt=\"Refer to caption\"></p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span><span class=\"ltx_text ltx_font_bold\">Visualization of Semantic Planning on Blinking Ball.</span> Long-horizon predictions over 1,000 timesteps. I-JEPA breaks pattern consistency around t=600 despite initial accuracy, while Discrete-JEPA maintains systematic pattern integrity throughout, demonstrating deliberate planning in semantic token space. Additional visualization examples are provided in Appendix <a href=\"#A3.T3\" title=\"Table 3 ‣ Appendix C Additional Visualization Results for Semantic Planning ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</figcaption>\n</figure>\n</section>\n</section>\n</section>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Limitations and Future Work</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our approach presents several key limitations that open avenues for future research. (1) <span class=\"ltx_text ltx_font_italic\">Abstraction-Precision Trade-off</span>: Discrete semantic tokens excel at capturing high-level patterns but sacrifice fine-grained spatial information, evident in position prediction tasks where I-JEPA initially outperforms our method. (2) <span class=\"ltx_text ltx_font_italic\">Limited Scope</span>: Our evaluation focuses on controlled synthetic datasets that, while enabling precise assessment of symbolic reasoning, may not capture real-world complexity. (3) <span class=\"ltx_text ltx_font_italic\">Baseline Coverage</span>: Comparisons primarily involve I-JEPA, limiting our understanding relative to other contemporary tokenization approaches like VQGAN or TiTok.</p>\n</div>\n<div id=\"S6.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Future work should address these limitations through several promising directions. (1) <span class=\"ltx_text ltx_font_italic\">Real-world Applications</span>: Evaluating Discrete-JEPA on robotics planning and complex video understanding tasks would validate its practical utility beyond controlled settings. (2) <span class=\"ltx_text ltx_font_italic\">Hierarchical Representation</span>: Developing multi-level semantic abstraction could address the abstraction-precision trade-off by maintaining tokens at different granularities.</p>\n</div>\n<div id=\"S6.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">Despite these limitations, our work establishes a promising foundation for advancing discrete semantic tokenization in latent predictive coding approaches, with demonstrated benefits for long-horizon prediction and compelling evidence of systematic reasoning capabilities.</p>\n</div>\n</section>\n<section id=\"S7\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Conclusion</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">This work addresses a key challenge in current visual representation learning: limitations of existing tokenization methods to effectively support symbolic reasoning and long-horizon planning. While recent advances in image tokenization have achieved remarkable success in reconstruction and generation tasks, they fail to capture the high-level semantic abstractions necessary for systematic inference and deliberate planning—capabilities central to cognitive intelligence.</p>\n</div>\n<div id=\"S7.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We introduce Discrete-JEPA, which extends the Joint-Embedding Predictive Architecture with semantic-level discrete tokenization and complementary predictive objectives. Our approach learns discrete semantic tokens that capture global image context while preserving the benefits of latent-space predictive learning. Through carefully designed <span class=\"ltx_text ltx_font_typewriter\">S2P</span>, <span class=\"ltx_text ltx_font_typewriter\">P2S</span>, and <span class=\"ltx_text ltx_font_typewriter\">P2P</span> objectives, Discrete-JEPA enables effective information flow between semantic and spatial representations, resulting in robust tokenization for symbolic reasoning tasks.</p>\n</div>\n<div id=\"S7.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our experimental evaluation demonstrates clear advantages of discrete semantic tokenization over existing methods. On challenging visual sequence prediction tasks, Discrete-JEPA significantly outperforms I-JEPA baselines, maintaining stable performance over extended horizons while continuous methods suffer from accumulated prediction errors. Most notably, our visualization analysis reveals compelling evidence of systematic pattern maintenance and deliberate reasoning behavior within the learned semantic token space—suggesting the emergence of semantic planning capabilities.</p>\n</div>\n<div id=\"S7.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">While our current evaluation focuses on controlled synthetic environments, the demonstrated capabilities suggest potential applications in robotics planning and multi-modal reasoning,\nthough further evaluation on real-world tasks is needed. Discrete-JEPA represents a meaningful step toward developing AI systems that can perform deliberate reasoning rather than purely reactive prediction, establishing a foundation for future research in symbolic visual reasoning.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Accessibility</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We have embedded figures as text-readable PDF files wherever possible, including the use of vector graphics to support compatibility with screen readers and ensure clarity when zoomed. This manuscript has been prepared using the official ICML style files, which incorporate accessibility-focused formatting. Furthermore, we have provided explanatory text for all figures and tables in addition to their captions, to facilitate understanding for readers using assistive technologies.</p>\n</div>\n</section>\n<section id=\"Sx2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgements</h2>\n\n<div id=\"Sx2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">This work was supported by Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2024-00509279, Global AI Frontier Lab). CH is supported by the DoD NDSEG Fellowship.</p>\n</div>\n</section>\n<section id=\"Sx3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Impact Statement</h2>\n\n<div id=\"Sx3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our work advances AI systems with enhanced symbolic visual reasoning capabilities, offering significant benefits through improved systematic reasoning abilities. The discretization of latent visual representations enables a better understanding of AI visual processing, contributing to more trustworthy and coherent systems. These capabilities hold promise, particularly for scientific research, safer autonomous driving, and systematic logical reasoning tasks.</p>\n</div>\n<div id=\"Sx3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">However, enhanced symbolic reasoning and planning abilities carry inherent risks requiring careful consideration. More capable AI agents could potentially be misused for surveillance, information manipulation, or autonomous decision-making without oversight. Improved planning capabilities might enable systems to pursue objectives in unexpected ways if misaligned. While our work represents early-stage research in highly synthetic and controlled settings, we encourage continued development of safety frameworks, noting that improved interpretability came with discretization, may facilitate better monitoring of future AI system behavior.</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Assran et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nAssran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning from images with a joint-embedding predictive architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  15619–15629, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bachmann et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nBachmann, R., Allardice, J., Mizrahi, D., Fini, E., Kar, O. F., Amirloo, E., El-Nouby, A., Zamir, A., and Dehghan, A.\n\n</span>\n<span class=\"ltx_bibblock\">Flextok: Resampling images into 1d token sequences of flexible length.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2502.13967</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baek et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nBaek, J., Wu, Y.-F., Singh, G., and Ahn, S.\n\n</span>\n<span class=\"ltx_bibblock\">Dreamweaver: Learning compositional world models from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Thirteenth International Conference on Learning Representations</em>, 2025.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://openreview.net/forum?id=e5mTvjXG9u\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://openreview.net/forum?id=e5mTvjXG9u</a>.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baevski et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nBaevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M.\n\n</span>\n<span class=\"ltx_bibblock\">Data2vec: A general framework for self-supervised learning in speech, vision and language.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pp.  1298–1312. PMLR, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bao et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nBao, H., Dong, L., Piao, S., and Wei, F.\n\n</span>\n<span class=\"ltx_bibblock\">Beit: Bert pre-training of image transformers.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2106.08254</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Ponce, J., and LeCun, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Vicreg: Variance-invariance-covariance regularization for self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2105.04906</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al. (2023a)</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., Assran, M., and Ballas, N.\n\n</span>\n<span class=\"ltx_bibblock\">V-jepa: Latent video prediction for visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">2023a.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al. (2023b)</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Ponce, J., and LeCun, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Mc-jepa: A joint-embedding predictive architecture for self-supervised learning of motion and content features.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2307.12698</em>, 2023b.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bengio et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nBengio, Y. et al.\n\n</span>\n<span class=\"ltx_bibblock\">From system 1 deep learning to system 2 deep learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Neural Information Processing Systems</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of visual features by contrasting cluster assignments.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 33:9912–9924, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nCaron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A.\n\n</span>\n<span class=\"ltx_bibblock\">Emerging properties in self-supervised vision transformers.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  9650–9660, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chang et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T.\n\n</span>\n<span class=\"ltx_bibblock\">Maskgit: Masked generative image transformer.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  11315–11325, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2025a)</span>\n<span class=\"ltx_bibblock\">\nChen, D., Hu, J., Wei, X., and Wu, E.\n\n</span>\n<span class=\"ltx_bibblock\">Denoising with a joint-embedding predictive architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Thirteenth International Conference on Learning Representations</em>, 2025a.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://openreview.net/forum?id=d4njmzM7jf\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://openreview.net/forum?id=d4njmzM7jf</a>.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2025b)</span>\n<span class=\"ltx_bibblock\">\nChen, H., Han, Y., Chen, F., Li, X., Wang, Y., Wang, J., Wang, Z., Liu, Z., Zou, D., and Raj, B.\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders are effective tokenizers for diffusion models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2025b.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G.\n\n</span>\n<span class=\"ltx_bibblock\">A simple framework for contrastive learning of visual representations.\n\n</span>\n<span class=\"ltx_bibblock\">In III, H. D. and Singh, A. (eds.), <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 37th International Conference on Machine Learning</em>, volume 119 of <em class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning Research</em>, pp.  1597–1607. PMLR, 13–18 Jul 2020.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://proceedings.mlr.press/v119/chen20j.html\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://proceedings.mlr.press/v119/chen20j.html</a>.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dosovitskiy et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 16x16 words: Transformers for image recognition at scale.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2010.11929</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Esser et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nEsser, P., Rombach, R., and Ommer, B.\n\n</span>\n<span class=\"ltx_bibblock\">Taming transformers for high-resolution image synthesis.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  12873–12883, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Evans &amp; Stanovich (2013)</span>\n<span class=\"ltx_bibblock\">\nEvans, J. S. B. and Stanovich, K. E.\n\n</span>\n<span class=\"ltx_bibblock\">Dual-process theories of higher cognition: Advancing the debate.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Perspectives on psychological science</em>, 8(3):223–241, 2013.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fei et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nFei, Z., Fan, M., and Huang, J.\n\n</span>\n<span class=\"ltx_bibblock\">A-jepa: Joint-embedding predictive architecture can listen.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2311.15830</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Goyal et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nGoyal, A., Didolkar, A., Ke, N. R., Blundell, C., Beaudoin, P., Heess, N., Mozer, M. C., and Bengio, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Neural production systems.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 34:25673–25687, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grill et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nGrill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Bootstrap your own latent-a new approach to self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 33:21271–21284, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.\n\n</span>\n<span class=\"ltx_bibblock\">Momentum contrast for unsupervised visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  9729–9738, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nHe, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R.\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders are scalable vision learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  16000–16009, June 2022.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJiang, J., Deng, F., Singh, G., Lee, M., and Ahn, S.\n\n</span>\n<span class=\"ltx_bibblock\">Slot state space models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, 2024.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://openreview.net/forum?id=BJv1t4XNJW\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://openreview.net/forum?id=BJv1t4XNJW</a>.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kahneman (2011)</span>\n<span class=\"ltx_bibblock\">\nKahneman, D.\n\n</span>\n<span class=\"ltx_bibblock\">Thinking, fast and slow.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Farrar, Straus and Giroux</em>, 2011.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kim et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nKim, D., He, J., Yu, Q., Yang, C., Shen, X., Kwak, S., and Chen, L.-C.\n\n</span>\n<span class=\"ltx_bibblock\">Democratizing text-to-image masked generative models with compact text-aware one-dimensional tokens.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2501.07730</em>, 2025.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">LeCun (2022)</span>\n<span class=\"ltx_bibblock\">\nLeCun, Y.\n\n</span>\n<span class=\"ltx_bibblock\">A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Open Review</em>, 62(1):1–62, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lee et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nLee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S.\n\n</span>\n<span class=\"ltx_bibblock\">Autoregressive image generation using residual quantization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  11523–11532, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mentzer et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nMentzer, F., Minnen, D., Agustsson, E., and Tschannen, M.\n\n</span>\n<span class=\"ltx_bibblock\">Finite scalar quantization: Vq-vae made simple.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.15505</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Oquab et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nOquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P.\n\n</span>\n<span class=\"ltx_bibblock\">DINOv2: Learning robust visual features without supervision.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Transactions on Machine Learning Research</em>, 2024.\n\n</span>\n<span class=\"ltx_bibblock\">ISSN 2835-8856.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ramesh et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I.\n\n</span>\n<span class=\"ltx_bibblock\">Zero-shot text-to-image generation.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pp.  8821–8831. Pmlr, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Razavi et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nRazavi, A., Van den Oord, A., and Vinyals, O.\n\n</span>\n<span class=\"ltx_bibblock\">Generating diverse high-fidelity images with vq-vae-2.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 32, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sehgal et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nSehgal, A., Grayeli, A., Sun, J. J., and Chaudhuri, S.\n\n</span>\n<span class=\"ltx_bibblock\">Neurosymbolic grounding for compositional world models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2310.12690</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sobal et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nSobal, V., SV, J., Jalagam, S., Carion, N., Cho, K., and LeCun, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Joint embedding predictive architectures focus on slow features.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2211.10831</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Takida et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nTakida, Y., Shibuya, T., Liao, W., Lai, C.-H., Ohmura, J., Uesaka, T., Murata, N., Takahashi, S., Kumakura, T., and Mitsufuji, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Sq-vae: Variational bayes on discrete representation with self-annealed stochastic quantization.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2205.07547</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nTang, H., Key, D., and Ellis, K.\n\n</span>\n<span class=\"ltx_bibblock\">Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 37:70148–70212, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Van Balen &amp; Levy (2019)</span>\n<span class=\"ltx_bibblock\">\nVan Balen, J. and Levy, M.\n\n</span>\n<span class=\"ltx_bibblock\">Pq-vae: Efficient recommendation using quantized embeddings.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">RecSys (Late-Breaking Results)</em>, pp.  46–50, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Van Den Oord et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nVan Den Oord, A., Vinyals, O., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Neural discrete representation learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 30, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nWang, H., Suri, S., Ren, Y., Chen, H., and Shrivastava, A.\n\n</span>\n<span class=\"ltx_bibblock\">LARP: Tokenizing videos with a learned autoregressive generative prior.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Thirteenth International Conference on Learning Representations</em>, 2025.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://openreview.net/forum?id=Wr3UuEx72f\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://openreview.net/forum?id=Wr3UuEx72f</a>.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Watters et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nWatters, N., Matthey, L., Borgeaud, S., Kabra, R., and Lerchner, A.\n\n</span>\n<span class=\"ltx_bibblock\">Spriteworld: A flexible, configurable reinforcement learning environment.\n\n</span>\n<span class=\"ltx_bibblock\">https://github.com/deepmind/spriteworld/, 2019.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://github.com/deepmind/spriteworld/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://github.com/deepmind/spriteworld/</a>.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nWu, S., Fei, H., Li, X., Ji, J., Zhang, H., Chua, T.-S., and Yan, S.\n\n</span>\n<span class=\"ltx_bibblock\">Towards semantic equivalence of tokenization in multimodal llm.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2406.05127</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yan et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nYan, W., Hafner, D., James, S., and Abbeel, P.\n\n</span>\n<span class=\"ltx_bibblock\">Temporally consistent transformers for video generation.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pp.  39062–39098. PMLR, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nYu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Vector-quantized image modeling with improved vqgan.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2110.04627</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nYu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A. G., Yang, M.-H., Hao, Y., Essa, I., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Magvit: Masked generative video transformer.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  10459–10469, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L.-C.\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 32 tokens for reconstruction and generation.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 37:128940–128966, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nZhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T.\n\n</span>\n<span class=\"ltx_bibblock\">ibot: Image bert pre-training with online tokenizer.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations (ICLR)</em>, 2022.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Details of Dataset Design</h2>\n\n<figure id=\"A1.F6\" class=\"ltx_figure\">\n<p class=\"ltx_p ltx_align_center ltx_align_center\"><img src=\"./assets/x6.png\" id=\"A1.F6.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"789\" height=\"410\" alt=\"Refer to caption\"></p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span><span class=\"ltx_text ltx_font_bold\">Dataset Visualization.</span>\n</figcaption>\n</figure>\n<section id=\"A1.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Dancing-Sprites-Pattern Dataset</h3>\n\n<div id=\"A1.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">The Dancing-Sprites-Pattern dataset consists of 64×64 color image sequences, each containing a single object at a fixed spatial position. Built upon the Spriteworld environment <cite class=\"ltx_cite ltx_citemacro_citep\">(Watters et al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>, each object can take one of four possible shapes (star, square, circle, triangle) and one of seven possible colors. The dataset is designed to test the model’s ability to learn and predict abstract color transition patterns over time.</p>\n</div>\n<div id=\"A1.SS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pattern Types.</span> We define four distinct color pattern categories, where colors are indexed from 0 to 6:</p>\n<ol id=\"A1.I1\" class=\"ltx_enumerate\">\n<li id=\"A1.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span> \n<div id=\"A1.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Linear</span>: Colors progress sequentially with fixed hop intervals (e.g., 2-hop: 0→2→4→6→1→3→5→0→…). Both starting color and hop size are randomly determined.</p>\n</div>\n</li>\n<li id=\"A1.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span> \n<div id=\"A1.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Repeat-2</span>: Two randomly selected colors alternate repeatedly (e.g., 1→4→1→4→…).</p>\n</div>\n</li>\n<li id=\"A1.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span> \n<div id=\"A1.I1.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Zigzag-3</span>: Three randomly chosen colors follow a zigzag pattern (e.g., 1→5→7→5→1→5→7→…).</p>\n</div>\n</li>\n<li id=\"A1.I1.i4\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">4.</span> \n<div id=\"A1.I1.i4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Repeat-3</span>: Three randomly selected colors cycle sequentially (e.g., 1→5→7→1→5→7→…).</p>\n</div>\n</li>\n</ol>\n</div>\n<div id=\"A1.SS1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Protocol.</span> We employ a world model evaluation framework with a conditioning horizon of 4 frames and a prediction horizon of 4 frames. The world model takes 4 conditioning Discrete-JEPA tokens as input and predicts the subsequent 4 tokens. Predicted tokens are then processed through a pre-trained linear classifier to predict the color of each frame. Performance is measured using color prediction accuracy.</p>\n</div>\n</section>\n<section id=\"A1.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Blinking-Ball Dataset</h3>\n\n<div id=\"A1.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">The Blinking-Ball dataset features 64×64 color image sequences containing four white ball objects at fixed spatial positions. Following the experimental protocol from <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et al. (<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, at each time step, exactly one ball is colored with one of five possible non-white colors, while the remaining three balls remain white. This dataset tests the model’s capacity to simultaneously track positional and color patterns.</p>\n</div>\n<div id=\"A1.SS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pattern Structure.</span> Each sequence follows two interconnected pattern types:</p>\n<ol id=\"A1.I2\" class=\"ltx_enumerate\">\n<li id=\"A1.I2.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span> \n<div id=\"A1.I2.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Position Pattern</span>: A random permutation of the four balls determines the sequence in which balls will be colored across time steps.</p>\n</div>\n</li>\n<li id=\"A1.I2.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span> \n<div id=\"A1.I2.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Color Pattern</span>: A randomly selected subset of the five available colors determines the color sequence applied to the balls according to the position pattern.</p>\n</div>\n</li>\n</ol>\n</div>\n<div id=\"A1.SS2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">The interaction between position and color patterns creates complex temporal dependencies that require both spatial and chromatic reasoning capabilities.</p>\n</div>\n<div id=\"A1.SS2.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Protocol.</span> We use a conditioning horizon of 6 frames and evaluate prediction performance over the subsequent 6 frames. Predicted Discrete-JEPA tokens are decoded into images using a pre-trained image decoder, and performance is assessed through pixel-wise classification accuracy of the reconstructed sequences.\nBoth datasets provide controlled environments for evaluating symbolic reasoning capabilities while maintaining sufficient complexity to distinguish between different tokenization approaches. The fixed spatial layouts allow models to focus on learning temporal color and position patterns without the confounding factor of spatial prediction.</p>\n</div>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Implementation Details</h2>\n\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>World Model Architecture</h3>\n\n<div id=\"A2.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our world model employs a Vision Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy et al., <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> architecture to perform autoregressive prediction over token sequences. Given <math id=\"A2.SS1.p1.m1\" class=\"ltx_Math\" alttext=\"H_{c}\" display=\"inline\"><semantics><msub><mi>H</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">H_{c}</annotation></semantics></math> conditioning tokens from previous timesteps, the model predicts tokens for the subsequent <math id=\"A2.SS1.p1.m2\" class=\"ltx_Math\" alttext=\"H_{p}\" display=\"inline\"><semantics><msub><mi>H</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">H_{p}</annotation></semantics></math> prediction timesteps. The model then repeats this procedure autoregressively to predict additional future frames over extended horizons.</p>\n</div>\n<div id=\"A2.SS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We implement different world model variants tailored to each baseline’s representation type:</p>\n</div>\n<section id=\"A2.SS1.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">B.1.1 </span>Discrete-JEPA World Models</h4>\n\n<div id=\"A2.SS1.SSS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2I (Representation-to-Index)</span>: Takes quantized tokenizer output vectors as input and predicts tokenizer output indices for future timesteps using CrossEntropy loss.</p>\n</div>\n<div id=\"A2.SS1.SSS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">I2I (Index-to-Index)</span>: Takes quantized tokenizer output indices, re-embeds them through learned embeddings, and predicts tokenizer output indices for future timesteps using CrossEntropy loss.</p>\n</div>\n</section>\n<section id=\"A2.SS1.SSS2\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">B.1.2 </span>I-JEPA World Models</h4>\n\n<div id=\"A2.SS1.SSS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2R-Concat (Representation-to-Representation, Concatenated)</span>: Uses all 64 I-JEPA patch tokens as input to predict future patch token vectors using MSE loss.</p>\n</div>\n<div id=\"A2.SS1.SSS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">R2R-AvgPool (Representation-to-Representation, Average Pooled)</span>: Uses averaged I-JEPA patch tokens (single pooled token) as input to predict future averaged patch token vectors using MSE loss.</p>\n</div>\n</section>\n</section>\n<section id=\"A2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Image Decoder for Blinking-Ball Task</h3>\n\n<div id=\"A2.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">For the Blinking-Ball task, we implement a Transformer decoder without causal masking to enable pixel-level reconstruction from learned token representations. The decoder follows a standard Transformer architecture where tokenizer outputs serve as key and value vectors in the cross-attention layers.</p>\n</div>\n<div id=\"A2.SS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Input preprocessing</span> is carefully designed to ensure the tokenizer focuses exclusively on ball identification and coloring. We preprocess input images by resetting all ball colors to white (the default state) before patchifying and feeding them to the transformer. This design choice forces the model to rely on the tokenizer output for determining which balls should be colored and with what colors.</p>\n</div>\n<div id=\"A2.SS2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Output prediction</span> is formulated as a pixel-wise classification problem over 7 possible classes: black background, white balls, and 5 possible ball colors. The model is trained using CrossEntropy loss to predict the correct color for each pixel location.</p>\n</div>\n</section>\n<section id=\"A2.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Hyperparameters</h3>\n\n<div id=\"A2.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We provide comprehensive hyperparameter configurations for both experimental tasks to ensure reproducibility and fair comparison between Discrete-JEPA and I-JEPA baselines. Dancing-Sprites-Pattern uses smaller models due to simpler visual patterns, while Blinking-Ball requires larger capacity for complex spatial-temporal dependencies. Tables <a href=\"#A3.T2\" title=\"Table 2 ‣ Appendix C Additional Visualization Results for Semantic Planning ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a href=\"#A3.T3\" title=\"Table 3 ‣ Appendix C Additional Visualization Results for Semantic Planning ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> detail the configurations for each dataset.</p>\n</div>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Visualization Results for Semantic Planning</h2>\n\n<div id=\"A3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">This section provides additional visualization examples that complement the semantic planning analysis in Figure 5 of the main paper. Figure <a href=\"#A3.F7\" title=\"Figure 7 ‣ Appendix C Additional Visualization Results for Semantic Planning ‣ Discrete JEPA: Learning Discrete Token Representations without Reconstruction\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows three additional Blinking Ball sequence instances, demonstrating consistent pattern maintenance behavior across different initial conditions.\nThese examples reinforce our findings that Discrete-JEPA maintains systematic pattern integrity while I-JEPA exhibits pattern breakdown during long-horizon prediction, providing robust evidence for emergent planning capabilities in semantic token space.</p>\n</div>\n<figure id=\"A3.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Hyperparameters for Dancing-Sprites-Pattern Experiments</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Model</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">Component</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">Hyperparameter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Discrete-JEPA</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">I-JEPA</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Encoder Configuration</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Architecture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ViT-Base</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">ViT-Base</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Patch Size</th>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Random Masking</th>\n<td class=\"ltx_td ltx_align_center\">40%-60%</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">40%-60%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LR Schedule</th>\n<td class=\"ltx_td ltx_align_center\">5% warmup + cosine</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5% warmup + cosine</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Batch Size</th>\n<td class=\"ltx_td ltx_align_center\">128</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">128</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Tokenization</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Quantizer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SVQ</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Semantic Tokens</th>\n<td class=\"ltx_td ltx_align_center\">8 per image</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Token Dimension</th>\n<td class=\"ltx_td ltx_align_center\">96</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Codebook Size</th>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VQ Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-5 (15% warmup)</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">World Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Architecture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Transformer Encoder</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">Transformer Encoder</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Layers</th>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Attention Heads</th>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hidden Dimension</th>\n<td class=\"ltx_td ltx_align_center\">96</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Input Tokens</th>\n<td class=\"ltx_td ltx_align_center\">8 per image</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">64 (Concat) / 1 (Pool)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Property Prober</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Architecture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AvgPool + Linear</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">AvgPool + Linear</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center\">LARS</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">LARS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Training Steps</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20,000</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">20,000</td>\n</tr>\n</tbody>\n</table>\n</figure>\n<figure id=\"A3.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Hyperparameters for Blinking-Ball Experiments</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Model</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Component</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hyperparameter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Discrete-JEPA</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">I-JEPA</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Encoder Configuration</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Architecture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ViT-Base</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">ViT-Base</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Patch Size</th>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Random Masking</th>\n<td class=\"ltx_td ltx_align_center\">50%-70%</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">50%-70%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">1e-3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LR Schedule</th>\n<td class=\"ltx_td ltx_align_center\">5% warmup + cosine</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5% warmup + cosine</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Batch Size</th>\n<td class=\"ltx_td ltx_align_center\">128</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">128</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Training Steps</th>\n<td class=\"ltx_td ltx_align_center\">300K</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">300K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Tokenization</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Quantizer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SVQ</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Semantic Tokens</th>\n<td class=\"ltx_td ltx_align_center\">32 per image</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Token Dimension</th>\n<td class=\"ltx_td ltx_align_center\">96</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Codebook Size</th>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VQ Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-5 (15% warmup)</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">World Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Type</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">I2I (Index-to-Index)</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">R2R (Repr-to-Repr)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Architecture</th>\n<td class=\"ltx_td ltx_align_center\">Transformer Encoder</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Transformer Encoder</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Layers</th>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Attention Heads</th>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hidden Dimension</th>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Input Tokens</th>\n<td class=\"ltx_td ltx_align_center\">32 per image</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">32 per image</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">1e-3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LR Schedule</th>\n<td class=\"ltx_td ltx_align_center\">5% warmup + cosine</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5% warmup + cosine</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Training Steps</th>\n<td class=\"ltx_td ltx_align_center\">15K</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">15K</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Embedding</th>\n<td class=\"ltx_td ltx_align_center\">Index → 768dim</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Decoder</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Architecture</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Transformer w/o causal mask</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">Transformer w/o causal mask</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Layers</th>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Attention Heads</th>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hidden Dimension</th>\n<td class=\"ltx_td ltx_align_center\">64</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Input Projection</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">768 → 64 linear</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\">1e-3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">1e-3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Optimizer</th>\n<td class=\"ltx_td ltx_align_center\">AdamW</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LR Schedule</th>\n<td class=\"ltx_td ltx_align_center\">5% warmup + cosine</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5% warmup + cosine</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Training Steps</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">50K</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">50K</td>\n</tr>\n</tbody>\n</table>\n</figure>\n<figure id=\"A3.F7\" class=\"ltx_figure\">\n<p class=\"ltx_p ltx_align_center ltx_align_center\"><img src=\"./assets/x7.png\" id=\"A3.F7.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"747\" height=\"1000\" alt=\"Refer to caption\"></p>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>Additional Visualization of Semantic Planning on Blinking Ball.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div id=\"A3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\"></p>\n</div>\n</section>",
  "css": "",
  "arxiv_id": "2506.14373",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:26.800Z"
}
