{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p1\">\n<p class=\"ltx_p\">Humans are capable of learning continuously from a stream of unlabeled and uncurated perceptual inputs, such as video data, without needing to iterate through multiple exposures or epochs. Since early infancy, humans have accumulated knowledge about the world through a continuous flow of raw visual observations. This capability contrasts sharply with the training paradigm of current methods in self-supervised learning (SSL) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>; Grill et al., <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2020</a>; Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>; Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">2020</a>; Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">2022</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">2022</a>; Assran et al., <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">2023</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">2020</a>)</cite>. Despite making significant strides in learning from large unlabeled datasets, these approaches still predominantly rely on static and curated image datasets, such as ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2009</a>)</cite>, and require multiple epochs of training for effective learning. This difference in paradigm raises a compelling question: how can we learn good visual representations in a streaming setting—learning from visual inputs in their original temporal order without cycling back?</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"329\" id=\"S1.F1.g1\" src=\"./assets/x1.png\" width=\"755\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span><span class=\"ltx_text ltx_font_bold\">Memory Storyboard framework for streaming self-supervised learning (SSL) from egocentric videos.</span> Given a continuous stream of images from an egocentric video, Memory Storyboard effectively learns visual representations by clustering similar frames into temporal segments and updating their labels (text information for illustration purposes only) in the long-term memory buffer for replay. SSL involves contrastive learning at both the frame and temporal segment levels.</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p2\">\n<p class=\"ltx_p\">Motivated by the differences in mechanisms between human learning and standard SSL, we aim to build learning algorithms that can efficiently learn visual representations and concepts from streaming video. One especially relevant mechanism in the human brain is event segmentation <cite class=\"ltx_cite ltx_citemacro_citep\">(Newtson et al., <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">1977</a>; Zacks et al., <a class=\"ltx_ref\" href=\"#bib.bib78\" title=\"\">2001</a>; Yates et al., <a class=\"ltx_ref\" href=\"#bib.bib74\" title=\"\">2022</a>)</cite>, where we spontaneously segment visual streams into hierarchically structured events and identify the event boundaries. Take your recent vacation trip as an example—you probably remember separate events and activities like exploring a city, dining at a local restaurant, or relaxing at the beach. The event segmentation mechanism helps us organize memories, recall specific moments, and summarize lengthened experiences <cite class=\"ltx_cite ltx_citemacro_citep\">(Zacks et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2006</a>; Zacks &amp; Swallow, <a class=\"ltx_ref\" href=\"#bib.bib77\" title=\"\">2007</a>)</cite>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p3\">\n<p class=\"ltx_p\">Drawing inspiration from the way we organize our memory in the brain, we introduce <span class=\"ltx_text ltx_font_italic\">Memory Storyboard</span>, a novel approach for streaming self-supervised learning. Memory Storyboard features a temporal segmentation module, which groups video frames into semantically meaningful temporal segments, resembling the automatic event segmentation of human cognition. Through our temporal contrastive learning objective, these temporal segments effectively facilitate representation learning in streaming videos. To accommodate efficient temporal segmentation, we propose a two-tier hierarchical memory: temporal segmentation in the short-term memory is used to update the temporal class labels in the long-term memory, and a training batch consists of samples mixed from both memories. A high-level diagram of the algorithm is shown in Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p4\">\n<p class=\"ltx_p\">We conduct experiments on the SAYCam <cite class=\"ltx_cite ltx_citemacro_citep\">(Sullivan et al., <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">2021</a>)</cite> and KrishnaCam <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"#bib.bib58\" title=\"\">2016</a>)</cite> datasets of real-world egocentric videos. Memory Storyboard outperforms state-of-the-art unsupervised continual learning methods on downstream image classification and object detection tasks and significantly reduces the gap between streaming learning and the less flexible IID learning that requires persistent storage of the entire prior video data. We also experiment with different buffer sizes and batch sizes and offer insights into the optimal training batch composition under different memory constraints.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p5\">\n<p class=\"ltx_p\">We summarize our contributions as follows:</p>\n<ol class=\"ltx_enumerate\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1)</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\">We introduce Memory Storyboard, a novel streaming SSL framework that features temporal segmentation and a two-tier memory hierarchy for efficient learning and temporal abstraction.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2)</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\">We demonstrate that Memory Storyboard achieves state-of-the-art performance on downstream ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2009</a>)</cite> and iNaturalist <cite class=\"ltx_cite ltx_citemacro_citep\">(Van Horn et al., <a class=\"ltx_ref\" href=\"#bib.bib64\" title=\"\">2018</a>)</cite> classification tasks when trained on real-world egocentric video datasets. Among all the streaming self-supervised learning methods we evaluated, Memory Storyboard is the only one that is competitive with or even outperforms IID training when trained on these datasets.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3)</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\">We study the effects of training factors including label merging, subsampling rate, average segment length, memory buffer size, and training batch composition. These studies provide insight for more efficient streaming learning from videos. In particular, we explore the optimal composition ratio of the training batch from short-term vs. long-term memory, under different memory constraints. Larger batches from long-term memory improve performance when we can afford a large memory bank, while smaller batches can help prevent overfitting when we have a small memory bank.</p>\n</div>\n</li>\n</ol>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Streaming SSL from Egocentric Videos</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.p1\">\n<p class=\"ltx_p\">In streaming self-supervised learning, the goal is to learn useful visual representations from a continuous stream of inputs <math alttext=\"(x_{1},x_{2},\\dots)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{1},x_{2},\\dots)</annotation></semantics></math>. Similar to continual learning, we impose a memory budget so that storing the entire video would violate the constraint. Different from standard continual learning, there is no explicit notion of task, and the data distribution shift follows directly from the scene transitions of a video. The learner needs to make changes to the model as it sees new inputs, and finishes learning as soon as it receives the last input of the stream. The streaming setting is similar to Online Continual Learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Mai et al., <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">2021</a>; Guo et al., <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2022</a>; Wei et al., <a class=\"ltx_ref\" href=\"#bib.bib70\" title=\"\">2023</a>)</cite>, but the focus here is primarily on streaming video frames instead of a fixed dataset of static images. We argue that streaming learning from sequential video frames enables better modeling of naturalistic scene transitions in real-world data streams because a stream of image collections often includes artificial class transitions.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Streaming Training Batches.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">At each training step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m1\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model fetches a new batch of <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m2\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> images <math alttext=\"X_{t}=x_{tb:(t+1)b}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m3\"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><msub><mi>x</mi><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>b</mi></mrow></mrow></msub></mrow><annotation encoding=\"application/x-tex\">X_{t}=x_{tb:(t+1)b}</annotation></semantics></math> from the video stream and updates its parameters upon receiving <math alttext=\"X_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m4\"><semantics><msub><mi>X</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">X_{t}</annotation></semantics></math>. At the end of the video, we evaluate the final model checkpoint on various downstream tasks such as object classification and detection, which are fundamental tasks for visual scene understanding as they enable models to recognize and interpret the contents of complex environments.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Standard SSL Fails on Streaming Video.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Directly applying the SSL method sequentially on <math alttext=\"X_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px2.p1.m1\"><semantics><msub><mi>X</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">X_{t}</annotation></semantics></math> gives very poor performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>; Ren et al., <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">2021</a>)</cite>. This is not only due to catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(McCloskey &amp; Cohen, <a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">1989</a>)</cite> caused by the non-stationary distribution of visual features in the stream, but also due to the high temporal correlation of images in the stream (illustrated in Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). This temporal correlation breaks the IID assumption held by common optimization algorithms like SGD or Adam <cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma &amp; Ba, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2015</a>)</cite>. For contrastive learning algorithms like SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>, the similarity across different frames in the same training batch would violate the assumption that each image is different.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Memory Replay.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Similar to previous works <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et al., <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2022</a>; Yu et al., <a class=\"ltx_ref\" href=\"#bib.bib76\" title=\"\">2023</a>; Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>, we use a replay buffer <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m1\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> with finite size <math alttext=\"|M|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m2\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M|</annotation></semantics></math> to mitigate these issues. The model can store some of the fetched images in the replay buffer, and use both samples from the replay buffer and the new frames to form a training batch of size <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.m3\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>. By sampling from the replay buffer we de-correlate the frames in the training batch and at the same time reduce the distribution shift between training batches.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Benefits of Streaming SSL over Other Settings.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">Compared to the traditional self-supervised learning setting, where all the frames are shuffled and uniformly sampled for each batch (we refer to this as \"IID learning\" in the text below), streaming SSL allows embodied agents to learn good visual representations from natural, uncurated video streams. It also involves less computation delay and less memory storage. For instance, a robot in a new environment can continuously adapt the visual representations from its own egocentric video feed without any human curation.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Related Work</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p1\">\n<p class=\"ltx_p\">In this section, we discuss the most relevant prior works. Please refer to Appendix <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Related Work ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for additional related work.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Unsupervised Continual Learning.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">Unsupervised Continual Learning (UCL) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rao et al., <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">2019</a>; Smith et al., <a class=\"ltx_ref\" href=\"#bib.bib59\" title=\"\">2021</a>; Madaan et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2022</a>; Fini et al., <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">2022</a>; Gomez-Villa et al., <a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">2024</a>; Cheng et al., <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite> aims at learning a good representation through an unlabeled non-stationary data stream. Existing works in UCL often assume that the data stream is composed of a series of episodes and a stationary data distribution within each episode. This is not as naturalistic and human-like as our streaming setting, where the data distribution changes continuously through the data stream, and each image appears in the data stream only once. Meanwhile, we showed that existing UCL methods are also effective in our streaming video setting, and can be used together with the supervised contrastive objective.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Streaming Learning from Videos.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">While a number of recent papers have studied streaming learning from images <cite class=\"ltx_cite ltx_citemacro_citep\">(Hayes et al., <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2019</a>; Hayes &amp; Kanan, <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2020</a>; Hayes et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2020</a>; Banerjee et al., <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">2021</a>)</cite> or IID self-supervised learning from video frames <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib65\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib68\" title=\"\">2024</a>)</cite>, limited works have investigated the problem of streaming learning from a continuous video stream. <cite class=\"ltx_cite ltx_citemacro_citet\">Roady et al. (<a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">2020</a>)</cite> introduces a benchmark for streaming classification and novelty detection from videos. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al. (<a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>)</cite> benchmarks many self-supervised learning methods in real-time and life-long learning settings in streaming video, assuming infinite replay buffer size which is unrealistic. Most similar to our setup, <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al. (<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite> studies the task of continuous representation learning with a SimSiam objective <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite> and proposes using a minimum-redundancy replay buffer. Their work also belongs to the broader range of works that study replay buffer sampling strategies in continual learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Aljundi et al., <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2019</a>; Wiewel &amp; Yang, <a class=\"ltx_ref\" href=\"#bib.bib71\" title=\"\">2021</a>; Tiwari et al., <a class=\"ltx_ref\" href=\"#bib.bib63\" title=\"\">2022</a>; Hacohen &amp; Tuytelaars, <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2024</a>)</cite>. Our work extends these prior works by adopting a two-tier replay buffer and a temporal segmentation component. Also relevant to our work, <cite class=\"ltx_cite ltx_citemacro_citet\">Carreira et al. (<a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">2024</a>)</cite> studies online learning from a continuous video stream using a pixel-to-pixel reconstruction loss for representation learning. Their findings on the effect of pre-training and different optimization schemes are orthogonal with the ones in our work. It is worth pointing out that their exploration mainly focuses on settings without data augmentation and replay, limiting the efficacy of their framework.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Memory Storyboard</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.p1\">\n<p class=\"ltx_p\">We present Memory Storyboard, an effective method for streaming SSL from egocentric videos. Memory Storyboard includes a temporal segmentation module and a two-tier memory hierarchy. It combines a standard self-supervised contrastive loss with a temporal contrastive objective that leverages the temporal class labels produced by the temporal segmentation module. Figure <a class=\"ltx_ref\" href=\"#S4.F2\" title=\"Figure 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the details of our method. The overall data processing and training procedure is summarized in Algorithm <a class=\"ltx_ref\" href=\"#alg2\" title=\"Algorithm 2 ‣ Warm-Start Training. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Segmentation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We describe our temporal segmentation algorithm as follows. Similar to <cite class=\"ltx_cite ltx_citemacro_citet\">Potapov et al. (<a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2014</a>)</cite>, we are given a down-sampled video frame sequence of length <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, with frames <math alttext=\"x_{1},x_{2},\\cdots,x_{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{1},x_{2},\\cdots,x_{L}</annotation></semantics></math>, and a feature extractor <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m3\"><semantics><msub><mi>f</mi><mi>θ</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>. We aim to find change points <math alttext=\"t_{1},t_{2},\\cdots,t_{n-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m4\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><msub><mi>t</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{1},t_{2},\\cdots,t_{n-1}</annotation></semantics></math> so that the video is divided into <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m5\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> semantically-consistent segments <math alttext=\"[x_{1},x_{t_{1}}],[x_{t_{1}},x_{t_{2}}],\\cdots,[x_{t_{n-1}},x_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m6\"><semantics><mrow><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><msub><mi>t</mi><mn>1</mn></msub></msub><mo stretchy=\"false\">]</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><msub><mi>t</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>x</mi><msub><mi>t</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">]</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><msub><mi>t</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">[x_{1},x_{t_{1}}],[x_{t_{1}},x_{t_{2}}],\\cdots,[x_{t_{n-1}},x_{L}]</annotation></semantics></math>. We also define <math alttext=\"t_{0}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m7\"><semantics><mrow><msub><mi>t</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">t_{0}=0</annotation></semantics></math> and <math alttext=\"t_{n}=L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m8\"><semantics><mrow><msub><mi>t</mi><mi>n</mi></msub><mo>=</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">t_{n}=L</annotation></semantics></math>. In this work, we determine the number of segments with <math alttext=\"n=\\frac{L}{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m9\"><semantics><mrow><mi>n</mi><mo>=</mo><mfrac><mi>L</mi><mi>T</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">n=\\frac{L}{T}</annotation></semantics></math>, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m10\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> refers to the average segment length and is a hyper-parameter.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px1.p2\">\n<p class=\"ltx_p\">The optimization objective of our segmentation algorithm is to maximize the average within-class similarity, such that each temporal segment captures a coherent scene, i.e.</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\max_{t_{1},t_{2},\\cdots,t_{n-1}}\\sum_{i=2}^{n}\\frac{1}{t_{i}-t_{i-1}}\\sum_{j=t_{i-1}}^{t_{i}}\\sum_{k=j}^{t_{i}}sim(x_{j},x_{k}).\" class=\"ltx_Math\" display=\"block\" id=\"S4.E1.m1\"><semantics><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><msub><mi>t</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></munder><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>−</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>j</mi><mo>=</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><msub><mi>t</mi><mi>i</mi></msub></munderover><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>k</mi><mo>=</mo><mi>j</mi></mrow><msub><mi>t</mi><mi>i</mi></msub></munderover><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\max_{t_{1},t_{2},\\cdots,t_{n-1}}\\sum_{i=2}^{n}\\frac{1}{t_{i}-t_{i-1}}\\sum_{j=t_{i-1}}^{t_{i}}\\sum_{k=j}^{t_{i}}sim(x_{j},x_{k}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math alttext=\"sim(x_{j},x_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m1\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sim(x_{j},x_{k})</annotation></semantics></math> denotes the cosine similarity between the embeddings <math alttext=\"f_{\\theta}(x_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m2\"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{\\theta}(x_{j})</annotation></semantics></math> and <math alttext=\"f_{\\theta}(x_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m3\"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{\\theta}(x_{k})</annotation></semantics></math>.\nWe compute the approximate solution to this optimization problem with a greedy approach, as detailed in Algorithm <a class=\"ltx_ref\" href=\"#alg1\" title=\"Algorithm 1 ‣ Warm-Start Training. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We adopt this simple temporal segmentation approach in order to get good segmentation results in the beginning when the encoder network does not provide good representations. We leave it to future work to investigate different temporal segmentation strategies.</p>\n</div>\n<figure class=\"ltx_figure ltx_align_floatright\" id=\"S4.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"511\" id=\"S4.F2.g1\" src=\"./assets/x2.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span><span class=\"ltx_text ltx_font_bold\">Details of our two-tier memory in Memory Storyboard.</span> Long-term memory is updated with reservoir sampling and short-term memory with first-in-first-out (FIFO). Temporal segmentation is applied on the short-term memory, which then updates the labels of corresponding images in the long-term memory.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Two-tier Memory Hierarchy.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Inspired by the Complementary Learning Systems (CLS) theory <cite class=\"ltx_cite ltx_citemacro_citep\">(McClelland et al., <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">1995</a>; O’Reilly et al., <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">2014</a>)</cite> of the human brain, we propose a two-tier memory hierarchy to accommodate efficient temporal segmentation. Shown in Figure <a class=\"ltx_ref\" href=\"#S4.F2\" title=\"Figure 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system includes a long-term memory <math alttext=\"M_{long}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m1\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> updated with reservoir sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Vitter, <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">1985</a>)</cite>, and a short-term memory storyboard <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m2\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> updated with a first-in-first-out (FIFO) strategy. We store the temporal index and the temporal class of each frame along with the image in the memory. The short-term memory size <math alttext=\"|M_{short}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m3\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{short}|</annotation></semantics></math> is much smaller than the long-term memory size <math alttext=\"|M_{long}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m4\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{long}|</annotation></semantics></math>, allowing efficient temporal segmentation of the recent past. The change points produced by the temporal segmentation component on <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m5\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> are then used to update the temporal class labels in <math alttext=\"M_{long}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m6\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math>.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px2.p2\">\n<p class=\"ltx_p\">To increase the horizon of the memory storyboard, we subsample the frames coming from the current stream before adding it to <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p2.m1\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>. The subsampling also reduces the temporal correlation between the frames in the training batch sampled from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p2.m2\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>. Compared to using a single replay buffer as memory, the two-tier memory hierarchy helps avoid overfitting the replay buffer and makes sure that the new frames are seen by the model.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Label Merging.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Same objects and scenes often repeat in egocentric video streams.\nTo efficiently share visual concept labels, we introduce here a label merging mechanism. When a new temporal segment is added to <math alttext=\"M_{\\text{long}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m1\"><semantics><msub><mi>M</mi><mtext>long</mtext></msub><annotation encoding=\"application/x-tex\">M_{\\text{long}}</annotation></semantics></math>, we compute the cosine similarity between its average frame embedding and those of existing segments. If the maximum similarity exceeds a threshold <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m2\"><semantics><mi>δ</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, the new segment inherits the class label of the most similar segment. This mechanism is activated only after the first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m3\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> segments, as early-stage embeddings tend to be uniformly high in similarity. Formally, let <math alttext=\"v_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m4\"><semantics><msub><mi>v</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">v_{i}</annotation></semantics></math> denote the average embedding of segment <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m5\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> in <math alttext=\"M_{\\text{long}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m6\"><semantics><msub><mi>M</mi><mtext>long</mtext></msub><annotation encoding=\"application/x-tex\">M_{\\text{long}}</annotation></semantics></math>, and <math alttext=\"v_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m7\"><semantics><msub><mi>v</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">v_{n}</annotation></semantics></math> for the new segment <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m8\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>. Define <math alttext=\"j=\\arg\\max_{i}\\text{sim}(v_{i},v_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m9\"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace=\"0.167em\">⁡</mo><mrow><msub><mi>max</mi><mi>i</mi></msub><mo lspace=\"0.167em\">⁡</mo><mtext>sim</mtext></mrow></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">j=\\arg\\max_{i}\\text{sim}(v_{i},v_{n})</annotation></semantics></math> and let <math alttext=\"c_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m10\"><semantics><msub><mi>c</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">c_{j}</annotation></semantics></math> be the label of segment <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m11\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>. Then,</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.Ex1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"c_{n}=\\begin{cases}c_{j}&amp;\\text{if }\\text{sim}(v_{j},v_{n})&gt;\\delta\\text{ and }n&gt;C\\\\\n\\text{new label}&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S4.Ex1.m1\"><semantics><mrow><msub><mi>c</mi><mi>n</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><msub><mi>c</mi><mi>j</mi></msub></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mrow><mtext>if </mtext><mtext>sim</mtext></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>j</mi></msub><mo>,</mo><msub><mi>v</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>δ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mtext> and </mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow><mo>&gt;</mo><mi>C</mi></mrow></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext>new label</mtext></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mtext>otherwise</mtext><mo lspace=\"0em\">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">c_{n}=\\begin{cases}c_{j}&amp;\\text{if }\\text{sim}(v_{j},v_{n})&gt;\\delta\\text{ and }n&gt;C\\\\\n\\text{new label}&amp;\\text{otherwise}.\\end{cases}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">In practice, choosing a fixed threshold <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m12\"><semantics><mi>δ</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> that generalizes across methods and datasets is challenging. To address this, rather than fixing <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m13\"><semantics><mi>δ</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> manually, we define it dynamically based on a quantile threshold <math alttext=\"\\tau\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m14\"><semantics><mrow><mi>τ</mi><mo>∈</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\in(0,1)</annotation></semantics></math>. Specifically, we set <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m15\"><semantics><mi>δ</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> as the <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px3.p1.m16\"><semantics><mi>τ</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>-quantile of all off-diagonal values in the similarity matrix.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Contrastive Loss.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">To effectively utilize the temporal class labels for representation learning, we adopt the supervised contrastive (SupCon) loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Khosla et al., <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2020</a>)</cite>, which takes the samples with the same temporal class label in a batch as positives and contrasts them from the remainder of the batch.\nLet <math alttext=\"f_{proj}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m1\"><semantics><msub><mi>f</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">f_{proj}</annotation></semantics></math> be a projector network. For a batch of images with size <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m2\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>, we take two random augmentations of each image to get an augmented batch <math alttext=\"\\tilde{x}_{1},\\tilde{x}_{2},\\dots,\\tilde{x}_{2B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m3\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>B</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}_{1},\\tilde{x}_{2},\\dots,\\tilde{x}_{2B}</annotation></semantics></math>, and compute <math alttext=\"z_{i}=f_{proj}(f_{\\theta}(\\tilde{x}_{j}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m4\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{i}=f_{proj}(f_{\\theta}(\\tilde{x}_{j}))</annotation></semantics></math> be the projected features of each augmented image <math alttext=\"\\tilde{x_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m5\"><semantics><mover accent=\"true\"><msub><mi>x</mi><mi>i</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{x_{i}}</annotation></semantics></math>. Let <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m6\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> be the temporal class label of <math alttext=\"\\tilde{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m7\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{x}_{i}</annotation></semantics></math> and <math alttext=\"P(i)=\\{p\\in\\{1,2,\\dots,2B\\}\\backslash\\{i\\}:y_{p}=y_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px4.p1.m8\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>p</mi><mo>∈</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>B</mi></mrow><mo rspace=\"0.222em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">\\</mo><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo rspace=\"0.278em\" stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"0.278em\">:</mo><mrow><msub><mi>y</mi><mi>p</mi></msub><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(i)=\\{p\\in\\{1,2,\\dots,2B\\}\\backslash\\{i\\}:y_{p}=y_{i}\\}</annotation></semantics></math>.</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S4.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathcal{L}_{TCL}=\\sum_{i}\\frac{-1}{|P(i)|}\\sum_{p\\in P(i)}\\log\\frac{\\exp(z_{i}\\cdot z_{p}/\\tau)}{\\sum_{a\\neq i}\\exp(z_{i}\\cdot z_{a}/\\tau)}.\" class=\"ltx_Math\" display=\"block\" id=\"S4.E2.m1\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\">∑</mo><mi>i</mi></munder><mrow><mfrac><mrow><mo>−</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow><mi>p</mi><mo>∈</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><mi>log</mi><mo lspace=\"0.167em\">⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>z</mi><mi>p</mi></msub></mrow><mo>/</mo><mi>τ</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo>∑</mo><mrow><mi>a</mi><mo>≠</mo><mi>i</mi></mrow></msub><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>z</mi><mi>a</mi></msub></mrow><mo>/</mo><mi>τ</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{TCL}=\\sum_{i}\\frac{-1}{|P(i)|}\\sum_{p\\in P(i)}\\log\\frac{\\exp(z_{i}\\cdot z_{p}/\\tau)}{\\sum_{a\\neq i}\\exp(z_{i}\\cdot z_{a}/\\tau)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We refer to this as the temporal contrastive loss. It is conceptually similar to the temporal classification loss proposed in <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite>. However, in the temporal classification loss, the size of the classification layer needs to be gradually expanded as more data is processed by the model and more temporal classes are formed. Hence, the temporal contrastive loss is more flexible and more suitable for the streaming SSL setting.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Overall Loss Function.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px5.p1\">\n<p class=\"ltx_p\">In addition to the temporal contrastive loss, we also incorporate a standard self-supervised loss <math alttext=\"\\mathcal{L}_{SSL}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px5.p1.m1\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{SSL}</annotation></semantics></math>. In particular, we experimented with the SimCLR loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>; Sohn, <a class=\"ltx_ref\" href=\"#bib.bib60\" title=\"\">2016</a>)</cite> and the SimSiam loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite> because they were shown to work well in lifelong self-supervised learning in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>; Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe overall loss function is a sum of the temporal contrastive loss and the self-supervised contrastive loss <math alttext=\"\\mathcal{L}=\\mathcal{L}_{TCL}+\\mathcal{L}_{SSL}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px5.p1.m2\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{TCL}+\\mathcal{L}_{SSL}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS0.SSS0.Px6\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Warm-Start Training.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS0.SSS0.Px6.p1\">\n<p class=\"ltx_p\">At the beginning of training, the model has only seen a very limited amount of data from the video stream. Even with a memory buffer, there is a likely high temporal correlation between the sampled frames which can cause instability in the training. To alleviate this problem, we warm-start the system by making no model updates on the first <math alttext=\"M_{long}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px6.p1.m1\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> frames of the stream and just use them to fill the memory. The warm-start phase ensures that the model is trained on de-correlated samples from the buffer starting from the beginning.</p>\n</div>\n<figure class=\"ltx_float ltx_minipage ltx_align_top ltx_framed ltx_framed_top\" id=\"alg1\" style=\"width:211.4pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 1</span> </span> Temporal Segmentation</figcaption>\n<div class=\"ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing\" style=\"background-color:#FFFFFF;\">\n<div class=\"ltx_listing_data\"><a download=\"\" href=\"data:text/plain;base64,IyBuOiBudW1iZXIgb2YgY2x1c3RlcnMKIyBmZWF0czogZmVhdHVyZXMgb2YgdGhlIGZyYW1lcyBpbiB0aGUgc2VxdWVuY2UKIyBGOiBtYXhpbWl6YXRpb24gb2JqZWN0aXZlIChkZWZpbmVkIGJ5IEVxdWF0aW9uIDEpLgojIFJldHVybnM6IGRldGVjdGVkIGNoYW5nZSBwb2ludHMgaW4gdGhlIHN0cmVhbSAoc29ydGVkKQoKZGVmIHRlbXBvcmFsX3NlZ21lbnQobiwgZmVhdHMsIEYpOgogICAgUyA9IGZlYXRzIEAgZmVhdHMuVAogICAgTCA9IGxlbihTKQogICAgY2hhbmdlcHRzID0gW10KICAgIGZvciBpIGluIHJhbmdlKDEsIG4pOgogICAgICAgIGJlc3RzY29yZSA9IDAKICAgICAgICBmb3IgY2hhbmdlcHQgaW4gcmFuZ2UoMSwgTCk6CiAgICAgICAgICAgIHRlbXAgPSBjaGFuZ2VwdHMgKyBbY2hhbmdlcHRdCiAgICAgICAgICAgIHNjb3JlID0gRihzb3J0ZWQodGVtcCkpCiAgICAgICAgICAgIGlmIHNjb3JlID4gYmVzdHNjb3JlOgogICAgICAgICAgICAgICAgYmVzdHNjb3JlID0gc2NvcmUKICAgICAgICAgICAgICAgIGJlc3RjaGFuZ2VwdCA9IGNoYW5nZXB0CiAgICAgICAgY2hhbmdlcHRzLmFwcGVuZChiZXN0Y2hhbmdlcHQpCiAgICByZXR1cm4gc29ydGVkKGNoYW5nZXB0cyk=\">⬇</a></div>\n<div class=\"ltx_listingline\" id=\"lstnumberx1\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>n:<span class=\"ltx_text ltx_lst_space\"> </span>number<span class=\"ltx_text ltx_lst_space\"> </span>of<span class=\"ltx_text ltx_lst_space\"> </span>clusters</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx2\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>feats:<span class=\"ltx_text ltx_lst_space\"> </span>features<span class=\"ltx_text ltx_lst_space\"> </span>of<span class=\"ltx_text ltx_lst_space\"> </span>the<span class=\"ltx_text ltx_lst_space\"> </span>frames<span class=\"ltx_text ltx_lst_space\"> </span>in<span class=\"ltx_text ltx_lst_space\"> </span>the<span class=\"ltx_text ltx_lst_space\"> </span>sequence</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx3\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>F:<span class=\"ltx_text ltx_lst_space\"> </span>maximization<span class=\"ltx_text ltx_lst_space\"> </span>objective<span class=\"ltx_text ltx_lst_space\"> </span>(defined<span class=\"ltx_text ltx_lst_space\"> </span>by<span class=\"ltx_text ltx_lst_space\"> </span>Equation<span class=\"ltx_text ltx_lst_space\"> </span>1).</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx4\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Returns:<span class=\"ltx_text ltx_lst_space\"> </span>detected<span class=\"ltx_text ltx_lst_space\"> </span>change<span class=\"ltx_text ltx_lst_space\"> </span>points<span class=\"ltx_text ltx_lst_space\"> </span>in<span class=\"ltx_text ltx_lst_space\"> </span>the<span class=\"ltx_text ltx_lst_space\"> </span>stream<span class=\"ltx_text ltx_lst_space\"> </span>(sorted)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx5\">\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx6\">\n<span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">def</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temporal_segment</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">F</span><span class=\"ltx_text ltx_font_typewriter\">):</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx7\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">S</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">@</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">T</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx8\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">L</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">len</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">S</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx9\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">[]</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx10\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">for</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">i</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">in</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">range</span><span class=\"ltx_text ltx_font_typewriter\">(1,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_font_typewriter\">):</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx11\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestscore</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">0</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx12\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">for</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changept</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">in</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">range</span><span class=\"ltx_text ltx_font_typewriter\">(1,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">L</span><span class=\"ltx_text ltx_font_typewriter\">):</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx13\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temp</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">+</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">[</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changept</span><span class=\"ltx_text ltx_font_typewriter\">]</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx14\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">score</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">F</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">sorted</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temp</span><span class=\"ltx_text ltx_font_typewriter\">))</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx15\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">if</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">score</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">&gt;</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestscore</span><span class=\"ltx_text ltx_font_typewriter\">:</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx16\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestscore</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">score</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx17\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestchangept</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changept</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx18\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">append</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestchangept</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx19\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">return</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">sorted</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n</div>\n</figure>\n<figure class=\"ltx_float ltx_minipage ltx_align_top ltx_framed ltx_framed_top\" id=\"alg2\" style=\"width:244.3pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 2</span> </span> Memory Storyboard Streaming SSL</figcaption>\n<div class=\"ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing\" style=\"background-color:#FFFFFF;\">\n<div class=\"ltx_listing_data\"><a download=\"\" href=\"data:text/plain;base64,IyBEOiBzdHJlYW1pbmcgZGF0YSBsb2FkZXIKIyBNX3M6IHNob3J0LXRlcm0gbWVtb3J5IGJ1ZmZlcgojIE1fbDogbG9uZy10ZXJtIG1lbW9yeSBidWZmZXIKIyBCX3MsIEJfbDogYmF0Y2ggc2l6ZSBmb3IgTV9zLCBNX2wKIyBUOiBkZWZhdWx0IHNlZ21lbnQgbGVuZ3RoCiMgcjogc3Vic2FtcGxpbmcgcmF0ZQoKd2hpbGUgVHJ1ZTogIyBMb29wIHVudGlsIGVuZCBvZiBzdHJlYW0KICAgIHggPSBELm5leHQoKQogICAgeF9zdWIgPSBzdWJzYW1wbGUoeCwgcikKICAgIE1fbC5hZGQoeCkgIyBVcGRhdGVkIHdpdGggUmVzZXJ2b2lyCiAgICBNX3MuYWRkKHhfc3ViKSAjIFVwZGF0ZWQgd2l0aCBGSUZPCiAgICBpZiBNX3NbMF0ubGFiZWwgPiB0Y19sYWJlbDoKICAgICAgICB0Y19sYWJlbCA9IE1fc1swXS5sYWJlbAogICAgICAgIG4gPSBsZW4oTV9zKSAvIFQKICAgICAgICBmZWF0cyA9IG5vcm1hbGl6ZShmZWF0dXJlcyhNX3MpKQogICAgICAgIGNoYW5nZXMgPSB0ZW1wb3JhbF9zZWdtZW50KG4sIGZlYXRzLCBGKQogICAgICAgIHVwZGF0ZV9sYWJlbHMoTV9zLCBjaGFuZ2VzKQogICAgICAgIHVwZGF0ZV9sYWJlbHMoTV9sLCBjaGFuZ2VzKQogICAgZGF0YSA9IHNhbXBsZShNX2wsIEJfbCwgTV9zLCBCX3MpCiAgICBsb3NzID0gVENMX2xvc3MoZGF0YSkgKyBTU0xfbG9zcyhkYXRhKQogICAgbW9kZWwudXBkYXRlKGxvc3Mp\">⬇</a></div>\n<div class=\"ltx_listingline\" id=\"lstnumberx20\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>D:<span class=\"ltx_text ltx_lst_space\"> </span>streaming<span class=\"ltx_text ltx_lst_space\"> </span>data<span class=\"ltx_text ltx_lst_space\"> </span>loader</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx21\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>M_s:<span class=\"ltx_text ltx_lst_space\"> </span>short-term<span class=\"ltx_text ltx_lst_space\"> </span>memory<span class=\"ltx_text ltx_lst_space\"> </span>buffer</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx22\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>M_l:<span class=\"ltx_text ltx_lst_space\"> </span>long-term<span class=\"ltx_text ltx_lst_space\"> </span>memory<span class=\"ltx_text ltx_lst_space\"> </span>buffer</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx23\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>B_s,<span class=\"ltx_text ltx_lst_space\"> </span>B_l:<span class=\"ltx_text ltx_lst_space\"> </span>batch<span class=\"ltx_text ltx_lst_space\"> </span>size<span class=\"ltx_text ltx_lst_space\"> </span>for<span class=\"ltx_text ltx_lst_space\"> </span>M_s,<span class=\"ltx_text ltx_lst_space\"> </span>M_l</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx24\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>T:<span class=\"ltx_text ltx_lst_space\"> </span>default<span class=\"ltx_text ltx_lst_space\"> </span>segment<span class=\"ltx_text ltx_lst_space\"> </span>length</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx25\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>r:<span class=\"ltx_text ltx_lst_space\"> </span>subsampling<span class=\"ltx_text ltx_lst_space\"> </span>rate</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx26\">\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx27\">\n<span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">while</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">True</span><span class=\"ltx_text ltx_font_typewriter\">:</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Loop<span class=\"ltx_text ltx_lst_space\"> </span>until<span class=\"ltx_text ltx_lst_space\"> </span>end<span class=\"ltx_text ltx_lst_space\"> </span>of<span class=\"ltx_text ltx_lst_space\"> </span>stream</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx28\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">D</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">next</span><span class=\"ltx_text ltx_font_typewriter\">()</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx29\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x_sub</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">subsample</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">r</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx30\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_l</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">add</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Updated<span class=\"ltx_text ltx_lst_space\"> </span>with<span class=\"ltx_text ltx_lst_space\"> </span>Reservoir</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx31\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">add</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x_sub</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Updated<span class=\"ltx_text ltx_lst_space\"> </span>with<span class=\"ltx_text ltx_lst_space\"> </span>FIFO</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx32\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">if</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">[0].</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">label</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">&gt;</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">tc_label</span><span class=\"ltx_text ltx_font_typewriter\">:</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx33\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">tc_label</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">[0].</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">label</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx34\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">len</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">/</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">T</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx35\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">normalize</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">features</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">))</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx36\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changes</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temporal_segment</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">F</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx37\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">update_labels</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changes</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx38\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">update_labels</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_l</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changes</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx39\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">data</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">sample</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_l</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">B_l</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">B_s</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx40\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">loss</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">TCL_loss</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">data</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">+</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">SSL_loss</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">data</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div class=\"ltx_listingline\" id=\"lstnumberx41\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">model</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">update</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">loss</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n</div>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span>Experiment Setup</h3>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We use two real-world egocentric video datasets in the experiments: (1) the child S subset of SAYCam dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Sullivan et al., <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">2021</a>)</cite>, which contains 221 hours of video data collected from a head-mounted camera on the child from age 6-32 months, decoded at 25 fps;\n(2) the KrishnaCam dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"#bib.bib58\" title=\"\">2016</a>)</cite>, which contains 70 hours of video data spanning nine months of the life of a graduate student, decoded at 10 fps. These two datasets have also been adopted in a number of existing self-supervised learning literature <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>; Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>; Zhuang et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>; Vong et al., <a class=\"ltx_ref\" href=\"#bib.bib67\" title=\"\">2024</a>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Following the architectural choices of Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite>, we use ResNet-50 <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">2016</a>)</cite> as the feature extractor with group normalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2018</a>)</cite> and the Mish activation function <cite class=\"ltx_cite ltx_citemacro_citep\">(Misra, <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">2020</a>)</cite>. Unless otherwise specified, the default hyperparameter values we use in our experiments are <math alttext=\"b=64\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">b=64</annotation></semantics></math>, <math alttext=\"B=512\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">B=512</annotation></semantics></math>, <math alttext=\"T=4.5K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>4.5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T=4.5K</annotation></semantics></math> for SAYCam and <math alttext=\"T=1.8K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m4\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>1.8</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T=1.8K</annotation></semantics></math> for KrishnaCam (both corresponding to 3 minutes of raw video), subsampling rate <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m5\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math> for SAYCam and <math alttext=\"r=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m6\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">r=4</annotation></semantics></math> for KrishnaCam. We train the models with two sets of memory sizes to evaluate their performance across different memory constraints: a larger memory constraint with <math alttext=\"|M|=50K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m7\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M|=50K</annotation></semantics></math>, <math alttext=\"|M_{short}|=5K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m8\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{short}|=5K</annotation></semantics></math>, <math alttext=\"|M_{long}|=45K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m9\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>45</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{long}|=45K</annotation></semantics></math>, and a smaller memory constraint with <math alttext=\"|M|=10K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m10\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>10</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M|=10K</annotation></semantics></math>, <math alttext=\"|M_{short}|=1K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m11\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{short}|=1K</annotation></semantics></math>, <math alttext=\"|M_{long}|=9K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m12\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>9</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{long}|=9K</annotation></semantics></math>. For context, there are a total of 18.2M frames in the SAYCam training set and 2.5M frames in the KrishnaCam training set. Therefore, even the large memory constraint of 50K frames only stores 0.27% and 2.01% of the total training frames in the memory buffer for SAYCam and KrishnaCam respectively. For the main experiments (Tables <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"#S5.T2\" title=\"Table 2 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we employ label merging with <math alttext=\"\\tau=0.998\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m13\"><semantics><mrow><mi>τ</mi><mo>=</mo><mn>0.998</mn></mrow><annotation encoding=\"application/x-tex\">\\tau=0.998</annotation></semantics></math> (i.e., the similarity threshold <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m14\"><semantics><mi>δ</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is the top <math alttext=\"0.002\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m15\"><semantics><mn>0.002</mn><annotation encoding=\"application/x-tex\">0.002</annotation></semantics></math> quantile of the off-diagonal values in the similarity matrix); for all other experiments, label merging is not employed unless explicitly specified otherwise. Each experiment is run on one A100 GPU.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\">For object classification, we use <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet classification task for both SAYCam and KrishnaCam models. For each dataset, we also pick another downstream task that evaluates the learned representations of the training data itself. Evaluation tasks are summarized below.</p>\n<ul class=\"ltx_itemize\" id=\"S5.I1\">\n<li class=\"ltx_item\" id=\"S5.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">mini<span class=\"ltx_text ltx_font_upright\">-ImageNet Classification:</span></span> Following a similar evaluation protocol as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al. (<a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>)</cite>, we evaluate the learned representations on downstream classification of a subsampled ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2009</a>)</cite> dataset (<span class=\"ltx_text ltx_font_italic\">mini</span>-INet). We extract the features of the model and train a support vector machine (SVM) to measure its classification performance. The <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet dataset contains 20K training images and 5K test images across 100 classes.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ImageNet-1K and iNaturalist Classification:</span> Similar to the evaluation protocol used in <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al. (<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>, we further evaluate the classification performance with a linear classifier on the larger ImageNet-1K <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2009</a>)</cite> dataset (INet) with 1.28M training images and 50K test images across 1K classes, and the iNaturalist-2018 <cite class=\"ltx_cite ltx_citemacro_citep\">(Van Horn et al., <a class=\"ltx_ref\" href=\"#bib.bib64\" title=\"\">2018</a>)</cite> dataset (iNat) with 437K training images and 24K test images across 8142 classes.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i3.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Labeled-S Classification:</span> For SAYCam models, we evaluate the classification performance on the Labeled-S dataset, following <cite class=\"ltx_cite ltx_citemacro_citet\">Orhan et al. (<a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite>. The Labeled-S dataset is a labeled subset of the SAYCam frames, containing a total of 5786 images across 26 classes after 10x subsampling of frames. We randomly use 50% as training data and 50% as test data.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.I1.i4.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OAK Object Detection:</span> For KrishnaCam models, we evaluate the object detection performance on the Objects Around Krishna (OAK) dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2021</a>)</cite>, which includes bounding box annotations of 105 object categories on a subset of the KrishnaCam frames. We fine-tune the model on the entire training set of OAK for 10 epochs before evaluating on the OAK validation set, and report the AP50 metric.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS1.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS1.SSS0.Px4.p1\">\n<p class=\"ltx_p\">We compare Memory Storyboard with a number of competitive SSL methods for image and video representation learning, and different memory buffer strategies:</p>\n<ul class=\"ltx_itemize\" id=\"S5.I2\">\n<li class=\"ltx_item\" id=\"S5.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SimCLR:</span> In prior studies, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al. (<a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>)</cite> showed that SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite> is the strongest self-supervised learning method under streaming video setting, outperforming other SSL methods such as BYOL <cite class=\"ltx_cite ltx_citemacro_citep\">(Grill et al., <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2020</a>)</cite> and Barlow Twins <cite class=\"ltx_cite ltx_citemacro_citep\">(Zbontar et al., <a class=\"ltx_ref\" href=\"#bib.bib80\" title=\"\">2021</a>)</cite>.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SimSiam:</span> In prior work, <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al. (<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite> showed that SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite> is able to learn good representations from egocentric video data.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i3.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Osiris:</span> Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite> is a state-of-the-art unsupervised continual learning method that is developed towards static image sequences.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i4.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TC:</span> Temporal classification (TC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite> is a simple self-supervised learning method that is shown to work well on the SAYCam dataset under IID setting. It also uses temporal segments as a source of self-supervision; however, it does not actively group the frames together but instead relies on fixed intervals.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i5.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reservoir Sampling:</span> We mainly use reservoir sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Vitter, <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">1985</a>)</cite> as a default baseline approach for updating the memory buffer, which uniformly samples from all the seen images in the memory.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I2.i6.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MinRed Buffer:</span> The minimum redundancy (MinRed) buffer <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite> is a streaming self-supervised learning algorithm that alleviates temporal correlations in continuous video streams by storing minimally redundant samples in the replay buffer.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I2.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.I2.i7.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Two-tier Buffer:</span> As in MemStoryboard, we use a long-term memory updated with reservoir sampling and short-term memory updated with first-in-first-out (FIFO), but we do not apply the temporal contrastive loss or the temporal segmentation module.</p>\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:429.3pt;height:188.6pt;vertical-align:-188.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-141.5pt,0.0pt) scale(0.602664498016578,0.602664498016578) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Labeled-S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Labeled-S</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">IID SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">59.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">29.02</td>\n<td class=\"ltx_td ltx_align_center\">20.92</td>\n<td class=\"ltx_td ltx_align_center\">4.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">42.71</td>\n<td class=\"ltx_td ltx_align_center\">29.02</td>\n<td class=\"ltx_td ltx_align_center\">20.92</td>\n<td class=\"ltx_td ltx_align_center\">4.91</td>\n<td class=\"ltx_td ltx_align_center\">42.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR No Replay</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam No Replay</th>\n<td class=\"ltx_td ltx_align_center\">6.44</td>\n<td class=\"ltx_td ltx_align_center\">1.47</td>\n<td class=\"ltx_td ltx_align_center\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.03</td>\n<td class=\"ltx_td ltx_align_center\">6.44</td>\n<td class=\"ltx_td ltx_align_center\">1.47</td>\n<td class=\"ltx_td ltx_align_center\">0.04</td>\n<td class=\"ltx_td ltx_align_center\">22.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 10k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 50k</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">45.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TC <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">33.92</td>\n<td class=\"ltx_td ltx_align_center\">19.03</td>\n<td class=\"ltx_td ltx_align_center\">5.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">48.09</td>\n<td class=\"ltx_td ltx_align_center\">36.68</td>\n<td class=\"ltx_td ltx_align_center\">22.72</td>\n<td class=\"ltx_td ltx_align_center\">8.24</td>\n<td class=\"ltx_td ltx_align_center\">52.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">33.02</td>\n<td class=\"ltx_td ltx_align_center\">20.13</td>\n<td class=\"ltx_td ltx_align_center\">4.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">49.29</td>\n<td class=\"ltx_td ltx_align_center\">37.96</td>\n<td class=\"ltx_td ltx_align_center\">23.75</td>\n<td class=\"ltx_td ltx_align_center\">6.91</td>\n<td class=\"ltx_td ltx_align_center\">53.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">33.62</td>\n<td class=\"ltx_td ltx_align_center\">20.21</td>\n<td class=\"ltx_td ltx_align_center\">5.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">48.88</td>\n<td class=\"ltx_td ltx_align_center\">38.66</td>\n<td class=\"ltx_td ltx_align_center\">24.10</td>\n<td class=\"ltx_td ltx_align_center\">6.81</td>\n<td class=\"ltx_td ltx_align_center\">54.75</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +Two-tier (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">33.80</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">20.70</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">5.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">49.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">39.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">24.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">7.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">55.43</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +MemStoryboard (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">34.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">22.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">51.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">38.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">26.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">8.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">56.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">20.90</td>\n<td class=\"ltx_td ltx_align_center\">13.72</td>\n<td class=\"ltx_td ltx_align_center\">2.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">39.12</td>\n<td class=\"ltx_td ltx_align_center\">26.66</td>\n<td class=\"ltx_td ltx_align_center\">14.44</td>\n<td class=\"ltx_td ltx_align_center\">3.79</td>\n<td class=\"ltx_td ltx_align_center\">43.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">22.68</td>\n<td class=\"ltx_td ltx_align_center\">17.85</td>\n<td class=\"ltx_td ltx_align_center\">3.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">39.78</td>\n<td class=\"ltx_td ltx_align_center\">25.58</td>\n<td class=\"ltx_td ltx_align_center\">18.99</td>\n<td class=\"ltx_td ltx_align_center\">4.24</td>\n<td class=\"ltx_td ltx_align_center\">40.37</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +Two-tier (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">21.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">16.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">2.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">39.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">28.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">20.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">3.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">42.95</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +MemStoryboard (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">36.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">26.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">8.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">49.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">41.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">28.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">10.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">53.78</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span><span class=\"ltx_text ltx_font_bold\">Results on streaming SSL from SAYCam <cite class=\"ltx_cite ltx_citemacro_citep\">(Sullivan et al., <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">2021</a>)</cite>.</span> Downstream evaluation on object classification (Accuracy %) for SSL models trained under the streaming setting. For “No Replay” and “IID” the results are the same for different memory buffer sizes. The “IID” methods are not under the streaming setting and are for reference only as a performance “upper bound” with the same number of gradient updates. Unless specified, standard reservoir sampling is used in the replay buffer.</figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S5.T2\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OAK</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OAK</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">IID SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">39.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.54</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">28.58</td>\n<td class=\"ltx_td ltx_align_center\">22.28</td>\n<td class=\"ltx_td ltx_align_center\">4.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">44.86</td>\n<td class=\"ltx_td ltx_align_center\">28.58</td>\n<td class=\"ltx_td ltx_align_center\">22.28</td>\n<td class=\"ltx_td ltx_align_center\">4.16</td>\n<td class=\"ltx_td ltx_align_center\">44.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR No Replay</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">14.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam No Replay</th>\n<td class=\"ltx_td ltx_align_center\">8.88</td>\n<td class=\"ltx_td ltx_align_center\">1.92</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">27.34</td>\n<td class=\"ltx_td ltx_align_center\">8.88</td>\n<td class=\"ltx_td ltx_align_center\">1.92</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center\">27.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 10k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 50k</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TC <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">32.58</td>\n<td class=\"ltx_td ltx_align_center\">19.19</td>\n<td class=\"ltx_td ltx_align_center\">6.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">32.61</td>\n<td class=\"ltx_td ltx_align_center\">32.94</td>\n<td class=\"ltx_td ltx_align_center\">20.50</td>\n<td class=\"ltx_td ltx_align_center\">6.25</td>\n<td class=\"ltx_td ltx_align_center\">28.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">31.46</td>\n<td class=\"ltx_td ltx_align_center\">19.09</td>\n<td class=\"ltx_td ltx_align_center\">4.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">31.92</td>\n<td class=\"ltx_td ltx_align_center\">34.98</td>\n<td class=\"ltx_td ltx_align_center\">22.37</td>\n<td class=\"ltx_td ltx_align_center\">5.19</td>\n<td class=\"ltx_td ltx_align_center\">33.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">31.56</td>\n<td class=\"ltx_td ltx_align_center\">19.93</td>\n<td class=\"ltx_td ltx_align_center\">4.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">34.78</td>\n<td class=\"ltx_td ltx_align_center\">34.84</td>\n<td class=\"ltx_td ltx_align_center\">22.29</td>\n<td class=\"ltx_td ltx_align_center\">5.30</td>\n<td class=\"ltx_td ltx_align_center\">35.65</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +Two-tier (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">33.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">20.39</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">5.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">33.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">35.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">22.42</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">5.29</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">35.68</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +MemStoryboard (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">33.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">22.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">5.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">36.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">36.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">25.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">6.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">38.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">19.16</td>\n<td class=\"ltx_td ltx_align_center\">12.94</td>\n<td class=\"ltx_td ltx_align_center\">2.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">39.38</td>\n<td class=\"ltx_td ltx_align_center\">21.84</td>\n<td class=\"ltx_td ltx_align_center\">14.13</td>\n<td class=\"ltx_td ltx_align_center\">3.56</td>\n<td class=\"ltx_td ltx_align_center\">41.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">20.90</td>\n<td class=\"ltx_td ltx_align_center\">14.53</td>\n<td class=\"ltx_td ltx_align_center\">3.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">43.74</td>\n<td class=\"ltx_td ltx_align_center\">22.88</td>\n<td class=\"ltx_td ltx_align_center\">17.64</td>\n<td class=\"ltx_td ltx_align_center\">5.12</td>\n<td class=\"ltx_td ltx_align_center\">44.17</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +Two-tier (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">20.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">13.76</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">2.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">43.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">22.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">17.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">44.41</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"background-color:#E6E6FF;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">   +MemStoryboard (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">33.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">23.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">6.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">45.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"background-color:#E6E6FF;\">34.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">25.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">6.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"background-color:#E6E6FF;\">46.17</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span class=\"ltx_text ltx_font_bold\">Results on streaming SSL from KrishnaCam <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"#bib.bib58\" title=\"\">2016</a>)</cite>.</span> Downstream evaluation on object classification (Accuracy %) and object detection (AP50 %) for SSL models trained under the streaming setting. The structure of the table is otherwise similar to Table <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\n</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2 </span>Main Results</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p1\">\n<p class=\"ltx_p\">In Tables <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"#S5.T2\" title=\"Table 2 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we report the main results on streaming SSL on both SAYCam and KrishnaCam. Firstly, we observe that all SSL methods work poorly in the streaming setting without replay, and larger memory leads to better performance. In terms of memory buffer strategies, our two-tier memory hierarchy and MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">(Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite> outperform reservoir sampling.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p2\">\n<p class=\"ltx_p\">Memory Storyboard achieves superior performance in all readout tasks compared to other streaming SSL models. For SimCLR-based methods, Memory Storyboard considerably narrows the gap between streaming learning and IID training. Memory Storyboard also significantly outperforms all baseline methods with a considerable gap on both the ImageNet classification and the challenging OAK object detection benchmark. For SimSiam-based methods, Memory Storyboard not only outperforms all streaming learning baselines by a considerable margin but also beats IID SimSiam training on all readout tasks.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p3\">\n<p class=\"ltx_p\">Memory Storyboard with SimSiam achieves the overall best performance across different training datasets and evaluation metrics. We hypothesize that Memory Storyboard works better with SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite> than SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite> in our experiments because the SimCLR loss can be conflicting with the temporal contrastive objective. SimCLR treats some highly correlated images in the same batch as negative samples during training, despite the fact that the other images in the batch sampled from the short-term buffer might be very similar to the current image since they are temporally close to each other. This issue is exacerbated in the SAYCam experiments due to the high frequency (25 fps) of the SAYCam video stream. By incorporating the temporal contrastive loss in Memory Storyboard, we successfully address this issue by utilizing only images in other temporal classes as negative samples.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.p4\">\n<p class=\"ltx_p\">Overall, the results demonstrate that Memory Storyboard is effective at learning good representations from a streaming video source, and the learned representations can be successfully transferred to downstream vision tasks on the training dataset itself or an external dataset.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S5.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Qualitative Results.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We visualize the temporal segments produced by Memory Storyboard at the end of training in Figure <a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 ‣ Qualitative Results. ‣ 5.2 Main Results ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The results demonstrate that our temporal segmentation module can produce semantically meaningful temporal segments, showing its strong temporal abstraction capability. We emphasize that the representations are entirely developed during the streaming SSL training as the networks are trained from scratch. We provide additional qualitative results in Appendix <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"344\" id=\"S5.F3.g1\" src=\"./assets/x3.png\" width=\"568\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of the temporal segments produced by Memory Storyboard on (a) SAYCam (b)(c) KrishnaCam at the end of training.</span> The images are sampled at 10 seconds per frame. Each color bar corresponds to a temporal class (the first and the last class might be incomplete). Temporal segments produced at the beginning of training are provided in the appendix for comparison.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3 </span>Ablation Experiments and Other Training Factors</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\">In this section, we study how varying different training factors affect the performance of Memory Storyboard, including label merging, subsampling rate, and average segment length. We use SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite> as the base SSL method for training, and a long-term memory size of 50K unless otherwise specified. Please refer to Appendix <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for additional ablation experiments.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:429.3pt;height:97.7pt;vertical-align:-97.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-77.5pt,0.0pt) scale(0.734654125099513,0.734654125099513) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Base SSL Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Label Merging</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Labeled-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Labeled-S</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 10k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 50k</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">✗</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">51.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">56.29</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">✓</th>\n<td class=\"ltx_td ltx_align_center\">34.18</td>\n<td class=\"ltx_td ltx_align_center\">22.59</td>\n<td class=\"ltx_td ltx_align_center\">6.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">51.09</td>\n<td class=\"ltx_td ltx_align_center\">38.84</td>\n<td class=\"ltx_td ltx_align_center\">26.87</td>\n<td class=\"ltx_td ltx_align_center\">8.17</td>\n<td class=\"ltx_td ltx_align_center\">56.26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimSiam</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">✗</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">SimSiam</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">✓</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">26.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">8.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">49.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">41.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">28.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">10.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.78</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Ablation on label merging for MemStoryboard trained on SAYCam.</figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S5.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:411.9pt;height:102.7pt;vertical-align:-102.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-60.9pt,0.0pt) scale(0.77190390422572,0.77190390422572) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Base SSL Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Label Merging</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OAK</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_italic\">mini</span><span class=\"ltx_text ltx_font_bold\">-INet</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">INet</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">iNat</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OAK</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 10k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Replay - 50k</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">✗</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">36.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">✓</th>\n<td class=\"ltx_td ltx_align_center\">33.30</td>\n<td class=\"ltx_td ltx_align_center\">22.52</td>\n<td class=\"ltx_td ltx_align_center\">5.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">36.01</td>\n<td class=\"ltx_td ltx_align_center\">36.08</td>\n<td class=\"ltx_td ltx_align_center\">25.37</td>\n<td class=\"ltx_td ltx_align_center\">6.28</td>\n<td class=\"ltx_td ltx_align_center\">38.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimSiam</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">✗</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">45.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">46.64</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">SimSiam</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">✓</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">33.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">23.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">6.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">45.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">25.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">6.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">46.17</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Ablation on label merging for MemStoryboard trained on KrishnaCam.</figcaption>\n</figure>\n<section class=\"ltx_paragraph\" id=\"S5.SS3.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Label Merging.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We train MemStoryboard with and without the label merging mechanism and present the results in Tables <a class=\"ltx_ref\" href=\"#S5.T3\" title=\"Table 3 ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"#S5.T4\" title=\"Table 4 ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that incorporating label merging consistently improves performance on the ImageNet and iNaturalist classification tasks, which are out-of-domain classification settings. This suggests that grouping labels for representation learning is helpful in recognizing new classes at test time. While the improvements are generally modest in absolute terms, they are robust across different datasets and buffer sizes. Performance on the other benchmarks (<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet, Labeled-S, and OAK) remains largely stable with or without label merging.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS3.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Subsampling Rate.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We train Memory Storyboard with different subsampling rates when adding data fetched from the current stream to the short-term memory. Results are shown in Table <a class=\"ltx_ref\" href=\"#S5.T5\" title=\"Table 5 ‣ Subsampling Rate. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. A subsampling ratio of 8 works best for SAYCam, while a ratio of 4 works best for KrishnaCam. Since the two datasets are decoded at different frequencies (25 fps for SAYCam and 10 fps for KrishnaCam), the effective frequency of frames entering the short-term buffer is 3.13 and 2.50 fps respectively. The result suggests that an effective frequency of around 3 fps is preferable although the optimal subsample ratio is dependent on the nature of the video stream. Intuitively, when the subsampling ratio is too small, the images entering the short-term buffer may have too much temporal correlation and hence would hurt the performance; when the subsampling ratio is too big, the model skips too many frames without training on them and the temporal clustering may also become less precise.</p>\n</div>\n<figure class=\"ltx_table ltx_minipage ltx_align_top\" id=\"S5.T5\" style=\"width:225.5pt;\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:465.1pt;height:195.7pt;vertical-align:-94.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(82.8pt,-18.0pt) scale(1.55314964301406,1.55314964301406) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Subsample</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Ratio</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Labeled-S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OAK AP50</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">2<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\">37.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">55.43</td>\n<td class=\"ltx_td ltx_align_center\">35.60</td>\n<td class=\"ltx_td ltx_align_center\">37.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">4<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\">38.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">55.84</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">36.36</span></td>\n<td class=\"ltx_td ltx_align_center\">38.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">8<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m4\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.29</span></td>\n<td class=\"ltx_td ltx_align_center\">35.48</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">16<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m5\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">55.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.22</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Effect of subsampling ratio for <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m7\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> in Memory Storyboard.</figcaption>\n</figure>\n<figure class=\"ltx_table ltx_minipage ltx_align_top\" id=\"S5.T6\" style=\"width:225.5pt;\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:432.2pt;height:197.5pt;vertical-align:-95.4pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(78.2pt,-18.5pt) scale(1.5675500858355,1.5675500858355) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.m1\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Labeled-S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OAK AP50</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1 min</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">2 min</th>\n<td class=\"ltx_td ltx_align_center\">39.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">56.53</td>\n<td class=\"ltx_td ltx_align_center\">36.30</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">3 min</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">56.29</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">36.36</span></td>\n<td class=\"ltx_td ltx_align_center\">38.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">5 min</th>\n<td class=\"ltx_td ltx_align_center\">39.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.64</span></td>\n<td class=\"ltx_td ltx_align_center\">36.28</td>\n<td class=\"ltx_td ltx_align_center\">38.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10 min</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">55.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">37.53</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Performance of Memory Storyboard using different average temporal segment lengths.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS3.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Average Segment Length.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.SSS0.Px3.p1\">\n<p class=\"ltx_p\">We trained Memory Storyboard with different average segment lengths <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px3.p1.m1\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> ranging from 1 minute to 10 minutes on SAYCam and KrishnaCam. The results are shown in Table <a class=\"ltx_ref\" href=\"#S5.T6\" title=\"Table 6 ‣ Subsampling Rate. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We demonstrate that the performance of Memory Storyboard is generally robust to average segment length (which determines the number of temporal segments in the segmentation module). We also find that the performance on downstream tasks becomes worse when the average segment length is very long <math alttext=\"(T=10\\text{ min})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px3.p1.m2\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>=</mo><mrow><mn>10</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mtext> min</mtext></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T=10\\text{ min})</annotation></semantics></math> on both datasets. This observation is different from that of temporal classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite> which claims longer segments are more helpful.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"223\" id=\"S5.F4.g1\" src=\"./assets/x4.png\" width=\"890\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>\n<span class=\"ltx_text ltx_font_bold\">Memory Storyboard model performance on SAYCam with different long-term memory sizes</span> (5k, 10k, 50k, and 100k) <span class=\"ltx_text ltx_font_bold\">and varying training batch compositions</span> (12.5% – 75.0% from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F4.m2\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>) using SVM readout. Each colored line represents the performance of different training batch compositions when <span class=\"ltx_text ltx_font_bold\">the model has seen the same amount of data</span> from the stream. Each black line represents the performance of different training batch compositions when the model has taken <span class=\"ltx_text ltx_font_bold\">the same number of gradient updates</span>.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.4 </span>Optimal Batch Composition Under Different Memory Constraints</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.p1\">\n<p class=\"ltx_p\">In Memory Storyboard, the training batch is composed of samples from both the long-term memory and the short-term memory (see Figure <a class=\"ltx_ref\" href=\"#S4.F2\" title=\"Figure 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). However, the optimal composition ratio of the training batch, i.e. the optimal percentage of data in the training batch that comes from the short-term memory, is yet to be explored. Sampling more data from the short-term memory means we can digest more data within a fixed number of training steps, but there will be more distribution shift between different training batches. On the other hand, sampling more data from the long-term memory buffer may result in overfitting on the long-term memory data. In this section, we experiment with different memory sizes and training composition and demonstrate the optimal batch composition under different memory constraints.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS4.p2\">\n<p class=\"ltx_p\">We fix the size of the short-term memory <math alttext=\"|M_{short}|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m1\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{short}|</annotation></semantics></math> to be <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m2\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> and vary the memory constraint for the long-term memory <math alttext=\"|M_{long}|=5K,10K,50K,100K\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m3\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><mo>,</mo><mrow><mn>10</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><mo>,</mo><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><mo>,</mo><mrow><mn>100</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{long}|=5K,10K,50K,100K</annotation></semantics></math>. For each long-term memory size, we experiment with batch size from data stream <math alttext=\"b=64,128,192,256,320,384\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m4\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><mn>64</mn><mo>,</mo><mn>128</mn><mo>,</mo><mn>192</mn><mo>,</mo><mn>256</mn><mo>,</mo><mn>320</mn><mo>,</mo><mn>384</mn></mrow></mrow><annotation encoding=\"application/x-tex\">b=64,128,192,256,320,384</annotation></semantics></math> (which corresponds to 12.5% though 75% of the training batch size). We sample <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m5\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> images from the short-term memory and <math alttext=\"512-b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m6\"><semantics><mrow><mn>512</mn><mo>−</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">512-b</annotation></semantics></math> images from the long-term memory to compose a training batch. We evaluate the model with SVM readout on <span class=\"ltx_text ltx_font_italic\">mini-</span>ImageNet after the model has seen every 10% of the entire data stream and plot the results in Figure <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We discuss the different observations for large memory size and small memory size respectively.</p>\n<ul class=\"ltx_itemize\" id=\"S5.I3\">\n<li class=\"ltx_item\" id=\"S5.I3.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I3.i1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large <math alttext=\"|M_{long}|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i1.p1.m1\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{long}|</annotation></semantics></math>:</span> When <math alttext=\"|M_{long}|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i1.p1.m2\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{long}|</annotation></semantics></math> is large (Figures <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(c) and <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(d)), overfitting on the memory is unlikely and hence we can sample more data from the long-term memory and the performance still keeps increasing as the model sees more data. Hence, with the same amount of data seen by the model (colored curves), it is better to sample only a small batch from the short-term memory. However, when we control the number of model update steps to the same (black curves), neither focusing on the short-term memory nor focusing on the long-term memory is preferable. In such cases, the optimal batch size from the short-term memory is at roughly 50% of the training batch.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S5.I3.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.I3.i2.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Small <math alttext=\"|M_{long}|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i2.p1.m1\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{long}|</annotation></semantics></math>:</span> When <math alttext=\"|M_{long}|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i2.p1.m2\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{long}|</annotation></semantics></math> is small (Figures <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a) and <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b)), the model is prone to overfitting on the memory. As a result, with the same number of model update steps (black curves), taking more images from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i2.p1.m3\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> gives better results. With the same amount of data seen by the model (colored curves), getting a higher percentage of data from <math alttext=\"M_{long}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i2.p1.m4\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> has an advantage in the beginning when there is less memory overfitting. Ultimately, focusing on <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I3.i2.p1.m5\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> is more beneficial in the late stage.</p>\n</div>\n</li>\n</ul>\n<p class=\"ltx_p\">To summarize, the optimal training batch composition depends on memory and compute constraints. More samples from the long-term memory are preferred when a large memory is available (e.g., 50K images from a 200-hour stream) and model performance is evaluated after seeing a fixed amount of data. More samples from the short-term memory are preferred when memory is limited to prevent overfitting on the long-term memory data. When both memory and compute are sufficient and performance is measured under a fixed compute budget, a balanced batch composition is most effective for real-time learning.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p1\">\n<p class=\"ltx_p\">The ability to continuously learn from large-scale uncurated streaming video data is crucial for applying self-supervised learning methods in real-world embodied agents. Existing works have limited exploration of this problem, have mainly focused on static datasets, and do not perform well in the streaming video setting. Inspired by the event segmentation mechanism in human cognition, in this work, we propose Memory Storyboard, which leverages temporal segmentation to produce a two-tier memory hierarchy akin to the short-term and long-term memory of humans. Memory Storyboard combines a temporal contrastive objective and a standard self-supervised contrastive objective to facilitate representation learning from scratch through streaming video experiences. Memory Storyboard achieves state-of-the-art performance on downstream classification and object detection tasks when trained on real-world large egocentric video datasets. By studying the effects of subsampling rates, average segment length, normalization, and optimal batch composition under different compute and memory constraints, we also offer valuable insights on the design choices for streaming self-supervised learning.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgements</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">We thank James McClelland for a helpful discussion on event segmentation and pointers to relevant literature. We thank Peiqi Liu and Anh Ta for their explorations on training self-supervised learning models on the SAYCam dataset. The work is supported in part by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) under grant RS-2024-00469482, funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research. YY is supported by the Meta AI Mentorship Program. The compute is supported through the NYU IT High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Afham et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nMohamed Afham, Satya Narayan Shukla, Omid Poursaeed, Pengchuan Zhang, Ashish Shah, and Sernam Lim.\n\n</span>\n<span class=\"ltx_bibblock\">Revisiting kernel temporal segmentation as an adaptive tokenizer for long-form video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  1189–1194, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Aljundi et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.\n\n</span>\n<span class=\"ltx_bibblock\">Gradient based sample selection for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 32, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Assran et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning from images with a joint-embedding predictive architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  15619–15629, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baldassano et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nChristopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Kenneth A Norman.\n\n</span>\n<span class=\"ltx_bibblock\">Discovering event structure in continuous narrative perception and memory.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Neuron</em>, 95(3):709–721, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baldwin et al. (2001)</span>\n<span class=\"ltx_bibblock\">\nDare A Baldwin, Jodie A Baird, Megan M Saylor, and M Angela Clark.\n\n</span>\n<span class=\"ltx_bibblock\">Infants parse dynamic action.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Child development</em>, 72(3):708–717, 2001.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Banerjee et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nSoumya Banerjee, Vinay Kumar Verma, Toufiq Parag, Maneesh Singh, and Vinay P Namboodiri.\n\n</span>\n<span class=\"ltx_bibblock\">Class incremental online streaming learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2110.10741</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nAdrien Bardes, Jean Ponce, and Yann LeCun.\n\n</span>\n<span class=\"ltx_bibblock\">Vicreg: Variance-invariance-covariance regularization for self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Tenth International Conference on Learning Representations</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.\n\n</span>\n<span class=\"ltx_bibblock\">Deep clustering for unsupervised learning of visual features.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the European conference on computer vision (ECCV)</em>, pp.  132–149, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of visual features by contrasting cluster assignments.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 33:9912–9924, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Carreira et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJoão Carreira, Michael King, Viorica Patraucean, Dilara Gokay, Catalin Ionescu, Yi Yang, Daniel Zoran, Joseph Heyward, Carl Doersch, Yusuf Aytar, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Learning from one continuous video stream.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  28751–28761, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\n\n</span>\n<span class=\"ltx_bibblock\">A simple framework for contrastive learning of visual representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pp.  1597–1607. PMLR, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen &amp; He (2021)</span>\n<span class=\"ltx_bibblock\">\nXinlei Chen and Kaiming He.\n\n</span>\n<span class=\"ltx_bibblock\">Exploring simple siamese representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  15750–15758, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cheng et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nHaoyang Cheng, Haitao Wen, Xiaoliang Zhang, Heqian Qiu, Lanxiao Wang, and Hongliang Li.\n\n</span>\n<span class=\"ltx_bibblock\">Contrastive continuity on augmentation stability rehearsal for continual self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  5707–5717, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Deng et al. (2009)</span>\n<span class=\"ltx_bibblock\">\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\n\n</span>\n<span class=\"ltx_bibblock\">Imagenet: A large-scale hierarchical image database.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">2009 IEEE conference on computer vision and pattern recognition</em>, pp.  248–255. Ieee, 2009.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Doersch et al. (2015)</span>\n<span class=\"ltx_bibblock\">\nCarl Doersch, Abhinav Gupta, and Alexei A Efros.\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised visual representation learning by context prediction.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE international conference on computer vision</em>, pp.  1422–1430, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">DuBrow &amp; Davachi (2013)</span>\n<span class=\"ltx_bibblock\">\nSarah DuBrow and Lila Davachi.\n\n</span>\n<span class=\"ltx_bibblock\">The influence of context boundaries on memory for the sequential order of events.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Experimental Psychology: General</em>, 142:1277–1286, 08 2013.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1037/a0034024</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ezzyat &amp; Davachi (2011)</span>\n<span class=\"ltx_bibblock\">\nYoussef Ezzyat and Lila Davachi.\n\n</span>\n<span class=\"ltx_bibblock\">What constitutes an episode in episodic memory?\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Psychological science</em>, 22:243–52, 02 2011.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1177/0956797610393742</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fini et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nEnrico Fini, Victor G Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised models are continual learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9621–9630, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gidaris et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis.\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised representation learning by predicting image rotations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">6th International Conference on Learning Representations</em>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gomez-Villa et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nAlex Gomez-Villa, Bartlomiej Twardowski, Lu Yu, Andrew D Bagdanov, and Joost Van de Weijer.\n\n</span>\n<span class=\"ltx_bibblock\">Continually learning self-supervised representations with projected functional regularization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  3867–3877, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gomez-Villa et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nAlex Gomez-Villa, Bartlomiej Twardowski, Kai Wang, and Joost Van de Weijer.\n\n</span>\n<span class=\"ltx_bibblock\">Plasticity-optimized complementary networks for unsupervised continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp.  1690–1700, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grill et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Bootstrap your own latent-a new approach to self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 33:21271–21284, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guo et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nYiduo Guo, Bing Liu, and Dongyan Zhao.\n\n</span>\n<span class=\"ltx_bibblock\">Online continual learning through mutual information maximization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pp.  8109–8126. PMLR, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hacohen &amp; Tuytelaars (2024)</span>\n<span class=\"ltx_bibblock\">\nGuy Hacohen and Tinne Tuytelaars.\n\n</span>\n<span class=\"ltx_bibblock\">Forgetting order of continual learning: Examples that are learned first are forgotten last.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2406.09935</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hayes &amp; Kanan (2020)</span>\n<span class=\"ltx_bibblock\">\nTyler L Hayes and Christopher Kanan.\n\n</span>\n<span class=\"ltx_bibblock\">Lifelong machine learning with deep streaming linear discriminant analysis.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</em>, pp.  220–221, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hayes et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nTyler L Hayes, Nathan D Cahill, and Christopher Kanan.\n\n</span>\n<span class=\"ltx_bibblock\">Memory efficient experience replay for streaming learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">2019 International Conference on Robotics and Automation (ICRA)</em>, pp.  9769–9776. IEEE, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hayes et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nTyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher Kanan.\n\n</span>\n<span class=\"ltx_bibblock\">Remind your neural network to prevent catastrophic forgetting.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">European conference on computer vision</em>, pp.  466–483. Springer, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. (2016)</span>\n<span class=\"ltx_bibblock\">\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\n</span>\n<span class=\"ltx_bibblock\">Deep residual learning for image recognition.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  770–778, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\n\n</span>\n<span class=\"ltx_bibblock\">Momentum contrast for unsupervised visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  9729–9738, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders are scalable vision learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  16000–16009, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nDapeng Hu, Shipeng Yan, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang, Zhenguo Li, Xinchao Wang, and Jiashi Feng.\n\n</span>\n<span class=\"ltx_bibblock\">How well does self-supervised pre-training perform with streaming data?\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Tenth International Conference on Learning Representations</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ioffe &amp; Szegedy (2015)</span>\n<span class=\"ltx_bibblock\">\nSergey Ioffe and Christian Szegedy.\n\n</span>\n<span class=\"ltx_bibblock\">Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 32nd International Conference on Machine Learning</em>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Khosla et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.\n\n</span>\n<span class=\"ltx_bibblock\">Supervised contrastive learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 33:18661–18673, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kingma &amp; Ba (2015)</span>\n<span class=\"ltx_bibblock\">\nDiederik P. Kingma and Jimmy Ba.\n\n</span>\n<span class=\"ltx_bibblock\">Adam: A method for stochastic optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lassiter &amp; Slaw (1991)</span>\n<span class=\"ltx_bibblock\">\nG. Lassiter and David Slaw.\n\n</span>\n<span class=\"ltx_bibblock\">The unitization and memory of events.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Experimental Psychology: General</em>, 120:80–82, 03 1991.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1037/0096-3445.120.1.80</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Madaan et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nDivyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang.\n\n</span>\n<span class=\"ltx_bibblock\">Representational continuity for unsupervised continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">The Tenth International Conference on Learning Representations</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mai et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nZheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner.\n\n</span>\n<span class=\"ltx_bibblock\">Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  3589–3599, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McClelland et al. (1995)</span>\n<span class=\"ltx_bibblock\">\nJames L McClelland, Bruce L McNaughton, and Randall C O’Reilly.\n\n</span>\n<span class=\"ltx_bibblock\">Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Psychological review</em>, 102(3):419, 1995.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McCloskey &amp; Cohen (1989)</span>\n<span class=\"ltx_bibblock\">\nMichael McCloskey and Neal J Cohen.\n\n</span>\n<span class=\"ltx_bibblock\">Catastrophic interference in connectionist networks: The sequential learning problem.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Psychology of Learning and Motivation</em>, volume 24, pp.  109–165. Elsevier, 1989.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Misra (2020)</span>\n<span class=\"ltx_bibblock\">\nDiganta Misra.\n\n</span>\n<span class=\"ltx_bibblock\">Mish: A self regularized non-monotonic activation function.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">31st British Machine Vision Conference</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Misra &amp; Maaten (2020)</span>\n<span class=\"ltx_bibblock\">\nIshan Misra and Laurens van der Maaten.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning of pretext-invariant representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  6707–6717, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Newtson et al. (1977)</span>\n<span class=\"ltx_bibblock\">\nDarren Newtson, Gretchen A Engquist, and Joyce Bois.\n\n</span>\n<span class=\"ltx_bibblock\">The objective basis of behavior units.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Personality and social psychology</em>, 35(12):847, 1977.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Noroozi &amp; Favaro (2016)</span>\n<span class=\"ltx_bibblock\">\nMehdi Noroozi and Paolo Favaro.\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of visual representations by solving jigsaw puzzles.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">European conference on computer vision</em>, pp.  69–84. Springer, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Oord et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n\n</span>\n<span class=\"ltx_bibblock\">Representation learning with contrastive predictive coding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1807.03748</em>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Orhan et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nEmin Orhan, Vaibhav Gupta, and Brenden M Lake.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning through the eyes of a child.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 33:9960–9971, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">O’Reilly et al. (2014)</span>\n<span class=\"ltx_bibblock\">\nRandall C O’Reilly, Rajan Bhattacharyya, Michael D Howard, and Nicholas Ketz.\n\n</span>\n<span class=\"ltx_bibblock\">Complementary learning systems.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Cognitive science</em>, 38(6):1229–1248, 2014.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Pathak et al. (2016)</span>\n<span class=\"ltx_bibblock\">\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros.\n\n</span>\n<span class=\"ltx_bibblock\">Context encoders: Feature learning by inpainting.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  2536–2544, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Potapov et al. (2014)</span>\n<span class=\"ltx_bibblock\">\nDanila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid.\n\n</span>\n<span class=\"ltx_bibblock\">Category-specific video summarization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13</em>, pp.  540–555. Springer, 2014.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Purushwalkam et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nSenthil Purushwalkam, Pedro Morgado, and Abhinav Gupta.\n\n</span>\n<span class=\"ltx_bibblock\">The challenges of continuous self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">European Conference on Computer Vision</em>, pp.  702–721. Springer, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rao et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nDushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell.\n\n</span>\n<span class=\"ltx_bibblock\">Continual unsupervised representation learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 32, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ren et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nMengye Ren, Tyler R. Scott, Michael L. Iuzzolino, Michael C. Mozer, and Richard S. Zemel.\n\n</span>\n<span class=\"ltx_bibblock\">Online unsupervised learning of visual representations and categories.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2109.05675</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ren et al. (2015)</span>\n<span class=\"ltx_bibblock\">\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n\n</span>\n<span class=\"ltx_bibblock\">Faster r-cnn: Towards real-time object detection with region proposal networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, volume 28, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Roady et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nRyne Roady, Tyler L Hayes, Hitesh Vaidya, and Christopher Kanan.\n\n</span>\n<span class=\"ltx_bibblock\">Stream-51: Streaming classification and novelty detection from videos.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, pp.  228–229, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rochan et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nMrigank Rochan, Linwei Ye, and Yang Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Video summarization using fully convolutional sequence networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the European conference on computer vision (ECCV)</em>, pp.  347–363, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sasmita &amp; Swallow (2022)</span>\n<span class=\"ltx_bibblock\">\nKaren Sasmita and Khena Swallow.\n\n</span>\n<span class=\"ltx_bibblock\">Measuring event segmentation: An investigation into the stability of event boundary agreement across groups.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Behavior Research Methods</em>, 55, 04 2022.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.3758/s13428-022-01832-5</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Saylor et al. (2007)</span>\n<span class=\"ltx_bibblock\">\nMegan M Saylor, Dare A Baldwin, Jodie A Baird, and Jennifer LaBounty.\n\n</span>\n<span class=\"ltx_bibblock\">Infants’ on-line segmentation of dynamic human action.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Cognition and Development</em>, 8(1):113–128, 2007.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Silva et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nMarta Silva, Christopher Baldassano, and Lluís Fuentemilla.\n\n</span>\n<span class=\"ltx_bibblock\">Rapid memory reactivation at movie event boundaries promotes episodic encoding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Neuroscience</em>, 39(43):8538–8548, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Singh et al. (2016)</span>\n<span class=\"ltx_bibblock\">\nKrishna Kumar Singh, Kayvon Fatahalian, and Alexei A Efros.\n\n</span>\n<span class=\"ltx_bibblock\">Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">2016 IEEE Winter Conference on Applications of Computer Vision</em>, pp.  1–9. IEEE, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Smith et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nJames Seale Smith, Cameron E. Taylor, Seth Baer, and Constantine Dovrolis.\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised progressive learning and the STAM architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sohn (2016)</span>\n<span class=\"ltx_bibblock\">\nKihyuk Sohn.\n\n</span>\n<span class=\"ltx_bibblock\">Improved deep metric learning with multi-class n-pair loss objective.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 29, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sullivan et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nJessica Sullivan, Michelle Mei, Andrew Perfors, Erica Wojcik, and Michael C Frank.\n\n</span>\n<span class=\"ltx_bibblock\">Saycam: A large, longitudinal audiovisual dataset recorded from the infant’s perspective.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Open mind</em>, 5:20–29, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tian et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nYonglong Tian, Dilip Krishnan, and Phillip Isola.\n\n</span>\n<span class=\"ltx_bibblock\">Contrastive multiview coding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16</em>, pp.  776–794. Springer, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tiwari et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nRishabh Tiwari, Krishnateja Killamsetty, Rishabh Iyer, and Pradeep Shenoy.\n\n</span>\n<span class=\"ltx_bibblock\">Gcr: Gradient coreset based replay buffer selection for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  99–108, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Van Horn et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie.\n\n</span>\n<span class=\"ltx_bibblock\">The inaturalist species classification and detection dataset.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  8769–8778, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Venkataramanan et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nShashanka Venkataramanan, Mamshad Nayeem Rizve, João Carreira, Yuki M Asano, and Yannis Avrithis.\n\n</span>\n<span class=\"ltx_bibblock\">Is imagenet worth 1 video? learning strong image encoders from 1 long unlabelled video.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2310.08584</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vitter (1985)</span>\n<span class=\"ltx_bibblock\">\nJeffrey S Vitter.\n\n</span>\n<span class=\"ltx_bibblock\">Random sampling with a reservoir.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">ACM Transactions on Mathematical Software (TOMS)</em>, 11(1):37–57, 1985.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vong et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nWai Keen Vong, Wentao Wang, A Emin Orhan, and Brenden M Lake.\n\n</span>\n<span class=\"ltx_bibblock\">Grounded language acquisition through the eyes and ears of a single child.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Science</em>, 383(6682):504–511, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nAlex N Wang, Christopher Hoang, Yuwen Xiong, Yann LeCun, and Mengye Ren.\n\n</span>\n<span class=\"ltx_bibblock\">Poodle: Pooled and dense self-supervised learning from naturalistic videos.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2408.11208</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nJianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta.\n\n</span>\n<span class=\"ltx_bibblock\">Wanderlust: Online continual object detection in the real world.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  10829–10838, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nYujie Wei, Jiaxin Ye, Zhizhong Huang, Junping Zhang, and Hongming Shan.\n\n</span>\n<span class=\"ltx_bibblock\">Online prototype learning for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  18764–18774, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wiewel &amp; Yang (2021)</span>\n<span class=\"ltx_bibblock\">\nFelix Wiewel and Bin Yang.\n\n</span>\n<span class=\"ltx_bibblock\">Entropy-based sample selection for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">2020 28th European signal processing conference (EUSIPCO)</em>, pp.  1477–1481. IEEE, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nJay Zhangjie Wu, David Junhao Zhang, Wynne Hsu, Mengmi Zhang, and Mike Zheng Shou.\n\n</span>\n<span class=\"ltx_bibblock\">Label-efficient online continual object detection in streaming video.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  19246–19255, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib73\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu &amp; He (2018)</span>\n<span class=\"ltx_bibblock\">\nYuxin Wu and Kaiming He.\n\n</span>\n<span class=\"ltx_bibblock\">Group normalization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the European conference on computer vision (ECCV)</em>, pp.  3–19, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib74\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yates et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nTristan S Yates, Lena J Skalaban, Cameron T Ellis, Angelika J Bracher, Christopher Baldassano, and Nicholas B Turk-Browne.\n\n</span>\n<span class=\"ltx_bibblock\">Neural event segmentation of continuous experience in human infants.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Proceedings of the National Academy of Sciences</em>, 119(43):e2200257119, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib75\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">You et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nYang You, Igor Gitman, and Boris Ginsburg.\n\n</span>\n<span class=\"ltx_bibblock\">Large batch training of convolutional networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1708.03888</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib76\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nXiaofan Yu, Yunhui Guo, Sicun Gao, and Tajana Rosing.\n\n</span>\n<span class=\"ltx_bibblock\">Scale: Online self-supervised lifelong learning without prior knowledge.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  2484–2495, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib77\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zacks &amp; Swallow (2007)</span>\n<span class=\"ltx_bibblock\">\nJeffrey M Zacks and Khena M Swallow.\n\n</span>\n<span class=\"ltx_bibblock\">Event segmentation.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Current directions in psychological science</em>, 16(2):80–84, 2007.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib78\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zacks et al. (2001)</span>\n<span class=\"ltx_bibblock\">\nJeffrey M Zacks, Barbara Tversky, and Gowri Iyer.\n\n</span>\n<span class=\"ltx_bibblock\">Perceiving, remembering, and communicating structure in events.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of experimental psychology: General</em>, 130(1):29, 2001.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib79\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zacks et al. (2006)</span>\n<span class=\"ltx_bibblock\">\nJeffrey M Zacks, Nicole K Speer, Jean M Vettel, and Larry L Jacoby.\n\n</span>\n<span class=\"ltx_bibblock\">Event understanding and memory in healthy aging and dementia of the alzheimer type.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Psychology and aging</em>, 21(3):466, 2006.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib80\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zbontar et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny.\n\n</span>\n<span class=\"ltx_bibblock\">Barlow twins: Self-supervised learning via redundancy reduction.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pp.  12310–12320. PMLR, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib81\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2016)</span>\n<span class=\"ltx_bibblock\">\nKe Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman.\n\n</span>\n<span class=\"ltx_bibblock\">Video summarization with long short-term memory.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14</em>, pp.  766–782. Springer, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib82\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYipeng Zhang, Laurent Charlin, Richard Zemel, and Mengye Ren.\n\n</span>\n<span class=\"ltx_bibblock\">Integrating present and past in unsupervised continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2404.19132</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib83\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nWencheng Zhu, Jiwen Lu, Jiahao Li, and Jie Zhou.\n\n</span>\n<span class=\"ltx_bibblock\">Dsnet: A flexible detect-to-summarize network for video summarization.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">IEEE Transactions on Image Processing</em>, 30:948–962, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib84\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhuang et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nChengxu Zhuang, Ziyu Xiang, Yoon Bai, Xiaoxuan Jia, Nicholas Turk-Browne, Kenneth Norman, James J DiCarlo, and Dan Yamins.\n\n</span>\n<span class=\"ltx_bibblock\">How well do unsupervised learning algorithms model human real-time and life-long learning?\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 35:22628–22642, 2022.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Experiment Details</h2>\n<section class=\"ltx_paragraph\" id=\"A1.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Model Architecture.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">On top of the ResNet backbone, we use a two-layer MLP with 2048 hidden units, 128 output units, and ReLU activation function as the projector. In Memory Storyboard, we create two separate projectors for <math alttext=\"\\mathcal{L}_{TCL}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px1.p1.m1\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{TCL}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{SSL}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px1.p1.m2\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{SSL}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A1.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">For all experiments in Tables <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"#S5.T2\" title=\"Table 2 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we used a total batch size of 512 (64 from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px2.p1.m1\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> and 448 from <math alttext=\"M_{long}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS0.SSS0.Px2.p1.m2\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> by default). The input resolution of the images to the model is 112. We apply a standard data augmentation pipeline for SSL methods following <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al. (<a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>)</cite>, which include random resized crop, random horizontal flip, random color jitter, random grayscale, random Gaussian filter, and color-normalization with ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2009</a>)</cite>. For the SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>, Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite>, and TC <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite> experiments, we used the Adam <cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma &amp; Ba, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2015</a>)</cite> optimizer with a constant learning rate of 0.001, and a projector with 2 MLP layers of size 2048 and 128 respectively. For the SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite> experiments, we used the SGD optimizer with learning rate 0.05, momentum 0.9, and weight decay 1e-4, and a projector with 3 MLP layers of size 2048.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A1.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet and Labeled-S evaluations, the streaming SSL models are evaluated every 5% of the entire dataset. That is, we store 20 model checkpoints throughout the streaming training and evaluate them on <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet and Labeled-S with SVM readout. The <span class=\"ltx_text ltx_font_italic\">best</span> results among these checkpoints are reported. Similar to <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al. (<a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2022</a>)</cite>, for SVM readout, we report the best performance among learning rate values {1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2}.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.SS0.SSS0.Px3.p2\">\n<p class=\"ltx_p\">For ImageNet-1K and iNaturalist-2018 evaluations, we evaluate the final model after streaming SSL training on the entire dataset. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al. (<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>, we train a linear classifier on top of the normalized learned representations and report the classification accuracy. We used a batch size of 1024. For ImageNet-1K, we used the LARS <cite class=\"ltx_cite ltx_citemacro_citep\">(You et al., <a class=\"ltx_ref\" href=\"#bib.bib75\" title=\"\">2017</a>)</cite> optimizer with learning rate 3.0, momentum 0.9, and cosine learning rate schedule for 10 epochs. For iNaturalist-2018, we used the LARS <cite class=\"ltx_cite ltx_citemacro_citep\">(You et al., <a class=\"ltx_ref\" href=\"#bib.bib75\" title=\"\">2017</a>)</cite> optimizer with learning rate 12.0, momentum 0.9, and cosine learning rate schedule for 20 epochs.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.SS0.SSS0.Px3.p3\">\n<p class=\"ltx_p\">For OAK evaluations, we use Faster R-CNN <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2015</a>)</cite>, a popular two-stage object detector. We initialize the ResNet-50 <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">2016</a>)</cite> backbone with the backbone of the final checkpoint of the streaming SSL model, and fine-tune the entire model on OAK with IID training for 10 epochs, following the training configurations of <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"#bib.bib72\" title=\"\">2023</a>)</cite>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Related Work</h2>\n<section class=\"ltx_paragraph\" id=\"A2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Self-Supervised Learning.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">A large number of self-supervised representation learning methods in computer vision follows the contrastive learning framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Oord et al., <a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2018</a>; Misra &amp; Maaten, <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">2020</a>; Tian et al., <a class=\"ltx_ref\" href=\"#bib.bib62\" title=\"\">2020</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">2020</a>; Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>; Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite> which maximizes the agreement of representations of two augmented views of the same image and minimizes that of different images. Extending this idea, the supervised contrastive (SupCon) method <cite class=\"ltx_cite ltx_citemacro_citep\">(Khosla et al., <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2020</a>)</cite> uses the labels as an extra supervision signal to get multiple positive crops for each anchor image. Other recent self-supervised learning works include pretext tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Doersch et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2015</a>; Noroozi &amp; Favaro, <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">2016</a>; Gidaris et al., <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2018</a>; Pathak et al., <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">2016</a>)</cite>, feature space clustering <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2018</a>; <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">2020</a>; Ren et al., <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">2021</a>)</cite>, distillation with asymmetric architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Grill et al., <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2020</a>; Chen &amp; He, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2021</a>)</cite>, redundancy reduction <cite class=\"ltx_cite ltx_citemacro_citep\">(Zbontar et al., <a class=\"ltx_ref\" href=\"#bib.bib80\" title=\"\">2021</a>; Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">2022</a>)</cite>, and masked autoencoding <cite class=\"ltx_cite ltx_citemacro_citep\">(He et al., <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">2022</a>)</cite>. Most relevant of these to our work, <cite class=\"ltx_cite ltx_citemacro_citet\">Orhan et al. (<a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite> proposes the temporal classification objective, which outperforms contrastive learning objectives on the SAYCam dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Sullivan et al., <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">2021</a>)</cite>. Our work enhances the temporal classification method by using a more flexible supervised contrastive objective, and leveraging temporal segmentation <cite class=\"ltx_cite ltx_citemacro_citep\">(Potapov et al., <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2014</a>; Afham et al., <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">2023</a>)</cite>, which have been used extensively in video summarization <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et al., <a class=\"ltx_ref\" href=\"#bib.bib83\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib81\" title=\"\">2016</a>; Rochan et al., <a class=\"ltx_ref\" href=\"#bib.bib54\" title=\"\">2018</a>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Segmentation in Human Cognition.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Prior research in psychology and cognitive sciences has shown that humans, including infants, are able to identify boundaries between action segments <cite class=\"ltx_cite ltx_citemacro_citep\">(Newtson et al., <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">1977</a>; Zacks et al., <a class=\"ltx_ref\" href=\"#bib.bib78\" title=\"\">2001</a>; Baldwin et al., <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2001</a>; Saylor et al., <a class=\"ltx_ref\" href=\"#bib.bib56\" title=\"\">2007</a>; Baldassano et al., <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2017</a>; Yates et al., <a class=\"ltx_ref\" href=\"#bib.bib74\" title=\"\">2022</a>)</cite>. Evidence in neuro-imaging further shows that event segmentation is an automatic component in human perception <cite class=\"ltx_cite ltx_citemacro_citep\">(Zacks &amp; Swallow, <a class=\"ltx_ref\" href=\"#bib.bib77\" title=\"\">2007</a>)</cite>. Temporal event segmentation has proven to be critical for memory formation and retrieval <cite class=\"ltx_cite ltx_citemacro_citep\">(Lassiter &amp; Slaw, <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">1991</a>; Ezzyat &amp; Davachi, <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2011</a>; DuBrow &amp; Davachi, <a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">2013</a>; Silva et al., <a class=\"ltx_ref\" href=\"#bib.bib57\" title=\"\">2019</a>; Sasmita &amp; Swallow, <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a>)</cite>. The temporal segmentation component in our proposed framework is motivated by how humans interpret videos as segments with coherent semantics. We demonstrate that temporal segmentation can improve the learned visual representation.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Results</h2>\n<section class=\"ltx_subsection\" id=\"A3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.1 </span>Performance of Different Normalization Layers</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS1.p1\">\n<p class=\"ltx_p\">We experimented with a variation of Memory Storyboard as well as three baseline methods (SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020</a>)</cite>, Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite>, and Temporal Classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Orhan et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2020</a>)</cite>) where the group normalization layers in the ResNet backbone are replaced with batch normalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Ioffe &amp; Szegedy, <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">2015</a>)</cite> layers. The models are trained on SAYCam and evaluated on the downstream <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet classification task with an SVM. The resulting accuracies are shown in Table <a class=\"ltx_ref\" href=\"#A3.T7\" title=\"Table 7 ‣ C.1 Performance of Different Normalization Layers ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that GroupNorm significantly outperforms BatchNorm for all the models examined. This result is aligned with the conclusion in <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite> that BatchNorm is less compatible with unsupervised continual learning, and extends the conclusion to streaming SSL.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T7\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Osiris</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MemStoryboard</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Batch Norm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Group Norm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">37.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>\nGroup norm is better at dealing with temporal non-stationarity for streaming SSL.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.2 </span>Separating Short-term Memory Batch and Long-term Memory Batch</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS2.p1\">\n<p class=\"ltx_p\">Inspired by the design of separating the loss on the new data and the replay data in Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">2024</a>)</cite>, we investigate the optimal strategy of applying the temporal contrastive loss on the training batch. We consider applying the temporal contrastive loss only on data from short-term memory, only on data from long-term memory, separately on data from short-term and long-term memory and average the losses, and on the entire training batch (concatenated data from short-term and long-term memory). We report the results in Table <a class=\"ltx_ref\" href=\"#A3.T8\" title=\"Table 8 ‣ C.2 Separating Short-term Memory Batch and Long-term Memory Batch ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. For experiments in the main paper, we apply the temporal contrastive loss only on data from long-term memory.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS2.p2\">\n<p class=\"ltx_p\">The results here demonstrate that applying the temporal contrastive loss only on data from long-term memory or on the entire training batch achieves the best performance. Applying the temporal contrastive loss only on data from short-term memory achieves inferior performance due to the limited number of temporal classes in the short-term buffer.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T8\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Labeled-S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OAK mAP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Short Only</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">52.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Long Only</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.29</span></td>\n<td class=\"ltx_td ltx_align_center\">36.36</td>\n<td class=\"ltx_td ltx_align_center\">21.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Concatenate</th>\n<td class=\"ltx_td ltx_align_center\">38.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">55.43</td>\n<td class=\"ltx_td ltx_align_center\">36.08</td>\n<td class=\"ltx_td ltx_align_center\">21.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Separate</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">54.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">21.40</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Performance of Memory Storyboard when the temporal contrastive loss is applied on different parts of the training batch.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.3 </span>Memory Storyboard in the IID Setting</h3>\n<div class=\"ltx_para\" id=\"A3.SS3.p1\">\n<p class=\"ltx_p\">To provide more context for the performance of Memory Storyboard in the Streaming Learning setting, we investigate the performance of Memory Storyboard in the IID setting. We take a SimCLR IID pre-trained model and use it to generate temporal segmentations on the entire dataset. Then we assign pseudo-labels to each frame according to the temporal segmentation results and train Memory Storyboard in the IID setting, taking the same number of gradient steps as the streaming setting. We observe that IID Memory Storyboard outperforms IID SimCLR and IID Simsiam, demonstrating the effectiveness of the temporal contrastive loss even in the IID setting.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T9\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">iNat</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR MemStoryboard (50K)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR MemStoryboard (50K)</th>\n<td class=\"ltx_td ltx_align_center\">41.32</td>\n<td class=\"ltx_td ltx_align_center\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.85</td>\n<td class=\"ltx_td ltx_align_center\">35.20</td>\n<td class=\"ltx_td ltx_align_center\">22.75</td>\n<td class=\"ltx_td ltx_align_center\">6.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimCLR</th>\n<td class=\"ltx_td ltx_align_center\">44.04</td>\n<td class=\"ltx_td ltx_align_center\">30.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.69</td>\n<td class=\"ltx_td ltx_align_center\">36.90</td>\n<td class=\"ltx_td ltx_align_center\">23.77</td>\n<td class=\"ltx_td ltx_align_center\">5.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimSiam</th>\n<td class=\"ltx_td ltx_align_center\">29.02</td>\n<td class=\"ltx_td ltx_align_center\">20.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.91</td>\n<td class=\"ltx_td ltx_align_center\">28.58</td>\n<td class=\"ltx_td ltx_align_center\">22.28</td>\n<td class=\"ltx_td ltx_align_center\">4.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimCLR MemStoryboard</th>\n<td class=\"ltx_td ltx_align_center\">44.54</td>\n<td class=\"ltx_td ltx_align_center\">29.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.92</td>\n<td class=\"ltx_td ltx_align_center\">37.60</td>\n<td class=\"ltx_td ltx_align_center\">24.43</td>\n<td class=\"ltx_td ltx_align_center\">7.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">IID SimSiam MemStoryboard</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">42.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">31.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">10.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.06</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Performance of Memory Storyboard in the IID Setting.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.4 </span>Memory Storyboard with the Cross Entropy Loss</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS4.p1\">\n<p class=\"ltx_p\">One alternative objective for the temporal contrastive loss in Memory Storyboard is the cross entropy (CE) loss.\nWe investigate the performance of Memory Storyboard with the CE loss instead of the Supervised Contrastive (SupCon) loss. Results are summarized in Table <a class=\"ltx_ref\" href=\"#A3.T10\" title=\"Table 10 ‣ C.4 Memory Storyboard with the Cross Entropy Loss ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. We observe that the CE loss outperforms the SupCon loss as the temporal contrastive loss when trained jointly with the SimCLR objective. We opted for the SupCon loss in the main text due to its flexibility. With the CE loss, we either need to know the number of temporal segments beforehand or gradually increase the size of the final classifier layer as the model sees more data, which is not ideal for streaming learning on a never-ending video stream. With the SupCon loss, we can learn from a very large number of temporal segments with the same model size.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T10\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:465.1pt;height:75.2pt;vertical-align:-75.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-67.3pt,0.0pt) scale(0.775610538532468,0.775610538532468) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Temp. Contrast Loss</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 50K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 50K</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</td>\n<td class=\"ltx_td ltx_align_center\">INet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">iNat</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</td>\n<td class=\"ltx_td ltx_align_center\">INet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">iNat</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</td>\n<td class=\"ltx_td ltx_align_center\">INet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">iNat</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</td>\n<td class=\"ltx_td ltx_align_center\">INet</td>\n<td class=\"ltx_td ltx_align_center\">iNat</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">SupCon</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">35.02</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">20.72</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">5.65</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">39.58</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">24.78</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">7.77</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">33.72</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">20.13</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">5.64</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">36.36</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">22.75</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">6.10</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Cross Entropy</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">24.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">6.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">40.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">8.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">5.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.73</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Results on Memory Storyboard (using SimCLR as the base SSL method) with the cross entropy-loss instead of the supervised contrastive loss as the temporal contrastive loss.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.5 </span>Memory Storyboard with only the Temporal Contrastive Loss</h3>\n<div class=\"ltx_para\" id=\"A3.SS5.p1\">\n<p class=\"ltx_p\">To demonstrate the need for both the self-supervised loss and the temporal contrastive loss in our training objective, we experiment with using only\nthe temporal contrastive (SupCon) loss and not the self-supervised loss (only using the self-supervised loss (SimCLR or SimSiam) and not the temporal contrastive loss has been experimented in the “two-tier” buffer baseline in Tables <a class=\"ltx_ref\" href=\"#S5.T1\" title=\"Table 1 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and  <a class=\"ltx_ref\" href=\"#S5.T2\" title=\"Table 2 ‣ Baselines. ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Results are shown in Table <a class=\"ltx_ref\" href=\"#A3.T11\" title=\"Table 11 ‣ C.5 Memory Storyboard with only the Temporal Contrastive Loss ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>. We observe that the performance of only using the SupCon loss is also inferior to the full memory storyboard method, demonstrating the necessity of joint training on both losses for best performance.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T11\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:465.1pt;height:88.1pt;vertical-align:-88.1pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-71.1pt,0.0pt) scale(0.765936563231137,0.765936563231137) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 50K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 50K</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">iNat</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR MemStoryboard</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam MemStoryboard</th>\n<td class=\"ltx_td ltx_align_center\">36.72</td>\n<td class=\"ltx_td ltx_align_center\">22.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.66</td>\n<td class=\"ltx_td ltx_align_center\">41.32</td>\n<td class=\"ltx_td ltx_align_center\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.85</td>\n<td class=\"ltx_td ltx_align_center\">33.78</td>\n<td class=\"ltx_td ltx_align_center\">21.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.51</td>\n<td class=\"ltx_td ltx_align_center\">35.20</td>\n<td class=\"ltx_td ltx_align_center\">22.75</td>\n<td class=\"ltx_td ltx_align_center\">6.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Supcon Only</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">21.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">6.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">24.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">8.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">31.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">21.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">5.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">23.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.56</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Results on Memory Storyboard with only the temporal contrastive Loss.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS6\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.6 </span>Memory Storyboard with Multiple Gradient Steps per Batch</h3>\n<div class=\"ltx_para\" id=\"A3.SS6.p1\">\n<p class=\"ltx_p\">Using multiple gradient steps for each batch is a widely used technique in online continual learning <cite class=\"ltx_cite ltx_citemacro_cite\">Madaan et al. (<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2022</a>)</cite>. We investigate the performance of Memory Storyboard when we take multiple gradient steps on each batch. Results are shown in Table <a class=\"ltx_ref\" href=\"#A3.T12\" title=\"Table 12 ‣ C.6 Memory Storyboard with Multiple Gradient Steps per Batch ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>. We observe that using multiple gradient steps (2 or 4) produces a sizable improvement on the ImageNet readout evaluation on KrishnaCam but not on the other benchmarks. We also observed that the improvement of multiple gradient steps is a lot smaller on SAYCam (sometimes even harming the performance), presumably due to the fact that SAYCam is a much larger training dataset than KrishnaCam and streaming learning without multiple gradient steps is sufficient for the model to capture a wide range of visual concepts.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T12\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:465.1pt;height:121.6pt;vertical-align:-121.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-90.6pt,0.0pt) scale(0.719645315429084,0.719645315429084) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Grad</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 50K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 50K</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Steps</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">iNat</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR MemStoryboard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR MemStoryboard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td class=\"ltx_td ltx_align_center\">34.56</td>\n<td class=\"ltx_td ltx_align_center\">23.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">5.66</td>\n<td class=\"ltx_td ltx_align_center\">38.44</td>\n<td class=\"ltx_td ltx_align_center\">26.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">7.67</td>\n<td class=\"ltx_td ltx_align_center\">33.00</td>\n<td class=\"ltx_td ltx_align_center\">23.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">5.45</td>\n<td class=\"ltx_td ltx_align_center\">35.30</td>\n<td class=\"ltx_td ltx_align_center\">26.36</td>\n<td class=\"ltx_td ltx_align_center\">6.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR MemStoryboard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td class=\"ltx_td ltx_align_center\">33.26</td>\n<td class=\"ltx_td ltx_align_center\">22.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.21</td>\n<td class=\"ltx_td ltx_align_center\">35.78</td>\n<td class=\"ltx_td ltx_align_center\">25.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">5.96</td>\n<td class=\"ltx_td ltx_align_center\">32.40</td>\n<td class=\"ltx_td ltx_align_center\">23.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">5.61</td>\n<td class=\"ltx_td ltx_align_center\">35.34</td>\n<td class=\"ltx_td ltx_align_center\">26.11</td>\n<td class=\"ltx_td ltx_align_center\">6.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam MemStoryboard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1</th>\n<td class=\"ltx_td ltx_align_center\">36.72</td>\n<td class=\"ltx_td ltx_align_center\">22.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.66</td>\n<td class=\"ltx_td ltx_align_center\">41.32</td>\n<td class=\"ltx_td ltx_align_center\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.85</td>\n<td class=\"ltx_td ltx_align_center\">33.78</td>\n<td class=\"ltx_td ltx_align_center\">21.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.51</td>\n<td class=\"ltx_td ltx_align_center\">35.20</td>\n<td class=\"ltx_td ltx_align_center\">22.75</td>\n<td class=\"ltx_td ltx_align_center\">6.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam MemStoryboard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td class=\"ltx_td ltx_align_center\">37.04</td>\n<td class=\"ltx_td ltx_align_center\">26.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">7.13</td>\n<td class=\"ltx_td ltx_align_center\">40.20</td>\n<td class=\"ltx_td ltx_align_center\">30.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.27</td>\n<td class=\"ltx_td ltx_align_center\">33.86</td>\n<td class=\"ltx_td ltx_align_center\">25.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.27</td>\n<td class=\"ltx_td ltx_align_center\">35.66</td>\n<td class=\"ltx_td ltx_align_center\">26.21</td>\n<td class=\"ltx_td ltx_align_center\">6.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">SimSiam MemStoryboard</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">4</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">22.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">6.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">7.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">33.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">24.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">6.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.62</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Results on Memory Storyboard with different number of gradient update steps per batch.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS7\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.7 </span>Memory Storyboard with Class-Balanced Buffer</h3>\n<div class=\"ltx_para\" id=\"A3.SS7.p1\">\n<p class=\"ltx_p\">Inspired by other methods with use smart memory storage policies <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a class=\"ltx_ref\" href=\"#bib.bib76\" title=\"\">2023</a>; Purushwalkam et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2022</a>)</cite>, we investigate the performance of Memory Storyboard with a class-balanced memory. When we attempt to add a new data point to the long-term memory that is already full, we randomly remove one of the data points from the class with the most samples in the memory. Results are shown in Table <a class=\"ltx_ref\" href=\"#A3.T13\" title=\"Table 13 ‣ C.7 Memory Storyboard with Class-Balanced Buffer ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe that using the class-balanced memory produces mild improvements over the reservoir sampling baseline, though results on specific runs are mixed. We think that the memory storyboard method should work well with many different buffer sampling strategies, and advancements in buffer sampling strategies are orthogonal to the contribution of this work.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A3.T13\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:465.1pt;height:94.8pt;vertical-align:-94.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-93.8pt,0.0pt) scale(0.712601354505378,0.712601354505378) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Base SSL Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Bal. Buffer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SAYCam 50K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 10K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam 50K</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">iNat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">INet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">iNat</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">✗</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">✓</th>\n<td class=\"ltx_td ltx_align_center\">34.04</td>\n<td class=\"ltx_td ltx_align_center\">22.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.41</td>\n<td class=\"ltx_td ltx_align_center\">38.58</td>\n<td class=\"ltx_td ltx_align_center\">26.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">7.58</td>\n<td class=\"ltx_td ltx_align_center\">31.92</td>\n<td class=\"ltx_td ltx_align_center\">21.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">5.70</td>\n<td class=\"ltx_td ltx_align_center\">34.66</td>\n<td class=\"ltx_td ltx_align_center\">23.64</td>\n<td class=\"ltx_td ltx_align_center\">5.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimSiam</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">✗</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">SimSiam</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">✓</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">34.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">23.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">6.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">37.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">24.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">7.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">33.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">22.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">6.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">36.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.09</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>Results on Memory Storyboard with class-balanced buffer.\n</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS8\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.8 </span>Additional Qualitative Results</h3>\n<section class=\"ltx_paragraph\" id=\"A3.SS8.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">OAK Object Detection Results.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS8.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We visualize the object detection results produced by Memory Storyboard when fine-tuned on the OAK dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2021</a>)</cite> in Figure <a class=\"ltx_ref\" href=\"#A3.F5\" title=\"Figure 5 ‣ Temporal Segmentation by Randomly Initialized Models. ‣ C.8 Additional Qualitative Results ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We observe that the fine-tuned model can successfully detect objects in cluttered environments. The results show that the representations learned by Memory Storyboard can be effectively transferred to downstream tasks which requires more fine-grained features.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A3.SS8.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Label Merging Results.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS8.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We visualize the class labels produced by the label merging mechanism in Memory Storyboard in Figure <a class=\"ltx_ref\" href=\"#A3.F7\" title=\"Figure 7 ‣ Temporal Segmentation by Randomly Initialized Models. ‣ C.8 Additional Qualitative Results ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that Memory Storyboard can successfully group semantically similar scenes together, which helps improve the representation learning performance.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A3.SS8.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Segmentation by Randomly Initialized Models.</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS8.SSS0.Px3.p1\">\n<p class=\"ltx_p\">We visualize the temporal segments produced by randomly initialized models in Figure <a class=\"ltx_ref\" href=\"#A3.F6\" title=\"Figure 6 ‣ Temporal Segmentation by Randomly Initialized Models. ‣ C.8 Additional Qualitative Results ‣ Appendix C Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. By comparing to Figure <a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 ‣ Qualitative Results. ‣ 5.2 Main Results ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we observe that randomly initialized models fail to capture intricate transitions between scenes and cannot create accurate temporal segments, while Memory Storyboard training enables the model to learn better image representations to capture more intricate scene transitions.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"325\" id=\"A3.F5.g1\" src=\"./assets/x5.png\" width=\"568\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of object detection results on the OAK validation set.</span> The Memory Storyboard model is trained on KrishnaCam and fine-tuned on the OAK training set. Red boxes show the predictions and the green boxes are ground truth bounding boxes.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"358\" id=\"A3.F6.g1\" src=\"./assets/x6.png\" width=\"592\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of the temporal segments produced by randomly initialized models on (a) SAYCam (b)(c) KrishnaCam.</span> The images are the same as the ones in Figure <a class=\"ltx_ref\" href=\"#S5.F3\" title=\"Figure 3 ‣ Qualitative Results. ‣ 5.2 Main Results ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. We observe that Memory Storyboard training enables to model to capture more intricate transitions between scenes.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"214\" id=\"A3.F7.g1\" src=\"./assets/x7.png\" width=\"592\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of label merging by Memory Storyboard on SAYCam.</span> Each image represents a temporal segment; segments sharing the same color bar have been merged. Memory Storyboard successfully groups semantically similar scenes—e.g., segments marked with the light blue bar are all associated with the dining table.\n</figcaption>\n</figure>\n</section>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Optimal Batch Composition for SimCLR</h2>\n<figure class=\"ltx_figure\" id=\"A4.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"223\" id=\"A4.F8.g1\" src=\"./assets/x8.png\" width=\"890\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>\n<span class=\"ltx_text ltx_font_bold\">SimCLR model performance on SAYCam with different long-term memory sizes</span> (5k, 10k, 50k, and 100k) <span class=\"ltx_text ltx_font_bold\">and varying training batch compositions</span> (12.5% – 75.0% from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.F8.m2\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>) using SVM readout. Each colored line represents the performance of different training batch compositions when <span class=\"ltx_text ltx_font_bold\">the model has seen the same amount of data</span> from the stream. Each black line represents the performance of different training batch compositions when the model has taken <span class=\"ltx_text ltx_font_bold\">the same number of gradient updates</span>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"228\" id=\"A4.F9.g1\" src=\"./assets/x9.png\" width=\"890\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>\n<span class=\"ltx_text ltx_font_bold\">Comparison of Memory Storyboard (solid lines) and SimCLR (dashed lines) model performance</span> on SAYCam using SVM readout, controlling the amount of data the model has seen from the stream.\n</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"A4.p1\">\n<p class=\"ltx_p\">We replicate the experiments in Figure <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> on SimCLR models with two-tier memory, and plot the results in Figure <a class=\"ltx_ref\" href=\"#A4.F8\" title=\"Figure 8 ‣ Appendix D Optimal Batch Composition for SimCLR ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. We observe that the analysis and the conclusions of section <a class=\"ltx_ref\" href=\"#S5.SS4\" title=\"5.4 Optimal Batch Composition Under Different Memory Constraints ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> still hold: when we have a large memory, we either prefer balanced training batches (with a fixed amount of computation) or a bigger batch from long-term memory (with a fixed amount of data); when we can only afford a small memory, we prefer a smaller batch from long-term memory. We also want to note that the SVM readout results start to go down towards the end of the streaming training in SimCLR experiments more often than Memory Storyboard experiments, suggesting the better scalability of Memory Storyboard to larger-scale streaming training.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A4.p2\">\n<p class=\"ltx_p\">These results demonstrate that the analysis and observations in section <a class=\"ltx_ref\" href=\"#S5.SS4\" title=\"5.4 Optimal Batch Composition Under Different Memory Constraints ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> regarding the optimal batch composition for streaming SSL training under different memory and compute constraints are general, and apply to standard SSL methods in addition to Memory Storyboard.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A5\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>More Comprehensive Comparison between Memory Storyboard and SimCLR</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.p1\">\n<p class=\"ltx_p\">With the experiment results in Figure <a class=\"ltx_ref\" href=\"#S5.F4\" title=\"Figure 4 ‣ Average Segment Length. ‣ 5.3 Ablation Experiments and Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"#A4.F8\" title=\"Figure 8 ‣ Appendix D Optimal Batch Composition for SimCLR ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we provide a more comprehensive comparison between Memory Storyboard and SimCLR performance under different memory constraints and batch compositions in Figure <a class=\"ltx_ref\" href=\"#A4.F9\" title=\"Figure 9 ‣ Appendix D Optimal Batch Composition for SimCLR ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. We observe that Memory Storyboard outperforms SimCLR under the same amount of seen data, across a wide range of memory sizes and batch compositions. In particular, we note that Memory Storyboard significantly outperforms SimCLR when we sample more data from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p1.m1\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> (towards the right side of the <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p1.m2\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>-axis). This results in higher optimal performance when the memory size is small, where a larger batch from <math alttext=\"M_{short}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p1.m3\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> is needed to prevent overfitting on the long-term memory for better performance. We argue that, with temporal segmentation and the temporal contrastive loss, Memory Storyboard is able to provide better memory efficiency and also alleviate the temporal correlation issue suffered by SimCLR when we sample a large batch from the short-term memory.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section><div about=\"\" class=\"ltx_rdf\" content=\"Overleaf Example\" property=\"dcterms:title\"></div>",
  "css": "",
  "arxiv_id": "2501.12254",
  "source": "arxiv-experimental",
  "generated": "2025-10-18T01:15:19.304Z"
}
