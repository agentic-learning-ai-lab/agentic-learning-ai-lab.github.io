{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Humans are capable of learning continuously from a stream of unlabeled and uncurated perceptual inputs, such as video data, without needing to iterate through multiple exposures or epochs. Since early infancy, humans have accumulated knowledge about the world through a continuous flow of raw visual observations. This capability contrasts sharply with the training paradigm of current methods in self-supervised learning (SSL). While recent SSL approaches have made great strides in learning from large unlabeled datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>, <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>, they still predominantly rely on static and curated image datasets, such as ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>, and needs multiple epochs of training for effective learning. This difference in paradigm raises a compelling question: how can we learn good visual representations in a streaming setting—learning from visual inputs in their original temporal order without cycling back?</p>\n</div>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Motivated by the differences of mechanisms between human learning and standard SSL, we aim to build learning algorithms that can efficiently learn visual representations and concepts from streaming video. One especially relevant mechanism in the human brain is event segmentation <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>, <a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">70</a>]</cite>, where we spontaneously segment visual streams into hierarchically structured events and identify the event boundaries. Take your recent vacation trip as an example—you probably remember separate events and activities like exploring a city, dining at a local restaurant, or relaxing at the beach. The event segmentation mechanism helps us organize memories, recall specific moments, and summarize from lengthened experiences <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib68\" title=\"\" class=\"ltx_ref\">68</a>, <a href=\"#bib.bib69\" title=\"\" class=\"ltx_ref\">69</a>]</cite>.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">Drawing inspiration from the way we organize our memory in the brain,\nwe introduce <span class=\"ltx_text ltx_font_italic\">Memory Storyboard</span>, a novel approach for streaming self-supervised learning. Memory Storyboard features a temporal segmentation module, which groups video frames into semantically meaningful temporal segments, resembling the automatic event segmentation of human cognition. Through our temporal contrastive learning objective, these temporal segments effectively facilitate representation learning in streaming videos. To accommodate efficient temporal segmentation, we propose a two-tier hierarchical memory: temporal segmentation in the short-term memory are used to update the temporal class labels in the long-term memory, and a training batch consists of samples mixed from both memories. A high-level diagram of the algorithm is shown in Figure <a href=\"#S0.F1\" title=\"Figure 1 ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">We conduct experiments on the SAYCam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a>]</cite> and KrishnaCam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite> datasets of real-world egocentric videos. Memory Storyboard outperforms state-of-the-art unsupervised continual learning methods on downstream image classification and object detection tasks, and significantly reduces the gap between streaming learning and the less flexible IID learning that requires persistent storage of the entire prior video data. We also experiment with different buffer sizes and batch sizes and offer insights on the optimal training batch composition under different memory constraints.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para\">\n<p class=\"ltx_p\">We summarize our contributions as follows:</p>\n<ol id=\"S1.I1\" class=\"ltx_enumerate\">\n<li id=\"S1.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1)</span> \n<div id=\"S1.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We introduce Memory Storyboard, a novel streaming SSL framework that features temporal segmentation and a two-tier memory hierarchy for efficient learning and temporal abstraction.</p>\n</div>\n</li>\n<li id=\"S1.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2)</span> \n<div id=\"S1.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We demonstrate that Memory Storyboard achieves state-of-the-art downstream performance when trained on real-world egocentric video datasets. Among all the streaming self-supervised learning methods we evaluated, Memory Storyboard is the only one that is competitive with or even outperforms IID training when trained on these datasets.</p>\n</div>\n</li>\n<li id=\"S1.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3)</span> \n<div id=\"S1.I1.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We study the effects of training factors including subsampling rate, average segment length, memory buffer size and training batch composition. These studies provide insight for more efficient streaming learning from videos. In particular, we explore the optimal composition ratio of the training batch from short-term vs. long-term memory, under different memory constraints. Larger batches from long-term memory improve performance when we can afford a large memory bank, while smaller batches can help prevent overfitting when we have a small memory bank.</p>\n</div>\n</li>\n</ol>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n\n<section id=\"S2.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Unsupervised Continual Learning.</h4>\n\n<div id=\"S2.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Unsupervised Continual Learning (UCL) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">44</a>, <a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">53</a>]</cite> aims at learning a good representation through an unlabeled non-stationary data stream. Existing works in UCL, notably CaSSLe <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite> and Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>, assume that the data stream is composed of a series of episodes, and a stationary data distribution within each episode. This is not as naturalistic and human-like as our streaming setting, where the data distribution changes continuously in through the data stream, and each image appear in the data stream only once. Meanwhile, we showed that existing UCL methods are also effective in our streaming video setting, and can be used together with the supervised contrastive objective.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Self-Supervised Learning.</h4>\n\n<div id=\"S2.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">A large number of self-supervised representation learning methods in computer vision follows the contrastive learning framework <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>, <a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>, <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">56</a>, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite> which maximizes the agreement of representations of two augmented views of the same image and minimizes that of different images. Extending this idea, the supervised contrastive (SupCon) method <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite> uses the labels as an extra supervision signal to get multiple positive crops for each anchor image. Other recent self-supervised learning works include pretext tasks <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>, <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>, <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>, <a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">41</a>]</cite>, feature space clustering <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>, <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite>, distillation with asymmetric architectures <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>, redundancy reduction <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib71\" title=\"\" class=\"ltx_ref\">71</a>, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>, and masked autoencoding <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>. Most relevant of these to our work, <cite class=\"ltx_cite ltx_citemacro_citet\">Orhan et al., [<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite> proposes the temporal classification objective, which outperforms contrastive learning objectives on the SAYCam dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a>]</cite>. Our work enhances the temporal classification method with using a more flexible supervised contrastive objective, and leveraging temporal segmentation <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">1</a>]</cite>, which have been used extensively in video summarization <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib74\" title=\"\" class=\"ltx_ref\">74</a>, <a href=\"#bib.bib72\" title=\"\" class=\"ltx_ref\">72</a>, <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">48</a>]</cite>.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Streaming Learning from Videos.</h4>\n\n<div id=\"S2.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">While a number of recent papers have studied streaming learning from images <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>, <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>, limited works have investigated the problem of streaming learning from a continuous video stream. <cite class=\"ltx_cite ltx_citemacro_citet\">Roady et al., [<a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">47</a>]</cite> introduces a benchmark for streaming classification and novelty detection from videos. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al., [<a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>]</cite> benchmarks many self-supervised learning methods in real-time and life-long learning settings in streaming video, assuming infinite replay buffer size which is unrealistic. Most similar to our setup, <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al., [<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite> studies the task of continuous representation learning with a SimSiam objective <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite> and proposes using a minimum-redundancy replay buffer. Their work also belongs to the broader range of works that study replay buffer sampling strategies in continual learning <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>, <a href=\"#bib.bib62\" title=\"\" class=\"ltx_ref\">62</a>, <a href=\"#bib.bib57\" title=\"\" class=\"ltx_ref\">57</a>, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>. Our work extends these prior works by adopting a two-tier replay buffer and a temporal segmentation component. Also relevant to our work, <cite class=\"ltx_cite ltx_citemacro_citet\">Carreira et al., [<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite> studies online learning from a continuous video stream with pixel-to-pixel modeling, but their exploration mainly focuses on settings without data augmentation and replay, limiting the efficacy of their framework.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Segmentation in Human Cognition.</h4>\n\n<div id=\"S2.SS0.SSS0.Px4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Prior research in psychology and cognitive sciences has shown that humans, including infants, are able to identify boundaries between action segments <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>, <a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">70</a>, <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>, <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">50</a>, <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>, <a href=\"#bib.bib65\" title=\"\" class=\"ltx_ref\">65</a>]</cite>. Evidence in neuro-imaging further show that event segmentation is an automatic component in human perception <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib69\" title=\"\" class=\"ltx_ref\">69</a>]</cite>. Temporal event segmentation has proven to be critical for memory formation and retrieval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>, <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">16</a>, <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>, <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>. The temporal segmentation component in our proposed framework is motivated by how humans interpret videos as segments with coherent semantics. We demonstrate that temporal segmentation can improve the learned visual representation.</p>\n</div>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Streaming SSL from Egocentric Videos</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In streaming self-supervised learning, the goal is to learn useful visual representations from a continuous stream of inputs <math id=\"S3.p1.m1\" class=\"ltx_Math\" alttext=\"(x_{1},x_{2},\\dots)\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{1},x_{2},\\dots)</annotation></semantics></math>. Here, we focus on the setting where inputs are uniformly sampled from a video stream.\nSimilar to continual learning, we impose a memory budget so that storing the entire video would violate the constraint.\nDifferent from standard continual learning, there is no explicit notion of task, and the data distribution shift follows directly from the scene transitions of a video.\nThe learner needs to make changes to the model as it sees new inputs, and finishes learning as soon as it receives the last input of the stream. The streaming setting is similar to Online Continual Learning <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>, <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>, <a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">61</a>]</cite>, but the focus here is primarily on streaming video frames instead of a fixed dataset of static images.</p>\n</div>\n<section id=\"S3.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Incorporating Training Batches.</h4>\n\n<div id=\"S3.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We use <math id=\"S3.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"x_{start:end}\" display=\"inline\"><semantics><msub><mi>x</mi><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow></mrow></msub><annotation encoding=\"application/x-tex\">x_{start:end}</annotation></semantics></math> to denote the batch of <math id=\"S3.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"(end-start)\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow><mo>−</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(end-start)</annotation></semantics></math> images between <math id=\"S3.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"x_{start}\" display=\"inline\"><semantics><msub><mi>x</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">x_{start}</annotation></semantics></math> and <math id=\"S3.SS0.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"x_{end}\" display=\"inline\"><semantics><msub><mi>x</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">x_{end}</annotation></semantics></math>. At each training step <math id=\"S3.SS0.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the model fetches a new batch of <math id=\"S3.SS0.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> images <math id=\"S3.SS0.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"X_{curr}=x_{tb:(t+1)b}\" display=\"inline\"><semantics><mrow><msub><mi>X</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi></mrow></msub><mo>=</mo><msub><mi>x</mi><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>b</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>b</mi></mrow></mrow></msub></mrow><annotation encoding=\"application/x-tex\">X_{curr}=x_{tb:(t+1)b}</annotation></semantics></math> from the video stream. The model produces model updates on its parameters <math id=\"S3.SS0.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"\\theta_{t+1}\" display=\"inline\"><semantics><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\theta_{t+1}</annotation></semantics></math> upon receiving <math id=\"S3.SS0.SSS0.Px1.p1.m9\" class=\"ltx_Math\" alttext=\"X_{curr}\" display=\"inline\"><semantics><msub><mi>X</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">X_{curr}</annotation></semantics></math>. At the end of the video, we evaluate the final model checkpoint <math id=\"S3.SS0.SSS0.Px1.p1.m10\" class=\"ltx_Math\" alttext=\"\\theta_{T}\" display=\"inline\"><semantics><msub><mi>θ</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{T}</annotation></semantics></math> on various downstream tasks such as object classification and detection, which are fundamental tasks for visual scene understanding as they enable models to recognize and interpret the contents of complex environments.</p>\n</div>\n</section>\n<section id=\"S3.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Standard SSL Fails on Streaming Video.</h4>\n\n<div id=\"S3.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Directly applying the SSL method on <math id=\"S3.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"X_{curr}\" display=\"inline\"><semantics><msub><mi>X</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">X_{curr}</annotation></semantics></math> gives very poor performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>, <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite>. This is mainly due to two reasons:</p>\n<ol id=\"S3.I1\" class=\"ltx_enumerate\">\n<li id=\"S3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1)</span> \n<div id=\"S3.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">the non-stationary distribution of visual features in the stream, similar to the catastrophic forgetting problem <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite> in supervised continual learning;</p>\n</div>\n</li>\n<li id=\"S3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2)</span> \n<div id=\"S3.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">the high temporal correlation of images in the stream (illustrated in Figure <a href=\"#S0.F1\" title=\"Figure 1 ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). This temporal correlation breaks the IID assumption held by common optimization algorithms like SGD or Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite>. For contrastive learning algorithms like SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>, the similarity across different frames in the same training batch would violate the assumption that each image is different.</p>\n</div>\n</li>\n</ol>\n</div>\n</section>\n<section id=\"S3.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Memory Replay.</h4>\n\n<div id=\"S3.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Similar to previous works <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a>, <a href=\"#bib.bib67\" title=\"\" class=\"ltx_ref\">67</a>, <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>, we use a replay buffer <math id=\"S3.SS0.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> with finite size <math id=\"S3.SS0.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"|M|\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M|</annotation></semantics></math> to mitigate these issues. The model can store some of the fetched images in the replay buffer, and use both samples from the replay buffer and the new frames to form a training batch of size <math id=\"S3.SS0.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>. By sampling from the replay buffer we de-correlate the frames in the training batch and at the same time reduce distribution shift between training batches.</p>\n</div>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Memory Storyboard</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We present Memory Storyboard, an effective method for streaming SSL from egocentric videos. Memory Storyboard includes a temporal segmentation module and a two-tier memory hierarchy. It combines a standard self-supervised contrastive loss with a temporal contrastive objective with leverages the temporal class labels produced by the temporal segmentation module. Figure <a href=\"#S4.F2\" title=\"Figure 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the details of our method. The overall data processing and training procedure is summarized in Algorithm <a href=\"#alg2\" title=\"Algorithm 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n</div>\n<section id=\"S4.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Segmentation.</h4>\n\n<div id=\"S4.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We describe our temporal segmentation algorithm as follows. Similar to <cite class=\"ltx_cite ltx_citemacro_citet\">Potapov et al., [<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite>, we are given a down-sampled video frame sequence <math id=\"S4.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"x_{1},x_{2},\\cdots,x_{L}\" display=\"inline\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{1},x_{2},\\cdots,x_{L}</annotation></semantics></math>, and a feature extractor <math id=\"S4.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"f_{\\theta}\" display=\"inline\"><semantics><msub><mi>f</mi><mi>θ</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>. We aim to find change points <math id=\"S4.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"t_{1},t_{2},\\cdots,t_{n-1}\" display=\"inline\"><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><msub><mi>t</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{1},t_{2},\\cdots,t_{n-1}</annotation></semantics></math> so that the video is divided into <math id=\"S4.SS0.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> semantically-consistent segments <math id=\"S4.SS0.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"[x_{1},x_{t_{1}}],[x_{t_{1}},x_{t_{2}}],\\cdots,[x_{t_{n-1}},x_{L}]\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><msub><mi>t</mi><mn>1</mn></msub></msub><mo stretchy=\"false\">]</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><msub><mi>t</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>x</mi><msub><mi>t</mi><mn>2</mn></msub></msub><mo stretchy=\"false\">]</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><msub><mi>t</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></msub><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">[x_{1},x_{t_{1}}],[x_{t_{1}},x_{t_{2}}],\\cdots,[x_{t_{n-1}},x_{L}]</annotation></semantics></math>. We also define <math id=\"S4.SS0.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"t_{0}=0\" display=\"inline\"><semantics><mrow><msub><mi>t</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">t_{0}=0</annotation></semantics></math> and <math id=\"S4.SS0.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"t_{n}=L\" display=\"inline\"><semantics><mrow><msub><mi>t</mi><mi>n</mi></msub><mo>=</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">t_{n}=L</annotation></semantics></math>. In this work, we determine the number of segments with <math id=\"S4.SS0.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"n=\\frac{L}{T}\" display=\"inline\"><semantics><mrow><mi>n</mi><mo>=</mo><mfrac><mi>L</mi><mi>T</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">n=\\frac{L}{T}</annotation></semantics></math>, where <math id=\"S4.SS0.SSS0.Px1.p1.m9\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> refers to the average segment length and is a hyper-parameter.</p>\n</div>\n<div id=\"S4.SS0.SSS0.Px1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">The optimization objective of our segmentation algorithm is to maximize the average within-class similarity, i.e.</p>\n<table id=\"S4.E1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E1.m1\" class=\"ltx_Math\" alttext=\"\\max_{t_{1},t_{2},\\cdots,t_{n-1}}\\sum_{i=1}^{n}\\frac{1}{t_{i}-t_{i-1}}\\sum_{j=t_{i-1}}^{t_{i}}\\sum_{k=j}^{t_{i}}sim(x_{j},x_{k}).\" display=\"block\"><semantics><mrow><mrow><munder><mi>max</mi><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">⋯</mi><mo>,</mo><msub><mi>t</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></munder><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>−</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>j</mi><mo>=</mo><msub><mi>t</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><msub><mi>t</mi><mi>i</mi></msub></munderover><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>k</mi><mo>=</mo><mi>j</mi></mrow><msub><mi>t</mi><mi>i</mi></msub></munderover><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\max_{t_{1},t_{2},\\cdots,t_{n-1}}\\sum_{i=1}^{n}\\frac{1}{t_{i}-t_{i-1}}\\sum_{j=t_{i-1}}^{t_{i}}\\sum_{k=j}^{t_{i}}sim(x_{j},x_{k}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS0.SSS0.Px1.p2.m1\" class=\"ltx_Math\" alttext=\"sim(x_{j},x_{k})\" display=\"inline\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sim(x_{j},x_{k})</annotation></semantics></math> denotes the cosine similarity between the embeddings <math id=\"S4.SS0.SSS0.Px1.p2.m2\" class=\"ltx_Math\" alttext=\"f_{\\theta}(x_{j})\" display=\"inline\"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{\\theta}(x_{j})</annotation></semantics></math> and <math id=\"S4.SS0.SSS0.Px1.p2.m3\" class=\"ltx_Math\" alttext=\"f_{\\theta}(x_{k})\" display=\"inline\"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{\\theta}(x_{k})</annotation></semantics></math>.\nWe compute the approximate solution to this optimization problem with a greedy approach, as detailed in Algorithm <a href=\"#alg1\" title=\"Algorithm 1 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We adopt this simple temporal segmentation approach in order to get good segmentation results in the beginning, when encoder network does not provide good representations. We leave it to future work for investigating different temporal segmentation strategies.</p>\n</div>\n<figure id=\"alg1\" class=\"ltx_float ltx_minipage ltx_align_top ltx_framed ltx_framed_top\" style=\"width:203.8pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 1</span> </span> Temporal Segmentation</figcaption>\n<div class=\"ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing\" style=\"background-color:#FFFFFF;\">\n<div class=\"ltx_listing_data\"><a href=\"data:text/plain;base64,IyBuOiBudW1iZXIgb2YgY2x1c3RlcnMKIyBmZWF0czogZmVhdHVyZXMgb2YgdGhlIGZyYW1lcyBpbiB0aGUgc2VxdWVuY2UKIyBGOiBtYXhpbWl6YXRpb24gb2JqZWN0aXZlIChkZWZpbmVkIGJ5IEVxdWF0aW9uIDEpLgojIFJldHVybnM6IGRldGVjdGVkIGNoYW5nZSBwb2ludHMgaW4gdGhlIHN0cmVhbSAoc29ydGVkKQoKZGVmIHRlbXBvcmFsX3NlZ21lbnQobiwgZmVhdHMsIEYpOgogICAgUyA9IGZlYXRzIEAgZmVhdHMuVAogICAgTCA9IGxlbihTKQogICAgY2hhbmdlcHRzID0gW10KICAgIGZvciBpIGluIHJhbmdlKDEsIG4pOgogICAgICAgIGJlc3RzY29yZSA9IDAKICAgICAgICBmb3IgY2hhbmdlcHQgaW4gcmFuZ2UoMSwgTCk6CiAgICAgICAgICAgIHRlbXAgPSBjaGFuZ2VwdHMgKyBbY2hhbmdlcHRdCiAgICAgICAgICAgIHNjb3JlID0gRihzb3J0ZWQodGVtcCkpCiAgICAgICAgICAgIGlmIHNjb3JlID4gYmVzdHNjb3JlOgogICAgICAgICAgICAgICAgYmVzdHNjb3JlID0gc2NvcmUKICAgICAgICAgICAgICAgIGJlc3RjaGFuZ2VwdCA9IGNoYW5nZXB0CiAgICAgICAgY2hhbmdlcHRzLmFwcGVuZChiZXN0Y2hhbmdlcHQpCiAgICByZXR1cm4gc29ydGVkKGNoYW5nZXB0cyk=\" download=\"\">⬇</a></div>\n<div id=\"lstnumberx1\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>n:<span class=\"ltx_text ltx_lst_space\"> </span>number<span class=\"ltx_text ltx_lst_space\"> </span>of<span class=\"ltx_text ltx_lst_space\"> </span>clusters</span>\n</div>\n<div id=\"lstnumberx2\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>feats:<span class=\"ltx_text ltx_lst_space\"> </span>features<span class=\"ltx_text ltx_lst_space\"> </span>of<span class=\"ltx_text ltx_lst_space\"> </span>the<span class=\"ltx_text ltx_lst_space\"> </span>frames<span class=\"ltx_text ltx_lst_space\"> </span>in<span class=\"ltx_text ltx_lst_space\"> </span>the<span class=\"ltx_text ltx_lst_space\"> </span>sequence</span>\n</div>\n<div id=\"lstnumberx3\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>F:<span class=\"ltx_text ltx_lst_space\"> </span>maximization<span class=\"ltx_text ltx_lst_space\"> </span>objective<span class=\"ltx_text ltx_lst_space\"> </span>(defined<span class=\"ltx_text ltx_lst_space\"> </span>by<span class=\"ltx_text ltx_lst_space\"> </span>Equation<span class=\"ltx_text ltx_lst_space\"> </span>1).</span>\n</div>\n<div id=\"lstnumberx4\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Returns:<span class=\"ltx_text ltx_lst_space\"> </span>detected<span class=\"ltx_text ltx_lst_space\"> </span>change<span class=\"ltx_text ltx_lst_space\"> </span>points<span class=\"ltx_text ltx_lst_space\"> </span>in<span class=\"ltx_text ltx_lst_space\"> </span>the<span class=\"ltx_text ltx_lst_space\"> </span>stream<span class=\"ltx_text ltx_lst_space\"> </span>(sorted)</span>\n</div>\n<div id=\"lstnumberx5\" class=\"ltx_listingline\">\n</div>\n<div id=\"lstnumberx6\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">def</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temporal_segment</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">F</span><span class=\"ltx_text ltx_font_typewriter\">):</span>\n</div>\n<div id=\"lstnumberx7\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">S</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">@</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">T</span>\n</div>\n<div id=\"lstnumberx8\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">L</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">len</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">S</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx9\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">[]</span>\n</div>\n<div id=\"lstnumberx10\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">for</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">i</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">in</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">range</span><span class=\"ltx_text ltx_font_typewriter\">(1,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_font_typewriter\">):</span>\n</div>\n<div id=\"lstnumberx11\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestscore</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">0</span>\n</div>\n<div id=\"lstnumberx12\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">for</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changept</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">in</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">range</span><span class=\"ltx_text ltx_font_typewriter\">(1,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">L</span><span class=\"ltx_text ltx_font_typewriter\">):</span>\n</div>\n<div id=\"lstnumberx13\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">            </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temp</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">+</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">[</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changept</span><span class=\"ltx_text ltx_font_typewriter\">]</span>\n</div>\n<div id=\"lstnumberx14\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">            </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">score</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">F</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">sorted</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temp</span><span class=\"ltx_text ltx_font_typewriter\">))</span>\n</div>\n<div id=\"lstnumberx15\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">            </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">if</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">score</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">&gt;</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestscore</span><span class=\"ltx_text ltx_font_typewriter\">:</span>\n</div>\n<div id=\"lstnumberx16\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">                </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestscore</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">score</span>\n</div>\n<div id=\"lstnumberx17\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">                </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestchangept</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changept</span>\n</div>\n<div id=\"lstnumberx18\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">append</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">bestchangept</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx19\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">return</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">sorted</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changepts</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n</div>\n</figure>\n<figure id=\"alg2\" class=\"ltx_float ltx_minipage ltx_align_top ltx_framed ltx_framed_top\" style=\"width:203.8pt;\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 2</span> </span> Memory Storyboard Streaming SSL</figcaption>\n<div class=\"ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing\" style=\"background-color:#FFFFFF;\">\n<div class=\"ltx_listing_data\"><a href=\"data:text/plain;base64,IyBEOiBzdHJlYW1pbmcgZGF0YSBsb2FkZXIKIyBNX3M6IHNob3J0LXRlcm0gbWVtb3J5IGJ1ZmZlcgojIE1fbDogbG9uZy10ZXJtIG1lbW9yeSBidWZmZXIKIyBCX3MsIEJfbDogYmF0Y2ggc2l6ZSBmb3IgTV9zLCBNX2wKIyBUOiBkZWZhdWx0IHNlZ21lbnQgbGVuZ3RoCiMgcjogc3Vic2FtcGxpbmcgcmF0ZQoKd2hpbGUgVHJ1ZTogIyBMb29wIHVudGlsIGVuZCBvZiBzdHJlYW0KICAgIHggPSBELm5leHQoKQogICAgeF9zdWIgPSBzdWJzYW1wbGUoeCwgcikKICAgIE1fbC5hZGQoeCkgIyBVcGRhdGVkIHdpdGggUmVzZXJ2b2lyCiAgICBNX3MuYWRkKHhfc3ViKSAjIFVwZGF0ZWQgd2l0aCBGSUZPCiAgICBpZiBNX3NbMF0ubGFiZWwgPiB0Y19sYWJlbDoKICAgICAgICB0Y19sYWJlbCA9IE1fc1swXS5sYWJlbAogICAgICAgIG4gPSBsZW4oTV9zKSAvIFQKICAgICAgICBmZWF0cyA9IG5vcm1hbGl6ZShmZWF0dXJlcyhNX3MpKQogICAgICAgIGNoYW5nZXMgPSB0ZW1wb3JhbF9zZWdtZW50KG4sIGZlYXRzLCBGKQogICAgICAgIHVwZGF0ZV9sYWJlbHMoTV9zLCBjaGFuZ2VzKQogICAgICAgIHVwZGF0ZV9sYWJlbHMoTV9sLCBjaGFuZ2VzKQogICAgZGF0YSA9IHNhbXBsZShNX2wsIEJfbCwgTV9zLCBCX3MpCiAgICBsb3NzID0gVENMX2xvc3MoZGF0YSkgKyBDTF9sb3NzKGRhdGEpCiAgICBtb2RlbC51cGRhdGUobG9zcyk=\" download=\"\">⬇</a></div>\n<div id=\"lstnumberx20\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>D:<span class=\"ltx_text ltx_lst_space\"> </span>streaming<span class=\"ltx_text ltx_lst_space\"> </span>data<span class=\"ltx_text ltx_lst_space\"> </span>loader</span>\n</div>\n<div id=\"lstnumberx21\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>M_s:<span class=\"ltx_text ltx_lst_space\"> </span>short-term<span class=\"ltx_text ltx_lst_space\"> </span>memory<span class=\"ltx_text ltx_lst_space\"> </span>buffer</span>\n</div>\n<div id=\"lstnumberx22\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>M_l:<span class=\"ltx_text ltx_lst_space\"> </span>long-term<span class=\"ltx_text ltx_lst_space\"> </span>memory<span class=\"ltx_text ltx_lst_space\"> </span>buffer</span>\n</div>\n<div id=\"lstnumberx23\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>B_s,<span class=\"ltx_text ltx_lst_space\"> </span>B_l:<span class=\"ltx_text ltx_lst_space\"> </span>batch<span class=\"ltx_text ltx_lst_space\"> </span>size<span class=\"ltx_text ltx_lst_space\"> </span>for<span class=\"ltx_text ltx_lst_space\"> </span>M_s,<span class=\"ltx_text ltx_lst_space\"> </span>M_l</span>\n</div>\n<div id=\"lstnumberx24\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>T:<span class=\"ltx_text ltx_lst_space\"> </span>default<span class=\"ltx_text ltx_lst_space\"> </span>segment<span class=\"ltx_text ltx_lst_space\"> </span>length</span>\n</div>\n<div id=\"lstnumberx25\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>r:<span class=\"ltx_text ltx_lst_space\"> </span>subsampling<span class=\"ltx_text ltx_lst_space\"> </span>rate</span>\n</div>\n<div id=\"lstnumberx26\" class=\"ltx_listingline\">\n</div>\n<div id=\"lstnumberx27\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">while</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">True</span><span class=\"ltx_text ltx_font_typewriter\">:</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Loop<span class=\"ltx_text ltx_lst_space\"> </span>until<span class=\"ltx_text ltx_lst_space\"> </span>end<span class=\"ltx_text ltx_lst_space\"> </span>of<span class=\"ltx_text ltx_lst_space\"> </span>stream</span>\n</div>\n<div id=\"lstnumberx28\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">D</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">next</span><span class=\"ltx_text ltx_font_typewriter\">()</span>\n</div>\n<div id=\"lstnumberx29\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x_sub</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">subsample</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">r</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx30\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_l</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">add</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Updated<span class=\"ltx_text ltx_lst_space\"> </span>with<span class=\"ltx_text ltx_lst_space\"> </span>Reservoir</span>\n</div>\n<div id=\"lstnumberx31\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">add</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">x_sub</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_comment ltx_font_typewriter\" style=\"color:#408080;\">#<span class=\"ltx_text ltx_lst_space\"> </span>Updated<span class=\"ltx_text ltx_lst_space\"> </span>with<span class=\"ltx_text ltx_lst_space\"> </span>FIFO</span>\n</div>\n<div id=\"lstnumberx32\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_keyword ltx_font_typewriter\">if</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">[0].</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">label</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">&gt;</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">tc_label</span><span class=\"ltx_text ltx_font_typewriter\">:</span>\n</div>\n<div id=\"lstnumberx33\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">tc_label</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">[0].</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">label</span>\n</div>\n<div id=\"lstnumberx34\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter\">len</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">/</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">T</span>\n</div>\n<div id=\"lstnumberx35\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">normalize</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">features</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">))</span>\n</div>\n<div id=\"lstnumberx36\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changes</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">temporal_segment</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">n</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">feats</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">F</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx37\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">update_labels</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changes</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx38\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">        </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">update_labels</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_l</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">changes</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx39\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">data</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">sample</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_l</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">B_l</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">M_s</span><span class=\"ltx_text ltx_font_typewriter\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">B_s</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx40\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">loss</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">=</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">TCL_loss</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">data</span><span class=\"ltx_text ltx_font_typewriter\">)</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_font_typewriter\">+</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">CL_loss</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">data</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n<div id=\"lstnumberx41\" class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_lst_space ltx_font_typewriter\">    </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">model</span><span class=\"ltx_text ltx_font_typewriter\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">update</span><span class=\"ltx_text ltx_font_typewriter\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\">loss</span><span class=\"ltx_text ltx_font_typewriter\">)</span>\n</div>\n</div>\n</figure>\n<figure id=\"S4.F2\" class=\"ltx_figure ltx_align_floatright\"><img src=\"./assets/x2.png\" id=\"S4.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"436\" height=\"264\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span><span class=\"ltx_text ltx_font_bold\">Details of our two-tier memory in Memory Storyboard.</span> Long-term memory is updated with reservoir sampling, and short-term memory with first-in-first-out (FIFO). Temporal segmentation is applied on the short-term memory, which then updates the labels of corresponding images in the long-term memory.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Two-tier Memory Hierarchy.</h4>\n\n<div id=\"S4.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">To accommodate efficient temporal segmentation, we propose a two-tier memory hierarchy. Shown in Figure <a href=\"#S4.F2\" title=\"Figure 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system includes a long-term memory <math id=\"S4.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"M_{long}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> updated with reservoir sampling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">58</a>]</cite>, and a short-term memory storyboard <math id=\"S4.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> updated with a first-in-first-out (FIFO) strategy. We store the temporal index and the temporal class of each frame along with the image in the memory. The short-term memory size <math id=\"S4.SS0.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"|M_{short}|\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{short}|</annotation></semantics></math> is much smaller than the long-term memory size <math id=\"S4.SS0.SSS0.Px2.p1.m4\" class=\"ltx_Math\" alttext=\"|M_{long}|\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{long}|</annotation></semantics></math>, allowing efficient temporal segmentation of the recent past. The change points produced by the temporal segmentation component on <math id=\"S4.SS0.SSS0.Px2.p1.m5\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> are then used to update the temporal class labels in <math id=\"S4.SS0.SSS0.Px2.p1.m6\" class=\"ltx_Math\" alttext=\"M_{long}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math>.</p>\n</div>\n<div id=\"S4.SS0.SSS0.Px2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">To increase the horizon of the memory storyboard, we subsample the frames coming from the current stream before adding it to <math id=\"S4.SS0.SSS0.Px2.p2.m1\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>. The subsampling also reduces the temporal correlation between the frames in the training batch sampled from <math id=\"S4.SS0.SSS0.Px2.p2.m2\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>. Compared to using a single replay buffer as memory, the two-tier memory hierarchy helps avoid overfitting on the replay buffer and makes sure that the new frames are seen by the model.</p>\n</div>\n</section>\n<section id=\"S4.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Contrastive Loss.</h4>\n\n<div id=\"S4.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">To effectively utilize the temporal class labels for representation learning, we adopt the supervised contrastive (SupCon) loss <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite>, which takes the samples with the same temporal class label in a batch as positives and contrasts them from the remainder of the batch.\nLet <math id=\"S4.SS0.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"f_{proj}\" display=\"inline\"><semantics><msub><mi>f</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">f_{proj}</annotation></semantics></math> be a projector network. For a batch of images with size <math id=\"S4.SS0.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math>, we take two random augmentations of each image to get an augmented batch <math id=\"S4.SS0.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"\\tilde{x_{1}},\\tilde{x_{2}},\\dots,\\tilde{x_{2B}}\" display=\"inline\"><semantics><mrow><mover accent=\"true\"><msub><mi>x</mi><mn>1</mn></msub><mo>~</mo></mover><mo>,</mo><mover accent=\"true\"><msub><mi>x</mi><mn>2</mn></msub><mo>~</mo></mover><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><mover accent=\"true\"><msub><mi>x</mi><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>B</mi></mrow></msub><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde{x_{1}},\\tilde{x_{2}},\\dots,\\tilde{x_{2B}}</annotation></semantics></math>, and compute <math id=\"S4.SS0.SSS0.Px3.p1.m4\" class=\"ltx_Math\" alttext=\"z_{i}=f_{proj}(f_{\\theta}(\\tilde{x_{j}}))\" display=\"inline\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><msub><mi>x</mi><mi>j</mi></msub><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{i}=f_{proj}(f_{\\theta}(\\tilde{x_{j}}))</annotation></semantics></math> be the projected features of each augmented image <math id=\"S4.SS0.SSS0.Px3.p1.m5\" class=\"ltx_Math\" alttext=\"\\tilde{x_{i}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>x</mi><mi>i</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{x_{i}}</annotation></semantics></math>. Let <math id=\"S4.SS0.SSS0.Px3.p1.m6\" class=\"ltx_Math\" alttext=\"y_{i}\" display=\"inline\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> be the temporal class label of <math id=\"S4.SS0.SSS0.Px3.p1.m7\" class=\"ltx_Math\" alttext=\"\\tilde{x_{i}}\" display=\"inline\"><semantics><mover accent=\"true\"><msub><mi>x</mi><mi>i</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{x_{i}}</annotation></semantics></math> and <math id=\"S4.SS0.SSS0.Px3.p1.m8\" class=\"ltx_Math\" alttext=\"P(i)=\\{p\\in\\{1,2,\\dots,2B\\}\\backslash\\{i\\}:y_{p}=y_{i}\\}\" display=\"inline\"><semantics><mrow><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>p</mi><mo>∈</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>B</mi></mrow><mo rspace=\"0.222em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">\\</mo><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo rspace=\"0.278em\" stretchy=\"false\">}</mo></mrow></mrow></mrow><mo rspace=\"0.278em\">:</mo><mrow><msub><mi>y</mi><mi>p</mi></msub><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(i)=\\{p\\in\\{1,2,\\dots,2B\\}\\backslash\\{i\\}:y_{p}=y_{i}\\}</annotation></semantics></math>.</p>\n<table id=\"S4.E2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E2.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{TCL}=\\sum_{i}\\frac{-1}{|P(i)|}\\sum_{p\\in P(i)}\\log\\frac{\\exp(z_{i}\\cdot z_{p}/\\tau)}{\\sum_{a\\neq i}\\exp(z_{i}\\cdot z_{a}/\\tau)}.\" display=\"block\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\">∑</mo><mi>i</mi></munder><mrow><mfrac><mrow><mo>−</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow><mi>p</mi><mo>∈</mo><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><mi>log</mi><mo lspace=\"0.167em\">⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>z</mi><mi>p</mi></msub></mrow><mo>/</mo><mi>τ</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><msub><mo>∑</mo><mrow><mi>a</mi><mo>≠</mo><mi>i</mi></mrow></msub><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>z</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>z</mi><mi>a</mi></msub></mrow><mo>/</mo><mi>τ</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{TCL}=\\sum_{i}\\frac{-1}{|P(i)|}\\sum_{p\\in P(i)}\\log\\frac{\\exp(z_{i}\\cdot z_{p}/\\tau)}{\\sum_{a\\neq i}\\exp(z_{i}\\cdot z_{a}/\\tau)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We refer to this as the temporal contrastive loss. It is conceptually similar to the temporal classification loss proposed in <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>. However, in the temporal classification loss, the size of the classification layer needs to be gradually expanded as more data is processed by the model and more temporal classes are formed. Hence the temporal contrastive loss is more flexible and more suitable for the streaming SSL setting.</p>\n</div>\n</section>\n<section id=\"S4.SS0.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Overall Loss Function.</h4>\n\n<div id=\"S4.SS0.SSS0.Px4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In addition to the temporal contrastive loss, we also incorporate a standard self-supervised contrastive loss <math id=\"S4.SS0.SSS0.Px4.p1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{CL}\" display=\"inline\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CL}</annotation></semantics></math>. In particular, we experimented with the SimCLR loss <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>, <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">54</a>]</cite> and the SimSiam loss <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite> because they were shown to work well in lifelong self-supervised learning in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>, <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>.\nThe overall loss function is a sum of the temporal contrastive loss and the self-supervised contrastive loss (Equation <a href=\"#S4.E3\" title=\"Equation 3 ‣ Overall Loss Function. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n<table id=\"S4.E3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E3.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}=\\mathcal{L}_{TCL}+\\mathcal{L}_{CL}.\" display=\"block\"><semantics><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{TCL}+\\mathcal{L}_{CL}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n</div>\n<figure id=\"S4.T1\" class=\"ltx_table\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:429.3pt;height:200.9pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-113.5pt,52.9pt) scale(0.654207918921436,0.654207918921436) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\">Labeled-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Labeled-S</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SVM</td>\n<td class=\"ltx_td ltx_align_center\">kNN</td>\n<td class=\"ltx_td ltx_align_center\">Linear</td>\n<td class=\"ltx_td ltx_align_center\">SVM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">kNN</td>\n<td class=\"ltx_td ltx_align_center\">SVM</td>\n<td class=\"ltx_td ltx_align_center\">kNN</td>\n<td class=\"ltx_td ltx_align_center\">Linear</td>\n<td class=\"ltx_td ltx_align_center\">SVM</td>\n<td class=\"ltx_td ltx_align_center\">kNN</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">IID SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">59.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">29.02</td>\n<td class=\"ltx_td ltx_align_center\">31.60</td>\n<td class=\"ltx_td ltx_align_center\">20.92</td>\n<td class=\"ltx_td ltx_align_center\">42.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">56.84</td>\n<td class=\"ltx_td ltx_align_center\">29.02</td>\n<td class=\"ltx_td ltx_align_center\">31.60</td>\n<td class=\"ltx_td ltx_align_center\">20.92</td>\n<td class=\"ltx_td ltx_align_center\">42.71</td>\n<td class=\"ltx_td ltx_align_center\">56.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR No Replay</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam No Replay</th>\n<td class=\"ltx_td ltx_align_center\">6.44</td>\n<td class=\"ltx_td ltx_align_center\">6.32</td>\n<td class=\"ltx_td ltx_align_center\">1.47</td>\n<td class=\"ltx_td ltx_align_center\">22.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">23.16</td>\n<td class=\"ltx_td ltx_align_center\">6.44</td>\n<td class=\"ltx_td ltx_align_center\">6.32</td>\n<td class=\"ltx_td ltx_align_center\">1.47</td>\n<td class=\"ltx_td ltx_align_center\">22.03</td>\n<td class=\"ltx_td ltx_align_center\">23.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Replay - 10k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Replay - 50k</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TC <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">33.92</td>\n<td class=\"ltx_td ltx_align_center\">24.28</td>\n<td class=\"ltx_td ltx_align_center\">19.03</td>\n<td class=\"ltx_td ltx_align_center\">48.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.29</td>\n<td class=\"ltx_td ltx_align_center\">36.68</td>\n<td class=\"ltx_td ltx_align_center\">24.08</td>\n<td class=\"ltx_td ltx_align_center\">22.72</td>\n<td class=\"ltx_td ltx_align_center\">52.22</td>\n<td class=\"ltx_td ltx_align_center\">54.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">33.02</td>\n<td class=\"ltx_td ltx_align_center\">26.82</td>\n<td class=\"ltx_td ltx_align_center\">20.13</td>\n<td class=\"ltx_td ltx_align_center\">49.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">53.57</td>\n<td class=\"ltx_td ltx_align_center\">37.96</td>\n<td class=\"ltx_td ltx_align_center\">30.92</td>\n<td class=\"ltx_td ltx_align_center\">23.75</td>\n<td class=\"ltx_td ltx_align_center\">53.67</td>\n<td class=\"ltx_td ltx_align_center\">56.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">33.62</td>\n<td class=\"ltx_td ltx_align_center\">27.38</td>\n<td class=\"ltx_td ltx_align_center\">20.21</td>\n<td class=\"ltx_td ltx_align_center\">48.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">52.33</td>\n<td class=\"ltx_td ltx_align_center\">38.66</td>\n<td class=\"ltx_td ltx_align_center\">31.26</td>\n<td class=\"ltx_td ltx_align_center\">24.10</td>\n<td class=\"ltx_td ltx_align_center\">54.75</td>\n<td class=\"ltx_td ltx_align_center\">55.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +Two-tier (Ours)</th>\n<td class=\"ltx_td ltx_align_center\">33.80</td>\n<td class=\"ltx_td ltx_align_center\">26.96</td>\n<td class=\"ltx_td ltx_align_center\">20.70</td>\n<td class=\"ltx_td ltx_align_center\">49.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">52.81</td>\n<td class=\"ltx_td ltx_align_center\">39.22</td>\n<td class=\"ltx_td ltx_align_center\">31.86</td>\n<td class=\"ltx_td ltx_align_center\">24.93</td>\n<td class=\"ltx_td ltx_align_center\">55.43</td>\n<td class=\"ltx_td ltx_align_center\">56.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +MemStoryboard (Ours)</th>\n<td class=\"ltx_td ltx_align_center\">35.02</td>\n<td class=\"ltx_td ltx_align_center\">27.42</td>\n<td class=\"ltx_td ltx_align_center\">20.72</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">51.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.91</td>\n<td class=\"ltx_td ltx_align_center\">39.58</td>\n<td class=\"ltx_td ltx_align_center\">31.92</td>\n<td class=\"ltx_td ltx_align_center\">24.78</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">56.29</span></td>\n<td class=\"ltx_td ltx_align_center\">57.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">20.90</td>\n<td class=\"ltx_td ltx_align_center\">27.02</td>\n<td class=\"ltx_td ltx_align_center\">13.72</td>\n<td class=\"ltx_td ltx_align_center\">39.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.09</td>\n<td class=\"ltx_td ltx_align_center\">26.66</td>\n<td class=\"ltx_td ltx_align_center\">30.04</td>\n<td class=\"ltx_td ltx_align_center\">14.44</td>\n<td class=\"ltx_td ltx_align_center\">43.09</td>\n<td class=\"ltx_td ltx_align_center\">56.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">22.68</td>\n<td class=\"ltx_td ltx_align_center\">27.12</td>\n<td class=\"ltx_td ltx_align_center\">17.85</td>\n<td class=\"ltx_td ltx_align_center\">39.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">51.84</td>\n<td class=\"ltx_td ltx_align_center\">25.58</td>\n<td class=\"ltx_td ltx_align_center\">27.42</td>\n<td class=\"ltx_td ltx_align_center\">18.99</td>\n<td class=\"ltx_td ltx_align_center\">40.37</td>\n<td class=\"ltx_td ltx_align_center\">53.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +Two-tier (Ours)</th>\n<td class=\"ltx_td ltx_align_center\">21.78</td>\n<td class=\"ltx_td ltx_align_center\">27.92</td>\n<td class=\"ltx_td ltx_align_center\">16.87</td>\n<td class=\"ltx_td ltx_align_center\">39.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.05</td>\n<td class=\"ltx_td ltx_align_center\">28.34</td>\n<td class=\"ltx_td ltx_align_center\">28.98</td>\n<td class=\"ltx_td ltx_align_center\">20.24</td>\n<td class=\"ltx_td ltx_align_center\">42.95</td>\n<td class=\"ltx_td ltx_align_center\">55.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +MemStoryboard (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">30.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">22.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">49.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">41.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">34.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">26.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">59.01</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span><span class=\"ltx_text ltx_font_bold\">Results on streaming SSL from SAYCam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a>]</cite>.</span> Downstream evaluation on object classification for SSL models trained under the streaming setting. For “No Replay” and “IID” the results are the same for different memory buffer sizes. The “IID” methods are not under the streaming setting and are for reference only as a performance “upper bound” with the same number of gradient updates. Unless specified, standard reservoir sampling is used in the replay buffer.</figcaption>\n</figure>\n<figure id=\"S4.T2\" class=\"ltx_table\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:429.3pt;height:198.9pt;vertical-align:-0.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-116.5pt,53.8pt) scale(0.648171531426623,0.648171531426623) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\">OAK</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ImageNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">OAK</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SVM</td>\n<td class=\"ltx_td ltx_align_center\">kNN</td>\n<td class=\"ltx_td ltx_align_center\">Linear</td>\n<td class=\"ltx_td ltx_align_center\">AP50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">AP75</td>\n<td class=\"ltx_td ltx_align_center\">SVM</td>\n<td class=\"ltx_td ltx_align_center\">kNN</td>\n<td class=\"ltx_td ltx_align_center\">Linear</td>\n<td class=\"ltx_td ltx_align_center\">AP50</td>\n<td class=\"ltx_td ltx_align_center\">AP75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">IID SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">IID SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">28.58</td>\n<td class=\"ltx_td ltx_align_center\">25.12</td>\n<td class=\"ltx_td ltx_align_center\">22.28</td>\n<td class=\"ltx_td ltx_align_center\">44.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">31.13</td>\n<td class=\"ltx_td ltx_align_center\">28.58</td>\n<td class=\"ltx_td ltx_align_center\">25.12</td>\n<td class=\"ltx_td ltx_align_center\">22.28</td>\n<td class=\"ltx_td ltx_align_center\">44.86</td>\n<td class=\"ltx_td ltx_align_center\">31.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimCLR No Replay</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam No Replay</th>\n<td class=\"ltx_td ltx_align_center\">8.88</td>\n<td class=\"ltx_td ltx_align_center\">7.52</td>\n<td class=\"ltx_td ltx_align_center\">1.92</td>\n<td class=\"ltx_td ltx_align_center\">27.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.47</td>\n<td class=\"ltx_td ltx_align_center\">8.88</td>\n<td class=\"ltx_td ltx_align_center\">7.52</td>\n<td class=\"ltx_td ltx_align_center\">1.92</td>\n<td class=\"ltx_td ltx_align_center\">27.34</td>\n<td class=\"ltx_td ltx_align_center\">9.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Replay - 10k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Replay - 50k</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">17.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TC <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">32.58</td>\n<td class=\"ltx_td ltx_align_center\">24.24</td>\n<td class=\"ltx_td ltx_align_center\">19.19</td>\n<td class=\"ltx_td ltx_align_center\">32.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.41</td>\n<td class=\"ltx_td ltx_align_center\">32.94</td>\n<td class=\"ltx_td ltx_align_center\">24.84</td>\n<td class=\"ltx_td ltx_align_center\">20.50</td>\n<td class=\"ltx_td ltx_align_center\">28.56</td>\n<td class=\"ltx_td ltx_align_center\">12.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">31.46</td>\n<td class=\"ltx_td ltx_align_center\">24.70</td>\n<td class=\"ltx_td ltx_align_center\">19.09</td>\n<td class=\"ltx_td ltx_align_center\">31.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.40</td>\n<td class=\"ltx_td ltx_align_center\">34.98</td>\n<td class=\"ltx_td ltx_align_center\">27.14</td>\n<td class=\"ltx_td ltx_align_center\">22.37</td>\n<td class=\"ltx_td ltx_align_center\">33.30</td>\n<td class=\"ltx_td ltx_align_center\">14.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">31.56</td>\n<td class=\"ltx_td ltx_align_center\">24.30</td>\n<td class=\"ltx_td ltx_align_center\">19.93</td>\n<td class=\"ltx_td ltx_align_center\">34.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.15</td>\n<td class=\"ltx_td ltx_align_center\">34.84</td>\n<td class=\"ltx_td ltx_align_center\">27.16</td>\n<td class=\"ltx_td ltx_align_center\">22.29</td>\n<td class=\"ltx_td ltx_align_center\">35.65</td>\n<td class=\"ltx_td ltx_align_center\">19.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +Two-tier (Ours)</th>\n<td class=\"ltx_td ltx_align_center\">33.26</td>\n<td class=\"ltx_td ltx_align_center\">25.56</td>\n<td class=\"ltx_td ltx_align_center\">20.39</td>\n<td class=\"ltx_td ltx_align_center\">33.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.73</td>\n<td class=\"ltx_td ltx_align_center\">35.78</td>\n<td class=\"ltx_td ltx_align_center\">27.18</td>\n<td class=\"ltx_td ltx_align_center\">22.42</td>\n<td class=\"ltx_td ltx_align_center\">35.68</td>\n<td class=\"ltx_td ltx_align_center\">19.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +MemStoryboard (Ours)</th>\n<td class=\"ltx_td ltx_align_center\">33.72</td>\n<td class=\"ltx_td ltx_align_center\">25.88</td>\n<td class=\"ltx_td ltx_align_center\">20.13</td>\n<td class=\"ltx_td ltx_align_center\">35.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.78</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">36.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">27.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">22.75</span></td>\n<td class=\"ltx_td ltx_align_center\">38.67</td>\n<td class=\"ltx_td ltx_align_center\">21.69</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">19.16</td>\n<td class=\"ltx_td ltx_align_center\">18.84</td>\n<td class=\"ltx_td ltx_align_center\">12.94</td>\n<td class=\"ltx_td ltx_align_center\">39.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">27.31</td>\n<td class=\"ltx_td ltx_align_center\">21.84</td>\n<td class=\"ltx_td ltx_align_center\">20.18</td>\n<td class=\"ltx_td ltx_align_center\">14.13</td>\n<td class=\"ltx_td ltx_align_center\">41.13</td>\n<td class=\"ltx_td ltx_align_center\">28.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">   +MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">20.90</td>\n<td class=\"ltx_td ltx_align_center\">18.28</td>\n<td class=\"ltx_td ltx_align_center\">14.53</td>\n<td class=\"ltx_td ltx_align_center\">43.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">27.35</td>\n<td class=\"ltx_td ltx_align_center\">22.88</td>\n<td class=\"ltx_td ltx_align_center\">20.36</td>\n<td class=\"ltx_td ltx_align_center\">17.64</td>\n<td class=\"ltx_td ltx_align_center\">44.17</td>\n<td class=\"ltx_td ltx_align_center\">29.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +Two-tier (Ours)</th>\n<td class=\"ltx_td ltx_align_center\">20.08</td>\n<td class=\"ltx_td ltx_align_center\">19.56</td>\n<td class=\"ltx_td ltx_align_center\">13.76</td>\n<td class=\"ltx_td ltx_align_center\">43.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">27.85</td>\n<td class=\"ltx_td ltx_align_center\">22.14</td>\n<td class=\"ltx_td ltx_align_center\">21.32</td>\n<td class=\"ltx_td ltx_align_center\">17.06</td>\n<td class=\"ltx_td ltx_align_center\">44.41</td>\n<td class=\"ltx_td ltx_align_center\">29.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>blue!10  +MemStoryboard (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">33.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">26.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">21.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">45.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">30.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">22.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">46.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">31.37</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span class=\"ltx_text ltx_font_bold\">Results on streaming SSL from KrishnaCam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>.</span> Downstream evaluation on object classification and object detection for SSL models trained on under the streaming setting.\nFor “No Replay” and “IID” the results are the same for different memory buffer sizes.\nThe “IID” methods are not under the streaming setting and are for reference only as a performance “upper bound” with the same number of gradient updates. Unless specified, standard reservoir sampling is used in the replay buffer.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS0.SSS0.Px5\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Warm-Start Training.</h4>\n\n<div id=\"S4.SS0.SSS0.Px5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">At the beginning of training, the model has only seen a very limited amount of data from the video stream. Even with a memory buffer, there is likely high temporal correlation between the sampled frames and could cause instability in the training. To alleviate this problem, we warm-start the system by making no model updates on the first <math id=\"S4.SS0.SSS0.Px5.p1.m1\" class=\"ltx_Math\" alttext=\"M_{long}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> frames of the stream and just use them to fill the memory. The warm-start phase ensures that the model is trained on de-correlated samples from the buffer starting from the beginning.</p>\n</div>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n\n<section id=\"S5.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span>Experiment Setup</h3>\n\n<section id=\"S5.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets.</h4>\n\n<div id=\"S5.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We use two real-world egocentric video datasets in the experiments: (1) the child S subset of SAYCam dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a>]</cite>, which contains 221 hours of video data collected from a head-mounted camera on the child from age 6-32 months, decoded at 25 fps;\n(2) the KrishnaCam dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>, which contains 70 hours of video data spanning nine months of the life of a graduate student, decoded at 10 fps. There two datasets have also been adopted in a number of existing self-supervised learning literature <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>, <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>, <a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>, <a href=\"#bib.bib59\" title=\"\" class=\"ltx_ref\">59</a>]</cite>.</p>\n</div>\n</section>\n<section id=\"S5.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training.</h4>\n\n<div id=\"S5.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Following the architectural choices of Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>, we use ResNet-50 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite> as the feature extractor with group normalization <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib64\" title=\"\" class=\"ltx_ref\">64</a>]</cite> and the Mish activation function <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">35</a>]</cite>. Unless otherwise specified, the default hyperparameter values we use in our experiments are <math id=\"S5.SS1.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"b=64\" display=\"inline\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">b=64</annotation></semantics></math>, <math id=\"S5.SS1.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"B=512\" display=\"inline\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding=\"application/x-tex\">B=512</annotation></semantics></math>, <math id=\"S5.SS1.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"T=4.5K\" display=\"inline\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>4.5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T=4.5K</annotation></semantics></math> for SAYCam and <math id=\"S5.SS1.SSS0.Px2.p1.m4\" class=\"ltx_Math\" alttext=\"T=1.8K\" display=\"inline\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mn>1.8</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T=1.8K</annotation></semantics></math> for KrishnaCam (both corresponding to 3 minutes of raw video), subsampling rate <math id=\"S5.SS1.SSS0.Px2.p1.m5\" class=\"ltx_Math\" alttext=\"r=8\" display=\"inline\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math> for SAYCam and <math id=\"S5.SS1.SSS0.Px2.p1.m6\" class=\"ltx_Math\" alttext=\"r=4\" display=\"inline\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">r=4</annotation></semantics></math> for KrishnaCam. We train the models with two sets of memory sizes to evaluate their performance across different memory constraints: a larger memory constraint with <math id=\"S5.SS1.SSS0.Px2.p1.m7\" class=\"ltx_Math\" alttext=\"|M|=50K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M|=50K</annotation></semantics></math>, <math id=\"S5.SS1.SSS0.Px2.p1.m8\" class=\"ltx_Math\" alttext=\"|M_{short}|=5K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{short}|=5K</annotation></semantics></math>, <math id=\"S5.SS1.SSS0.Px2.p1.m9\" class=\"ltx_Math\" alttext=\"|M_{long}|=45K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>45</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{long}|=45K</annotation></semantics></math>, and a smaller memory constraint with <math id=\"S5.SS1.SSS0.Px2.p1.m10\" class=\"ltx_Math\" alttext=\"|M|=10K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>M</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>10</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M|=10K</annotation></semantics></math>, <math id=\"S5.SS1.SSS0.Px2.p1.m11\" class=\"ltx_Math\" alttext=\"|M_{short}|=1K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{short}|=1K</annotation></semantics></math>, <math id=\"S5.SS1.SSS0.Px2.p1.m12\" class=\"ltx_Math\" alttext=\"|M_{long}|=9K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>9</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{long}|=9K</annotation></semantics></math>. For context, there are a total of 18.2M frames in the SAYCam training set and 2.5M frame in the KrishnaCam training set. Therefore, even the large memory constraint of 50K frames only stores 0.27% and 2.01% of the total training frames in the memory buffer for SAYCam and KrishnaCam respectively.</p>\n</div>\n</section>\n<section id=\"S5.SS1.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation.</h4>\n\n<div id=\"S5.SS1.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">For object classification, we use<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet classification task for both SAYCam and KrishnaCam models. For each dataset, we also pick another downstream task that evaluates the learned representations of the training data itself. Evaluation tasks are summarized below.</p>\n<ul id=\"S5.I1\" class=\"ltx_itemize\">\n<li id=\"S5.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">mini<span class=\"ltx_text ltx_font_upright\">-ImageNet classification.</span></span> Following a similar evaluation protocol as <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al., [<a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>]</cite>, we evaluate the learned representations on a downstream classification task on a subsampled ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite> dataset (<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet). We extract the features of the model and train a support vector machine (SVM) or a k-nearest neighbor (kNN) classifier to measure its classification performance. The <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet dataset contains 20K training images and 5K test images across 100 classes.</p>\n</div>\n</li>\n<li id=\"S5.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ImageNet-1K classification.</span> Similar to the evaluation protocol used in <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al., [<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>, we further evaluate the classification performance with a linear classifier on the larger ImageNet-1K <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite> dataset with 1.28M training images and 50K test images across 1K classes.</p>\n</div>\n</li>\n<li id=\"S5.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Labeled-S classification.</span> For SAYCam models, we evaluate the classification performance on the Labeled-S dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>. The the Labeled-S dataset is a labeled subset of the SAYCam frames, containing a total of 5786 images across 26 classes after 10x subsampling of frames. We randomly use 50% as training data and 50% as test data.</p>\n</div>\n</li>\n<li id=\"S5.I1.i4\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OAK object detection.</span> For KrishnaCam models, we evaluate the object detection performance on the Objects Around Krishna (OAK) dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">60</a>]</cite>, which includes bounding box annotations of 105 object categories on a subset of the KrishnaCam frames. We fine-tune the model on the entire training set of OAK for 10 epochs before evaluating on the OAK validation set.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section id=\"S5.SS1.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines.</h4>\n\n<div id=\"S5.SS1.SSS0.Px4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We compare a number of competitive SSL methods for image and video representation learning, and different memory buffer strategies:</p>\n<ul id=\"S5.I2\" class=\"ltx_itemize\">\n<li id=\"S5.I2.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SimCLR.</span> In prior studies, <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al., [<a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>]</cite> showed that SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite> is the strongest self-supervised learning method under streaming video setting, outperforming other SSL methods such as BYOL <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>]</cite> and Barlow Twins <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib71\" title=\"\" class=\"ltx_ref\">71</a>]</cite>.</p>\n</div>\n</li>\n<li id=\"S5.I2.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SimSiam.</span> In prior work, <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al., [<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite> showed that SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite> is able to learn good representations from egocentric video data.</p>\n</div>\n</li>\n<li id=\"S5.I2.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Osiris.</span> Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite> is a state-of-the-art unsupervised continual learning method that is developed towards static image sequences.</p>\n</div>\n</li>\n<li id=\"S5.I2.i4\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TC.</span> Temporal classification (TC) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite> is a simple self-supervised learning method that is shown to work well on the SAYCam dataset under IID setting. It also uses temporal segments as a source of self-supervision; however, it does not actively group the frames together but instead relies on fixed intervals.</p>\n</div>\n</li>\n<li id=\"S5.I2.i5\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reservoir Sampling.</span> We mainly use reservoir sampling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">58</a>]</cite> as a default baseline approach for updating the memory buffer, which uniformly samples from all the seen images in the memory.</p>\n</div>\n</li>\n<li id=\"S5.I2.i6\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MinRed Buffer.</span> The minimum redundancy (MinRed) buffer <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>, also designed for the streaming setting, alleviates the temporal correlation of data in the continuous video stream by maintaining minimally redundant samples in the replay buffer.</p>\n</div>\n</li>\n<li id=\"S5.I2.i7\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I2.i7.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Two-tier Buffer.</span> As in MemStoryboard, we use a long-term memory updated with reservoir sampling and short-term memory updated with first-in-first-out (FIFO), but we do not apply the temporal contrastive loss or the temporal segmentation module.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n</section>\n<section id=\"S5.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2 </span>Main Results</h3>\n\n<div id=\"S5.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In Tables <a href=\"#S4.T1\" title=\"Table 1 ‣ Overall Loss Function. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a href=\"#S4.T2\" title=\"Table 2 ‣ Overall Loss Function. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we report the main results on streaming SSL on both SAYCam and KrishnaCam. Firstly, we observe that all SSL methods work poorly in the streaming setting without replay, and larger memory leads to better performance. In terms of memory buffer strategies, our two-tier memory hierarchy and MinRed <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite> outperforms reservoir sampling.</p>\n</div>\n<div id=\"S5.SS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Memory Storyboard achieves superior performance in all readout tasks compared to other streaming SSL models. For SimCLR-based methods, Memory Storyboard outperforms the baseline Reservoir sampling method by an average of 1.85% on the SVM readout performance of <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet classification and considerably narrows the gap between streaming learning and IID training. Memory Storyboard also significantly outperforms all baseline methods with a considerable gap by around 3% on AP50 on the challenging OAK object detection benchmark. For SimSiam-based methods, Memory Storyboard not only outperforms all streaming learning baselines by a considerable margin but also beats IID SimSiam training on all readout tasks when using a 50K replay buffer size.</p>\n</div>\n<div id=\"S5.SS2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">Memory Storyboard with SimSiam achieves the overall best performance across different training datasets and evaluation metrics. We hypothesize that Memory Storyboard works better with SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite> than SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite> in our experiments due to the fact that SimCLR treats some highly correlated images in the same batch as negative samples during training, which hinders effective representation learning. This issue is exacerbated in the SAYCam experiments due to the high frequency (25 fps) of the SAYCam video stream. By incorporating the temporal contrastive loss in Memory Storyboard, we successfully address this issue by utilizing only images in other temporal classes as negative samples.</p>\n</div>\n<div id=\"S5.SS2.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">Overall, the results demonstrate that Memory Storyboard is effective at learning good representations from a streaming video source, and the learned representations can be successfully transferred to downstream vision tasks on the training dataset itself or an external dataset.</p>\n</div>\n<section id=\"S5.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Qualitative Results.</h4>\n\n<div id=\"S5.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We visualize the temporal segments produced Memory Storyboard at the end of training in Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ Qualitative Results. ‣ 5.2 Main Results ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The results demonstrate that the our temporal segmentation module can produce semantically meaningful temporal segments, showing its strong temporal abstraction capability. We emphasize that the representations are entirely developed during the streaming SSL training as the networks are trained from scratch.</p>\n</div>\n<div id=\"S5.SS2.SSS0.Px1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We also visualize the object detection results produced by Memory Storyboard when fine-tuned on the OAK dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">60</a>]</cite> in Figure <a href=\"#S5.F4\" title=\"Figure 4 ‣ Qualitative Results. ‣ 5.2 Main Results ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that the fine-tuned model can successfully detect objects in cluttered environments. The results show that the representations learned by Memory Storyboard can be effectively transferred to downstream tasks which requires more fine-grained features.</p>\n</div>\n<figure id=\"S5.F3\" class=\"ltx_figure\"><img src=\"./assets/x3.png\" id=\"S5.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"308\" height=\"186\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of the temporal segments produced by Memory Storyboard on (a) SAYCam (b)(c) KrishnaCam at the end of training.</span> The images are sampled at 10 seconds per frame. Each color bar correspond to a temporal class (the first and the last class might be incomplete). Temporal segments produced at the beginning of training are provided in the appendix for comparison.</figcaption>\n</figure>\n<figure id=\"S5.F4\" class=\"ltx_figure\"><img src=\"./assets/x4.png\" id=\"S5.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"311\" height=\"178\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of object detection results on the OAK validation set.</span> The Memory Storyboard model is trained on KrishnaCam and fine-tuned on the OAK training set. Red boxes show the predictions and the green boxes are ground truth bounding boxes.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S5.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3 </span>Other Training Factors</h3>\n\n<div id=\"S5.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In this section, we study how varying different training factors affects the performance of Memory Storyboard, including subsampling rate, average segment length, and normalization layers. For evaluation on the downstream tasks, we use SVM readout top-1 accuracy for classification tasks and AP50 for fine-tuning on OAK. We use SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite> as the base SSL method for training.</p>\n</div>\n<section id=\"S5.SS3.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Subsampling Rate.</h4>\n\n<div id=\"S5.SS3.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We train Memory Storyboard with different subsampling rates when adding data fetched from the current stream to the short-term memory. Results are shown in Table <a href=\"#S5.T3\" title=\"Table 3 ‣ Subsampling Rate. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. A subsampling ratio of 8 works best for SAYCam, while a ratio of 4 works best for KrishnaCam. Since the two datasets are decoded at different frequencies (25 fps for SAYCam and 10 fps for KrishnaCam), the effective frequency of frames entering the short-term buffer is 3.13 and 2.50 fps respectively. The result suggests that an effective frequency of around 3 fps is preferable although the optimal subsample ratio is dependent on nature of the video stream. Intuitively, when the subsampling ratio is too small, the images entering the short-term buffer may have too much temporal correlation and hence would hurt the performance; when the subsampling ratio is too big, the model skips too many frames without training on them and the temporal clustering may also become less precise.</p>\n</div>\n<figure id=\"S5.T3\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Subsample</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Ratio</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Labeled-S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OAK AP50</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1<math id=\"S5.T3.m1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">2<math id=\"S5.T3.m2\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\">37.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">55.43</td>\n<td class=\"ltx_td ltx_align_center\">35.60</td>\n<td class=\"ltx_td ltx_align_center\">37.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">4<math id=\"S5.T3.m3\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\">38.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">55.84</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">36.36</span></td>\n<td class=\"ltx_td ltx_align_center\">38.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">8<math id=\"S5.T3.m4\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.29</span></td>\n<td class=\"ltx_td ltx_align_center\">35.48</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">16<math id=\"S5.T3.m5\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics><mo>×</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">55.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.22</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Effect of subsampling ratio for <math id=\"S5.T3.m7\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> in Memory Storyboard.</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS3.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Average Segment Length.</h4>\n\n<div id=\"S5.SS3.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We trained Memory Storyboard with different average segment length <math id=\"S5.SS3.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> ranging from 1 minute to 10 minutes on SAYCam and KrishnaCam. The results are shown in Table <a href=\"#S5.T4\" title=\"Table 4 ‣ Average Segment Length. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We demonstrate that the performance of Memory Storyboard is generally robust to average segment length (which determines the number of temporal segments in the segmentation module). We also find that the performance on downstream tasks becomes worse when the average segment length is very long <math id=\"S5.SS3.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"(T=10\\text{ min})\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>=</mo><mrow><mn>10</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mtext> min</mtext></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T=10\\text{ min})</annotation></semantics></math> on both datasets. This observation is different from that of temporal classification <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite> which claims longer segments are more helpful.</p>\n</div>\n<figure id=\"S5.T4\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><math id=\"S5.T4.m1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Labeled-S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OAK AP50</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1 min</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">2 min</th>\n<td class=\"ltx_td ltx_align_center\">39.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">56.53</td>\n<td class=\"ltx_td ltx_align_center\">36.30</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">3 min</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">56.29</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">36.36</span></td>\n<td class=\"ltx_td ltx_align_center\">38.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">5 min</th>\n<td class=\"ltx_td ltx_align_center\">39.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.64</span></td>\n<td class=\"ltx_td ltx_align_center\">36.28</td>\n<td class=\"ltx_td ltx_align_center\">38.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10 min</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">38.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">55.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">35.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">37.53</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Performance of Memory Storyboard using different average temporal segment lengths.</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS3.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">BatchNorm vs. GroupNorm.</h4>\n\n<div id=\"S5.SS3.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We experimented with a variation of Memory Storyboard as well as three baseline methods (SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>, Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>, and Temporal Classification <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>) where the group normalization layers in the ResNet backbone are replaced with batch normalization <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite> layers. The models are trained on SAYCam and evaluated on the downstream <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet classification task with a SVM. The resulting accuracies are shown in Table <a href=\"#S5.T5\" title=\"Table 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We observe that GroupNorm significantly outperform BatchNorm for all the models examined. This result is aligned the conclusion in <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite> that BatchNorm is not compatible with unsupervised continual learning, and extends the conclusion to streaming SSL.</p>\n</div>\n<figure id=\"S5.T5\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SimCLR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Osiris</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MemStoryboard</th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Batch Norm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Group Norm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">37.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>\nGroup norm is better at dealing with temporal non-stationarity for streaming SSL.</figcaption>\n</figure>\n<figure id=\"S5.F5\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"S5.F5.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"305\" height=\"285\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>\n<span class=\"ltx_text ltx_font_bold\">Memory Storyboard model performance on SAYCam with different long-term memory sizes</span> (5k, 10k, 50k, and 100k) <span class=\"ltx_text ltx_font_bold\">and varying training batch compositions</span> (12.5% – 75.0% from <math id=\"S5.F5.m2\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>) using SVM readout. Each colored line represents the performance of different training batch compositions when <span class=\"ltx_text ltx_font_bold\">the model has seen the same amount of data</span> from the stream. Each black line represent the performance of different training batch compositions when the model has taken <span class=\"ltx_text ltx_font_bold\">the same number of gradient updates</span>.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S5.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.4 </span>Optimal Batch Composition Under Different Memory Constraints</h3>\n\n<div id=\"S5.SS4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In Memory Storyboard, the training batch is composed of samples from both the long-term memory and the short-term memory (see Figure <a href=\"#S4.F2\" title=\"Figure 2 ‣ Temporal Segmentation. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). However, the optimal composition ratio of the training batch, i.e. the optimal percentage of data in the training batch that comes from the short-term memory, is yet to be explored. Sampling more data from the short-term memory means we can digest more data within a fixed number of training steps, but there will be more distribution shift between different training batches. On the other hand, sampling more data from the long-term memory buffer may result in overfitting on the long-term memory data. In this section we experiment with different memory sizes and training composition, and demonstrate the optimal batch composition under different memory constraints.</p>\n</div>\n<div id=\"S5.SS4.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We fix the size of the short-term memory <math id=\"S5.SS4.p2.m1\" class=\"ltx_Math\" alttext=\"|M_{short}|\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|M_{short}|</annotation></semantics></math> to be <math id=\"S5.SS4.p2.m2\" class=\"ltx_Math\" alttext=\"5K\" display=\"inline\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math>, and vary the memory constraint for the long-term memory <math id=\"S5.SS4.p2.m3\" class=\"ltx_Math\" alttext=\"|M_{long}|=5K,10K,50K,100K\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><mo>,</mo><mrow><mn>10</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><mo>,</mo><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow><mo>,</mo><mrow><mn>100</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>K</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">|M_{long}|=5K,10K,50K,100K</annotation></semantics></math>. For each long-term memory size, we experiment with batch size from data stream <math id=\"S5.SS4.p2.m4\" class=\"ltx_Math\" alttext=\"b=64,128,192,256,320,384\" display=\"inline\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><mn>64</mn><mo>,</mo><mn>128</mn><mo>,</mo><mn>192</mn><mo>,</mo><mn>256</mn><mo>,</mo><mn>320</mn><mo>,</mo><mn>384</mn></mrow></mrow><annotation encoding=\"application/x-tex\">b=64,128,192,256,320,384</annotation></semantics></math> (which corresponds to 12.5% though 75% of the training batch size). We sample <math id=\"S5.SS4.p2.m5\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> images from the short-term memory and <math id=\"S5.SS4.p2.m6\" class=\"ltx_Math\" alttext=\"512-b\" display=\"inline\"><semantics><mrow><mn>512</mn><mo>−</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">512-b</annotation></semantics></math> images from the long-term memory to compose a training batch. We evaluate the model with SVM readout on <span class=\"ltx_text ltx_font_italic\">mini-</span>ImageNet after the model has seen every 10% of the entire data stream and plot the results in Figure <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. We discuss the different observations for large memory size and small memory size respectively.</p>\n<ul id=\"S5.I3\" class=\"ltx_itemize\">\n<li id=\"S5.I3.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I3.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large memory size.</span> When the long-term memory size is large (Figures <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(c) and <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(d)), overfitting on the memory is unlikely and hence we can sample more data from the long-term memory and the performance still keeps increasing as the model sees more data. Hence, with the same amount of data seen by the model (colored curves), it is better to sample only a small batch from the short-term memory. However, when we control the number of model update steps to the same (black curves), neither focusing on the short-term memory or focusing on the long-term memory is preferable. In such case, the optimal batch size from the short term memory is at roughly 50% of the training batch.</p>\n</div>\n</li>\n<li id=\"S5.I3.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I3.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Small memory size.</span> When the long-term memory size is small (Figures <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) and <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b)), the model is prone to overfitting on the memory. In our experiments, when at least 50% of the training batch (256 images) come from the short-term memory, the downstream task accuracy will start to decrease before we reach the end of the video stream. As a result, with the same number of model update steps (black curves), taking more images from the short-term memory gives better results. With the same amount of data seen by the model (colored curves), getting a higher percentage of long-term memory data has an advantage in the beginning when the memory size is not too small compared to the data seen by the model, but it is ultimately outperformed by models that focus more on short-term memory.</p>\n</div>\n</li>\n</ul>\n<p class=\"ltx_p\">To summarize, the optimal training batch composition is dependent on the memory and compute constraint. A bigger batch from the long-term memory is preferred when we can afford a relatively large memory (in our experiments, 50K images from a 200-hour video stream) and we care about the model’s performance after seeing a fixed amount of data. A smaller batch from the long-term memory is preferred when we cannot afford to a large memory to prevent overfitting on the memory buffer data. When we can afford a large memory buffer and we care about the model’s performance after a fixed amount of computation for real-time learning, a balanced training batch composition is preferred.</p>\n</div>\n</section>\n</section>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">The ability to continuously learn from large-scale uncurated streaming video data is crucial for applying self-supervised learning methods in real-world embodied agents. Existing works have limited exploration on this problem, have mainly focused on static datasets, and do not perform well in the streaming video setting. inspired by the event segmentation mechanism in human cognition, in this work, we propose Memory Storyboard, which leverages temporal segmentation to produce a two-tier memory hierarchy akin to the short-term and long-term memory of humans. Memory Storyboard combines a temporal contrastive objective and a standard self-supervised contrastive objective to facilitate representation learning from scratch through streaming video experiences. Memory Storyboard achieves state-of-the-art performance on downstream classification and object detection tasks when trained on real-world large egocentric video datasets. By studying the effects of subsampling rates, average segment length, normalization, and optimal batch composition under different compute and memory constraints, we also offer valuable insights on the design choices for streaming self-supervised learning.</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Afham et al.,  [2023]</span>\n<span class=\"ltx_bibblock\">\nAfham, M., Shukla, S. N., Poursaeed, O., Zhang, P., Shah, A., and Lim, S. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Revisiting kernel temporal segmentation as an adaptive tokenizer for long-form video understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 1189–1194.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Aljundi et al.,  [2019]</span>\n<span class=\"ltx_bibblock\">\nAljundi, R., Lin, M., Goujaud, B., and Bengio, Y. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Gradient based sample selection for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 32.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Assran et al.,  [2023]</span>\n<span class=\"ltx_bibblock\">\nAssran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning from images with a joint-embedding predictive architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 15619–15629.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baldassano et al.,  [2017]</span>\n<span class=\"ltx_bibblock\">\nBaldassano, C., Chen, J., Zadbood, A., Pillow, J. W., Hasson, U., and Norman, K. A. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Discovering event structure in continuous narrative perception and memory.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neuron</span>, 95(3):709–721.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Baldwin et al.,  [2001]</span>\n<span class=\"ltx_bibblock\">\nBaldwin, D. A., Baird, J. A., Saylor, M. M., and Clark, M. A. (2001).\n\n</span>\n<span class=\"ltx_bibblock\">Infants parse dynamic action.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Child development</span>, 72(3):708–717.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Banerjee et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nBanerjee, S., Verma, V. K., Parag, T., Singh, M., and Namboodiri, V. P. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Class incremental online streaming learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2110.10741</span>.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Ponce, J., and LeCun, Y. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Vicreg: Variance-invariance-covariance regularization for self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">The Tenth International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al.,  [2018]</span>\n<span class=\"ltx_bibblock\">\nCaron, M., Bojanowski, P., Joulin, A., and Douze, M. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Deep clustering for unsupervised learning of visual features.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the European conference on computer vision (ECCV)</span>, pages 132–149.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of visual features by contrasting cluster assignments.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 33:9912–9924.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Carreira et al.,  [2024]</span>\n<span class=\"ltx_bibblock\">\nCarreira, J., King, M., Patraucean, V., Gokay, D., Ionescu, C., Yang, Y., Zoran, D., Heyward, J., Doersch, C., Aytar, Y., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Learning from one continuous video stream.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 28751–28761.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">A simple framework for contrastive learning of visual representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1597–1607. PMLR.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen and He,  [2021]</span>\n<span class=\"ltx_bibblock\">\nChen, X. and He, K. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Exploring simple siamese representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 15750–15758.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Deng et al.,  [2009]</span>\n<span class=\"ltx_bibblock\">\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).\n\n</span>\n<span class=\"ltx_bibblock\">Imagenet: A large-scale hierarchical image database.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">2009 IEEE conference on computer vision and pattern recognition</span>, pages 248–255. Ieee.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Doersch et al.,  [2015]</span>\n<span class=\"ltx_bibblock\">\nDoersch, C., Gupta, A., and Efros, A. A. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised visual representation learning by context prediction.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE international conference on computer vision</span>, pages 1422–1430.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">DuBrow and Davachi,  [2013]</span>\n<span class=\"ltx_bibblock\">\nDuBrow, S. and Davachi, L. (2013).\n\n</span>\n<span class=\"ltx_bibblock\">The influence of context boundaries on memory for the sequential order of events.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Experimental Psychology: General</span>, 142:1277–1286.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ezzyat and Davachi,  [2011]</span>\n<span class=\"ltx_bibblock\">\nEzzyat, Y. and Davachi, L. (2011).\n\n</span>\n<span class=\"ltx_bibblock\">What constitutes an episode in episodic memory?\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Psychological science</span>, 22:243–52.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fini et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nFini, E., Da Costa, V. G. T., Alameda-Pineda, X., Ricci, E., Alahari, K., and Mairal, J. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised models are continual learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 9621–9630.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gidaris et al.,  [2018]</span>\n<span class=\"ltx_bibblock\">\nGidaris, S., Singh, P., and Komodakis, N. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised representation learning by predicting image rotations.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">6th International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grill et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nGrill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Bootstrap your own latent-a new approach to self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 33:21271–21284.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guo et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nGuo, Y., Liu, B., and Zhao, D. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Online continual learning through mutual information maximization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 8109–8126. PMLR.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hacohen and Tuytelaars,  [2024]</span>\n<span class=\"ltx_bibblock\">\nHacohen, G. and Tuytelaars, T. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Forgetting order of continual learning: Examples that are learned first are forgotten last.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2406.09935</span>.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hayes et al.,  [2019]</span>\n<span class=\"ltx_bibblock\">\nHayes, T. L., Cahill, N. D., and Kanan, C. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Memory efficient experience replay for streaming learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">2019 International Conference on Robotics and Automation (ICRA)</span>, pages 9769–9776. IEEE.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hayes et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nHayes, T. L., Kafle, K., Shrestha, R., Acharya, M., and Kanan, C. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Remind your neural network to prevent catastrophic forgetting.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European conference on computer vision</span>, pages 466–483. Springer.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hayes and Kanan,  [2020]</span>\n<span class=\"ltx_bibblock\">\nHayes, T. L. and Kanan, C. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Lifelong machine learning with deep streaming linear discriminant analysis.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</span>, pages 220–221.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nHe, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders are scalable vision learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 16000–16009.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Momentum contrast for unsupervised visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 9729–9738.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.,  [2016]</span>\n<span class=\"ltx_bibblock\">\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Deep residual learning for image recognition.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 770–778.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nHu, D., Yan, S., Lu, Q., Hong, L., Hu, H., Zhang, Y., Li, Z., Wang, X., and Feng, J. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">How well does self-supervised pre-training perform with streaming data?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">The Tenth International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ioffe and Szegedy,  [2015]</span>\n<span class=\"ltx_bibblock\">\nIoffe, S. and Szegedy, C. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the 32nd International Conference on Machine Learning</span>.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Khosla et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nKhosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., and Krishnan, D. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Supervised contrastive learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 33:18661–18673.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kingma and Ba,  [2015]</span>\n<span class=\"ltx_bibblock\">\nKingma, D. P. and Ba, J. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Adam: A method for stochastic optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</span>.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lassiter and Slaw,  [1991]</span>\n<span class=\"ltx_bibblock\">\nLassiter, G. and Slaw, D. (1991).\n\n</span>\n<span class=\"ltx_bibblock\">The unitization and memory of events.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Experimental Psychology: General</span>, 120:80–82.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mai et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nMai, Z., Li, R., Kim, H., and Sanner, S. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 3589–3599.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McCloskey and Cohen,  [1989]</span>\n<span class=\"ltx_bibblock\">\nMcCloskey, M. and Cohen, N. J. (1989).\n\n</span>\n<span class=\"ltx_bibblock\">Catastrophic interference in connectionist networks: The sequential learning problem.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Psychology of Learning and Motivation</span>, volume 24, pages 109–165. Elsevier.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Misra,  [2020]</span>\n<span class=\"ltx_bibblock\">\nMisra, D. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Mish: A self regularized non-monotonic activation function.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">31st British Machine Vision Conference</span>.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Misra and Maaten,  [2020]</span>\n<span class=\"ltx_bibblock\">\nMisra, I. and Maaten, L. v. d. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning of pretext-invariant representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 6707–6717.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Newtson et al.,  [1977]</span>\n<span class=\"ltx_bibblock\">\nNewtson, D., Engquist, G. A., and Bois, J. (1977).\n\n</span>\n<span class=\"ltx_bibblock\">The objective basis of behavior units.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Personality and social psychology</span>, 35(12):847.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Noroozi and Favaro,  [2016]</span>\n<span class=\"ltx_bibblock\">\nNoroozi, M. and Favaro, P. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of visual representations by solving jigsaw puzzles.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European conference on computer vision</span>, pages 69–84. Springer.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Oord et al.,  [2018]</span>\n<span class=\"ltx_bibblock\">\nOord, A. v. d., Li, Y., and Vinyals, O. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Representation learning with contrastive predictive coding.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1807.03748</span>.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Orhan et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nOrhan, E., Gupta, V., and Lake, B. M. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning through the eyes of a child.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, 33:9960–9971.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Pathak et al.,  [2016]</span>\n<span class=\"ltx_bibblock\">\nPathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A. A. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Context encoders: Feature learning by inpainting.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 2536–2544.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Potapov et al.,  [2014]</span>\n<span class=\"ltx_bibblock\">\nPotapov, D., Douze, M., Harchaoui, Z., and Schmid, C. (2014).\n\n</span>\n<span class=\"ltx_bibblock\">Category-specific video summarization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13</span>, pages 540–555. Springer.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Purushwalkam et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nPurushwalkam, S., Morgado, P., and Gupta, A. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">The challenges of continuous self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European Conference on Computer Vision</span>, pages 702–721. Springer.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rao et al.,  [2019]</span>\n<span class=\"ltx_bibblock\">\nRao, D., Visin, F., Rusu, A., Pascanu, R., Teh, Y. W., and Hadsell, R. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Continual unsupervised representation learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 32.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ren et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nRen, M., Scott, T. R., Iuzzolino, M. L., Mozer, M. C., and Zemel, R. S. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Online unsupervised learning of visual representations and categories.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2109.05675</span>.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ren et al.,  [2015]</span>\n<span class=\"ltx_bibblock\">\nRen, S., He, K., Girshick, R., and Sun, J. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Faster r-cnn: Towards real-time object detection with region proposal networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, volume 28.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Roady et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nRoady, R., Hayes, T. L., Vaidya, H., and Kanan, C. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Stream-51: Streaming classification and novelty detection from videos.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</span>, pages 228–229.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rochan et al.,  [2018]</span>\n<span class=\"ltx_bibblock\">\nRochan, M., Ye, L., and Wang, Y. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Video summarization using fully convolutional sequence networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the European conference on computer vision (ECCV)</span>, pages 347–363.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sasmita and Swallow,  [2022]</span>\n<span class=\"ltx_bibblock\">\nSasmita, K. and Swallow, K. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Measuring event segmentation: An investigation into the stability of event boundary agreement across groups.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Behavior Research Methods</span>, 55.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Saylor et al.,  [2007]</span>\n<span class=\"ltx_bibblock\">\nSaylor, M. M., Baldwin, D. A., Baird, J. A., and LaBounty, J. (2007).\n\n</span>\n<span class=\"ltx_bibblock\">Infants’ on-line segmentation of dynamic human action.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Cognition and Development</span>, 8(1):113–128.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Silva et al.,  [2019]</span>\n<span class=\"ltx_bibblock\">\nSilva, M., Baldassano, C., and Fuentemilla, L. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Rapid memory reactivation at movie event boundaries promotes episodic encoding.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Neuroscience</span>, 39(43):8538–8548.\n\n</span>\n</li>\n<li id=\"bib.bib52\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Singh et al.,  [2016]</span>\n<span class=\"ltx_bibblock\">\nSingh, K. K., Fatahalian, K., and Efros, A. A. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">2016 IEEE Winter Conference on Applications of Computer Vision</span>, pages 1–9. IEEE.\n\n</span>\n</li>\n<li id=\"bib.bib53\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Smith et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nSmith, J. S., Taylor, C. E., Baer, S., and Dovrolis, C. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised progressive learning and the STAM architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</span>.\n\n</span>\n</li>\n<li id=\"bib.bib54\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sohn,  [2016]</span>\n<span class=\"ltx_bibblock\">\nSohn, K. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Improved deep metric learning with multi-class n-pair loss objective.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 29.\n\n</span>\n</li>\n<li id=\"bib.bib55\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sullivan et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nSullivan, J., Mei, M., Perfors, A., Wojcik, E., and Frank, M. C. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Saycam: A large, longitudinal audiovisual dataset recorded from the infant’s perspective.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Open mind</span>, 5:20–29.\n\n</span>\n</li>\n<li id=\"bib.bib56\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tian et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nTian, Y., Krishnan, D., and Isola, P. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Contrastive multiview coding.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16</span>, pages 776–794. Springer.\n\n</span>\n</li>\n<li id=\"bib.bib57\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tiwari et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nTiwari, R., Killamsetty, K., Iyer, R., and Shenoy, P. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Gcr: Gradient coreset based replay buffer selection for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 99–108.\n\n</span>\n</li>\n<li id=\"bib.bib58\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vitter,  [1985]</span>\n<span class=\"ltx_bibblock\">\nVitter, J. S. (1985).\n\n</span>\n<span class=\"ltx_bibblock\">Random sampling with a reservoir.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM Transactions on Mathematical Software (TOMS)</span>, 11(1):37–57.\n\n</span>\n</li>\n<li id=\"bib.bib59\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vong et al.,  [2024]</span>\n<span class=\"ltx_bibblock\">\nVong, W. K., Wang, W., Orhan, A. E., and Lake, B. M. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Grounded language acquisition through the eyes and ears of a single child.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Science</span>, 383(6682):504–511.\n\n</span>\n</li>\n<li id=\"bib.bib60\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nWang, J., Wang, X., Shang-Guan, Y., and Gupta, A. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Wanderlust: Online continual object detection in the real world.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 10829–10838.\n\n</span>\n</li>\n<li id=\"bib.bib61\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et al.,  [2023]</span>\n<span class=\"ltx_bibblock\">\nWei, Y., Ye, J., Huang, Z., Zhang, J., and Shan, H. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Online prototype learning for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 18764–18774.\n\n</span>\n</li>\n<li id=\"bib.bib62\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wiewel and Yang,  [2021]</span>\n<span class=\"ltx_bibblock\">\nWiewel, F. and Yang, B. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Entropy-based sample selection for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">2020 28th European signal processing conference (EUSIPCO)</span>, pages 1477–1481. IEEE.\n\n</span>\n</li>\n<li id=\"bib.bib63\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et al.,  [2023]</span>\n<span class=\"ltx_bibblock\">\nWu, J. Z., Zhang, D. J., Hsu, W., Zhang, M., and Shou, M. Z. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Label-efficient online continual object detection in streaming video.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 19246–19255.\n\n</span>\n</li>\n<li id=\"bib.bib64\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu and He,  [2018]</span>\n<span class=\"ltx_bibblock\">\nWu, Y. and He, K. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Group normalization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the European conference on computer vision (ECCV)</span>, pages 3–19.\n\n</span>\n</li>\n<li id=\"bib.bib65\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yates et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nYates, T. S., Skalaban, L. J., Ellis, C. T., Bracher, A. J., Baldassano, C., and Turk-Browne, N. B. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Neural event segmentation of continuous experience in human infants.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Proceedings of the National Academy of Sciences</span>, 119(43):e2200257119.\n\n</span>\n</li>\n<li id=\"bib.bib66\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">You et al.,  [2017]</span>\n<span class=\"ltx_bibblock\">\nYou, Y., Gitman, I., and Ginsburg, B. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Large batch training of convolutional networks.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1708.03888</span>.\n\n</span>\n</li>\n<li id=\"bib.bib67\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al.,  [2023]</span>\n<span class=\"ltx_bibblock\">\nYu, X., Guo, Y., Gao, S., and Rosing, T. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Scale: Online self-supervised lifelong learning without prior knowledge.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 2484–2495.\n\n</span>\n</li>\n<li id=\"bib.bib68\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zacks et al.,  [2006]</span>\n<span class=\"ltx_bibblock\">\nZacks, J. M., Speer, N. K., Vettel, J. M., and Jacoby, L. L. (2006).\n\n</span>\n<span class=\"ltx_bibblock\">Event understanding and memory in healthy aging and dementia of the alzheimer type.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Psychology and aging</span>, 21(3):466.\n\n</span>\n</li>\n<li id=\"bib.bib69\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zacks and Swallow,  [2007]</span>\n<span class=\"ltx_bibblock\">\nZacks, J. M. and Swallow, K. M. (2007).\n\n</span>\n<span class=\"ltx_bibblock\">Event segmentation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Current directions in psychological science</span>, 16(2):80–84.\n\n</span>\n</li>\n<li id=\"bib.bib70\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zacks et al.,  [2001]</span>\n<span class=\"ltx_bibblock\">\nZacks, J. M., Tversky, B., and Iyer, G. (2001).\n\n</span>\n<span class=\"ltx_bibblock\">Perceiving, remembering, and communicating structure in events.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of experimental psychology: General</span>, 130(1):29.\n\n</span>\n</li>\n<li id=\"bib.bib71\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zbontar et al.,  [2021]</span>\n<span class=\"ltx_bibblock\">\nZbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Barlow twins: Self-supervised learning via redundancy reduction.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 12310–12320. PMLR.\n\n</span>\n</li>\n<li id=\"bib.bib72\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al.,  [2016]</span>\n<span class=\"ltx_bibblock\">\nZhang, K., Chao, W.-L., Sha, F., and Grauman, K. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Video summarization with long short-term memory.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14</span>, pages 766–782. Springer.\n\n</span>\n</li>\n<li id=\"bib.bib73\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al.,  [2024]</span>\n<span class=\"ltx_bibblock\">\nZhang, Y., Charlin, L., Zemel, R., and Ren, M. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Integrating present and past in unsupervised continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2404.19132</span>.\n\n</span>\n</li>\n<li id=\"bib.bib74\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu et al.,  [2020]</span>\n<span class=\"ltx_bibblock\">\nZhu, W., Lu, J., Li, J., and Zhou, J. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Dsnet: A flexible detect-to-summarize network for video summarization.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE Transactions on Image Processing</span>, 30:948–962.\n\n</span>\n</li>\n<li id=\"bib.bib75\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhuang et al.,  [2022]</span>\n<span class=\"ltx_bibblock\">\nZhuang, C., Xiang, Z., Bai, Y., Jia, X., Turk-Browne, N., Norman, K., DiCarlo, J. J., and Yamins, D. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">How well do unsupervised learning algorithms model human real-time and life-long learning?\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>, 35:22628–22642.\n\n</span>\n</li>\n</ul>\n</section>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Experiment Details</h2>\n\n<section id=\"A1.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Model Architecture</h4>\n\n<div id=\"A1.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">On top of the ResNet backbone, we use a two-layer MLP with 2048 hidden units, 128 output units, and ReLU activation function as the projector. In Memory Storyboard, we create two separate projectors for <math id=\"A1.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{TCL}\" display=\"inline\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{TCL}</annotation></semantics></math> and <math id=\"A1.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathcal{L}_{CL}\" display=\"inline\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CL}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"A1.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training</h4>\n\n<div id=\"A1.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">For all experiments in Tables <a href=\"#S4.T1\" title=\"Table 1 ‣ Overall Loss Function. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a href=\"#S4.T2\" title=\"Table 2 ‣ Overall Loss Function. ‣ 4 Memory Storyboard ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we used a total batch size of 512 (64 from <math id=\"A1.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> and 448 from <math id=\"A1.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"M_{long}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{long}</annotation></semantics></math> by default). The input resolution of the images to the model is 112. We apply a standard data augmentation pipeline for SSL methods following <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al., [<a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>]</cite>, which include random resized crop, random horizontal flip, random color jitter, random gray scale, random Gaussian filter, and color-normalization with ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>. For the SimCLR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>, Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>, and TC <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite> experiments, we used the Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite> optimizer with a constant learning rate of 0.001, and a projector with 2 MLP layers of size 2048 and 128 respectively. For the SimSiam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite> experiments, we used the SGD optimizer with learning rate 0.05, momentum 0.9, and weight decay 1e-4, and a projector with 3 MLP layers of size 2048.</p>\n</div>\n</section>\n<section id=\"A1.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation</h4>\n\n<div id=\"A1.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet and Labeled-S evaluations, the streaming SSL models are evaluated every 5% of the entire dataset. That is, we store 20 model checkpoints throughout the streaming training and evaluate them on <span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet and Labeled-S with SVM and kNN readout. The <span class=\"ltx_text ltx_font_italic\">best</span> result among these checkpoints are reported. Similar to <cite class=\"ltx_cite ltx_citemacro_citet\">Zhuang et al., [<a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">75</a>]</cite>, for SVM readout, we report the best performance among learning rate values {1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2}; for kNN readout, we report the best performance among <math id=\"A1.SS0.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> values {1, 3, 5, 10, 20, 50, 100, 150, 200, 250}.</p>\n</div>\n<div id=\"A1.SS0.SSS0.Px3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">For ImageNet-1K evaluations, we evaluate the final model after streaming SSL training on the entire dataset. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Purushwalkam et al., [<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>, we train a linear classifier on top of the normalized learned representations and report the classification accuracy. We used the LARS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">66</a>]</cite> optimizer with learning rate 3.0, momentum 0.9, and cosine learning rate schedule for 10 epochs. We used a batch size of 1024.</p>\n</div>\n<div id=\"A1.SS0.SSS0.Px3.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">For OAK evaluations, we use Faster R-CNN <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>, a popular two-stage object detector. We initialize the ResNet-50 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite> backbone with the backbone of the final checkpoint of the streaming SSL model, and fine-tune the entire model on OAK with IID training for 10 epochs, following the training configurations of <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">63</a>]</cite>.</p>\n</div>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Results</h2>\n\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Separating Short-term Memory Batch and Long-term Memory Batch</h3>\n\n<div id=\"A2.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Inspired by the design of separating the loss on the new data and the replay data in Osiris <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite>, we investigate the optimal strategy of applying the temporal contrastive loss on the training batch. We consider applying the temporal contrastive loss only on data from short-term memory, only on data from long-term memory, separately on data from short-term and long-term memory and average the losses, and on the entire training batch (concatenated data from short-term and long-term memory). We report the results in Table <a href=\"#A2.T6\" title=\"Table 6 ‣ B.1 Separating Short-term Memory Batch and Long-term Memory Batch ‣ Appendix B Additional Results ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For experiments in the main paper, we apply the temporal contrastive loss only on data from long-term memory.</p>\n</div>\n<div id=\"A2.SS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">The results here demonstrate that applying the temporal contrastive loss only on data from long-term memory or on the entire training batch achieve best performance. Applying the temporal contrastive loss only on data from short-term memory achieves inferior performance due to the limited number of temporal classes in the short-term buffer.</p>\n</div>\n<figure id=\"A2.T6\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SAYCam</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">KrishnaCam</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Labeled-S</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text ltx_font_italic\">mini</span>-ImageNet</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OAK mAP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Short Only</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">52.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Long Only</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">39.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">56.29</span></td>\n<td class=\"ltx_td ltx_align_center\">36.36</td>\n<td class=\"ltx_td ltx_align_center\">21.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Concatenate</th>\n<td class=\"ltx_td ltx_align_center\">38.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">55.43</td>\n<td class=\"ltx_td ltx_align_center\">36.08</td>\n<td class=\"ltx_td ltx_align_center\">21.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Separate</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">54.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">36.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">21.40</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Performance of Memory Storyboard when the temporal contrastive loss is applied on different parts of the training batch.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Optimal Batch Composition for SimCLR</h2>\n\n<figure id=\"A3.F6\" class=\"ltx_figure\"><img src=\"./assets/x6.png\" id=\"A3.F6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"432\" height=\"108\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>\n<span class=\"ltx_text ltx_font_bold\">SimCLR model performance on SAYCam with different long-term memory sizes</span> (5k, 10k, 50k, and 100k) <span class=\"ltx_text ltx_font_bold\">and varying training batch compositions</span> (12.5% – 75.0% from <math id=\"A3.F6.m2\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math>) using SVM readout. Each colored line represents the performance of different training batch compositions when <span class=\"ltx_text ltx_font_bold\">the model has seen the same amount of data</span> from the stream. Each black line represent the performance of different training batch compositions when the model has taken <span class=\"ltx_text ltx_font_bold\">the same number of gradient updates</span>.</figcaption>\n</figure>\n<figure id=\"A3.F7\" class=\"ltx_figure\"><img src=\"./assets/x7.png\" id=\"A3.F7.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"432\" height=\"111\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>\n<span class=\"ltx_text ltx_font_bold\">Comparison of Memory Storyboard (solid lines) and SimCLR (dashed lines) model performance</span> on SAYCam using SVM readout, controlling the amount of data the model has seen from the stream.\n</figcaption>\n</figure>\n<div id=\"A3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We replicate the experiments in Figure <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> on SimCLR models with two-tier memory, and plot the results in Figure <a href=\"#A3.F6\" title=\"Figure 6 ‣ Appendix C Optimal Batch Composition for SimCLR ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that the analysis and the conclusions of section <a href=\"#S5.SS4\" title=\"5.4 Optimal Batch Composition Under Different Memory Constraints ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> still holds: when we have a large memory, we either prefer balanced training batch (with fixed amount of computation) or a bigger batch from long-term memory (with fixed amount of data); when we can only afford a small memory, we prefer a smaller batch from long-term memory. We also want to note that, the SVM readout results starts to go down towards the end of the streaming training in SimCLR experiments more often than Memory Storyboard experiments, suggesting the better scalability of Memory Storyboard to larger-scale streaming training.</p>\n</div>\n<div id=\"A3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">These results demonstrate that the analysis and observations in section <a href=\"#S5.SS4\" title=\"5.4 Optimal Batch Composition Under Different Memory Constraints ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> regarding the optimal batch composition for streaming SSL training under different memory and compute constraints is general, and applies to standard SSL methods in addition to Memory Storyboard.</p>\n</div>\n</section>\n<section id=\"A4\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>More Comprehensive Comparison between Memory Storyboard and SimCLR</h2>\n\n<div id=\"A4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">With the experiment results in Figure <a href=\"#S5.F5\" title=\"Figure 5 ‣ BatchNorm vs. GroupNorm. ‣ 5.3 Other Training Factors ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Figure <a href=\"#A3.F6\" title=\"Figure 6 ‣ Appendix C Optimal Batch Composition for SimCLR ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we provide a more comprehensive comparision between Memory Storyboard and SimCLR performance under different memory constraints and batch compositions in Figure <a href=\"#A3.F7\" title=\"Figure 7 ‣ Appendix C Optimal Batch Composition for SimCLR ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. We observe that Memory Storyboard outperforms SimCLR under the same amount of seen data, across a wide range of memory sizes and batch compositions. In particular, we note that Memory Storyboard significantly outperform SimCLR when we sample more data from <math id=\"A4.p1.m1\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> (towards the right side of the <math id=\"A4.p1.m2\" class=\"ltx_Math\" alttext=\"x\" display=\"inline\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>-axis). This results in the higher optimal performance when the memory size is small, where a larger batch from <math id=\"A4.p1.m3\" class=\"ltx_Math\" alttext=\"M_{short}\" display=\"inline\"><semantics><msub><mi>M</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">M_{short}</annotation></semantics></math> is needed to prevent overfitting on the long-term memory for better performance. We argue that, with temporal segmentation and the temporal contrastive loss, Memory Storyboard is able to provide better memory efficiency and also alleviate the temporal correlation issue suffered by SimCLR when we sample a large batch from the short-term memory.</p>\n</div>\n<figure id=\"A4.F8\" class=\"ltx_figure\"><img src=\"./assets/x8.png\" id=\"A4.F8.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"308\" height=\"186\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>\n<span class=\"ltx_text ltx_font_bold\">Visualization of the temporal segments produced by randomly initialized models on (a) SAYCam (b)(c) KrishnaCam.</span> The images are the same as the ones in Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ Qualitative Results. ‣ 5.2 Main Results ‣ 5 Experiments ‣ Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. We observe that Memory Storyboard training enables to model to capture more intricate transitions between scenes.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2501.12254",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:28.549Z"
}
