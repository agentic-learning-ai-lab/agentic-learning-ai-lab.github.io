{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Humans and other animals can seemingly learn new skills and accumulate knowledge throughout their lifetimes.\nContinual learning algorithms aim to achieve this same capability: to produce systems that can learn from a sequence of tasks.\nOne of the main challenges is a phenomenon typically\ncalled catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(McCloskey &amp; Cohen, <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">1989</a>)</cite>, where the learner’s performance on a previously visited task degrades once it learns new tasks.\nA dominant theme in continual learning has been the development of methods\nto address catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Li &amp; Hoiem, <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2017</a>; Zenke et al., <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2017</a>; Rebuffi et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2017</a>; Kirkpatrick et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2017</a>; Prabhu et al., <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. However, there has been limited theoretical treatment of their efficacy.</p>\n</div>\n<div id=\"S1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In this work, we focus on a continual learning problem consisting of a sequence of linear regression tasks. The tasks are designed such that a single linear model is sufficient to solve the full sequence.\nThe aim of focusing on this problem is that\nit is simple enough to permit analysis while preserving some key challenges of the continual learning problem.\nA particularly noteworthy discovery of prior work is that even in this setting, catastrophic forgetting can occur <cite class=\"ltx_cite ltx_citemacro_citep\">(Evron et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.\nHowever, an open question remains: Can methods designed to combat forgetting succeed in this setting? As a first step towards answering this question, we study one such method, experience replay.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">There are many variations of experience replay. For example, <cite class=\"ltx_cite ltx_citemacro_citet\">van de Ven et al. (<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> introduce a brain-inspired form of generative replay and show that it has strong performance on complex benchmarks.\nIn this paper, we focus on the more standard form of sample replay, an intuitive technique where samples observed during continual learning are stored and repeated back to the learner during later tasks, to help retain solutions to prior tasks. This simple method has been shown\nto be effective at ameliorating forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Rebuffi et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2017</a>; Rolnick et al., <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2019</a>; Aljundi et al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2019b</a>; Wu et al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2019</a>; Chaudhry et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2019</a>; Tiwari et al., <a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, and is often used in dynamic learning settings like reinforcement learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">1992</a>)</cite>.\nReplay has also been a focus of study in neuroscience, as strong experimental evidence supports the hypothesis that replay plays an important role in memory consolidation <cite class=\"ltx_cite ltx_citemacro_citep\">(Rasch &amp; Born, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2007</a>; Oudiette &amp; Paller, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2013</a>)</cite>.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Some existing theoretical works in continual learning show that when tasks are repeatedly visited in a cyclical order, forgetting will be minimized. <cite class=\"ltx_cite ltx_citemacro_citet\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> shows this in the continual linear regression setting, while <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et al. (<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> show a similar result in a more general PAC-like continual learning setting.\nWhile these results show that forgetting decreases when the entire task sequence is replayed, they do not consider what happens when replay occurs between tasks, and involves a subset of samples.\nThis is a focus of this work.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We first prove that somewhat counter-intuitively there are worst-case scenarios where replay actually causes more forgetting. We then prove a surprising stronger result, that this can still hold in a certain average case sense:\neven when examples are sampled randomly from specific task subspaces, and replay samples are chosen randomly, replay can increase forgetting on average.\nWe emphasize that we are obtaining these results in a relatively benign (or non-adversarial) setting: the samples are not noisy, and the tasks share an optimal solution.\nThis increased forgetting occurs even when the tasks are close to each other, under a natural notion of distance.</p>\n</div>\n<div id=\"S1.p6\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In addition to our theoretical contributions, we provide an empirical investigation of forgetting with sample replay to support our theoretical findings. We verify our theoretical results and further show that the same surprising behavior exists in continual linear regression learning with neural networks. Through experiments on MNIST continual learning benchmarks, we show that there is significant variation in the effectiveness of replay, and\nfind a simple task sequence where replay can increase forgetting.</p>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Background and Setup</h2>\n\n<section id=\"S2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.1 </span>Background</h3>\n\n<div id=\"S2.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_cite\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> initiated the study of catastrophic forgetting in overparameterized linear regression.\nThey consider a sequence of linear tasks <math id=\"S2.SS1.p1.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left({\\bm{X}}_{t},{\\bm{y}}_{t}}}\\right)_{t=1}^{T}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>(</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>t</mi></msub></mrow></mrow><mo>)</mo><msub><mi></mi><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow></msub><msup><mi></mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left({\\bm{X}}_{t},{\\bm{y}}_{t}}}\\right)_{t=1}^{T}</annotation></semantics></math> where <math id=\"S2.SS1.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\\in\\mathbb{R}^{n_{t}\\times d}\" display=\"inline\"><semantics><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}\\in\\mathbb{R}^{n_{t}\\times d}</annotation></semantics></math>, <math id=\"S2.SS1.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{y}}_{t}\\in\\mathbb{R}^{n_{t}}\" display=\"inline\"><semantics><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>t</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{y}}_{t}\\in\\mathbb{R}^{n_{t}}</annotation></semantics></math> and <math id=\"S2.SS1.p1.m4\" class=\"ltx_Math\" alttext=\"n_{t},d\" display=\"inline\"><semantics><mrow><msub><mi>n</mi><mi>t</mi></msub><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">n_{t},d</annotation></semantics></math> are the number of samples per task and input dimension respectively.\nThey assume that the sequence of linear tasks share a solution that could be obtained by jointly training on all tasks, and for each task <math id=\"S2.SS1.p1.m5\" class=\"ltx_Math\" alttext=\"n_{t}&lt;d\" display=\"inline\"><semantics><mrow><msub><mi>n</mi><mi>t</mi></msub><mo>&lt;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">n_{t}&lt;d</annotation></semantics></math>, so any single task would not necessarily contain all the information needed to learn the common solution.\nDespite the existence of a common solution, they show that there are sequences of tasks such that learning them in a sequential manner with gradient descent will result in a significant amount of forgetting, which is defined to be the average error on all previously seen tasks (<a href=\"#S2.Thmtheorem3\" title=\"Definition 2.3 (Forgetting). ‣ Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Definition</span> <span class=\"ltx_text ltx_ref_tag\">2.3</span></a>).\nIn this setting, <math id=\"S2.SS1.p1.m6\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> many samples would be sufficient to recover the solution to each task.</p>\n</div>\n<div id=\"S2.SS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Another group of techniques used to mitigate forgetting is through regularization.\nFor example, forgetting can be eliminated using a Fisher information based weighting matrix\n<cite class=\"ltx_cite ltx_citemacro_citep\">(Kirkpatrick et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2017</a>; Evron et al., <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. We note that storing such matrices would take order <math id=\"S2.SS1.p2.m1\" class=\"ltx_Math\" alttext=\"d^{2}\" display=\"inline\"><semantics><msup><mi>d</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">d^{2}</annotation></semantics></math> bits of memory, which is of the same order as storing <math id=\"S2.SS1.p2.m2\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> samples.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> introduce a general notion of an ”ideal continual learner” and instantiate it for continual linear regression.\nTheir algorithm maintains the shared null space of the previous tasks.\nAgain, storing a null space could take order <math id=\"S2.SS1.p2.m3\" class=\"ltx_Math\" alttext=\"d^{2}\" display=\"inline\"><semantics><msup><mi>d</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">d^{2}</annotation></semantics></math> bits of memory.\nSample replay, on the other hand, allows the learner to store much less than <math id=\"S2.SS1.p2.m4\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> many samples, reducing memory requirements significantly.\nHowever, the effectiveness of replaying a few samples in this linear setting is an open question and the focus of this paper.</p>\n</div>\n</section>\n<section id=\"S2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.2 </span> General Setup</h3>\n\n<div id=\"S2.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Here we consider two different settings for sample replay: the worst case and the average case.\nWe first introduce the general setup,\nwhich is shared between the two settings and closely resembles the earlier formulation\ndescribed above, and then examine each separately, in Sections <a href=\"#S3.SS1\" title=\"3.1 Worst Case: From Vanishing to Catastrophic Forgetting via Replay Sample Selection ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> and <a href=\"#S3.SS2.SSS1\" title=\"3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>, respectively.\nWe start with the following two assumptions.</p>\n</div>\n<div id=\"S2.Thmtheorem1\" class=\"ltx_theorem ltx_theorem_assumption\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Assumption 2.1</span></span><span class=\"ltx_text ltx_font_bold\"> </span>(Over-parameterized linear regression)<span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S2.Thmtheorem1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We assume that each task is:</p>\n<ul id=\"S2.I1\" class=\"ltx_itemize\">\n<li id=\"S2.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S2.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Linear: there is <math id=\"S2.I1.i1.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}_{t}\\in\\mathbb{R}^{d}\" display=\"inline\"><semantics><mrow><msubsup><mi>𝒘</mi><mi>t</mi><mo>∗</mo></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}_{t}\\in\\mathbb{R}^{d}</annotation></semantics></math> such that <math id=\"S2.I1.i1.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}{\\bm{w}}^{*}_{t}={\\bm{y}}_{t}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒘</mi><mi>t</mi><mo>∗</mo></msubsup></mrow><mo>=</mo><msub><mi>𝒚</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}{\\bm{w}}^{*}_{t}={\\bm{y}}_{t}</annotation></semantics></math>.</p>\n</div>\n</li>\n<li id=\"S2.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S2.I1.i2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Over-parameterized: <math id=\"S2.I1.i2.p1.m1\" class=\"ltx_Math\" alttext=\"k_{t}\\vcentcolon=\\mathrm{rank}({\\bm{X}}_{t})&lt;d\" display=\"inline\"><semantics><mrow><msub><mi>k</mi><mi>t</mi></msub><mo>:=</mo><mrow><mi>rank</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">k_{t}\\vcentcolon=\\mathrm{rank}({\\bm{X}}_{t})&lt;d</annotation></semantics></math>.</p>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<div id=\"S2.SS2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We also assume\nrealizability,\nwhich ensures that the <math id=\"S2.SS2.p2.m1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tasks share a common solution:</p>\n</div>\n<div id=\"S2.Thmtheorem2\" class=\"ltx_theorem ltx_theorem_assumption\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Assumption 2.2</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S2.Thmtheorem2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">(Realizability). There exists <math id=\"S2.Thmtheorem2.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\\in\\bigcup_{t}\\mathrm{span}({\\bm{X}}_{t})\" display=\"inline\"><semantics><mrow><msup><mi>𝒘</mi><mo>∗</mo></msup><mo rspace=\"0.111em\">∈</mo><mrow><msub><mo>⋃</mo><mi>t</mi></msub><mrow><mi>span</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}\\in\\bigcup_{t}\\mathrm{span}({\\bm{X}}_{t})</annotation></semantics></math>, where <math id=\"S2.Thmtheorem2.p1.m2\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}^{*}}}}\\right\\|_{2}\\leq 1\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn></msub><mo lspace=\"0.0835em\">≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}^{*}}}}\\right\\|_{2}\\leq 1</annotation></semantics></math>, such that for all tasks <math id=\"S2.Thmtheorem2.p1.m3\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>,\n<math id=\"S2.Thmtheorem2.p1.m4\" class=\"ltx_Math\" alttext=\"{\\bm{y}}_{t}={\\bm{X}}_{t}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{y}}_{t}={\\bm{X}}_{t}{\\bm{w}}^{*}</annotation></semantics></math>.</p>\n</div>\n</div>\n<section id=\"S2.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Projections.</h5>\n\n<div id=\"S2.SS2.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Let <math id=\"S2.SS2.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{t}\" display=\"inline\"><semantics><msub><mi>𝚷</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{t}</annotation></semantics></math> be an orthogonal projection onto the row span of <math id=\"S2.SS2.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}</annotation></semantics></math>, i.e. the span of the samples of task <math id=\"S2.SS2.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.\nWe can write <math id=\"S2.SS2.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{i}={\\bm{X}}_{t}^{+}{\\bm{X}}_{t}\" display=\"inline\"><semantics><mrow><msub><mi>𝚷</mi><mi>i</mi></msub><mo>=</mo><mrow><msubsup><mi>𝑿</mi><mi>t</mi><mo>+</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑿</mi><mi>t</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{i}={\\bm{X}}_{t}^{+}{\\bm{X}}_{t}</annotation></semantics></math> where <math id=\"S2.SS2.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}^{+}\" display=\"inline\"><semantics><msubsup><mi>𝑿</mi><mi>t</mi><mo>+</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}^{+}</annotation></semantics></math> is the Moore-Penrose inverse of <math id=\"S2.SS2.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}^{+}\" display=\"inline\"><semantics><msubsup><mi>𝑿</mi><mi>t</mi><mo>+</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}^{+}</annotation></semantics></math>.\nAnother way to obtain the orthonormal projection is using a matrix <math id=\"S2.SS2.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑾</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{W}}_{t}</annotation></semantics></math> whose columns form an orthonormal basis for the row span of <math id=\"S2.SS2.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}</annotation></semantics></math>.\nGiven <math id=\"S2.SS2.SSS0.Px1.p1.m9\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑾</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{W}}_{t}</annotation></semantics></math>, we could write <math id=\"S2.SS2.SSS0.Px1.p1.m10\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{t}={\\bm{W}}_{t}{\\bm{W}}_{t}^{\\top}\" display=\"inline\"><semantics><mrow><msub><mi>𝚷</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝑾</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑾</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{t}={\\bm{W}}_{t}{\\bm{W}}_{t}^{\\top}</annotation></semantics></math>.\nWe use <math id=\"S2.SS2.SSS0.Px1.p1.m11\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{t}\\vcentcolon=\\mathbf{I}-{\\bm{\\Pi}}_{i}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo>:=</mo><mrow><mi>𝐈</mi><mo>−</mo><msub><mi>𝚷</mi><mi>i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{t}\\vcentcolon=\\mathbf{I}-{\\bm{\\Pi}}_{i}</annotation></semantics></math> to denote the orthogonal projection onto the null space of task <math id=\"S2.SS2.SSS0.Px1.p1.m12\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.\nWe use the term projection interchangeably with orthogonal projection throughout the rest of this paper.</p>\n</div>\n</section>\n<section id=\"S2.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Learning Procedure.</h5>\n\n<div id=\"S2.SS2.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Initially <math id=\"S2.SS2.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{0}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{0}</annotation></semantics></math> is set to the all-zero vector. For each task <math id=\"S2.SS2.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, starting with the solution <math id=\"S2.SS2.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{t-1}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{t-1}</annotation></semantics></math> from the previous task(s), the learning algorithm minimizes squared error <math id=\"S2.SS2.SSS0.Px2.p1.m4\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{w}}-{\\bm{y}}_{t}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mi>𝒘</mi><mo>−</mo><msub><mi>𝒚</mi><mi>t</mi></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{w}}-{\\bm{y}}_{t}}}}\\right\\|_{2}^{2}</annotation></semantics></math> using GD or SGD.</p>\n</div>\n<div id=\"S2.SS2.SSS0.Px2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">It is known that training with GD or SGD leads to a solution that has minimum distance to initialization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunasekar et al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2018</a>; Zhang et al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>, that is,</p>\n<table id=\"A4.EGx1\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S2.E1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S2.E1.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle{\\bm{w}}_{t}=\\arg\\min_{{\\bm{w}}}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}-{\\bm{w}}_{t-1}}}}\\right\\|_{2}\\;\\;\\mbox{s.t.}\\;{\\bm{X}}_{t}{\\bm{w}}={\\bm{y}}_{t}.\" display=\"inline\"><semantics><mrow><msub><mi>𝒘</mi><mi>t</mi></msub><mo>=</mo><mi>arg</mi><munder><mi>min</mi><mi>𝒘</mi></munder><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mi>𝒘</mi><mo>−</mo><msub><mi>𝒘</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn></msub><mtext>s.t.</mtext><msub><mi>𝑿</mi><mi>t</mi></msub><mi>𝒘</mi><mo>=</mo><msub><mi>𝒚</mi><mi>t</mi></msub><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{w}}_{t}=\\arg\\min_{{\\bm{w}}}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}-{\\bm{w}}_{t-1}}}}\\right\\|_{2}\\;\\;\\mbox{s.t.}\\;{\\bm{X}}_{t}{\\bm{w}}={\\bm{y}}_{t}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_italic\">parameter error</span> of the procedure in <a href=\"#S2.E1\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a> satisfies the following recursive relationship</p>\n<table id=\"A4.EGx2\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S2.E2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S2.E2.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{w}}_{t}-{\\bm{w}}^{*}={\\bm{P}}_{t}({\\bm{w}}_{t-1}-{\\bm{w}}^{*}).\" display=\"inline\"><semantics><mrow><mrow><mrow><msub><mi>𝒘</mi><mi>t</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝒘</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{w}}_{t}-{\\bm{w}}^{*}={\\bm{P}}_{t}({\\bm{w}}_{t-1}-{\\bm{w}}^{*}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We include a derivation of this relationship in\n<a href=\"#A1.SS1\" title=\"A.1 Derivation of Equation 2 ‣ Appendix A Additional Background and Definitions ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">A.1</span></a> for completeness.\nInitially, the parameter error vector is <math id=\"S2.SS2.SSS0.Px2.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{0}-{\\bm{w}}^{*}=-{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝒘</mi><mn>0</mn></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}_{0}-{\\bm{w}}^{*}=-{\\bm{w}}^{*}</annotation></semantics></math>, <a href=\"#S2.E2\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a> states that after training on task <math id=\"S2.SS2.SSS0.Px2.p2.m2\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the parameter error vector is projected onto the null space of task <math id=\"S2.SS2.SSS0.Px2.p2.m3\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. So the parameter error vector evolves as a sequence of orthonormal projections into task null spaces, while forgetting also takes into account projection of the parameter error onto the task samples.</p>\n</div>\n<div id=\"S2.Thmtheorem3\" class=\"ltx_theorem ltx_theorem_definition\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Definition 2.3</span></span><span class=\"ltx_text ltx_font_bold\"> </span>(Forgetting)<span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S2.Thmtheorem3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Given a sequence of training samples for tasks <math id=\"S2.Thmtheorem3.p1.m1\" class=\"ltx_math_unparsed\" alttext=\"S=\\mathopen{}\\mathclose{{\\left(({\\bm{X}}_{t},{\\bm{y}}_{t})}}\\right)_{t=1}^{T}\" display=\"inline\"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>)</mo><msub><mi></mi><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow></msub><msup><mi></mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">S=\\mathopen{}\\mathclose{{\\left(({\\bm{X}}_{t},{\\bm{y}}_{t})}}\\right)_{t=1}^{T}</annotation></semantics></math>, the forgetting with respect to the training samples is defined to be</p>\n<table id=\"S2.E3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S2.E3.m1\" class=\"ltx_math_unparsed\" alttext=\"F_{S}({\\bm{w}}_{T})=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{w}}_{T}-{\\bm{y}}_{t}}}}\\right\\|_{2}^{2}.\" display=\"block\"><semantics><mrow><msub><mi>F</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msub><mi>𝒚</mi><mi>t</mi></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">F_{S}({\\bm{w}}_{T})=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{w}}_{T}-{\\bm{y}}_{t}}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n</div>\n</div>\n<div id=\"S2.SS2.SSS0.Px2.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We drop the subscript <math id=\"S2.SS2.SSS0.Px2.p3.m1\" class=\"ltx_Math\" alttext=\"S\" display=\"inline\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> when it is clear from the context which sequence of tasks the forgetting is being computed over.\nNote that the average forgetting defined above is over <math id=\"S2.SS2.SSS0.Px2.p3.m2\" class=\"ltx_Math\" alttext=\"T-1\" display=\"inline\"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">T-1</annotation></semantics></math> tasks since forgetting on the last task is always zero.\nWe can consider forgetting for a certain task ordering catastrophic, when <math id=\"S2.SS2.SSS0.Px2.p3.m3\" class=\"ltx_Math\" alttext=\"\\lim_{T\\rightarrow\\infty}F_{S}({\\bm{w}}_{T})&gt;0\" display=\"inline\"><semantics><mrow><mrow><msub><mo>lim</mo><mrow><mi>T</mi><mo stretchy=\"false\">→</mo><mi mathvariant=\"normal\">∞</mi></mrow></msub><mrow><msub><mi>F</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lim_{T\\rightarrow\\infty}F_{S}({\\bm{w}}_{T})&gt;0</annotation></semantics></math>, or in other words, when it does not vanish with the number of tasks.</p>\n</div>\n<div id=\"S2.Thmtheorem4\" class=\"ltx_theorem ltx_theorem_remark\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_italic\">Remark 2.4</span></span><span class=\"ltx_text ltx_font_italic\">.</span>\n</h6>\n<div id=\"S2.Thmtheorem4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In our average case result, each task is given by a distribution, and forgetting is measured on new samples from previous tasks’ distributions. We introduce and discuss these details in <a href=\"#S3.SS2.SSS1\" title=\"3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.2.1</span></a>.</p>\n</div>\n</div>\n<div id=\"S2.SS2.SSS0.Px2.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Forgetting for the output of the learning procedure described in <a href=\"#S2.E1\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a> can be written as</p>\n<table id=\"A4.EGx3\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S2.E4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S2.E4.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle F_{S}({\\bm{w}}_{T})=\" display=\"inline\"><semantics><mrow><mrow><msub><mi>F</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle F_{S}({\\bm{w}}_{T})=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"S2.E4.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}P_{T-1}\\dots P_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2},\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><msub><mi>P</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant=\"normal\">…</mi><msub><mi>P</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}P_{T-1}\\dots P_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">see <a href=\"#A1.SS2\" title=\"A.2 Derivation of Equation 4 ‣ Appendix A Additional Background and Definitions ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for this derivation.\nWe can see from <a href=\"#S2.E4\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a> that forgetting not only depends on the parameter error vector but also on its\nrelationship with the training samples.</p>\n</div>\n</section>\n<section id=\"S2.SS2.SSS0.Px3\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Replay.</h5>\n\n<div id=\"S2.SS2.SSS0.Px3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We consider a simple and standard formulation of replay in the literature, where the learning algorithm can store up to <math id=\"S2.SS2.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> samples from the previously seen tasks in memory, sometimes called episodic memory <cite class=\"ltx_cite ltx_citemacro_citep\">(Chaudhry et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>. Let <math id=\"S2.SS2.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"\\{{\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}}\\}\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>𝑿</mi><mtext>mem</mtext></msup><mo>,</mo><msup><mi>𝒚</mi><mtext>mem</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}}\\}</annotation></semantics></math> denote the set of stored samples.\nDuring training on the current task <math id=\"S2.SS2.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, in addition to the current task’s samples, the model also trains on <math id=\"S2.SS2.SSS0.Px3.p1.m4\" class=\"ltx_Math\" alttext=\"\\{{\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}}\\}\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>𝑿</mi><mtext>mem</mtext></msup><mo>,</mo><msup><mi>𝒚</mi><mtext>mem</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}}\\}</annotation></semantics></math> to get the new iterate <math id=\"S2.SS2.SSS0.Px3.p1.m5\" class=\"ltx_Math\" alttext=\"\\tilde{w}_{t+1}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>w</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{w}_{t+1}</annotation></semantics></math>. There are many ways the algorithm can update <math id=\"S2.SS2.SSS0.Px3.p1.m6\" class=\"ltx_Math\" alttext=\"\\{{\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}}\\}\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mi>𝑿</mi><mtext>mem</mtext></msup><mo>,</mo><msup><mi>𝒚</mi><mtext>mem</mtext></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}}\\}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chaudhry et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>. In our worst case setup, this choice is adversarial, while in the average case setup we consider a random selection.</p>\n</div>\n</section>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Replay Can Provably Increase Forgetting in Continual Linear Regression</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In this section we show that replay can increase forgetting in two different settings. Each setting demonstrates a different scenario where replay can increase forgetting.\nThe worst case setting highlights how the relationships between individual samples could lead to catastrophic forgetting with replay, while the average case result goes beyond interactions between individual samples and highlights the role of task subspaces and the angle(s) between them.\nFor both of these results, interference within samples of each task plays an important role. Since samples within a task can be revisited many times during training, this intra-task sample interference does not matter without replay and we only see forgetting due to interference across tasks. With replay, however, since only a fraction of the samples within a task are trained on, intra-task sample interference could also contribute to forgetting.</p>\n</div>\n<section id=\"S3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Worst Case: From Vanishing to Catastrophic Forgetting via Replay Sample Selection</h3>\n\n<div id=\"S3.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our result in the worst case setting shows that the increase in forgetting due to replay can be significant.\nIn addition to Assumptions <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a> and <a href=\"#S2.Thmtheorem2\" title=\"Assumption 2.2. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, we require that samples have unit norm:</p>\n</div>\n<div id=\"S3.Thmtheorem1\" class=\"ltx_theorem ltx_theorem_assumption\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Assumption 3.1</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S3.Thmtheorem1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Let <math id=\"S3.Thmtheorem1.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{ti}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{ti}</annotation></semantics></math> be the <math id=\"S3.Thmtheorem1.p1.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>th row of <math id=\"S3.Thmtheorem1.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}</annotation></semantics></math>. Assume that <math id=\"S3.Thmtheorem1.p1.m4\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{ti}}}}\\right\\|_{2}=1\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn></msub><mo lspace=\"0.0835em\">=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{ti}}}}\\right\\|_{2}=1</annotation></semantics></math>.</p>\n</div>\n</div>\n<div id=\"S3.SS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">This assumption is not necessary to get the worst case result.\nWe have included it to make it explicit that the construction in the worst case does not rely on some samples have a much larger norm than the rest.</p>\n</div>\n<section id=\"S3.SS1.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.1.1 </span>Worst case result</h4>\n\n<div id=\"S3.SS1.SSS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In this worst case setting, we choose the tasks and a (possibly empty) subset of samples from each task to be replayed. In this setup, we show that forgetting could increase from vanishing, <math id=\"S3.SS1.SSS1.p1.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathcal{O}\\mathopen{}\\mathclose{{\\left(\\frac{1}{T}}}\\right)\" display=\"inline\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">𝒪</mi><mrow><mi></mi><mrow><mo>(</mo><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}\\mathopen{}\\mathclose{{\\left(\\frac{1}{T}}}\\right)</annotation></semantics></math>, to <math id=\"S3.SS1.SSS1.p1.m2\" class=\"ltx_Math\" alttext=\"\\Theta(1)\" display=\"inline\"><semantics><mrow><mi mathvariant=\"normal\">Θ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Theta(1)</annotation></semantics></math>, which\nwe have labeled catastrophic.</p>\n</div>\n<div id=\"S3.SS1.SSS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">At the core of the tasks constructed in the worst case setting are three samples <math id=\"S3.SS1.SSS1.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1},{\\bm{x}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1},{\\bm{x}}_{2}</annotation></semantics></math> and <math id=\"S3.SS1.SSS1.p2.m2\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>, where <math id=\"S3.SS1.SSS1.p2.m3\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> and <math id=\"S3.SS1.SSS1.p2.m4\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math> are orthogonal to one another, and <math id=\"S3.SS1.SSS1.p2.m5\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> is linearly independent but not orthogonal to <math id=\"S3.SS1.SSS1.p2.m6\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> or <math id=\"S3.SS1.SSS1.p2.m7\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>. Consider a sequence of two tasks where the samples for the first task consists of <math id=\"S3.SS1.SSS1.p2.m8\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1},{\\bm{x}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1},{\\bm{x}}_{2}</annotation></semantics></math>, and the second task contains only <math id=\"S3.SS1.SSS1.p2.m9\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>. After training on the first task, there is no error on <math id=\"S3.SS1.SSS1.p2.m10\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> and <math id=\"S3.SS1.SSS1.p2.m11\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>. When we train on the second task, since <math id=\"S3.SS1.SSS1.p2.m12\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math> is not orthogonal to <math id=\"S3.SS1.SSS1.p2.m13\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, training on <math id=\"S3.SS1.SSS1.p2.m14\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math> introduces some error on <math id=\"S3.SS1.SSS1.p2.m15\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> but doesn’t introduce error on <math id=\"S3.SS1.SSS1.p2.m16\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>, since it is othogonal to <math id=\"S3.SS1.SSS1.p2.m17\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>.\nNow suppose that <math id=\"S3.SS1.SSS1.p2.m18\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> is replayed, so for the second task, we train on both <math id=\"S3.SS1.SSS1.p2.m19\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> and <math id=\"S3.SS1.SSS1.p2.m20\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>. Since <math id=\"S3.SS1.SSS1.p2.m21\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> is not orthogonal to <math id=\"S3.SS1.SSS1.p2.m22\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, this will introduce error on <math id=\"S3.SS1.SSS1.p2.m23\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>. Thus in this three sample, two task setup, replay causes an <span class=\"ltx_text ltx_font_italic\">exchange</span> of the error on <math id=\"S3.SS1.SSS1.p2.m24\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> with error on <math id=\"S3.SS1.SSS1.p2.m25\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>. See <a href=\"#A2.F6.sf2\" title=\"In Figure 6 ‣ B.1 Intuition for the worst case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">6(b)</span></a> in <a href=\"#A2.SS1\" title=\"B.1 Intuition for the worst case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">B.1</span></a> for a geometric illustration of this phenomena.</p>\n</div>\n<div id=\"S3.Thmtheorem2\" class=\"ltx_theorem ltx_theorem_theorem\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Theorem 3.2</span></span><span class=\"ltx_text ltx_font_bold\"> </span>(Worst case replay)<span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S3.Thmtheorem2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Under assumptions <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a> <a href=\"#S2.Thmtheorem2\" title=\"Assumption 2.2. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a> and <a href=\"#S3.Thmtheorem1\" title=\"Assumption 3.1. ‣ 3.1 Worst Case: From Vanishing to Catastrophic Forgetting via Replay Sample Selection ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, for any <math id=\"S3.Thmtheorem2.p1.m1\" class=\"ltx_Math\" alttext=\"T\\geq 2,d\\geq 3\" display=\"inline\"><semantics><mrow><mrow><mi>T</mi><mo>≥</mo><mn>2</mn></mrow><mo>,</mo><mrow><mi>d</mi><mo>≥</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">T\\geq 2,d\\geq 3</annotation></semantics></math>, there is a sequence of <math id=\"S3.Thmtheorem2.p1.m2\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tasks and a sample <math id=\"S3.Thmtheorem2.p1.m3\" class=\"ltx_Math\" alttext=\"(\\tilde{{\\bm{x}}},\\tilde{y})\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>𝐱</mi><mo>~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{{\\bm{x}}},\\tilde{y})</annotation></semantics></math> such that without replay, forgetting is <math id=\"S3.Thmtheorem2.p1.m4\" class=\"ltx_math_unparsed\" alttext=\"F({\\bm{w}}_{T})=\\mathcal{O}\\mathopen{}\\mathclose{{\\left(\\frac{1}{T}}}\\right)\" display=\"inline\"><semantics><mrow><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐰</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mi class=\"ltx_font_mathcaligraphic\">𝒪</mi><mrow><mi></mi><mrow><mo>(</mo><mfrac><mn>1</mn><mi>T</mi></mfrac></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">F({\\bm{w}}_{T})=\\mathcal{O}\\mathopen{}\\mathclose{{\\left(\\frac{1}{T}}}\\right)</annotation></semantics></math>, while with replay of <math id=\"S3.Thmtheorem2.p1.m5\" class=\"ltx_Math\" alttext=\"(\\tilde{{\\bm{x}}},\\tilde{y})\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>𝐱</mi><mo>~</mo></mover><mo>,</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{{\\bm{x}}},\\tilde{y})</annotation></semantics></math>, forgetting is catastrophic, i.e., <math id=\"S3.Thmtheorem2.p1.m6\" class=\"ltx_Math\" alttext=\"F(\\tilde{{\\bm{w}}}_{T})=\\Theta(1)\" display=\"inline\"><semantics><mrow><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝐰</mi><mo>~</mo></mover><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi mathvariant=\"normal\">Θ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">F(\\tilde{{\\bm{w}}}_{T})=\\Theta(1)</annotation></semantics></math> .</span></p>\n</div>\n</div>\n<div id=\"S3.SS1.SSS1.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In the proof, which is given in <a href=\"#A3.SS1\" title=\"C.1 Proof of worst case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a> , we construct a scenario where this type of error exchange is detrimental.\nWe describe a sequence of tasks <math id=\"S3.SS1.SSS1.p3.m1\" class=\"ltx_Math\" alttext=\"t=1,\\ldots,T\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">t=1,\\ldots,T</annotation></semantics></math>, where the sample <math id=\"S3.SS1.SSS1.p3.m2\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> occurs in all but the last task, while the sample <math id=\"S3.SS1.SSS1.p3.m3\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> occurs in just one of these tasks. Here without replay there is no forgetting on any sample other than <math id=\"S3.SS1.SSS1.p3.m4\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>.\nAlso, replaying <math id=\"S3.SS1.SSS1.p3.m5\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> would not change the final iterate <math id=\"S3.SS1.SSS1.p3.m6\" class=\"ltx_Math\" alttext=\"w_{T}\" display=\"inline\"><semantics><msub><mi>w</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">w_{T}</annotation></semantics></math>, since without replay, there is no error on <math id=\"S3.SS1.SSS1.p3.m7\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>.\nHowever, if <math id=\"S3.SS1.SSS1.p3.m8\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> is replayed in the last task (<math id=\"S3.SS1.SSS1.p3.m9\" class=\"ltx_Math\" alttext=\"t=T\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">t=T</annotation></semantics></math>), causing error on <math id=\"S3.SS1.SSS1.p3.m10\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>, the forgetting will be much larger, growing with the number of tasks <math id=\"S3.SS1.SSS1.p3.m11\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, since <math id=\"S3.SS1.SSS1.p3.m12\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> occurs in almost all the tasks.</p>\n</div>\n</section>\n</section>\n<section id=\"S3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Average Case: Forgetting in a Random Sample Setting</h3>\n\n<div id=\"S3.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In the previous section, we saw that increased forgetting can arise from adversarially chosen replay samples.\nNext we will show that there are task subspaces such that replay increases forgetting, even when task samples are chosen randomly from those subspaces and replay samples are also chosen randomly from previous tasks.\nOn the other hand, in <a href=\"#S3.SS3\" title=\"3.3 When Replay Cannot Increase Forgetting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.3</span></a> we give some conditions under which replay cannot increase forgetting.</p>\n</div>\n<section id=\"S3.SS2.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.2.1 </span>Average Case Setup</h4>\n\n<div id=\"S3.SS2.SSS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The average case setup is more natural and general relative to the worst case setup. Each task’s samples are drawn from a distribution supported on some subspace, and the forgetting is measured on a set of new samples from that distribution. Additionally, the replay samples are chosen randomly.</p>\n</div>\n<div id=\"S3.SS2.SSS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In the average case construction, the task distributions are\nGaussians supported on specific subspaces.\nEach of these subspaces can be specified by an orthonormal basis <math id=\"S3.SS2.SSS1.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{t}\\in\\mathbb{R}^{d\\times k_{t}}\" display=\"inline\"><semantics><mrow><msub><mi>𝑾</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{W}}_{t}\\in\\mathbb{R}^{d\\times k_{t}}</annotation></semantics></math>.\nThe rows of the <math id=\"S3.SS2.SSS1.p2.m2\" class=\"ltx_Math\" alttext=\"n_{t}\\times d\" display=\"inline\"><semantics><mrow><msub><mi>n</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">n_{t}\\times d</annotation></semantics></math> dimensional matrix <math id=\"S3.SS2.SSS1.p2.m3\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{t}</annotation></semantics></math> consist of individual samples <math id=\"S3.SS2.SSS1.p2.m4\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{t1},\\dots,{\\mathbf{X}}_{tn_{t}}\" display=\"inline\"><semantics><mrow><msub><mi>𝐗</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝐗</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>n</mi><mi>t</mi></msub></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{t1},\\dots,{\\mathbf{X}}_{tn_{t}}</annotation></semantics></math>, where each sample</p>\n<table id=\"A4.EGx4\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S3.E5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S3.E5.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\mathbf{X}}_{tj}={\\bm{W}}_{t}{\\mathbf{Z}}_{tj}\\;\\;\\mathrm{and}\\;\\;{\\mathbf{Z}}_{tj}\\sim\\sf{N}(0,\\frac{\\mathbf{I}_{k_{t}}}{k_{t}}),\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝐗</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>𝑾</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>and</mi><mo lspace=\"0.560em\" rspace=\"0em\">​</mo><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub></mrow><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mstyle displaystyle=\"true\"><mfrac><msub><mi>𝐈</mi><msub><mi>𝗄</mi><mi>𝗍</mi></msub></msub><msub><mi>𝗄</mi><mi>𝗍</mi></msub></mfrac></mstyle><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\mathbf{X}}_{tj}={\\bm{W}}_{t}{\\mathbf{Z}}_{tj}\\;\\;\\mathrm{and}\\;\\;{\\mathbf{Z}}_{tj}\\sim\\sf{N}(0,\\frac{\\mathbf{I}_{k_{t}}}{k_{t}}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">and <math id=\"S3.SS2.SSS1.p2.m5\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{tj}</annotation></semantics></math> are iid for <math id=\"S3.SS2.SSS1.p2.m6\" class=\"ltx_Math\" alttext=\"j\\in[n_{t}]\" display=\"inline\"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>n</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">j\\in[n_{t}]</annotation></semantics></math>.\nSince <math id=\"S3.SS2.SSS1.p2.m7\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{tj}</annotation></semantics></math> are rows of <math id=\"S3.SS2.SSS1.p2.m8\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{t}</annotation></semantics></math>, we can write <math id=\"S3.SS2.SSS1.p2.m9\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{t}={\\mathbf{Z}}_{t}{\\bm{W}}_{t}^{\\top}\" display=\"inline\"><semantics><mrow><msub><mi>𝐗</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝐙</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑾</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{t}={\\mathbf{Z}}_{t}{\\bm{W}}_{t}^{\\top}</annotation></semantics></math>, where\n<math id=\"S3.SS2.SSS1.p2.m10\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{t}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{t}</annotation></semantics></math> is a <math id=\"S3.SS2.SSS1.p2.m11\" class=\"ltx_Math\" alttext=\"n_{t}\\times k_{t}\" display=\"inline\"><semantics><mrow><msub><mi>n</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">n_{t}\\times k_{t}</annotation></semantics></math> dimensional matrix whose rows are <math id=\"S3.SS2.SSS1.p2.m12\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{tj}</annotation></semantics></math>.\nThen <math id=\"S3.SS2.SSS1.p2.m13\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> along with <math id=\"S3.SS2.SSS1.p2.m14\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{t}</annotation></semantics></math> determines <math id=\"S3.SS2.SSS1.p2.m15\" class=\"ltx_Math\" alttext=\"{\\bm{y}}_{t}={\\mathbf{X}}_{t}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝐗</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{y}}_{t}={\\mathbf{X}}_{t}{\\bm{w}}^{*}</annotation></semantics></math>.</p>\n</div>\n<div id=\"S3.SS2.SSS1.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In this average case setting, we need to have enough samples for each task to span each task subspace <math id=\"S3.SS2.SSS1.p3.m1\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑾</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{W}}_{t}</annotation></semantics></math>. The sample generation process described above ensures that this condition is met as long as the number of samples for each task is larger than the task’s rank.</p>\n</div>\n<div id=\"S3.Thmtheorem3\" class=\"ltx_theorem ltx_theorem_assumption\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Assumption 3.3</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S3.Thmtheorem3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Assume that the number of the samples for each task is at least as large as the rank of the given subspace <math id=\"S3.Thmtheorem3.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑾</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{W}}_{t}</annotation></semantics></math>, that is, <math id=\"S3.Thmtheorem3.p1.m2\" class=\"ltx_Math\" alttext=\"k_{t}\\leq n_{t}\" display=\"inline\"><semantics><mrow><msub><mi>k</mi><mi>t</mi></msub><mo>≤</mo><msub><mi>n</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">k_{t}\\leq n_{t}</annotation></semantics></math>.</p>\n</div>\n</div>\n<div id=\"S3.SS2.SSS1.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">While it will always be the case that <math id=\"S3.SS2.SSS1.p4.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{rank}({\\mathbf{X}}_{t})\\leq n_{t}\" display=\"inline\"><semantics><mrow><mrow><mi>rank</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐗</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>≤</mo><msub><mi>n</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathrm{rank}({\\mathbf{X}}_{t})\\leq n_{t}</annotation></semantics></math>, the condition above is to ensure that <math id=\"S3.SS2.SSS1.p4.m2\" class=\"ltx_Math\" alttext=\"\\mathrm{rank}({\\mathbf{X}}_{t})=k_{t}\" display=\"inline\"><semantics><mrow><mrow><mi>rank</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐗</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathrm{rank}({\\mathbf{X}}_{t})=k_{t}</annotation></semantics></math>.\nWe consider the expectation of forgetting with respect to <math id=\"S3.SS2.SSS1.p4.m3\" class=\"ltx_Math\" alttext=\"k_{t}\" display=\"inline\"><semantics><msub><mi>k</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">k_{t}</annotation></semantics></math> test samples from previous tasks. To mark this difference we use <math id=\"S3.SS2.SSS1.p4.m4\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}^{\\prime}_{t},{\\mathbf{y}}^{\\prime}_{t}\" display=\"inline\"><semantics><mrow><msubsup><mi>𝐗</mi><mi>t</mi><mo>′</mo></msubsup><mo>,</mo><msubsup><mi>𝐲</mi><mi>t</mi><mo>′</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{X}}^{\\prime}_{t},{\\mathbf{y}}^{\\prime}_{t}</annotation></semantics></math> to denote the test samples and <math id=\"S3.SS2.SSS1.p4.m5\" class=\"ltx_Math\" alttext=\"F_{S^{\\prime}}({\\bm{w}}_{T})\" display=\"inline\"><semantics><mrow><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">F_{S^{\\prime}}({\\bm{w}}_{T})</annotation></semantics></math> to denote forgetting with respect to these new samples. That is</p>\n<table id=\"A4.EGx5\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S3.E6\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S3.E6.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle F_{S^{\\prime}}({\\bm{w}}_{T})=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\mathbf{X}}^{\\prime}_{t}{\\bm{w}}_{T}-{\\mathbf{y}}^{\\prime}_{t}}}}\\right\\|_{2}^{2}=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\mathbf{X}}^{\\prime}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msubsup><mi>𝐗</mi><mi>t</mi><mo>′</mo></msubsup><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msubsup><mi>𝐲</mi><mi>t</mi><mo>′</mo></msubsup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msubsup><mi>𝐗</mi><mi>t</mi><mo>′</mo></msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle F_{S^{\\prime}}({\\bm{w}}_{T})=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\mathbf{X}}^{\\prime}_{t}{\\bm{w}}_{T}-{\\mathbf{y}}^{\\prime}_{t}}}}\\right\\|_{2}^{2}=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\mathbf{X}}^{\\prime}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We assume that there are <math id=\"S3.SS2.SSS1.p4.m6\" class=\"ltx_Math\" alttext=\"k_{t}\" display=\"inline\"><semantics><msub><mi>k</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">k_{t}</annotation></semantics></math> test samples. This would ensure that there are enough test samples to span the task’s subspace.\nNext proposition gives the expected forgetting (without replay) in this setting, the proof is given in <a href=\"#A3.SS4\" title=\"C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.</p>\n</div>\n<div id=\"S3.Thmtheorem4\" class=\"ltx_theorem ltx_theorem_proposition\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Proposition 3.4</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S3.Thmtheorem4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Suppose that <math id=\"S3.Thmtheorem4.p1.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{tj}</annotation></semantics></math> are sampled according to <a href=\"#S3.E5\" title=\"In 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>, then</span></p>\n<table id=\"A4.EGx6\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S3.E7\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S3.E7.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{T})}}\\right]=\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{T})}}\\right]=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"S3.E7.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(7)</span></td>\n</tr></tbody>\n</table>\n</div>\n</div>\n</section>\n<section id=\"S3.SS2.SSS2\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.2.2 </span>Average Case Results</h4>\n\n<div id=\"S3.SS2.SSS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We now present one of our main results\nwhich states that replay can increase forgetting even in an average-case replay setting.</p>\n</div>\n<div id=\"S3.Thmtheorem5\" class=\"ltx_theorem ltx_theorem_theorem\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Theorem 3.5</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S3.Thmtheorem5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Suppose that Assumptions <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>, <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>, and <a href=\"#S2.Thmtheorem2\" title=\"Assumption 2.2. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a> hold.\nThen for any <math id=\"S3.Thmtheorem5.p1.m1\" class=\"ltx_Math\" alttext=\"{{\\bm{w}}^{*}}\\in\\mathbb{R}^{d}\" display=\"inline\"><semantics><mrow><msup><mi>𝐰</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding=\"application/x-tex\">{{\\bm{w}}^{*}}\\in\\mathbb{R}^{d}</annotation></semantics></math>, there exists constants <math id=\"S3.Thmtheorem5.p1.m2\" class=\"ltx_Math\" alttext=\"0&lt;c_{1},c_{2},c_{3}\" display=\"inline\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>,</mo><msub><mi>c</mi><mn>3</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">0&lt;c_{1},c_{2},c_{3}</annotation></semantics></math> such that for <math id=\"S3.Thmtheorem5.p1.m3\" class=\"ltx_Math\" alttext=\"c_{1}&lt;d\" display=\"inline\"><semantics><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>&lt;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">c_{1}&lt;d</annotation></semantics></math>, <math id=\"S3.Thmtheorem5.p1.m4\" class=\"ltx_Math\" alttext=\"c_{2}m&lt;d-1\" display=\"inline\"><semantics><mrow><mrow><msub><mi>c</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mo>&lt;</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">c_{2}m&lt;d-1</annotation></semantics></math> and <math id=\"S3.Thmtheorem5.p1.m5\" class=\"ltx_Math\" alttext=\"d-1&lt;\\frac{\\exp(m\\log m)}{c_{3}}\" display=\"inline\"><semantics><mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo>&lt;</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><mi>log</mi><mo lspace=\"0.167em\">⁡</mo><mi>m</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><msub><mi>c</mi><mn>3</mn></msub></mfrac></mrow><annotation encoding=\"application/x-tex\">d-1&lt;\\frac{\\exp(m\\log m)}{c_{3}}</annotation></semantics></math>, there\nis a sequences of two tasks such that replay of <math id=\"S3.Thmtheorem5.p1.m6\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> randomly chosen samples from the first task increases forgetting in expectation.</span></p>\n</div>\n</div>\n<div id=\"S3.SS2.SSS2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">This result shows that there can be task sequences where most choices of sample options for replay are unfavorable, so that it is not only choices of replay samples that matters but also the relationship between the tasks.\nNote that replay could increase forgetting even when the number of replay samples is linear in the dimension.\nThe proof of <a href=\"#S3.Thmtheorem5\" title=\"Theorem 3.5. ‣ 3.2.2 Average Case Results ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">3.5</span></a> is given in <a href=\"#A3.SS3\" title=\"C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.3</span></a>. Note the statement in <a href=\"#S3.Thmtheorem5\" title=\"Theorem 3.5. ‣ 3.2.2 Average Case Results ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">3.5</span></a> concerns the <span class=\"ltx_text ltx_font_italic\">expected</span> forgetting with replay; it does not imply that replaying any particular sample from the first subspace would increase forgetting. In fact, there are directions in the first task’s subspace such that replaying a sample in those directions would reduce forgetting.</p>\n</div>\n<div id=\"S3.SS2.SSS2.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Now we give an overview of our average case construction, and the intuition behind it.\nFix an orthonormal bases <math id=\"S3.SS2.SSS2.p3.m1\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d}</annotation></semantics></math> for <math id=\"S3.SS2.SSS2.p3.m2\" class=\"ltx_Math\" alttext=\"\\mathbb{R}^{d}\" display=\"inline\"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{d}</annotation></semantics></math>.\nSuppose that the first task’s null space is spanned by a vector <math id=\"S3.SS2.SSS2.p3.m3\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{2}</annotation></semantics></math> that is very close, but not equal to <math id=\"S3.SS2.SSS2.p3.m4\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>.\nThe second task’s null space is <math id=\"S3.SS2.SSS2.p3.m5\" class=\"ltx_Math\" alttext=\"d-1\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d-1</annotation></semantics></math> dimensional and spanned by <math id=\"S3.SS2.SSS2.p3.m6\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{v}}_{2},\\dots,{\\bm{v}}_{d-1}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{v}}_{2},\\dots,{\\bm{v}}_{d-1}}}\\right\\}</annotation></semantics></math>.\nIt is known that forgetting depends on the angle between the task null spaces in a non-monotonic way; see <a href=\"#A1.SS3\" title=\"A.3 The Angle between Null Spaces and Forgetting ‣ Appendix A Additional Background and Definitions ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">A.3</span></a> for more details.\nWithout replay, this would be the angle between the <math id=\"S3.SS2.SSS2.p3.m7\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{2}</annotation></semantics></math> and <math id=\"S3.SS2.SSS2.p3.m8\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>.\nReplay would reduce the second task’s subspace to a subset of it, determined by the replay samples.\nWe show that the projections of the replayed samples into the second task’s null space approximately follows a Gaussian distribution. This means that for sufficiently large <math id=\"S3.SS2.SSS2.p3.m9\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>,\nthe span of the projected samples will not be too close to <math id=\"S3.SS2.SSS2.p3.m10\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>.\nTherefore, the remaining subspace, which would be the null space of the second task with replay, would be close to <math id=\"S3.SS2.SSS2.p3.m11\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>, but not fully include <math id=\"S3.SS2.SSS2.p3.m12\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>.\nConsequently, the angle with <math id=\"S3.SS2.SSS2.p3.m13\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> would increase, but not by too much, becoming closer to <math id=\"S3.SS2.SSS2.p3.m14\" class=\"ltx_Math\" alttext=\"\\pi/4\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/4</annotation></semantics></math>, resulting in increased forgetting.</p>\n</div>\n<div id=\"S3.SS2.SSS2.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Note that the above Theorem holds when the underlying dimension <math id=\"S3.SS2.SSS2.p4.m1\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is a sufficiently large. However, the result holds even when the dimension <math id=\"S3.SS2.SSS2.p4.m2\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is very small, but with a somewhat more complicated argument. In <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a> (see <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a>) we\nprove a similar result when <math id=\"S3.SS2.SSS2.p4.m3\" class=\"ltx_Math\" alttext=\"d=3\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">d=3</annotation></semantics></math>.</p>\n</div>\n</section>\n</section>\n<section id=\"S3.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>When Replay Cannot Increase Forgetting</h3>\n\n<div id=\"S3.SS3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">These results naturally raise the question of when replay is guaranteed to be benign, i.e. not result in higher forgetting relative to no replay.\nWe give some simple sufficient conditions for benign replay.</p>\n<ul id=\"S3.I1\" class=\"ltx_itemize\">\n<li id=\"S3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S3.I1.i1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">If forgetting without replay is zero.\nIn our setting where after training the loss on each task is zero, zero forgetting implies that loss on all previous tasks is zero.\nIf loss on all previous tasks is zero, then loss on any set of selected replay samples is zero. This means that, the gradient of the loss with respect to those samples would also be zero, resulting in no change to the solution.\nThis holds regardless of whether we are in the average case setting or the worst case one.\nIt also holds for longer sequences of tasks.\nThe results in <cite class=\"ltx_cite ltx_citemacro_citet\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> give some hints on when forgetting would be zero for longer sequences of tasks.\nIt would happen, for example, when the tasks are orthogonal to each other.\nTheir Theorem 6 fully characterizes when forgetting is zero for two tasks.</p>\n</div>\n</li>\n<li id=\"S3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S3.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">When principal angles between task subspaces is large. We can think of the principal angles between task subspaces as distances between tasks. These principal angles are the same as the principal angles between null spaces, see Claim 19 of <cite class=\"ltx_cite ltx_citemacro_citep\">(Evron et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. The following proposition says that if the smallest principal angle between the task null spaces is larger than <math id=\"S3.I1.i2.p1.m1\" class=\"ltx_Math\" alttext=\"\\pi/4\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/4</annotation></semantics></math>, then replay would not hurt forgetting.</p>\n</div>\n<div id=\"S3.Thmtheorem6\" class=\"ltx_theorem ltx_theorem_proposition\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Proposition 3.6</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"S3.Thmtheorem6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Suppose that assumptions <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>, <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>, <a href=\"#S2.Thmtheorem2\" title=\"Assumption 2.2. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a> hold, and we are in the average case setup.\nConsider any two tasks where <math id=\"S3.Thmtheorem6.p1.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}}}}\\right\\|_{\\text{op}}\\leq\\frac{\\sqrt{2}}{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝐏</mi><mn>2</mn></msub><msub><mi>𝐏</mi><mn>1</mn></msub></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mtext class=\"ltx_mathvariant_italic\">op</mtext></msub><mo lspace=\"0.0835em\">≤</mo><mfrac><msqrt><mn>2</mn></msqrt><mn>2</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}}}}\\right\\|_{\\text{op}}\\leq\\frac{\\sqrt{2}}{2}</annotation></semantics></math> and <math id=\"S3.Thmtheorem6.p1.m2\" class=\"ltx_Math\" alttext=\"{{\\bm{w}}^{*}}\\sim\\sf{N}(0,\\mathbf{I})\" display=\"inline\"><semantics><mrow><msup><mi>𝐰</mi><mo>∗</mo></msup><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mi>𝐈</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{{\\bm{w}}^{*}}\\sim\\sf{N}(0,\\mathbf{I})</annotation></semantics></math>. Then forgetting with replay of any number of samples, averaged over <math id=\"S3.Thmtheorem6.p1.m3\" class=\"ltx_Math\" alttext=\"{{\\bm{w}}^{*}}\" display=\"inline\"><semantics><msup><mi>𝐰</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{{\\bm{w}}^{*}}</annotation></semantics></math> is never larger than forgetting without replay. That is, <math id=\"S3.Thmtheorem6.p1.m4\" class=\"ltx_math_unparsed\" alttext=\"\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F(\\tilde{{\\bm{w}}}_{2})}}\\right]\\leq\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F({\\bm{w}}_{2})}}\\right]\" display=\"inline\"><semantics><mrow><mrow><msub><mo>𝔼</mo><msup><mi>𝐰</mi><mo>∗</mo></msup></msub><mrow><mi></mi><mrow><mo>[</mo><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝐰</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">≤</mo><msub><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><msup><mi>𝐰</mi><mo>∗</mo></msup></msub><mrow><mi></mi><mrow><mo>[</mo><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐰</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F(\\tilde{{\\bm{w}}}_{2})}}\\right]\\leq\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F({\\bm{w}}_{2})}}\\right]</annotation></semantics></math>.</span></p>\n</div>\n</div>\n<div id=\"S3.I1.i2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Proof of this proposition is given in <a href=\"#A3.SS4\" title=\"C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.\nNote that in the two task setting, as described in <cite class=\"ltx_cite ltx_citemacro_citet\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, forgetting would be small when the principal angles are away from <math id=\"S3.I1.i2.p2.m1\" class=\"ltx_Math\" alttext=\"\\pi/4\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/4</annotation></semantics></math>, so either larger or smaller but not intermediate values. Our results show that replay differentiates between these two scenarios.</p>\n</div>\n</li>\n</ul>\n<p class=\"ltx_p\">This raises the interesting question of finding necessary and sufficient conditions under which replay would always be benign.</p>\n</div>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We have included three sets of experiments. The first set of experiments explores the extent to which our theoretical results hold empirically when training with more complex (non-linear) networks.\nThe other two experiments involve MNIST and are in a classification setting. In the second set of experiments, we replay one sample and examine how the class of the replayed sample affects forgetting. In the third set, we compare replay of different numbers of samples for two pairs of related task sequences.\nOverall, the aim of these experiments is to verify the theoretical results and explore whether they can hold in more general settings, such as training with nonlinear models and classification.</p>\n</div>\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Empirical Evaluation of the Theoretical Results</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">So far, we have shown that there are tasks that are realizable by linear models where sample replay increases forgetting when a linear model is trained sequentially.\nIn this set of experiments, we verify these findings empirically and show that this behavior is not restricted to training with linear models. The experiments here show that there are (linear) tasks where replay can increase forgetting even when training nonlinear models on these tasks.\nWe investigate the effect of replay on forgetting using a multi-layer perceptron (MLP) with one hidden layer and ReLU activations on a sequence of two tasks that are based on our worst case construction.</p>\n</div>\n<div id=\"S4.SS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We consider two models: a linear model,\nand a MLP with one hidden layer.\nWe consider two task sequence constructions. The first construction is in <math id=\"S4.SS1.p2.m1\" class=\"ltx_Math\" alttext=\"\\mathbb{R}^{3}\" display=\"inline\"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{3}</annotation></semantics></math> and is based on the construction given in <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a>. The second task sequence is an extension of that construction into a higher dimensional space (<math id=\"S4.SS1.p2.m2\" class=\"ltx_Math\" alttext=\"d=50\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">d=50</annotation></semantics></math>). Each sequence of tasks consists of a pair of tasks. The model is trained on the first task and then on the second task. Forgetting is then measured as the mean squared error of the final model on the first task’s samples.\nWhen training with replay, <math id=\"S4.SS1.p2.m3\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> samples from the first task are randomly selected without replacement and are combined with each batch during training on the second task. See <a href=\"#A4\" title=\"Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">D</span></a> for further details on the experimental setup and how the construction in\n<a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a> was extended to a higher dimensional input.</p>\n</div>\n<div id=\"S4.SS1.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Another experiment included in <a href=\"#A4.SS1.SSS1\" title=\"D.1.1 The effect of the angle between tasks while training with a MLP ‣ D.1 Continual Linear Regression Experiments ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">D.1.1</span></a>, provides empirical evidence that forgetting for the nonlinear models in this setting is affected by the angle between task null spaces through a mechanism similar to linear models. This provides some insight into the behavior of the nonlinear models on the continual linear regression problem, as seen in <a href=\"#S4.F1\" title=\"In 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<figure id=\"S4.F1\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S4.F1.sf1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x1.png\" id=\"S4.F1.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"830\" height=\"456\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(a) </span> The input data for the two tasks are given by the three dimensional construction given in <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a>. Each point is averaged over 150 runs. </figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S4.F1.sf2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x2.png\" id=\"S4.F1.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"830\" height=\"446\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(b) </span>An extension of the three dimensional construction to <math id=\"S4.F1.sf2.m2\" class=\"ltx_Math\" alttext=\"d=50\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">d=50</annotation></semantics></math> displaying a similar behavior. Each point is averaged over 60 runs.\n</figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Forgetting versus the number of replayed samples. Each plot shows forgetting of a linear model and a neural net with one hidden layer. We can see that forgetting initially increases with a small number of replay samples and then eventually decreases. The dashed lines show the baseline of no replay and the error bars indicate standard mean error. </figcaption>\n</figure>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Experiments on MNIST</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The aim of the experiments in this section is to explore how the factors that led to harmful forgetting would affect replay in classification in more empirical settings.\nIn all the experiments in this section, a MLP with two hidden layers of size <math id=\"S4.SS2.p1.m1\" class=\"ltx_Math\" alttext=\"256\" display=\"inline\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and ReLU activations was used. See <a href=\"#A4.SS2\" title=\"D.2 Experiments on MNIST ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">D.2</span></a> for more detail on these experiments.</p>\n</div>\n<section id=\"S4.SS2.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.2.1 </span>Rotated MNIST</h4>\n\n<div id=\"S4.SS2.SSS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We study the role of replay samples in this experiment using Rotated MNIST <cite class=\"ltx_cite ltx_citemacro_citep\">(Lopez-Paz &amp; Ranzato, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2017</a>)</cite> in a task incremental setting. We consider two tasks, where the first task is MNIST and the second task is Rotated MNIST, which has the same training data except that each digit\nis rotated in the image. Forgetting is measured as the drop in classification accuracy in test data from the first task.\nIn each run, after training a MLP on the first task, two copies of the network are made. One version of the network is trained on the second task without replay and the other one is trained with replay of a single sample from the first task. The results of these experiments are shown in <a href=\"#S4.F2\" title=\"In 4.2.1 Rotated MNIST ‣ 4.2 Experiments on MNIST ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nOn average, the extent to which replay decreases forgetting depends significantly on the class of the replayed sample.\nFor example, we can see that for <math id=\"S4.SS2.SSS1.p1.m1\" class=\"ltx_Math\" alttext=\"45\" display=\"inline\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math> degrees rotation, replaying the digit <math id=\"S4.SS2.SSS1.p1.m2\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> reduces forgetting by about <math id=\"S4.SS2.SSS1.p1.m3\" class=\"ltx_Math\" alttext=\"3\\%\" display=\"inline\"><semantics><mrow><mn>3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3\\%</annotation></semantics></math>, while replaying the digit <math id=\"S4.SS2.SSS1.p1.m4\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> does not seem to make much of a difference.\nAdditionally, the relationship between tasks, in this case characterized by the degree of the rotation, also affects the behavior of replay with respect to the class of the replayed sample.\nFor example, replaying a <math id=\"S4.SS2.SSS1.p1.m5\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> seems to be more beneficial in the <math id=\"S4.SS2.SSS1.p1.m6\" class=\"ltx_Math\" alttext=\"45\" display=\"inline\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math> degrees rotation case than the <math id=\"S4.SS2.SSS1.p1.m7\" class=\"ltx_Math\" alttext=\"90\" display=\"inline\"><semantics><mn>90</mn><annotation encoding=\"application/x-tex\">90</annotation></semantics></math> degrees.</p>\n</div>\n<div id=\"S4.SS2.SSS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We take a closer look at replaying the digit <math id=\"S4.SS2.SSS1.p2.m1\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> in the <math id=\"S4.SS2.SSS1.p2.m2\" class=\"ltx_Math\" alttext=\"45\" display=\"inline\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math> degree rotation case, where replay seems to have minimal effect on forgetting. <a href=\"#S4.F3\" title=\"In 4.2.1 Rotated MNIST ‣ 4.2 Experiments on MNIST ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the distribution of differences in forgetting, that is, forgetting without replay minus forgetting with replay of a randomly selected digit <math id=\"S4.SS2.SSS1.p2.m3\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> sample.\nWe can see that even when on average this difference is not statistically significant, in many cases forgetting with replay exceeds without replay.\nThese results suggest that there could be significant differences in the efficacy of replay depending on which examples are replayed.</p>\n</div>\n<figure id=\"S4.F2\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S4.F2.sf1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x3.png\" id=\"S4.F2.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"830\" height=\"622\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(a) </span><math id=\"S4.F2.sf1.m5\" class=\"ltx_Math\" alttext=\"45\" display=\"inline\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math> degrees rotation. Differences in means for classes <math id=\"S4.F2.sf1.m6\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, <math id=\"S4.F2.sf1.m7\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, and <math id=\"S4.F2.sf1.m8\" class=\"ltx_Math\" alttext=\"8\" display=\"inline\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> are statistically significant. </figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S4.F2.sf2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x4.png\" id=\"S4.F2.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"830\" height=\"622\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(b) </span><math id=\"S4.F2.sf2.m4\" class=\"ltx_Math\" alttext=\"90\" display=\"inline\"><semantics><mn>90</mn><annotation encoding=\"application/x-tex\">90</annotation></semantics></math> degrees rotation. Differences in the sample means are statistically significant except for <math id=\"S4.F2.sf2.m5\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> and <math id=\"S4.F2.sf2.m6\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>.</figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Class of the replayed sample affects forgetting. The x-axis shows the class of the replay sample while the y-axis shows the amount of forgetting in Rotated MNIST. The points depict the averages and the error bars show mean standard error over <math id=\"S4.F2.m2\" class=\"ltx_Math\" alttext=\"80\" display=\"inline\"><semantics><mn>80</mn><annotation encoding=\"application/x-tex\">80</annotation></semantics></math> runs.\nComparing the average forgetting without replay to the one with replay of a single sample from each class shows that the effect of replay varies significantly across classes.\n</figcaption>\n</figure>\n<figure id=\"S4.F3\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"S4.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"415\" height=\"311\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>A histogram of differences in forgetting without replay and with replay of a digit <math id=\"S4.F3.m3\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> sample in Rotated MNIST, where the second task is rotated by <math id=\"S4.F3.m4\" class=\"ltx_Math\" alttext=\"45\" display=\"inline\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math> degrees.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS2.SSS2\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.2.2 </span>Split MNIST</h4>\n\n<div id=\"S4.SS2.SSS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In this experiment, we study whether the relationship between tasks affects forgetting with replay in a class incremental setting.\nIn one task sequence the first task involves discriminating <math id=\"S4.SS2.SSS2.p1.m1\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn>0</mn></math>’s from <math id=\"S4.SS2.SSS2.p1.m2\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>’s, and the second <math id=\"S4.SS2.SSS2.p1.m3\" class=\"ltx_Math\" alttext=\"6\" display=\"inline\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>’s from <math id=\"S4.SS2.SSS2.p1.m4\" class=\"ltx_Math\" alttext=\"7\" display=\"inline\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>’s; in the other sequence the first task is <math id=\"S4.SS2.SSS2.p1.m5\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn>0</mn></math> vs. <math id=\"S4.SS2.SSS2.p1.m6\" class=\"ltx_Math\" alttext=\"6\" display=\"inline\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>, the second <math id=\"S4.SS2.SSS2.p1.m7\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> vs. <math id=\"S4.SS2.SSS2.p1.m8\" class=\"ltx_Math\" alttext=\"7\" display=\"inline\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>.\n<a href=\"#S4.F4\" title=\"In 4.2.2 Split MNIST ‣ 4.2 Experiments on MNIST ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that in the first task sequence\nreplay consistently helps, but in the second forgetting initially increases, and only begins helping with 4 or more samples.\nWe hypothesize that the visual similarity of the digits in the first task in the <math id=\"S4.SS2.SSS2.p1.m9\" class=\"ltx_Math\" alttext=\"0,6\" display=\"inline\"><semantics><mrow><mn>0</mn><mo>,</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">0,6</annotation></semantics></math> - <math id=\"S4.SS2.SSS2.p1.m10\" class=\"ltx_Math\" alttext=\"1,7\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>,</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">1,7</annotation></semantics></math> sequence makes the forgetting worse with a small number of replay samples, as the challenging discrimination task is sensitive to the selection of samples.</p>\n</div>\n<div id=\"S4.SS2.SSS2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">This empirical result bears important similarities to our theoretical results, in the average case setting.\nThe relationship between the tasks in a continual learning sequence determines the effect of replay on forgetting. And even in this more complicated, classification problem, there exists sequences where forgetting can increase with replay.</p>\n</div>\n<figure id=\"S4.F4\" class=\"ltx_figure\"><img src=\"./assets/x6.png\" id=\"S4.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"415\" height=\"311\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>Role of tasks in replay for Split MNIST. For each task sequence, we have plotted the average forgetting against the number of replay samples. The error bars show mean standard error over <math id=\"S4.F4.m5\" class=\"ltx_Math\" alttext=\"80\" display=\"inline\"><semantics><mn>80</mn><annotation encoding=\"application/x-tex\">80</annotation></semantics></math> runs. Values for <math id=\"S4.F4.m6\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn>0</mn></math> replay samples show forgetting without replay.\nThe differences in average forgetting across the two task sequences are statistically significant in all replay cases except for the no replay case. For the <math id=\"S4.F4.m7\" class=\"ltx_Math\" alttext=\"0,6-1,7\" display=\"inline\"><semantics><mrow><mn>0</mn><mo>,</mo><mrow><mn>6</mn><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">0,6-1,7</annotation></semantics></math> task sequence, the differences in the means for no replay and replay of <math id=\"S4.F4.m8\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> samples is statistically significant, so the observed increase in forgetting is not noise.\n</figcaption>\n</figure>\n</section>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Related Work</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">For general background on continual or lifelong learning, see the surveys by <cite class=\"ltx_cite ltx_citemacro_citet\">De Lange et al. (<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2022</a>); Parisi et al. (<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2019</a>); Wang et al. (<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.\nTheoretical studies of continual learning, and especially replay have been relatively more scarce and recent.\nNevertheless, there are a few studies of continual learning in a linear setting.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Doan et al. (<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> initiate the study of catastrophic forgetting of neural nets in the NTK regime, which also applies to linear models.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Goldfarb &amp; Hand (<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> study the effect of over-parameterization on forgetting in a linear regression setting for two tasks whose task subspaces are effectively low rank and picked randomly.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin et al. (<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> study generalization and a slightly different notion of forgetting in continual linear regression. They allow the tasks to be realized by different linear functions, while the task subspaces are essentially random.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Li et al. (<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> study the trade-off between stability and plasticity in a linear regression setting similar to <cite class=\"ltx_cite ltx_citemacro_citep\">(Evron et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, with the distinction that they use <math id=\"S5.p1.m1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">ℓ</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\ell_{2}</annotation></semantics></math>- regularization while training on the second task.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Shan et al. (<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> study continual learning in deep learning from a statistical-mechanics point of view.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> define a general continual learner that has no memory constraints and incurs zero forgetting. They propose an instantiation of it for continual linear regression that could require up to order <math id=\"S5.p2.m1\" class=\"ltx_Math\" alttext=\"d^{2}\" display=\"inline\"><semantics><msup><mi>d</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">d^{2}</annotation></semantics></math> bits of memory.\nAdditionally, through their framework, they derive uniform convergence type bounds and justify a form of sample replay where the learner keeps a task-balanced set of samples from previous tasks in memory and for each new task trains on the stored and new task’s samples from scratch. Note that without knowing the exact constants in the bounds, this result does not provide much information on replay of fewer number of samples.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et al. (<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> make a similar argument empirically and show that training from scratch on the samples stored in memory plus the most recent task’s samples outperforms many methods that were specifically designed to address catastrophic forgetting.</p>\n</div>\n<div id=\"S5.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">There is a large body of empirical work on continual learning and methods used to mitigate forgetting, many of which use sample replay as a main strategy to address forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Rebuffi et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2017</a>; Chaudhry et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2019</a>; Wu et al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2019</a>; Aljundi et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2019a</a>; Caccia et al., <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.\nAlthough there has been some concern in the literature that replaying a small number of samples might lead to overfitting <cite class=\"ltx_cite ltx_citemacro_citep\">(Lopez-Paz &amp; Ranzato, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>, <cite class=\"ltx_cite ltx_citemacro_citet\">Chaudhry et al. (<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> find that replay of even one sample per class improves performance and does not hurt generalization even when the model memorizes the replay samples. <cite class=\"ltx_cite ltx_citemacro_citet\">Verwimp et al. (<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> give a more complex picture of this in terms of the loss landscape. They find that while replay could keep the solution within a low loss region for previous tasks it could also pull the solution towards an unstable region, affecting generalization.\nMotivated by forgetting in continual learning, <cite class=\"ltx_cite ltx_citemacro_citet\">Toneva et al. (<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2018</a>)</cite> study forgetting of the examples across batches while training on a task.\nThey find that there are complex examples that are prone to forgetting across different model architectures.\nOur results differ from these empirical results in that we focus on the existence of task sequences where replay would provably increase forgetting in a seemingly non adversarial setting.</p>\n</div>\n<div id=\"S5.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In a recent work, <cite class=\"ltx_cite ltx_citemacro_citet\">Banayeeanzade et al. (<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> also study replay in continual linear regression. Their setup differs from ours in at least two significant ways.\nFirst, the sequence of learned tasks lack any structured relationship, as samples for all the tasks are from the Gaussian <math id=\"S5.p4.m1\" class=\"ltx_Math\" alttext=\"\\sf{N}(0,\\mathbf{I}_{d})\" display=\"inline\"><semantics><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><msub><mi>𝐈</mi><mi>𝖽</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sf{N}(0,\\mathbf{I}_{d})</annotation></semantics></math>. In the over-parameterized regime where the number of samples per task is less than <math id=\"S5.p4.m2\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>, each task subspace is chosen uniformly at random.\nSecondly, they allow the labels for each tasks to be generated by different linear functions, so their notion of task similarity is based on the distance between the parameters of each task rather than the relationship between subspaces.\nAssuming that all the tasks share the same linear function, their expression for sample replay is monotonically decreasing with respect to the number of samples.</p>\n</div>\n</section>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We have shown that replay could increase forgetting in continual linear regression for some close pairs of tasks.\nOur experimental results show that an increase in forgetting is not unique to linear models and can be present when sequentially training with simple MLPs and even on a more natural class incremental continual learning problem.</p>\n</div>\n<div id=\"S6.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">This work opens a number of avenues for future study.\nOne interesting direction aims to form a method for identifying tasks in a dataset where replay could increase forgetting.\nA second direction considers relevant work in the cognitive science literature. For example, studies of Retrieval-Induced Forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Anderson et al., <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">1994</a>; <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2000</a>)</cite> find that retrieving some information in a category makes a subject more likely to forget the information in the same category that was not retrieved, which bears similarity to our finding\nthat replaying samples from a task could increase forgetting on the other samples in the same task that were not replayed.\nFinally a valuable but challenging direction endeavors to develop a\nmore complete characterization of when replay increases versus decreases forgetting.</p>\n</div>\n</section>\n<section id=\"S7\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Acknowledgements</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">This work is supported by the funds provided by the National Science Foundation and by DoD OUSD (R<math id=\"S7.p1.m1\" class=\"ltx_Math\" alttext=\"\\&amp;\" display=\"inline\"><semantics><mo>&amp;</mo><annotation encoding=\"application/x-tex\">\\&amp;</annotation></semantics></math>E) under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence).</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Aljundi et al. (2019a)</span>\n<span class=\"ltx_bibblock\">\nRahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia.\n\n</span>\n<span class=\"ltx_bibblock\">Online continual learning with maximal interfered retrieval.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 32, 2019a.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Aljundi et al. (2019b)</span>\n<span class=\"ltx_bibblock\">\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.\n\n</span>\n<span class=\"ltx_bibblock\">Gradient based sample selection for online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>, 32, 2019b.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Anderson et al. (1994)</span>\n<span class=\"ltx_bibblock\">\nM. C. Anderson, R. A. Bjork, and E. L. Bjork.\n\n</span>\n<span class=\"ltx_bibblock\">Remembering can cause forgetting: Retrieval dynamics in long-term memory.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, (20(5)):1063–1087, 1994.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Anderson et al. (2000)</span>\n<span class=\"ltx_bibblock\">\nM C Anderson, E L Bjork, and R A Bjork.\n\n</span>\n<span class=\"ltx_bibblock\">Retrieval-induced forgetting: evidence for a recall-specific mechanism.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, 7(3):522–530, Sep 2000.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Banayeeanzade et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nMohammadamin Banayeeanzade, Mahdi Soltanolkotabi, and Mohammad Rostami.\n\n</span>\n<span class=\"ltx_bibblock\">Theoretical insights into overparameterized models in multi-task and replay-based continual learning, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caccia et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nLucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky.\n\n</span>\n<span class=\"ltx_bibblock\">New insights on reducing abrupt representation change in online continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2104.05025</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chaudhry et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet Kumar Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato.\n\n</span>\n<span class=\"ltx_bibblock\">On tiny episodic memories in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv: Learning</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nXi Chen, Christos Papadimitriou, and Binghui Peng.\n\n</span>\n<span class=\"ltx_bibblock\">Memory bounds for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)</em>, pp.  519–530, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dasgupta &amp; Gupta (2003)</span>\n<span class=\"ltx_bibblock\">\nSanjoy Dasgupta and Anupam Gupta.\n\n</span>\n<span class=\"ltx_bibblock\">An elementary proof of a theorem of johnson and lindenstrauss.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Random Structures &amp; Algorithms</em>, 22, 2003.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">De Lange et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.\n\n</span>\n<span class=\"ltx_bibblock\">A continual learning survey: Defying forgetting in classification tasks.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 44(7):3366–3385, 2022.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1109/TPAMI.2021.3057446</span>.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Doan et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nThang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre Alquier.\n\n</span>\n<span class=\"ltx_bibblock\">A theoretical analysis of catastrophic forgetting through the ntk overlap matrix.\n\n</span>\n<span class=\"ltx_bibblock\">In Arindam Banerjee and Kenji Fukumizu (eds.), <em class=\"ltx_emph ltx_font_italic\">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</em>, volume 130 of <em class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning Research</em>, pp.  1072–1080. PMLR, 13–15 Apr 2021.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Evron et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nItay Evron, Edward Moroshko, Rachel Ward, Nathan Srebro, and Daniel Soudry.\n\n</span>\n<span class=\"ltx_bibblock\">How catastrophic can catastrophic forgetting be in linear regression?\n\n</span>\n<span class=\"ltx_bibblock\">In Po-Ling Loh and Maxim Raginsky (eds.), <em class=\"ltx_emph ltx_font_italic\">Proceedings of Thirty Fifth Conference on Learning Theory</em>, volume 178 of <em class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning Research</em>, pp.  4028–4079. PMLR, 02–05 Jul 2022.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Evron et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nItay Evron, Edward Moroshko, Gon Buzaglo, Maroun Khriesh, Badea Marjieh, Nathan Srebro, and Daniel Soudry.\n\n</span>\n<span class=\"ltx_bibblock\">Continual learning in linear classification on separable data.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 40th International Conference on Machine Learning</em>, ICML’23. JMLR.org, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Goldfarb &amp; Hand (2023)</span>\n<span class=\"ltx_bibblock\">\nDaniel Goldfarb and Paul Hand.\n\n</span>\n<span class=\"ltx_bibblock\">Analysis of catastrophic forgetting for random orthogonal transformation tasks in the overparameterized regime.\n\n</span>\n<span class=\"ltx_bibblock\">In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (eds.), <em class=\"ltx_emph ltx_font_italic\">Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</em>, volume 206 of <em class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning Research</em>, pp.  2975–2993. PMLR, 25–27 Apr 2023.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gunasekar et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nSuriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.\n\n</span>\n<span class=\"ltx_bibblock\">Characterizing implicit bias in terms of optimization geometry.\n\n</span>\n<span class=\"ltx_bibblock\">In Jennifer Dy and Andreas Krause (eds.), <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of <em class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning Research</em>, pp.  1832–1841. PMLR, 10–15 Jul 2018.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Horn &amp; Johnson (1985)</span>\n<span class=\"ltx_bibblock\">\nRoger A. Horn and Charles R. Johnson.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Matrix Analysis</em>.\n\n</span>\n<span class=\"ltx_bibblock\">Cambridge University Press, 1985.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kingma &amp; Ba (2014)</span>\n<span class=\"ltx_bibblock\">\nDiederik P. Kingma and Jimmy Ba.\n\n</span>\n<span class=\"ltx_bibblock\">Adam: A method for stochastic optimization.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">CoRR</em>, abs/1412.6980, 2014.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kirkpatrick et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Overcoming catastrophic forgetting in neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Proceedings of the national academy of sciences</em>, 114(13):3521–3526, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nHaoran Li, Jingfeng Wu, and Vladimir Braverman.\n\n</span>\n<span class=\"ltx_bibblock\">Fixed design analysis of regularization-based continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 2nd Conference on Lifelong Learning Agents (CoLLAs)</em>, pp.  513–533, 2023.\n\n</span>\n<span class=\"ltx_bibblock\">arXiv:2303.10263.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li &amp; Hoiem (2017)</span>\n<span class=\"ltx_bibblock\">\nZhizhong Li and Derek Hoiem.\n\n</span>\n<span class=\"ltx_bibblock\">Learning without forgetting.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">IEEE transactions on pattern analysis and machine intelligence</em>, 40(12):2935–2947, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lin (1992)</span>\n<span class=\"ltx_bibblock\">\nLong-Ji Lin.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Reinforcement learning for robots using neural networks</em>.\n\n</span>\n<span class=\"ltx_bibblock\">Carnegie Mellon University, 1992.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lin et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nSen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff.\n\n</span>\n<span class=\"ltx_bibblock\">Theory on forgetting and generalization of continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 40th International Conference on Machine Learning</em>, ICML’23. JMLR.org, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lopez-Paz &amp; Ranzato (2017)</span>\n<span class=\"ltx_bibblock\">\nDavid Lopez-Paz and Marc’Aurelio Ranzato.\n\n</span>\n<span class=\"ltx_bibblock\">Gradient episodic memory for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, NIPS’17, pp.  6470–6479, Red Hook, NY, USA, 2017. Curran Associates Inc.\n\n</span>\n<span class=\"ltx_bibblock\">ISBN 9781510860964.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McCloskey &amp; Cohen (1989)</span>\n<span class=\"ltx_bibblock\">\nMichael McCloskey and Neal J Cohen.\n\n</span>\n<span class=\"ltx_bibblock\">Catastrophic interference in connectionist networks: The sequential learning problem.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Psychology of learning and motivation</em>, volume 24, pp.  109–165. Elsevier, 1989.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Oudiette &amp; Paller (2013)</span>\n<span class=\"ltx_bibblock\">\nDelphine Oudiette and Ken Paller.\n\n</span>\n<span class=\"ltx_bibblock\">Upgrading the sleeping brain with targeted memory reactivation.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Trends in Cognitive Science</em>, 3(17):pp. 142–149, 2013.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Parisi et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nGerman I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter.\n\n</span>\n<span class=\"ltx_bibblock\">Continual lifelong learning with neural networks: A review.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Neural Networks</em>, 113:54–71, 2019.\n\n</span>\n<span class=\"ltx_bibblock\">ISSN 0893-6080.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">https://doi.org/10.1016/j.neunet.2019.01.012</span>.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Peng et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nLiangzu Peng, Paris V. Giampouras, and René Vidal.\n\n</span>\n<span class=\"ltx_bibblock\">The ideal continual learner: an agent that never forgets.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 40th International Conference on Machine Learning</em>, ICML’23. JMLR.org, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Prabhu et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nAmeya Prabhu, Philip H. S. Torr, and Puneet K. Dokania.\n\n</span>\n<span class=\"ltx_bibblock\">Gdumb: A simple approach that questions our progress in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), <em class=\"ltx_emph ltx_font_italic\">Computer Vision – ECCV 2020</em>, pp.  524–540, Cham, 2020. Springer International Publishing.\n\n</span>\n<span class=\"ltx_bibblock\">ISBN 978-3-030-58536-5.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rasch &amp; Born (2007)</span>\n<span class=\"ltx_bibblock\">\nBjorn Rasch and Jan Born.\n\n</span>\n<span class=\"ltx_bibblock\">Maintaining memories by reactivation.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Current Opinions in Neurobiology</em>, 6(17):pp. 698–703, 2007.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rebuffi et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\n</span>\n<span class=\"ltx_bibblock\">icarl: Incremental classifier and representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</em>, pp.  2001–2010, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rolnick et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne.\n\n</span>\n<span class=\"ltx_bibblock\">Experience replay for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 32, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shan et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nHaozhe Shan, Qianyi Li, and Haim Sompolinsky.\n\n</span>\n<span class=\"ltx_bibblock\">Order parameters and phase transitions of continual learning in deep neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2407.10315</em>, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tiwari et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nRishabh Tiwari, Krishnateja Killamsetty, Rishabh Iyer, and Pradeep Shenoy.\n\n</span>\n<span class=\"ltx_bibblock\">Gcr: Gradient coreset based replay buffer selection for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  99–108, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Toneva et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nMariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon.\n\n</span>\n<span class=\"ltx_bibblock\">An empirical study of example forgetting during deep neural network learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1812.05159</em>, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">van de Ven et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nGido M. van de Ven, Hava T. Siegelmann, and Andreas S. Tolias.\n\n</span>\n<span class=\"ltx_bibblock\">Brain-inspired replay for continual learning with artificial neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Nature Communications</em>, 11(1):4069, 2020.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1038/s41467-020-17866-2</span>.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Verwimp et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nEli Verwimp, Matthias De Lange, and Tinne Tuytelaars.\n\n</span>\n<span class=\"ltx_bibblock\">Rehearsal revealed: The limits and merits of revisiting samples in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  9365–9374, 2021.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1109/ICCV48922.2021.00925</span>.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nLiyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.\n\n</span>\n<span class=\"ltx_bibblock\">A comprehensive survey of continual learning: Theory, method and application, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nYue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.\n\n</span>\n<span class=\"ltx_bibblock\">Large scale incremental learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  374–382, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zenke et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nFriedemann Zenke, Ben Poole, and Surya Ganguli.\n\n</span>\n<span class=\"ltx_bibblock\">Continual learning through synaptic intelligence.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pp.  3987–3995. PMLR, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.\n\n</span>\n<span class=\"ltx_bibblock\">Understanding deep learning (still) requires rethinking generalization.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\">Commun. ACM</em>, 64(3):107–115, feb 2021.\n\n</span>\n<span class=\"ltx_bibblock\">ISSN 0001-0782.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1145/3446776</span>.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Background and Definitions</h2>\n\n<section id=\"A1.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Derivation of <a href=\"#S2.E2\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a>\n</h3>\n\n<div id=\"A1.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The following is based on <cite class=\"ltx_cite ltx_citemacro_citet\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> and included here for completeness.\nMore information on the properties of this solution and different forms of it are provided in their work.</p>\n</div>\n<div id=\"A1.SS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We start with the closed form solution of <a href=\"#S2.E1\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nRecall that <math id=\"A1.SS1.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{t},{\\bm{P}}_{t}\" display=\"inline\"><semantics><mrow><msub><mi>𝚷</mi><mi>t</mi></msub><mo>,</mo><msub><mi>𝑷</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{t},{\\bm{P}}_{t}</annotation></semantics></math> are orthogonal projections onto the spans of row spaces and null space of <math id=\"A1.SS1.p2.m2\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}</annotation></semantics></math> respectively.\nEvery solution to the equation\n<math id=\"A1.SS1.p2.m3\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}{\\bm{w}}={\\bm{y}}_{t}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒘</mi></mrow><mo>=</mo><msub><mi>𝒚</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}{\\bm{w}}={\\bm{y}}_{t}</annotation></semantics></math> can be written as\n<math id=\"A1.SS1.p2.m4\" class=\"ltx_Math\" alttext=\"{\\bm{w}}={\\bm{\\Pi}}_{t}{\\bm{w}}^{*}+{\\bm{P}}_{t}{\\bm{v}}\" display=\"inline\"><semantics><mrow><mi>𝒘</mi><mo>=</mo><mrow><mrow><msub><mi>𝚷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>+</mo><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒗</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}={\\bm{\\Pi}}_{t}{\\bm{w}}^{*}+{\\bm{P}}_{t}{\\bm{v}}</annotation></semantics></math> for some vector <math id=\"A1.SS1.p2.m5\" class=\"ltx_Math\" alttext=\"{\\bm{v}}\" display=\"inline\"><semantics><mi>𝒗</mi><annotation encoding=\"application/x-tex\">{\\bm{v}}</annotation></semantics></math>. It is then easy to see that <math id=\"A1.SS1.p2.m6\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{t}w_{t-1}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{t}w_{t-1}</annotation></semantics></math> would minimize <math id=\"A1.SS1.p2.m7\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}-{\\bm{w}}_{t-1}}}}\\right\\|_{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mi>𝒘</mi><mo>−</mo><msub><mi>𝒘</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><msub><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}-{\\bm{w}}_{t-1}}}}\\right\\|_{2}</annotation></semantics></math> and is in the null space of <math id=\"A1.SS1.p2.m8\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}</annotation></semantics></math>, so the closed form solution would be <math id=\"A1.SS1.p2.m9\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{t}={\\bm{\\Pi}}_{t}{\\bm{w}}^{*}+{\\bm{P}}_{t}{\\bm{w}}_{t-1}\" display=\"inline\"><semantics><mrow><msub><mi>𝒘</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><msub><mi>𝚷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>+</mo><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒘</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}_{t}={\\bm{\\Pi}}_{t}{\\bm{w}}^{*}+{\\bm{P}}_{t}{\\bm{w}}_{t-1}</annotation></semantics></math>.\nSubtracting <math id=\"A1.SS1.p2.m10\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> from both sides would give <a href=\"#S2.E2\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n</div>\n</section>\n<section id=\"A1.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Derivation of <a href=\"#S2.E4\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>\n</h3>\n\n<div id=\"A1.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">This derivation is also in given in <cite class=\"ltx_cite ltx_citemacro_citet\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> and is included here for completeness.\nUsing <a href=\"#S2.Thmtheorem2\" title=\"Assumption 2.2. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, we can write each <math id=\"A1.SS2.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{y}}_{t}={\\bm{X}}_{t}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝒚</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{y}}_{t}={\\bm{X}}_{t}{\\bm{w}}^{*}</annotation></semantics></math> and the forgetting as</p>\n<table id=\"A4.EGx7\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A1.E8\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A1.E8.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle F_{S}({\\bm{w}}_{T})=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><msub><mi>F</mi><mi>S</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle F_{S}({\\bm{w}}_{T})=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(8)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A1.SS2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Applying <a href=\"#S2.E2\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a> repeatedly, the parameter error vector after task <math id=\"A1.SS2.p2.m1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> would be</p>\n<table id=\"A4.EGx8\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A1.E9\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A1.E9.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{w}}_{T}-{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{w}}_{T}-{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A1.E9.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{P}}_{T}({\\bm{w}}_{T-1}-{\\bm{w}}^{*})={\\bm{P}}_{T}\\dots\\bm{P}_{1}({\\bm{w}}_{0}-{\\bm{w}}^{*})=-{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}.\" display=\"inline\"><semantics><mrow><mrow><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝒘</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝒘</mi><mn>0</mn></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{P}}_{T}({\\bm{w}}_{T-1}-{\\bm{w}}^{*})={\\bm{P}}_{T}\\dots\\bm{P}_{1}({\\bm{w}}_{0}-{\\bm{w}}^{*})=-{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(9)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A1.SS2.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Plugging the equation above into each term in forgetting, we have</p>\n<table id=\"A4.EGx9\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A1.E10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A1.E10.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle F_{S}({\\bm{w}}_{T})=\" display=\"inline\"><semantics><mrow><mrow><msub><mi>F</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle F_{S}({\\bm{w}}_{T})=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A1.E10.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(10)</span></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"A1.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>The Angle between Null Spaces and Forgetting</h3>\n\n<div id=\"A1.SS3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Principal angles between two subspaces are a generalization of angles between two vectors.\nWe start with the following simple two task example. Let <math id=\"A1.SS3.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> be an arbitrary unit vector and consider two tasks whose null spaces are <math id=\"A1.SS3.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1}={\\bm{a}}_{1}{\\bm{a}}_{1}^{\\top}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo>=</mo><mrow><msub><mi>𝒂</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒂</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1}={\\bm{a}}_{1}{\\bm{a}}_{1}^{\\top}</annotation></semantics></math> and <math id=\"A1.SS3.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{2}={\\bm{a}}_{2}{\\bm{a}}_{2}^{\\top}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>𝒂</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒂</mi><mn>2</mn><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{2}={\\bm{a}}_{2}{\\bm{a}}_{2}^{\\top}</annotation></semantics></math>, where <math id=\"A1.SS3.p1.m4\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1},{\\bm{a}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒂</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒂</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1},{\\bm{a}}_{2}</annotation></semantics></math> are unit vectors such that <math id=\"A1.SS3.p1.m5\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{a}}_{1}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}&gt;0\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msubsup><mi>𝒂</mi><mn>1</mn><mo>⊤</mo></msubsup><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn></msub><mo lspace=\"0.0835em\">&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{a}}_{1}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}&gt;0</annotation></semantics></math>.\nIn this two task case, the expected forgetting, given in <a href=\"#S3.E7\" title=\"In Proposition 3.4. ‣ 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>, is proportional to <math id=\"A1.SS3.p1.m6\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}</annotation></semantics></math>.</p>\n</div>\n<div id=\"A1.SS3.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Note that <math id=\"A1.SS3.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1}{\\bm{w}}^{*}={\\bm{a}}_{1}{\\bm{a}}_{1}^{\\top}{\\bm{w}}^{*}=c_{1}{\\bm{a}}_{1}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><msub><mi>𝒂</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒂</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒂</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1}{\\bm{w}}^{*}={\\bm{a}}_{1}{\\bm{a}}_{1}^{\\top}{\\bm{w}}^{*}=c_{1}{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.SS3.p2.m2\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=c_{1}{\\bm{a}}_{2}{\\bm{a}}_{2}^{\\top}{\\bm{a}}_{1}=c_{2}{\\bm{a}}_{2}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒂</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒂</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒂</mi><mn>1</mn></msub></mrow><mo>=</mo><mrow><msub><mi>c</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒂</mi><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=c_{1}{\\bm{a}}_{2}{\\bm{a}}_{2}^{\\top}{\\bm{a}}_{1}=c_{2}{\\bm{a}}_{2}</annotation></semantics></math> with <math id=\"A1.SS3.p2.m3\" class=\"ltx_Math\" alttext=\"c_{1}={\\bm{a}}_{1}^{\\top}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>=</mo><mrow><msubsup><mi>𝒂</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">c_{1}={\\bm{a}}_{1}^{\\top}{\\bm{w}}^{*}</annotation></semantics></math>, and <math id=\"A1.SS3.p2.m4\" class=\"ltx_Math\" alttext=\"c_{2}=c_{1}{\\bm{a}}_{2}^{\\top}{\\bm{a}}_{1}\" display=\"inline\"><semantics><mrow><msub><mi>c</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>c</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒂</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒂</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">c_{2}=c_{1}{\\bm{a}}_{2}^{\\top}{\\bm{a}}_{1}</annotation></semantics></math>.\n<a href=\"#A1.F5\" title=\"In A.3 The Angle between Null Spaces and Forgetting ‣ Appendix A Additional Background and Definitions ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a> in <a href=\"#A1.SS3\" title=\"A.3 The Angle between Null Spaces and Forgetting ‣ Appendix A Additional Background and Definitions ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">A.3</span></a> shows how the norm of <math id=\"A1.SS3.p2.m5\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝚷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math> depends on the angle between <math id=\"A1.SS3.p2.m6\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.SS3.p2.m7\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math>. When the angle between <math id=\"A1.SS3.p2.m8\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.SS3.p2.m9\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math> is small, as in the left display, forgetting will be small.\nIn fact if the angle was zero, then forgetting would be zero. At the other extreme is when the tasks are almost orthogonal, as shown in the right display, which would also result in less forgetting.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> show that the maximum forgetting (over the choice of <math id=\"A1.SS3.p2.m10\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> and <math id=\"A1.SS3.p2.m11\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math>) in this two task setting is proportional to <math id=\"A1.SS3.p2.m12\" class=\"ltx_math_unparsed\" alttext=\"({\\bm{a}}_{1}^{\\top}{\\bm{a}}_{2})^{2}\\mathopen{}\\mathclose{{\\left(1-({\\bm{a}}_{1}^{\\top}{\\bm{a}}_{2})^{2}}}\\right)\" display=\"inline\"><semantics><mrow><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>𝒂</mi><mn>1</mn><mo>⊤</mo></msubsup><msub><mi>𝒂</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>𝒂</mi><mn>1</mn><mo>⊤</mo></msubsup><msub><mi>𝒂</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{a}}_{1}^{\\top}{\\bm{a}}_{2})^{2}\\mathopen{}\\mathclose{{\\left(1-({\\bm{a}}_{1}^{\\top}{\\bm{a}}_{2})^{2}}}\\right)</annotation></semantics></math> which is maximized when the angle between <math id=\"A1.SS3.p2.m13\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.SS3.p2.m14\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math> is <math id=\"A1.SS3.p2.m15\" class=\"ltx_Math\" alttext=\"\\pi/4\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/4</annotation></semantics></math>.</p>\n</div>\n<figure id=\"A1.F5\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"./assets/sections/sections/images/angles_1_cr.png\" id=\"A1.F5.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" width=\"195\" height=\"173\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"./assets/sections/sections/images/angles_2_cr.png\" id=\"A1.F5.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" width=\"195\" height=\"176\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"./assets/sections/sections/images/angles_3_cr.png\" id=\"A1.F5.g3\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" width=\"195\" height=\"175\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>A simple example to demonstrate how the angle between tasks with one dimensional null spaces affects forgetting. <math id=\"A1.F5.m15\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1},{\\bm{a}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒂</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒂</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1},{\\bm{a}}_{2}</annotation></semantics></math> span the null spaces of the first and second tasks respectively.\nThe two vectors that are outside <math id=\"A1.F5.m16\" class=\"ltx_Math\" alttext=\"\\mathrm{span}({\\bm{\\Pi}}_{1})\" display=\"inline\"><semantics><mrow><mi>span</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝚷</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{span}({\\bm{\\Pi}}_{1})</annotation></semantics></math> show the projections <math id=\"A1.F5.m17\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math> and <math id=\"A1.F5.m18\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math> which will be in the directions of <math id=\"A1.F5.m19\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.F5.m20\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math> as marked.\nEach display shows the effect of the angle between the null spaces, which is the angle between <math id=\"A1.F5.m21\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.F5.m22\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math>, on forgetting, which would be <math id=\"A1.F5.m23\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msub><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}</annotation></semantics></math>. As the angle between <math id=\"A1.F5.m24\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.F5.m25\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math> increases (from the left to right display), the forgetting first increases and then decreases. Forgetting is maximized when the angle between <math id=\"A1.F5.m26\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{1}</annotation></semantics></math> and <math id=\"A1.F5.m27\" class=\"ltx_Math\" alttext=\"{\\bm{a}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒂</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{a}}_{2}</annotation></semantics></math> is <math id=\"A1.F5.m28\" class=\"ltx_Math\" alttext=\"\\pi/4\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/4</annotation></semantics></math>.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Intuition for the proofs</h2>\n\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Intuition for the worst case result</h3>\n\n<figure id=\"A2.F6\" class=\"ltx_figure\">\n\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>Replay can transfer error between samples. The first task consists of two samples <math id=\"A2.F6.m21\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> and <math id=\"A2.F6.m22\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, while the second task has one sample <math id=\"A2.F6.m23\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>. All of the plots display the three samples <math id=\"A2.F6.m24\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1},{\\bm{x}}_{2},{\\bm{x}}_{3}\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1},{\\bm{x}}_{2},{\\bm{x}}_{3}</annotation></semantics></math>, the target parameter vector <math id=\"A2.F6.m25\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math>, and the iterates without replay <math id=\"A2.F6.m26\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{1}</annotation></semantics></math> and <math id=\"A2.F6.m27\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> from different angles. The vector <math id=\"A2.F6.m28\" class=\"ltx_Math\" alttext=\"v_{2}\" display=\"inline\"><semantics><msub><mi>v</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">v_{2}</annotation></semantics></math>, which is orthogonal to the samples <math id=\"A2.F6.m29\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> and <math id=\"A2.F6.m30\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>, is also displayed to show the orientation of the plots. The left figure in each row shows the general position of vectors of interest. Rest of the figures in the first row focus on the error of the final iterate <math id=\"A2.F6.m31\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> on the first task’s samples without replay.\nThe second row, additionally, shows the final iterate <math id=\"A2.F6.m32\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> after replay of <math id=\"A2.F6.m33\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, and the projection of <math id=\"A2.F6.m34\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> onto the first task’s samples. In each case, intersection of the dashed line originating from parameter vectors <math id=\"A2.F6.m35\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*},{\\bm{w}}_{1},\\dots\" display=\"inline\"><semantics><mrow><msup><mi>𝒘</mi><mo>∗</mo></msup><mo>,</mo><msub><mi>𝒘</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*},{\\bm{w}}_{1},\\dots</annotation></semantics></math> with the sample <math id=\"A2.F6.m36\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> or <math id=\"A2.F6.m37\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> shows the projection of the parameter vector onto that sample.\nThe error of each iterate <math id=\"A2.F6.m38\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> and <math id=\"A2.F6.m39\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> along a sample is marked in red by the discrepancy between its’ projection and the projection of <math id=\"A2.F6.m40\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> onto the sample. </figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"A2.F6.sf1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"./assets/x7.png\" id=\"A2.F6.sf1.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"270\" height=\"279\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"./assets/x8.png\" id=\"A2.F6.sf1.g2\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"305\" height=\"293\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"./assets/x9.png\" id=\"A2.F6.sf1.g3\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"310\" height=\"296\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(a) </span>Without replay: The first iterate <math id=\"A2.F6.sf1.m24\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{1}</annotation></semantics></math> is in the span of <math id=\"A2.F6.sf1.m25\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> and <math id=\"A2.F6.sf1.m26\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>. The final iterate <math id=\"A2.F6.sf1.m27\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> is obtained by training on <math id=\"A2.F6.sf1.m28\" class=\"ltx_Math\" alttext=\"({\\bm{x}}_{3},{\\bm{x}}_{3}^{\\top}{\\bm{w}}^{*})\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒙</mi><mn>3</mn></msub><mo>,</mo><mrow><msubsup><mi>𝒙</mi><mn>3</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{3},{\\bm{x}}_{3}^{\\top}{\\bm{w}}^{*})</annotation></semantics></math> starting from <math id=\"A2.F6.sf1.m29\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{1}</annotation></semantics></math>. The change from <math id=\"A2.F6.sf1.m30\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{1}</annotation></semantics></math> to <math id=\"A2.F6.sf1.m31\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> is in the direction of <math id=\"A2.F6.sf1.m32\" class=\"ltx_Math\" alttext=\"-{\\bm{x}}_{3}\" display=\"inline\"><semantics><mrow><mo>−</mo><msub><mi>𝒙</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">-{\\bm{x}}_{3}</annotation></semantics></math>, and since <math id=\"A2.F6.sf1.m33\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math> is orthogonal to <math id=\"A2.F6.sf1.m34\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>, <math id=\"A2.F6.sf1.m35\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> has no error along <math id=\"A2.F6.sf1.m36\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>. We can see this in the center figure where projections of <math id=\"A2.F6.sf1.m37\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> and <math id=\"A2.F6.sf1.m38\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> onto <math id=\"A2.F6.sf1.m39\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> coincide.\nAs we can see in the right figure, <math id=\"A2.F6.sf1.m40\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> will have some error along <math id=\"A2.F6.sf1.m41\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, since <math id=\"A2.F6.sf1.m42\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math> is not orthogonal to <math id=\"A2.F6.sf1.m43\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>. The discrepancy in the projections of <math id=\"A2.F6.sf1.m44\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> and <math id=\"A2.F6.sf1.m45\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> onto <math id=\"A2.F6.sf1.m46\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> is shown in red. </figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"A2.F6.sf2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"./assets/x10.png\" id=\"A2.F6.sf2.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"287\" height=\"288\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"./assets/x11.png\" id=\"A2.F6.sf2.g2\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"287\" height=\"269\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"./assets/x12.png\" id=\"A2.F6.sf2.g3\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"288\" height=\"288\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(b) </span>With replay: Since <math id=\"A2.F6.sf2.m15\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math> had error on <math id=\"A2.F6.sf2.m16\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, we consider replaying <math id=\"A2.F6.sf2.m17\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>. The first display shows the iterate after replay of <math id=\"A2.F6.sf2.m18\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, <math id=\"A2.F6.sf2.m19\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> relative to the other iterates and examples. Since <math id=\"A2.F6.sf2.m20\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> changes in the direction of <math id=\"A2.F6.sf2.m21\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> in addition to changing in the direction of <math id=\"A2.F6.sf2.m22\" class=\"ltx_Math\" alttext=\"-{\\bm{x}}_{3}\" display=\"inline\"><semantics><mrow><mo>−</mo><msub><mi>𝒙</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">-{\\bm{x}}_{3}</annotation></semantics></math>, it also moves in the direction of <math id=\"A2.F6.sf2.m23\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>. This causes it to incur some error in the direction of <math id=\"A2.F6.sf2.m24\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> as seen in the center figure. The right figure shows that in contrast to <math id=\"A2.F6.sf2.m25\" class=\"ltx_Math\" alttext=\"{\\bm{w}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒘</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{w}}_{2}</annotation></semantics></math>, <math id=\"A2.F6.sf2.m26\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> does not have any error along <math id=\"A2.F6.sf2.m27\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, since <math id=\"A2.F6.sf2.m28\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> was just replayed.\n</figcaption>\n</figure>\n</div>\n</div>\n</figure>\n</section>\n<section id=\"A2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Intuition for the lower dimensional average case result</h3>\n\n<div id=\"A2.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">For this intuition, we focus on the lower dimensional case (<math id=\"A2.SS2.p1.m1\" class=\"ltx_Math\" alttext=\"d=3\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">d=3</annotation></semantics></math>).\nFix an orthonormal basis <math id=\"A2.SS2.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}</annotation></semantics></math> of <math id=\"A2.SS2.p1.m3\" class=\"ltx_Math\" alttext=\"\\mathbb{R}^{3}\" display=\"inline\"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{3}</annotation></semantics></math> and consider two tasks in <math id=\"A2.SS2.p1.m4\" class=\"ltx_Math\" alttext=\"\\mathbb{R}^{3}\" display=\"inline\"><semantics><msup><mi>ℝ</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{3}</annotation></semantics></math>, where the first and second tasks’ null spaces has rank one and two respectively.\nLet the unit vector <math id=\"A2.SS2.p1.m5\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> span the first task’s null space. <math id=\"A2.SS2.p1.m6\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> is chosen such that it is in the span of <math id=\"A2.SS2.p1.m7\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{2},{\\bm{v}}_{3}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{2},{\\bm{v}}_{3}}}\\right\\}</annotation></semantics></math>, and is very close to <math id=\"A2.SS2.p1.m8\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>. The second task’s null space is spanned by <math id=\"A2.SS2.p1.m9\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{v}}_{2}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{v}}_{2}}}\\right\\}</annotation></semantics></math>.\nSee <a href=\"#A2.F7.sf1\" title=\"In Figure 7 ‣ B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7(a)</span></a> for an illustration of the null spaces.\nReplaying a sample in this setting would reduce the rank of the second task’s null space.\nBut this doesn’t necessarily mean that the forgetting will be smaller. It is known that forgetting depends on the angle between the task null spaces in a non-monotonic way; see <a href=\"#A1.SS3\" title=\"A.3 The Angle between Null Spaces and Forgetting ‣ Appendix A Additional Background and Definitions ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">A.3</span></a> for more details. Initially, this angle is the angle between <math id=\"A2.SS2.p1.m10\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> and <math id=\"A2.SS2.p1.m11\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>, which is very small.\nAfter replay, the second task’s null space will be reduced to a one dimensional null space, spanned by <math id=\"A2.SS2.p1.m12\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{p}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒑</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{p}}}_{2}</annotation></semantics></math>.\nDisplays (b) and (c) in <a href=\"#A2.F7\" title=\"In B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a> show some possible scenarios for <math id=\"A2.SS2.p1.m13\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{p}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒑</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{p}}}_{2}</annotation></semantics></math> where forgetting would increase, since the angle between <math id=\"A2.SS2.p1.m14\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> and <math id=\"A2.SS2.p1.m15\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{p}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒑</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{p}}}_{2}</annotation></semantics></math> would be slightly larger than the angle between <math id=\"A2.SS2.p1.m16\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> and <math id=\"A2.SS2.p1.m17\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>, but not much larger.\n<a href=\"#A2.F7.sf4\" title=\"In Figure 7 ‣ B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7(d)</span></a> on the other hand shows a scenario where forgetting would decrease with replay since the angle between <math id=\"A2.SS2.p1.m18\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{p}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒑</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{p}}}_{2}</annotation></semantics></math> and <math id=\"A2.SS2.p1.m19\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> is much larger than the angle between <math id=\"A2.SS2.p1.m20\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> and <math id=\"A2.SS2.p1.m21\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1}</annotation></semantics></math>.\nThe construction in <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a> is such that replay of most samples would result in cases like (b) and (c) in <a href=\"#A2.F7\" title=\"In B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n</div>\n<figure id=\"A2.F7\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_3\">\n<figure id=\"A2.F7.sf1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/sections/sections/images/avg_case_subspaces.png\" id=\"A2.F7.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"415\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(a) </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\">\n<figure id=\"A2.F7.sf2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/sections/sections/images/avg_case_replay1.png\" id=\"A2.F7.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"374\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(b) </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\">\n<figure id=\"A2.F7.sf3\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/sections/sections/images/avg_case_replay2.png\" id=\"A2.F7.sf3.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"428\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(c) </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"A2.F7.sf4\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/sections/sections/images/avg_case_replay3.png\" id=\"A2.F7.sf4.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"432\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">(d) </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span> An illustration of forgetting in the average case construction. The red plane in (a) shows the null space of task <math id=\"A2.F7.m11\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>.\nThe null space of the first task, spanned by <math id=\"A2.F7.m12\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math>, is in the span of <math id=\"A2.F7.m13\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{2},{\\bm{v}}_{3}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{2},{\\bm{v}}_{3}}}\\right\\}</annotation></semantics></math> and very close to <math id=\"A2.F7.m14\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>. Without replay, the angle between <math id=\"A2.F7.m15\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math> and <math id=\"A2.F7.m16\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> determines forgetting , while with replay of one sample, the angle between <math id=\"A2.F7.m17\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{p}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒑</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{p}}}_{2}</annotation></semantics></math> and <math id=\"A2.F7.m18\" class=\"ltx_Math\" alttext=\"{\\bm{p}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒑</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{p}}_{1}</annotation></semantics></math> determines forgetting. In displays <a href=\"#A2.F7.sf2\" title=\"Figure 7(b) ‣ Figure 7 ‣ B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a> and <a href=\"#A2.F7.sf3\" title=\"Figure 7(c) ‣ Figure 7 ‣ B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7(c)</span></a> where <math id=\"A2.F7.m19\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{p}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝒑</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{p}}}_{2}</annotation></semantics></math> is not too far from <math id=\"A2.F7.m20\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>, forgetting would increase with replay, while in <a href=\"#A2.F7.sf4\" title=\"Figure 7(d) ‣ Figure 7 ‣ B.2 Intuition for the lower dimensional average case result ‣ Appendix B Intuition for the proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7(d)</span></a>, it would decrease.\n</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Proofs</h2>\n\n<section id=\"A3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.1 </span>Proof of worst case results</h3>\n\n<div id=\"A3.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#S3.Thmtheorem2\" title=\"Theorem 3.2 (Worst case replay). ‣ 3.1.1 Worst case result ‣ 3.1 Worst Case: From Vanishing to Catastrophic Forgetting via Replay Sample Selection ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">3.2</span></a></span>\n  \nWe construct a sequence of tasks where all the examples have unit norm.\nFix an arbitrary orthonormal basis <math id=\"A3.SS1.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d}</annotation></semantics></math> for <math id=\"A3.SS1.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathbb{R}^{d}\" display=\"inline\"><semantics><msup><mi>ℝ</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{d}</annotation></semantics></math> and consider the subspaces spanned by <math id=\"A3.SS1.p1.m3\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}}}\\right\\}</annotation></semantics></math> and <math id=\"A3.SS1.p1.m4\" class=\"ltx_math_unparsed\" alttext=\"{\\mathbb{W}}=\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{4},\\dots,{\\bm{v}}_{d}}}\\right\\}\" display=\"inline\"><semantics><mrow><mi>𝕎</mi><mo>=</mo><mi>span</mi><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>4</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">{\\mathbb{W}}=\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{4},\\dots,{\\bm{v}}_{d}}}\\right\\}</annotation></semantics></math>.\nLet</p>\n<table id=\"A4.EGx10\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E11\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E11.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{x}}_{1}=\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{x}}_{1}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E11.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{v}}_{1},\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{v}}_{1},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(11)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E12\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E12.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{x}}_{2}=\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>2</mn></msub><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{x}}_{2}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E12.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2\\sqrt{2}}{\\bm{v}}_{1}+\\frac{1}{2\\sqrt{2}}{\\bm{v}}_{2}+\\frac{\\sqrt{3}}{2}{\\bm{v}}_{3},\" display=\"inline\"><semantics><mrow><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msqrt><mn>3</mn></msqrt><mn>2</mn></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{2\\sqrt{2}}{\\bm{v}}_{1}+\\frac{1}{2\\sqrt{2}}{\\bm{v}}_{2}+\\frac{\\sqrt{3}}{2}{\\bm{v}}_{3},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(12)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E13\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E13.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{x}}_{3}=\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>3</mn></msub><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{x}}_{3}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E13.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{v}}_{3},\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>3</mn></msub><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{v}}_{3},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(13)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">For each task <math id=\"A3.SS1.p2.m1\" class=\"ltx_Math\" alttext=\"1\\leq t\\leq T-1\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>≤</mo><mi>t</mi><mo>≤</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">1\\leq t\\leq T-1</annotation></semantics></math>, let <math id=\"A3.SS1.p2.m2\" class=\"ltx_Math\" alttext=\"{\\bm{X}}^{\\prime}_{t}\" display=\"inline\"><semantics><msubsup><mi>𝑿</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{X}}^{\\prime}_{t}</annotation></semantics></math> be a matrix containing a set of samples that are in the span of <math id=\"A3.SS1.p2.m3\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>. That is, span of each <math id=\"A3.SS1.p2.m4\" class=\"ltx_Math\" alttext=\"{\\bm{X}}^{\\prime}_{t}\" display=\"inline\"><semantics><msubsup><mi>𝑿</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{X}}^{\\prime}_{t}</annotation></semantics></math> is a subset of <math id=\"A3.SS1.p2.m5\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>. The number of samples in each <math id=\"A3.SS1.p2.m6\" class=\"ltx_Math\" alttext=\"{\\bm{X}}^{\\prime}_{t}\" display=\"inline\"><semantics><msubsup><mi>𝑿</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{X}}^{\\prime}_{t}</annotation></semantics></math> does not affect our construction.</p>\n</div>\n<div id=\"A3.SS1.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">For the first <math id=\"A3.SS1.p3.m1\" class=\"ltx_Math\" alttext=\"T-2\" display=\"inline\"><semantics><mrow><mi>T</mi><mo>−</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">T-2</annotation></semantics></math> tasks, set</p>\n</div>\n<div id=\"A3.SS1.p4\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx11\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E14\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E14.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{X}}_{t}=\\begin{bmatrix}{\\bm{x}}_{1}^{\\top}\\\\\n{\\bm{X}}^{\\prime}_{t}\\end{bmatrix}.\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑿</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝑿</mi><mi>t</mi><mo>′</mo></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{X}}_{t}=\\begin{bmatrix}{\\bm{x}}_{1}^{\\top}\\\\\n{\\bm{X}}^{\\prime}_{t}\\end{bmatrix}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(14)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">For the last two tasks we have</p>\n<table id=\"A4.EGx12\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E15\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E15.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{X}}_{T-1}=\\begin{bmatrix}{\\bm{x}}_{1}^{\\top}\\\\\n{\\bm{x}}_{2}^{\\top}\\\\\n{\\bm{X}}^{\\prime}_{T-1}\\end{bmatrix},\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑿</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝒙</mi><mn>2</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝑿</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo>′</mo></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{X}}_{T-1}=\\begin{bmatrix}{\\bm{x}}_{1}^{\\top}\\\\\n{\\bm{x}}_{2}^{\\top}\\\\\n{\\bm{X}}^{\\prime}_{T-1}\\end{bmatrix},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(15)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS1.p5\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">and finally for the last task</p>\n<table id=\"A4.EGx13\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E16\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E16.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{X}}_{T}=\\begin{bmatrix}{\\bm{x}}_{3}^{\\top}\\\\\n{{\\bm{w}}^{\\prime}_{1}}^{\\top}\\\\\n\\vdots\\\\\n{{\\bm{w}}^{\\prime}_{d-3}}^{\\top}\\end{bmatrix},\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑿</mi><mi>T</mi></msub><mo>=</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒙</mi><mn>3</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><mmultiscripts><mi>𝒘</mi><mn>1</mn><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mtd></mtr><mtr><mtd><mi mathvariant=\"normal\">⋮</mi></mtd></mtr><mtr><mtd><mmultiscripts><mi>𝒘</mi><mrow><mi>d</mi><mo>−</mo><mn>3</mn></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{X}}_{T}=\\begin{bmatrix}{\\bm{x}}_{3}^{\\top}\\\\\n{{\\bm{w}}^{\\prime}_{1}}^{\\top}\\\\\n\\vdots\\\\\n{{\\bm{w}}^{\\prime}_{d-3}}^{\\top}\\end{bmatrix},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(16)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"A3.SS1.p5.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{\\prime}_{1},\\dots\\bm{w}^{\\prime}_{d-3}\" display=\"inline\"><semantics><mrow><msubsup><mi>𝒘</mi><mn>1</mn><mo>′</mo></msubsup><mo>,</mo><mrow><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒘</mi><mrow><mi>d</mi><mo>−</mo><mn>3</mn></mrow><mo>′</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}^{\\prime}_{1},\\dots\\bm{w}^{\\prime}_{d-3}</annotation></semantics></math> span <math id=\"A3.SS1.p5.m2\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>.\nLet <math id=\"A3.SS1.p5.m3\" class=\"ltx_Math\" alttext=\"{\\bm{u}}=\\sqrt{\\frac{6}{7}}{\\bm{v}}_{2}-\\frac{1}{\\sqrt{7}}{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><mi>𝒖</mi><mo>=</mo><mrow><mrow><msqrt><mfrac><mn>6</mn><mn>7</mn></mfrac></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mfrac><mn>1</mn><msqrt><mn>7</mn></msqrt></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}=\\sqrt{\\frac{6}{7}}{\\bm{v}}_{2}-\\frac{1}{\\sqrt{7}}{\\bm{v}}_{3}</annotation></semantics></math> be a vector that is orthogonal to <math id=\"A3.SS1.p5.m4\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> and <math id=\"A3.SS1.p5.m5\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>.\nWe pick <math id=\"A3.SS1.p5.m6\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math> such that <math id=\"A3.SS1.p5.m7\" class=\"ltx_Math\" alttext=\"a\\vcentcolon={\\bm{u}}^{\\top}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mi>a</mi><mo>:=</mo><mrow><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">a\\vcentcolon={\\bm{u}}^{\\top}{\\bm{w}}^{*}</annotation></semantics></math> is bounded away from zero.\nTo compute forgetting without replay, we first compute\n<math id=\"A3.SS1.p5.m8\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}1{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝑷</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}1{\\bm{w}}^{*}</annotation></semantics></math>. For every <math id=\"A3.SS1.p5.m9\" class=\"ltx_Math\" alttext=\"t&lt;T-1\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">t&lt;T-1</annotation></semantics></math>, we decompose the null space <math id=\"A3.SS1.p5.m10\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{t}={\\bm{P}}^{\\prime}_{t}+\\bar{{\\bm{P}}}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo>=</mo><mrow><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup><mo>+</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{t}={\\bm{P}}^{\\prime}_{t}+\\bar{{\\bm{P}}}</annotation></semantics></math>, where <math id=\"A3.SS1.p5.m11\" class=\"ltx_Math\" alttext=\"\\bar{{\\bm{P}}}\" display=\"inline\"><semantics><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{{\\bm{P}}}</annotation></semantics></math> is a projection into the span of <math id=\"A3.SS1.p5.m12\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2},{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2},{\\bm{v}}_{3}</annotation></semantics></math> (which are orthogonal to <math id=\"A3.SS1.p5.m13\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math>) and <math id=\"A3.SS1.p5.m14\" class=\"ltx_Math\" alttext=\"{\\bm{P}}^{\\prime}_{t}\" display=\"inline\"><semantics><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{P}}^{\\prime}_{t}</annotation></semantics></math> gives a projection of the null space <math id=\"A3.SS1.p5.m15\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑷</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{P}}_{t}</annotation></semantics></math> into <math id=\"A3.SS1.p5.m16\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>. Note that <math id=\"A3.SS1.p5.m17\" class=\"ltx_Math\" alttext=\"{\\bm{P}}^{\\prime}_{t}\" display=\"inline\"><semantics><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup><annotation encoding=\"application/x-tex\">{\\bm{P}}^{\\prime}_{t}</annotation></semantics></math> and <math id=\"A3.SS1.p5.m18\" class=\"ltx_Math\" alttext=\"\\bar{{\\bm{P}}}\" display=\"inline\"><semantics><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{{\\bm{P}}}</annotation></semantics></math> are projections into orthogonal subspaces, hence <math id=\"A3.SS1.p5.m19\" class=\"ltx_Math\" alttext=\"{\\bm{P}}^{\\prime}_{t}\\bar{{\\bm{P}}}=\\bar{{\\bm{P}}}{\\bm{P}}^{\\prime}_{t}=0\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover></mrow><mo>=</mo><mrow><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}^{\\prime}_{t}\\bar{{\\bm{P}}}=\\bar{{\\bm{P}}}{\\bm{P}}^{\\prime}_{t}=0</annotation></semantics></math>.\nFor <math id=\"A3.SS1.p5.m20\" class=\"ltx_Math\" alttext=\"t&lt;T-2\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mrow><mi>T</mi><mo>−</mo><mn>2</mn></mrow></mrow><annotation encoding=\"application/x-tex\">t&lt;T-2</annotation></semantics></math>, we can write <math id=\"A3.SS1.p5.m21\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{t+1}{\\bm{P}}_{t}=({\\bm{P}}^{\\prime}_{t+1}+\\bar{{\\bm{P}}})({\\bm{P}}^{\\prime}_{t}+\\bar{{\\bm{P}}})={\\bm{P}}^{\\prime}_{t+1}{\\bm{P}}^{\\prime}_{t}+\\bar{{\\bm{P}}}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>t</mi></msub></mrow><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>𝑷</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>′</mo></msubsup><mo>+</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup><mo>+</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msubsup><mi>𝑷</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mi>t</mi><mo>′</mo></msubsup></mrow><mo>+</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{t+1}{\\bm{P}}_{t}=({\\bm{P}}^{\\prime}_{t+1}+\\bar{{\\bm{P}}})({\\bm{P}}^{\\prime}_{t}+\\bar{{\\bm{P}}})={\\bm{P}}^{\\prime}_{t+1}{\\bm{P}}^{\\prime}_{t}+\\bar{{\\bm{P}}}</annotation></semantics></math>. Then we have</p>\n<table id=\"A4.EGx14\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E17\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E17.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{P}}_{T-2}\\dots\\bm{P}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}={\\bm{P}}^{\\prime}_{T-2}\\dots\\bm{P}^{\\prime}_{2}{\\bm{P}}^{\\prime}_{1}{\\bm{w}}^{*}+\\bar{{\\bm{P}}}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>2</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><mrow><msubsup><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>2</mn></mrow><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mn>2</mn><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mn>1</mn><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>+</mo><mrow><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{P}}_{T-2}\\dots\\bm{P}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}={\\bm{P}}^{\\prime}_{T-2}\\dots\\bm{P}^{\\prime}_{2}{\\bm{P}}^{\\prime}_{1}{\\bm{w}}^{*}+\\bar{{\\bm{P}}}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(17)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">For the second to last task, we have <math id=\"A3.SS1.p5.m22\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{T-1}={\\bm{u}}{\\bm{u}}^{\\top}+{\\bm{P}}^{\\prime}_{T-1}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup></mrow><mo>+</mo><msubsup><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo>′</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{T-1}={\\bm{u}}{\\bm{u}}^{\\top}+{\\bm{P}}^{\\prime}_{T-1}</annotation></semantics></math>, so <math id=\"A3.SS1.p5.m23\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}={\\bm{u}}{\\bm{u}}^{\\top}\\bar{{\\bm{P}}}{\\bm{w}}^{*}+{\\bm{P}}^{\\prime}_{T-1}\\dots\\bm{P}^{\\prime}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>+</mo><mrow><msubsup><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mn>1</mn><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}={\\bm{u}}{\\bm{u}}^{\\top}\\bar{{\\bm{P}}}{\\bm{w}}^{*}+{\\bm{P}}^{\\prime}_{T-1}\\dots\\bm{P}^{\\prime}_{1}{\\bm{w}}^{*}</annotation></semantics></math>. Since <math id=\"A3.SS1.p5.m24\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{\\prime}_{1},\\dots\\bm{w}^{\\prime}_{d-3}\" display=\"inline\"><semantics><mrow><msubsup><mi>𝒘</mi><mn>1</mn><mo>′</mo></msubsup><mo>,</mo><mrow><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒘</mi><mrow><mi>d</mi><mo>−</mo><mn>3</mn></mrow><mo>′</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}^{\\prime}_{1},\\dots\\bm{w}^{\\prime}_{d-3}</annotation></semantics></math> span <math id=\"A3.SS1.p5.m25\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>, the last task’s null space <math id=\"A3.SS1.p5.m26\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{T}\" display=\"inline\"><semantics><msub><mi>𝑷</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{P}}_{T}</annotation></semantics></math> is a projection into the subspace spanned by <math id=\"A3.SS1.p5.m27\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},{\\bm{v}}_{2}</annotation></semantics></math>, therefore, <math id=\"A3.SS1.p5.m28\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{T}{\\bm{P}}^{\\prime}_{T-2}\\dots\\bm{P}^{\\prime}_{2}{\\bm{P}}^{\\prime}_{1}{\\bm{w}}^{*}=0\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>2</mn></mrow><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mn>2</mn><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑷</mi><mn>1</mn><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{T}{\\bm{P}}^{\\prime}_{T-2}\\dots\\bm{P}^{\\prime}_{2}{\\bm{P}}^{\\prime}_{1}{\\bm{w}}^{*}=0</annotation></semantics></math> and consequently</p>\n<table id=\"A4.EGx15\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E18\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E18.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}={\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}\\bar{{\\bm{P}}}{\\bm{w}}^{*}.\" display=\"inline\"><semantics><mrow><mrow><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}={\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}\\bar{{\\bm{P}}}{\\bm{w}}^{*}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(18)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Since <math id=\"A3.SS1.p5.m29\" class=\"ltx_Math\" alttext=\"{\\bm{u}}\" display=\"inline\"><semantics><mi>𝒖</mi><annotation encoding=\"application/x-tex\">{\\bm{u}}</annotation></semantics></math> is fully in the span of <math id=\"A3.SS1.p5.m30\" class=\"ltx_Math\" alttext=\"\\bar{{\\bm{P}}}\" display=\"inline\"><semantics><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><annotation encoding=\"application/x-tex\">\\bar{{\\bm{P}}}</annotation></semantics></math>, <math id=\"A3.SS1.p5.m31\" class=\"ltx_Math\" alttext=\"{\\bm{u}}{\\bm{u}}^{\\top}\\bar{{\\bm{P}}}={\\bm{u}}{\\bm{u}}^{\\top}\" display=\"inline\"><semantics><mrow><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover></mrow><mo>=</mo><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}{\\bm{u}}^{\\top}\\bar{{\\bm{P}}}={\\bm{u}}{\\bm{u}}^{\\top}</annotation></semantics></math>.\nFinally, the forgetting is</p>\n<table id=\"A4.EGx16\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E19\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E19.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi>𝒖</mi><msup><mi>𝒖</mi><mo>⊤</mo></msup><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(19)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS1.p6\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We first compute forgetting on the samples <math id=\"A3.SS1.p6.m1\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1},{\\bm{x}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒙</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1},{\\bm{x}}_{2}</annotation></semantics></math> and <math id=\"A3.SS1.p6.m2\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math>.\nSince <math id=\"A3.SS1.p6.m3\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}</annotation></semantics></math> is included in the last task, <math id=\"A3.SS1.p6.m4\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=0\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒙</mi><mn>3</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=0</annotation></semantics></math>, and since <math id=\"A3.SS1.p6.m5\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}^{\\top}{\\bm{P}}_{T}={\\bm{x}}_{1}^{\\top}\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub></mrow><mo>=</mo><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}^{\\top}{\\bm{P}}_{T}={\\bm{x}}_{1}^{\\top}</annotation></semantics></math>, we have <math id=\"A3.SS1.p6.m6\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}={\\bm{x}}_{1}^{\\top}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=0\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}={\\bm{x}}_{1}^{\\top}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=0</annotation></semantics></math>. Now, for <math id=\"A3.SS1.p6.m7\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> we have</p>\n<table id=\"A4.EGx17\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E20\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E20.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{x}}_{2}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒙</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{x}}_{2}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E20.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\begin{bmatrix}\\frac{1}{2\\sqrt{2}}&amp;\\frac{1}{2\\sqrt{2}}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}\\end{bmatrix}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mrow><mo>[</mo><mtable columnspacing=\"5pt\"><mtr><mtd><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac></mtd><mtd><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\begin{bmatrix}\\frac{1}{2\\sqrt{2}}&amp;\\frac{1}{2\\sqrt{2}}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}\\end{bmatrix}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(20)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E21\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E21.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E21.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2\\sqrt{2}}{\\bm{v}}_{2}^{\\top}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=\\frac{\\sqrt{3}}{2\\sqrt{7}}\\cdot a.\" display=\"inline\"><semantics><mrow><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><mstyle displaystyle=\"true\"><mfrac><msqrt><mn>3</mn></msqrt><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mn>7</mn></msqrt></mrow></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>a</mi></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{2\\sqrt{2}}{\\bm{v}}_{2}^{\\top}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=\\frac{\\sqrt{3}}{2\\sqrt{7}}\\cdot a.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(21)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">The last equality follows from <math id=\"A3.SS1.p6.m8\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}^{\\top}{\\bm{u}}=\\sqrt{\\frac{6}{7}}\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi></mrow><mo>=</mo><msqrt><mfrac><mn>6</mn><mn>7</mn></mfrac></msqrt></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}^{\\top}{\\bm{u}}=\\sqrt{\\frac{6}{7}}</annotation></semantics></math> and definition of <math id=\"A3.SS1.p6.m9\" class=\"ltx_Math\" alttext=\"a\" display=\"inline\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math>.\nThe rest of the samples are in <math id=\"A3.SS1.p6.m10\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>, and since the samples in <math id=\"A3.SS1.p6.m11\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{T}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{T}</annotation></semantics></math> span <math id=\"A3.SS1.p6.m12\" class=\"ltx_Math\" alttext=\"{\\mathbb{W}}\" display=\"inline\"><semantics><mi>𝕎</mi><annotation encoding=\"application/x-tex\">{\\mathbb{W}}</annotation></semantics></math>, we have that for all <math id=\"A3.SS1.p6.m13\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, <math id=\"A3.SS1.p6.m14\" class=\"ltx_Math\" alttext=\"{\\bm{X}}^{\\prime}_{t}{\\bm{P}}_{T}=0\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝑿</mi><mi>t</mi><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{X}}^{\\prime}_{t}{\\bm{P}}_{T}=0</annotation></semantics></math>, that is, they make no contribution to forgetting. So the only sample that contributes to forgetting is <math id=\"A3.SS1.p6.m15\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math>, which only occurs in task <math id=\"A3.SS1.p6.m16\" class=\"ltx_Math\" alttext=\"T-1\" display=\"inline\"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">T-1</annotation></semantics></math>. Then plugging in <a href=\"#A3.E20\" title=\"In C.1 Proof of worst case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">20</span></a> we have</p>\n<table id=\"A4.EGx18\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E22\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E22.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi>𝒖</mi><msup><mi>𝒖</mi><mo>⊤</mo></msup><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E22.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\mathopen{}\\mathclose{{\\left({\\bm{x}}_{2}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}\\right)^{2}\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mrow><mi></mi><mrow><mo>(</mo><msubsup><mi>𝒙</mi><mn>2</mn><mo>⊤</mo></msubsup><msub><mi>𝑷</mi><mi>T</mi></msub><mi>𝒖</mi><msup><mi>𝒖</mi><mo>⊤</mo></msup><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo>)</mo><msup><mi></mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\mathopen{}\\mathclose{{\\left({\\bm{x}}_{2}^{\\top}{\\bm{P}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}\\right)^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(22)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E23\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E23.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E23.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{3}{28(T-1)}\\cdot a^{2},\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>3</mn><mrow><mn>28</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msup><mi>a</mi><mn>2</mn></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{3}{28(T-1)}\\cdot a^{2},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(23)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">which is of order <math id=\"A3.SS1.p6.m17\" class=\"ltx_Math\" alttext=\"1/T\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">1/T</annotation></semantics></math> as long as <math id=\"A3.SS1.p6.m18\" class=\"ltx_Math\" alttext=\"a\" display=\"inline\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> is bounded away from zero.</p>\n</div>\n<div id=\"A3.SS1.p7\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Now, we compare this to forgetting with replay of the sample <math id=\"A3.SS1.p7.m1\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2},y_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒙</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2},y_{2}</annotation></semantics></math>, which is the only sample that contributed to forgetting. In the replay scenario, <math id=\"A3.SS1.p7.m2\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> is combined with <math id=\"A3.SS1.p7.m3\" class=\"ltx_Math\" alttext=\"{\\bm{X}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑿</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{X}}_{t}</annotation></semantics></math> to get <math id=\"A3.SS1.p7.m4\" class=\"ltx_Math\" alttext=\"\\tilde{X}_{T}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>T</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{X}_{T}</annotation></semantics></math>, consequently the null space <math id=\"A3.SS1.p7.m5\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{T}=\\frac{1}{2}({\\bm{v}}_{1}-{\\bm{v}}_{2})({\\bm{v}}_{1}-{\\bm{v}}_{2})^{\\top}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mi>T</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{T}=\\frac{1}{2}({\\bm{v}}_{1}-{\\bm{v}}_{2})({\\bm{v}}_{1}-{\\bm{v}}_{2})^{\\top}</annotation></semantics></math>. Recall that without replay <math id=\"A3.SS1.p7.m6\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{T}=\\begin{bmatrix}{\\bm{v}}_{1},{\\bm{v}}_{2}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}.\\end{bmatrix}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mi>T</mi></msub><mo>=</mo><mrow><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><mrow><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\">.</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{T}=\\begin{bmatrix}{\\bm{v}}_{1},{\\bm{v}}_{2}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}.\\end{bmatrix}</annotation></semantics></math></p>\n</div>\n<div id=\"A3.SS1.p8\" class=\"ltx_para\">\n<p class=\"ltx_p\">Now we compute the forgetting <math id=\"A3.SS1.p8.m1\" class=\"ltx_math_unparsed\" alttext=\"\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}\\tilde{{\\bm{P}}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><msubsup><mo rspace=\"0em\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mi>T</mi></msub><mi>𝒖</mi><msup><mi>𝒖</mi><mo>⊤</mo></msup><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}\\tilde{{\\bm{P}}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}</annotation></semantics></math>.\nSimilar to the no replay case, we can see that for all <math id=\"A3.SS1.p8.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, <math id=\"A3.SS1.p8.m3\" class=\"ltx_Math\" alttext=\"{\\bm{X}}^{\\prime}_{t}\\tilde{{\\bm{P}}}_{T}=0\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝑿</mi><mi>t</mi><mo>′</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mi>T</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{X}}^{\\prime}_{t}\\tilde{{\\bm{P}}}_{T}=0</annotation></semantics></math> and <math id=\"A3.SS1.p8.m4\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{3}\\tilde{{\\bm{P}}}_{T}=0\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝒙</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mi>T</mi></msub></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{x}}_{3}\\tilde{{\\bm{P}}}_{T}=0</annotation></semantics></math>, so these samples don’t contribute to forgetting. Additionally, since <math id=\"A3.SS1.p8.m5\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{2}</annotation></semantics></math> has just been replayed, it is in the span of <math id=\"A3.SS1.p8.m6\" class=\"ltx_Math\" alttext=\"\\tilde{X}_{T}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>T</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{X}_{T}</annotation></semantics></math> and doesn’t contribute to forgetting. We are left with</p>\n<table id=\"A4.EGx19\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E24\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E24.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{x}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{x}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E24.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{2}({\\bm{v}}_{1}-{\\bm{v}}_{2})^{\\top}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=-\\frac{6}{28}a\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mn>6</mn><mn>28</mn></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>a</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{2}({\\bm{v}}_{1}-{\\bm{v}}_{2})^{\\top}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}=-\\frac{6}{28}a</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(24)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Forgetting with replay is then <math id=\"A3.SS1.p8.m7\" class=\"ltx_math_unparsed\" alttext=\"\\frac{T-1}{T-1}\\mathopen{}\\mathclose{{\\left({\\bm{x}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}\\right)^{2}=\\frac{9}{196}a^{2}\" display=\"inline\"><semantics><mrow><mfrac><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mrow><mi></mi><mrow><mo>(</mo><msubsup><mi>𝒙</mi><mn>1</mn><mo>⊤</mo></msubsup><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mi>T</mi></msub><mi>𝒖</mi><msup><mi>𝒖</mi><mo>⊤</mo></msup><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo>)</mo><msup><mi></mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>9</mn><mn>196</mn></mfrac><mi>a</mi><msup><mi></mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{T-1}{T-1}\\mathopen{}\\mathclose{{\\left({\\bm{x}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{T}{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{w}}^{*}}}\\right)^{2}=\\frac{9}{196}a^{2}</annotation></semantics></math>, since <math id=\"A3.SS1.p8.m8\" class=\"ltx_Math\" alttext=\"{\\bm{x}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒙</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{x}}_{1}</annotation></semantics></math> appeared in all the first <math id=\"A3.SS1.p8.m9\" class=\"ltx_Math\" alttext=\"T-1\" display=\"inline\"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">T-1</annotation></semantics></math> tasks.\nComparing <math id=\"A3.SS1.p8.m10\" class=\"ltx_Math\" alttext=\"\\frac{3a^{2}}{28\\;(T-1)}\" display=\"inline\"><semantics><mfrac><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>a</mi><mn>2</mn></msup></mrow><mrow><mn>28</mn><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac><annotation encoding=\"application/x-tex\">\\frac{3a^{2}}{28\\;(T-1)}</annotation></semantics></math> to <math id=\"A3.SS1.p8.m11\" class=\"ltx_Math\" alttext=\"\\frac{9a^{2}}{196}\" display=\"inline\"><semantics><mfrac><mrow><mn>9</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>a</mi><mn>2</mn></msup></mrow><mn>196</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{9a^{2}}{196}</annotation></semantics></math>, we can see that as <math id=\"A3.SS1.p8.m12\" class=\"ltx_Math\" alttext=\"T\\rightarrow\\infty\" display=\"inline\"><semantics><mrow><mi>T</mi><mo stretchy=\"false\">→</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">T\\rightarrow\\infty</annotation></semantics></math>, forgetting vanishes without replay, while with replay, it is a constant.\n∎</p>\n</div>\n</section>\n<section id=\"A3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.2 </span>Lower dimensional average case results</h3>\n\n<div id=\"A3.Thmtheorem1\" class=\"ltx_theorem ltx_theorem_theorem\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Theorem C.1</span></span><span class=\"ltx_text ltx_font_bold\"> </span>(Average case replay)<span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"A3.Thmtheorem1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Suppose that assumptions <a href=\"#S2.Thmtheorem1\" title=\"Assumption 2.1 (Over-parameterized linear regression). ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>, <a href=\"#S2.Thmtheorem2\" title=\"Assumption 2.2. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, and <a href=\"#S3.Thmtheorem3\" title=\"Assumption 3.3. ‣ 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> hold.\nFor every <math id=\"A3.Thmtheorem1.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\\in\\mathbb{R}^{3}\" display=\"inline\"><semantics><mrow><msup><mi>𝐰</mi><mo>∗</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mn>3</mn></msup></mrow><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}\\in\\mathbb{R}^{3}</annotation></semantics></math> where <math id=\"A3.Thmtheorem1.p1.m2\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}^{*}}}}\\right\\|_{2}\\leq 1\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msup><mi>𝐰</mi><mo>∗</mo></msup></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn></msub><mo lspace=\"0.0835em\">≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{w}}^{*}}}}\\right\\|_{2}\\leq 1</annotation></semantics></math>, there exists a sequence of two task subspaces, such that replaying a randomly chosen sample from the first task’s samples increases expected forgetting.\nThat is</span></p>\n<table id=\"A4.EGx20\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E25\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E25.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{2})}}\\right]&lt;\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}(\\tilde{{\\bm{w}}}_{2})}}\\right],\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">&lt;</mo><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{2})}}\\right]&lt;\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}(\\tilde{{\\bm{w}}}_{2})}}\\right],</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(25)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">where <math id=\"A3.Thmtheorem1.p1.m3\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{w}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝐰</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{w}}}_{2}</annotation></semantics></math> is the iterate after the second task with replay.</span></p>\n</div>\n</div>\n<div id=\"A3.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a></span>\n  \nWe use the following claim which simplifies the form of expected forgetting in the two task case with replay. Proof of this claim is given in <a href=\"#A3.SS4\" title=\"C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.4</span></a></p>\n</div>\n<div id=\"A3.Thmtheorem2\" class=\"ltx_theorem ltx_theorem_claim\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Claim C.2</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"A3.Thmtheorem2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Suppose that we have a sequence <math id=\"A3.Thmtheorem2.p1.m1\" class=\"ltx_Math\" alttext=\"S\" display=\"inline\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> of two tasks in the average case setting (as described in <a href=\"#S3.SS2\" title=\"3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Let <math id=\"A3.Thmtheorem2.p1.m2\" class=\"ltx_Math\" alttext=\"R\" display=\"inline\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> be a set of <math id=\"A3.Thmtheorem2.p1.m3\" class=\"ltx_Math\" alttext=\"m\\leq n_{1}\" display=\"inline\"><semantics><mrow><mi>m</mi><mo>≤</mo><msub><mi>n</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">m\\leq n_{1}</annotation></semantics></math> randomly chosen (without replacement) indices of the samples to be replayed from the first task. Let <math id=\"A3.Thmtheorem2.p1.m4\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1},{\\bm{P}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝐏</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝐏</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1},{\\bm{P}}_{2}</annotation></semantics></math> be the task null spaces and <math id=\"A3.Thmtheorem2.p1.m5\" class=\"ltx_math_unparsed\" alttext=\"\\tilde{{\\bm{P}}}_{2}=\\tilde{{\\bm{P}}}_{2}(\\mathopen{}\\mathclose{{\\left\\{x_{1,j}}}\\right\\}_{j\\in R})\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝐏</mi><mo>~</mo></mover><mn>2</mn></msub><mo>=</mo><msub><mover accent=\"true\"><mi>𝐏</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mrow><mo stretchy=\"false\">(</mo><mrow><mi></mi><mrow><mo>{</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><mo>}</mo></mrow><mrow><mi>j</mi><mo>∈</mo><mi>R</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}=\\tilde{{\\bm{P}}}_{2}(\\mathopen{}\\mathclose{{\\left\\{x_{1,j}}}\\right\\}_{j\\in R})</annotation></semantics></math> be the null space of the second task with replay. Then the expected forgetting with respect to test samples and with replay of <math id=\"A3.Thmtheorem2.p1.m6\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> samples from the first task can be simplified to</span></p>\n<table id=\"A4.EGx21\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E26\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E26.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}(\\tilde{{\\bm{w}}}_{2})}}\\right]=\\;\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right],\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo>=</mo><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}(\\tilde{{\\bm{w}}}_{2})}}\\right]=\\;\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right],</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(26)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">where the expectation is over the randomness of <math id=\"A3.Thmtheorem2.p1.m7\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝐏</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math>.</span></p>\n</div>\n</div>\n<div id=\"A3.SS2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Given <math id=\"A3.SS2.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{w}}^{*}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{\\bm{w}}^{*}</annotation></semantics></math>, we can pick orthonormal basis <math id=\"A3.SS2.p2.m2\" class=\"ltx_Math\" alttext=\"W_{1}\" display=\"inline\"><semantics><msub><mi>W</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">W_{1}</annotation></semantics></math> and <math id=\"A3.SS2.p2.m3\" class=\"ltx_Math\" alttext=\"W_{2}\" display=\"inline\"><semantics><msub><mi>W</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">W_{2}</annotation></semantics></math> for the subspaces, <math id=\"A3.SS2.p2.m4\" class=\"ltx_Math\" alttext=\"n_{1}\" display=\"inline\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math>, and <math id=\"A3.SS2.p2.m5\" class=\"ltx_Math\" alttext=\"n_{2}\" display=\"inline\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math>.\nAlso recall the sample generation process described in <a href=\"#S3.E5\" title=\"In 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nWe start with describing the two task subspaces.\nWe first fix <math id=\"A3.SS2.p2.m6\" class=\"ltx_Math\" alttext=\"\\epsilon=\\sqrt{\\frac{1}{63}}\" display=\"inline\"><semantics><mrow><mi>ϵ</mi><mo>=</mo><msqrt><mfrac><mn>1</mn><mn>63</mn></mfrac></msqrt></mrow><annotation encoding=\"application/x-tex\">\\epsilon=\\sqrt{\\frac{1}{63}}</annotation></semantics></math>, though other small values of <math id=\"A3.SS2.p2.m7\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics><mi>ϵ</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> would work as well.\nThen we fix an orthonormal basis <math id=\"A3.SS2.p2.m8\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}</annotation></semantics></math> such that <math id=\"A3.SS2.p2.m9\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}&gt;0\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn></msub><mo lspace=\"0.0835em\">&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}&gt;0</annotation></semantics></math>, where <math id=\"A3.SS2.p2.m10\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1}\" display=\"inline\"><semantics><msub><mi>𝑷</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1}</annotation></semantics></math> is defined in <a href=\"#A3.E28\" title=\"In C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">28</span></a>.\nThe first task is spanned by orthonormal vectors <math id=\"A3.SS2.p2.m11\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},{\\bm{u}}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi>𝒖</mi></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},{\\bm{u}}</annotation></semantics></math>, where <math id=\"A3.SS2.p2.m12\" class=\"ltx_Math\" alttext=\"{\\bm{u}}=\\epsilon{\\bm{v}}_{2}+\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><mi>𝒖</mi><mo>=</mo><mrow><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}=\\epsilon{\\bm{v}}_{2}+\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}</annotation></semantics></math> for some <math id=\"A3.SS2.p2.m13\" class=\"ltx_Math\" alttext=\"1&gt;\\epsilon&gt;0\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>&gt;</mo><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">1&gt;\\epsilon&gt;0</annotation></semantics></math> that will be fixed later. That is</p>\n<table id=\"A4.EGx22\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E27\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E27.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{\\Pi}}_{1}={\\bm{W}}_{1}{\\bm{W}}_{1}^{\\top}=\\begin{bmatrix}{\\bm{v}}_{1},{\\bm{u}}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{u}}^{\\top}\\end{bmatrix}.\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝚷</mi><mn>1</mn></msub><mo>=</mo><mrow><msub><mi>𝑾</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝑾</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow><mo>=</mo><mrow><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi>𝒖</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msup><mi>𝒖</mi><mo>⊤</mo></msup></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{\\Pi}}_{1}={\\bm{W}}_{1}{\\bm{W}}_{1}^{\\top}=\\begin{bmatrix}{\\bm{v}}_{1},{\\bm{u}}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{u}}^{\\top}\\end{bmatrix}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(27)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">This leads to the following one dimensional null space for the first task</p>\n<table id=\"A4.EGx23\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E28\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E28.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{P}}_{1}=(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})^{\\top}.\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{P}}_{1}=(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})^{\\top}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(28)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The second task is spanned by <math id=\"A3.SS2.p3.m1\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{3}</annotation></semantics></math>, leading to the null space</p>\n<table id=\"A4.EGx24\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E29\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E29.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{P}}_{2}=\\begin{bmatrix}{\\bm{v}}_{1},{\\bm{v}}_{2}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}\\end{bmatrix}.\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo>=</mo><mrow><mrow><mo>[</mo><mtable><mtr><mtd><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{P}}_{2}=\\begin{bmatrix}{\\bm{v}}_{1},{\\bm{v}}_{2}\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}\\end{bmatrix}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(29)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Now we compute forgetting without replay using <a href=\"#S3.E7\" title=\"In Proposition 3.4. ‣ 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>. So we start with computing</p>\n<table id=\"A4.EGx25\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E30\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E30.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝚷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E30.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}+{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>+</mo><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}+{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(30)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E31\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E31.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E31.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}.\" display=\"inline\"><semantics><mrow><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(31)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">The last equality holds since <math id=\"A3.SS2.p4.m1\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1}^{\\top}{\\bm{P}}_{2}={\\bm{v}}_{1}^{\\top}\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub></mrow><mo>=</mo><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1}^{\\top}{\\bm{P}}_{2}={\\bm{v}}_{1}^{\\top}</annotation></semantics></math> and <math id=\"A3.SS2.p4.m2\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1}^{\\top}{\\bm{P}}_{1}=0\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1}^{\\top}{\\bm{P}}_{1}=0</annotation></semantics></math>.\nLet <math id=\"A3.SS2.p4.m3\" class=\"ltx_Math\" alttext=\"a\\vcentcolon=((\\sqrt{1-\\epsilon^{2}})\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{3})^{\\top}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mi>a</mi><mo>:=</mo><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">a\\vcentcolon=((\\sqrt{1-\\epsilon^{2}})\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{3})^{\\top}{\\bm{w}}^{*}</annotation></semantics></math>.\nWe have</p>\n<table id=\"A4.EGx26\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E32\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E32.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{u}}{\\bm{u}}^{\\top}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E32.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{u}}\\begin{bmatrix}0,\\epsilon\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}\\end{bmatrix}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mtable><mtr><mtd><mrow><mn>0</mn><mo>,</mo><mi>ϵ</mi></mrow></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>[</mo><mtable rowspacing=\"0pt\"><mtr><mtd><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup></mtd></mtr></mtable><mo>]</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{u}}\\begin{bmatrix}0,\\epsilon\\end{bmatrix}\\begin{bmatrix}{\\bm{v}}_{1}^{\\top}\\\\\n{\\bm{v}}_{2}^{\\top}\\end{bmatrix}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(32)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E33\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E33.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E33.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\epsilon\\cdot a\\cdot{\\bm{u}}{\\bm{v}}_{2}^{\\top}(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})\" display=\"inline\"><semantics><mrow><mrow><mi>ϵ</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>𝒖</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\epsilon\\cdot a\\cdot{\\bm{u}}{\\bm{v}}_{2}^{\\top}(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(33)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E34\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E34.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E34.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\epsilon\\cdot\\sqrt{1-\\epsilon^{2}}\\cdot a\\cdot{\\bm{u}}.\" display=\"inline\"><semantics><mrow><mrow><mi>ϵ</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>𝒖</mi></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\epsilon\\cdot\\sqrt{1-\\epsilon^{2}}\\cdot a\\cdot{\\bm{u}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(34)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Forgetting without replay is</p>\n<table id=\"A4.EGx27\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E35\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E35.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E35.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\cdot\\epsilon^{2}\\cdot(1-\\epsilon^{2})\\cdot a^{2}\\cdot\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mo rspace=\"0.222em\">⋅</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">⋅</mo><msup><mi>a</mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mi>𝒖</mi></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\cdot\\epsilon^{2}\\cdot(1-\\epsilon^{2})\\cdot a^{2}\\cdot\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(35)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E36\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E36.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E36.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\cdot\\epsilon^{2}\\cdot(1-\\epsilon^{2})\\cdot a^{2}.\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">⋅</mo><msup><mi>a</mi><mn>2</mn></msup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\cdot\\epsilon^{2}\\cdot(1-\\epsilon^{2})\\cdot a^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(36)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Note that forgetting on the last task is always zero.\nTo compute forgetting with replay of one sample, we need to understand the distribution of <math id=\"A3.SS2.p4.m4\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math> first.\nLet <math id=\"A3.SS2.p4.m5\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{1J}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{1J}</annotation></semantics></math> be a randomly chosen sample from the first task. By <a href=\"#S3.E5\" title=\"In 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>,\n<math id=\"A3.SS2.p4.m6\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{1J}={\\bm{W}}_{1}{\\mathbf{Z}}_{1J}\" display=\"inline\"><semantics><mrow><msub><mi>𝐗</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo>=</mo><mrow><msub><mi>𝑾</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{1J}={\\bm{W}}_{1}{\\mathbf{Z}}_{1J}</annotation></semantics></math> where <math id=\"A3.SS2.p4.m7\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{1j}\\sim\\sf{N}(0,\\frac{\\mathbf{I}_{2}}{2})\" display=\"inline\"><semantics><mrow><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mfrac><msub><mi>𝐈</mi><mn>𝟤</mn></msub><mn>𝟤</mn></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{1j}\\sim\\sf{N}(0,\\frac{\\mathbf{I}_{2}}{2})</annotation></semantics></math> and <math id=\"A3.SS2.p4.m8\" class=\"ltx_Math\" alttext=\"J\\in_{R}[n_{t}]\" display=\"inline\"><semantics><mrow><mi>J</mi><msub><mo>∈</mo><mi>R</mi></msub><mrow><mo stretchy=\"false\">[</mo><msub><mi>n</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">J\\in_{R}[n_{t}]</annotation></semantics></math> is an index that is picked uniformly at random from the set <math id=\"A3.SS2.p4.m9\" class=\"ltx_Math\" alttext=\"[n_{t}]\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>n</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[n_{t}]</annotation></semantics></math>. It’s important here to note that\n<math id=\"A3.SS2.p4.m10\" class=\"ltx_Math\" alttext=\"J\" display=\"inline\"><semantics><mi>J</mi><annotation encoding=\"application/x-tex\">J</annotation></semantics></math> and <math id=\"A3.SS2.p4.m11\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{1J}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{1J}</annotation></semantics></math> are independent and <math id=\"A3.SS2.p4.m12\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{1i}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{1i}</annotation></semantics></math> are iid.\nThen we have</p>\n<table id=\"A4.EGx28\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E37\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E37.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle p({\\mathbf{Z}}_{1J})=\" display=\"inline\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle p({\\mathbf{Z}}_{1J})=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E37.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\sum_{j=1}^{n_{t}}p({\\mathbf{Z}}_{1J}\\mid J=j)p(J=j)\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>t</mi></msub></munderover></mstyle><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo>∣</mo><mi>J</mi></mrow><mo>=</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>J</mi><mo>=</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\sum_{j=1}^{n_{t}}p({\\mathbf{Z}}_{1J}\\mid J=j)p(J=j)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(37)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E38\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E38.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E38.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{n_{t}}\\sum_{j=1}^{n_{t}}p({\\mathbf{Z}}_{1J}\\mid J=j)\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>n</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>t</mi></msub></munderover></mstyle><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo>∣</mo><mi>J</mi></mrow><mo>=</mo><mi>j</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{n_{t}}\\sum_{j=1}^{n_{t}}p({\\mathbf{Z}}_{1J}\\mid J=j)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(38)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E39\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E39.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E39.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{1}{n_{t}}\\sum_{j=1}^{n_{t}}p({\\mathbf{Z}}_{1j})=p({\\mathbf{Z}}_{11}),\" display=\"inline\"><semantics><mrow><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>n</mi><mi>t</mi></msub></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>t</mi></msub></munderover></mstyle><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝐙</mi><mn>11</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{n_{t}}\\sum_{j=1}^{n_{t}}p({\\mathbf{Z}}_{1j})=p({\\mathbf{Z}}_{11}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(39)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">and consequently <math id=\"A3.SS2.p4.m13\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{1J}\\sim\\sf{N}(0,\\frac{\\mathbf{I}_{2}}{2})\" display=\"inline\"><semantics><mrow><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mfrac><msub><mi>𝐈</mi><mn>𝟤</mn></msub><mn>𝟤</mn></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{1J}\\sim\\sf{N}(0,\\frac{\\mathbf{I}_{2}}{2})</annotation></semantics></math> and we can write</p>\n<table id=\"A4.EGx29\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E40\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E40.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle{\\mathbf{X}}_{1J}={\\bm{W}}_{1}{\\mathbf{Z}}_{1J}=\\frac{1}{2}\\mathopen{}\\mathclose{{\\left(\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}}}\\right),\" display=\"inline\"><semantics><mrow><msub><mi>𝐗</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo>=</mo><msub><mi>𝑾</mi><mn>1</mn></msub><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>1</mn></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><mi>𝒖</mi></mrow></mrow><mo>)</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\mathbf{X}}_{1J}={\\bm{W}}_{1}{\\mathbf{Z}}_{1J}=\\frac{1}{2}\\mathopen{}\\mathclose{{\\left(\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}}}\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(40)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"A3.SS2.p4.m14\" class=\"ltx_Math\" alttext=\"\\frac{1}{2}\\alpha_{1},\\frac{1}{2}\\alpha_{2}\\sim\\sf{N}(0,\\frac{1}{2})\" display=\"inline\"><semantics><mrow><mrow><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>α</mi><mn>1</mn></msub></mrow><mo>,</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>α</mi><mn>2</mn></msub></mrow></mrow><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mfrac><mn>𝟣</mn><mn>𝟤</mn></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{1}{2}\\alpha_{1},\\frac{1}{2}\\alpha_{2}\\sim\\sf{N}(0,\\frac{1}{2})</annotation></semantics></math> are the two iid coordinates of <math id=\"A3.SS2.p4.m15\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{1J}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{1J}</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS2.p5\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><math id=\"A3.SS2.p5.m1\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math> is a projection onto the null space of the space spanned by task two samples, which have the form <math id=\"A3.SS2.p5.m2\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{2j}={\\bm{v}}_{3}{\\mathbf{Z}}_{2j}\" display=\"inline\"><semantics><mrow><msub><mi>𝐗</mi><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msub><mi>𝒗</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐙</mi><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{2j}={\\bm{v}}_{3}{\\mathbf{Z}}_{2j}</annotation></semantics></math>, plus <math id=\"A3.SS2.p5.m3\" class=\"ltx_Math\" alttext=\"{\\mathbf{X}}_{1J}\" display=\"inline\"><semantics><msub><mi>𝐗</mi><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>J</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{X}}_{1J}</annotation></semantics></math>.\nIn another words, since all the samples for task <math id=\"A3.SS2.p5.m4\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> are colinear with <math id=\"A3.SS2.p5.m5\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{3}</annotation></semantics></math>, <math id=\"A3.SS2.p5.m6\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math> is a projection onto the null space of linear span of <math id=\"A3.SS2.p5.m7\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{3},\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>3</mn></msub><mo>,</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>1</mn></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><mi>𝒖</mi></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{3},\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}}}\\right\\}</annotation></semantics></math>.\nSince <math id=\"A3.SS2.p5.m8\" class=\"ltx_Math\" alttext=\"\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}=\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}+\\alpha_{2}\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}=\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}+\\alpha_{2}\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}</annotation></semantics></math>, we have that</p>\n<table id=\"A4.EGx30\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E41\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E41.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{3},\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}+\\alpha_{2}\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}}}\\right\\}\" display=\"inline\"><semantics><mrow><mi>span</mi><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>3</mn></msub><mo>,</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>1</mn></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>2</mn></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{3},\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}+\\alpha_{2}\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}}}\\right\\}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(41)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E42\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E42.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{3},\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}}}\\right\\}\" display=\"inline\"><semantics><mrow><mo>=</mo><mi>span</mi><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>3</mn></msub><mo>,</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>1</mn></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{3},\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}}}\\right\\}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(42)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p6\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Then <math id=\"A3.SS2.p6.m1\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math> is a projection onto a one dimensional vector space that is orthogonal to <math id=\"A3.SS2.p6.m2\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{3}</annotation></semantics></math> and <math id=\"A3.SS2.p6.m3\" class=\"ltx_Math\" alttext=\"\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}\\epsilon{\\bm{v}}_{2}</annotation></semantics></math>. We can write</p>\n<table id=\"A4.EGx31\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E43\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E43.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\tilde{{\\bm{P}}}_{2}=\\frac{1}{z}\\cdot\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top},\" display=\"inline\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo><msup><mi></mi><mo>⊤</mo></msup><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\tilde{{\\bm{P}}}_{2}=\\frac{1}{z}\\cdot\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(43)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"A3.SS2.p6.m4\" class=\"ltx_Math\" alttext=\"z\\vcentcolon=\\epsilon^{2}\\alpha_{2}^{2}+\\alpha_{1}^{2}\" display=\"inline\"><semantics><mrow><mi>z</mi><mo>:=</mo><mrow><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">z\\vcentcolon=\\epsilon^{2}\\alpha_{2}^{2}+\\alpha_{1}^{2}</annotation></semantics></math> is a normalizing constant.\nNext we compute each of the terms in</p>\n<table id=\"A4.EGx32\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E44\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E44.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝚷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E44.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}+{\\bm{u}}{\\bm{u}}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}.\" display=\"inline\"><semantics><mrow><mrow><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>+</mo><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}+{\\bm{u}}{\\bm{u}}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(44)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p7\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We have</p>\n<table id=\"A4.EGx33\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E45\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E45.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E45.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{z}\\cdot{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>1</mn></msub><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo><msup><mi></mi><mo>⊤</mo></msup><mi>𝑷</mi><msub><mi></mi><mn>1</mn></msub><mi>𝒘</mi><msup><mi></mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{z}\\cdot{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(45)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E46\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E46.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E46.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{\\alpha_{2}\\;\\epsilon}{z}\\cdot{\\bm{v}}_{1}\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><mi>ϵ</mi></mrow><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo><msup><mi></mi><mo>⊤</mo></msup><mi>𝑷</mi><msub><mi></mi><mn>1</mn></msub><mi>𝒘</mi><msup><mi></mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{\\alpha_{2}\\;\\epsilon}{z}\\cdot{\\bm{v}}_{1}\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(46)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E47\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E47.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E47.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{\\alpha_{2}\\;\\epsilon}{z}\\cdot{\\bm{v}}_{1}\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><mi>ϵ</mi></mrow><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo><msup><mi></mi><mo>⊤</mo></msup><mo stretchy=\"false\">(</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><msub><mi>𝒗</mi><mn>2</mn></msub><mo>−</mo><mi>ϵ</mi><msub><mi>𝒗</mi><mn>3</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{\\alpha_{2}\\;\\epsilon}{z}\\cdot{\\bm{v}}_{1}\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(47)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E48\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E48.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})^{\\top}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle(\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{2}-\\epsilon{\\bm{v}}_{3})^{\\top}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(48)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E49\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E49.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E49.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle-\\frac{\\alpha_{1}\\;\\alpha_{2}\\;\\epsilon\\;\\sqrt{1-\\epsilon^{2}}}{z}\\cdot a\\cdot{\\bm{v}}_{1},\" display=\"inline\"><semantics><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>ϵ</mi><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt></mrow><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle-\\frac{\\alpha_{1}\\;\\alpha_{2}\\;\\epsilon\\;\\sqrt{1-\\epsilon^{2}}}{z}\\cdot a\\cdot{\\bm{v}}_{1},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(49)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p8\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">and</p>\n</div>\n<div id=\"A3.SS2.p9\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx34\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E50\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E50.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\bm{u}}{\\bm{u}}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=\" display=\"inline\"><semantics><mrow><mrow><mi>𝒖</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒖</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\bm{u}}{\\bm{u}}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E50.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle-\\frac{\\alpha_{1}\\epsilon}{z}\\cdot{\\bm{u}}\\;\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}{\\bm{P}}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mo>−</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>ϵ</mi></mrow><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>𝒖</mi><mrow><mi></mi><mrow><mo>(</mo><msub><mi>α</mi><mn>2</mn></msub><mi>ϵ</mi><msub><mi>𝒗</mi><mn>1</mn></msub><mo>−</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><mo>)</mo><msup><mi></mi><mo>⊤</mo></msup><mi>𝑷</mi><msub><mi></mi><mn>1</mn></msub><mi>𝒘</mi><msup><mi></mi><mo>∗</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle-\\frac{\\alpha_{1}\\epsilon}{z}\\cdot{\\bm{u}}\\;\\mathopen{}\\mathclose{{\\left(\\alpha_{2}\\epsilon{\\bm{v}}_{1}-\\alpha_{1}{\\bm{v}}_{2}}}\\right)^{\\top}{\\bm{P}}_{1}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(50)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E51\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E51.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E51.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\alpha_{1}^{2}\\;\\epsilon\\;\\sqrt{1-\\epsilon^{2}}}{z}\\cdot a\\cdot{\\bm{u}}.\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><mi>ϵ</mi><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt></mrow><mi>z</mi></mfrac></mstyle><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>𝒖</mi></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{\\alpha_{1}^{2}\\;\\epsilon\\;\\sqrt{1-\\epsilon^{2}}}{z}\\cdot a\\cdot{\\bm{u}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(51)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p10\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Since <math id=\"A3.SS2.p10.m1\" class=\"ltx_Math\" alttext=\"{\\bm{u}}\" display=\"inline\"><semantics><mi>𝒖</mi><annotation encoding=\"application/x-tex\">{\\bm{u}}</annotation></semantics></math> and <math id=\"A3.SS2.p10.m2\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1}</annotation></semantics></math> are orthogonal to each other, we can write</p>\n<table id=\"A4.EGx35\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E52\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E52.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E52.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}{\\bm{u}}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}+\\mathopen{}\\mathclose{{\\left\\|{{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mi>𝒖</mi><msup><mi>𝒖</mi><mo>⊤</mo></msup><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\">+</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝒗</mi><mn>1</mn></msub><msubsup><mi>𝒗</mi><mn>1</mn><mo>⊤</mo></msubsup><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}{\\bm{u}}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}+\\mathopen{}\\mathclose{{\\left\\|{{\\bm{v}}_{1}{\\bm{v}}_{1}^{\\top}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(52)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E53\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E53.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E53.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\alpha_{1}^{4}\\epsilon^{2}(1-\\epsilon^{2})\\frac{a^{2}}{z^{2}}+\\alpha_{1}^{2}\\alpha_{2}^{2}\\;\\epsilon^{2}(1-\\epsilon^{2})\\;\\frac{a^{2}}{z^{2}}\" display=\"inline\"><semantics><mrow><mrow><msubsup><mi>α</mi><mn>1</mn><mn>4</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mstyle displaystyle=\"true\"><mfrac><msup><mi>a</mi><mn>2</mn></msup><msup><mi>z</mi><mn>2</mn></msup></mfrac></mstyle></mrow><mo>+</mo><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.280em\" rspace=\"0em\">​</mo><mstyle displaystyle=\"true\"><mfrac><msup><mi>a</mi><mn>2</mn></msup><msup><mi>z</mi><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\alpha_{1}^{4}\\epsilon^{2}(1-\\epsilon^{2})\\frac{a^{2}}{z^{2}}+\\alpha_{1}^{2}\\alpha_{2}^{2}\\;\\epsilon^{2}(1-\\epsilon^{2})\\;\\frac{a^{2}}{z^{2}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(53)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E54\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E54.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E54.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left(\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}+\\alpha_{1}^{4}}{z^{2}}}}\\right)\\epsilon^{2}(1-\\epsilon^{2})a^{2}.\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>4</mn></msubsup></mrow><msup><mi>z</mi><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><mo>)</mo><mi>ϵ</mi><msup><mi></mi><mn>2</mn></msup><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mi>a</mi><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left(\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}+\\alpha_{1}^{4}}{z^{2}}}}\\right)\\epsilon^{2}(1-\\epsilon^{2})a^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(54)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">By <a href=\"#A3.Thmtheorem2\" title=\"Claim C.2. ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a>, the expected forgetting with replay is</p>\n<table id=\"A4.EGx36\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E55\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E55.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}+\\alpha_{1}^{4}}{z^{2}}}}\\right]\\epsilon^{2}(1-\\epsilon^{2})a^{2}.\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">=</mo><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>4</mn></msubsup></mrow><msup><mi>z</mi><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><mo>]</mo><mi>ϵ</mi><msup><mi></mi><mn>2</mn></msup><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mi>a</mi><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}+\\alpha_{1}^{4}}{z^{2}}}}\\right]\\epsilon^{2}(1-\\epsilon^{2})a^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(55)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We compare this to expected forgetting without replay, which is <math id=\"A3.SS2.p10.m3\" class=\"ltx_Math\" alttext=\"\\epsilon^{2}(1-\\epsilon^{2})a^{2}\" display=\"inline\"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>a</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\epsilon^{2}(1-\\epsilon^{2})a^{2}</annotation></semantics></math>, and show that there exists <math id=\"A3.SS2.p10.m4\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics><mi>ϵ</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> such that</p>\n<table id=\"A4.EGx37\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E56\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E56.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}+\\alpha_{1}^{4}}{z^{2}}}}\\right]&gt;1.\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>4</mn></msubsup></mrow><msup><mi>z</mi><mn>2</mn></msup></mfrac></mstyle></mrow></mrow><mo>]</mo><mo>&gt;</mo><mn>1</mn><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}+\\alpha_{1}^{4}}{z^{2}}}}\\right]&gt;1.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(56)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p11\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Note that by definition, <math id=\"A3.SS2.p11.m1\" class=\"ltx_math_unparsed\" alttext=\"a^{2}=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">a^{2}=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}</annotation></semantics></math> and the orthonormal basis <math id=\"A3.SS2.p11.m2\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},{\\bm{v}}_{2},{\\bm{v}}_{3}</annotation></semantics></math> can be chosen such that\n<math id=\"A3.SS2.p11.m3\" class=\"ltx_Math\" alttext=\"a^{2}&gt;0\" display=\"inline\"><semantics><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">a^{2}&gt;0</annotation></semantics></math>.\n.</p>\n</div>\n<div id=\"A3.SS2.p12\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">By definition of <math id=\"A3.SS2.p12.m1\" class=\"ltx_Math\" alttext=\"z\" display=\"inline\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>, can write <math id=\"A3.SS2.p12.m2\" class=\"ltx_Math\" alttext=\"\\frac{\\alpha_{2}^{2}}{z}=(1-\\frac{\\alpha_{1}^{2}}{z})\\cdot\\frac{1}{\\epsilon^{2}}\" display=\"inline\"><semantics><mrow><mfrac><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mi>z</mi></mfrac><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mi>z</mi></mfrac></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">⋅</mo><mfrac><mn>1</mn><msup><mi>ϵ</mi><mn>2</mn></msup></mfrac></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{\\alpha_{2}^{2}}{z}=(1-\\frac{\\alpha_{1}^{2}}{z})\\cdot\\frac{1}{\\epsilon^{2}}</annotation></semantics></math> and simplify the first term inside expectation to\n<math id=\"A3.SS2.p12.m3\" class=\"ltx_math_unparsed\" alttext=\"\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}}{z^{2}}=\\frac{\\alpha_{1}^{2}}{z}\\cdot\\mathopen{}\\mathclose{{\\left(1-\\frac{\\alpha_{1}^{2}}{z}}}\\right)\\cdot\\frac{1}{\\epsilon^{2}}.\" display=\"inline\"><semantics><mrow><mfrac><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow><msup><mi>z</mi><mn>2</mn></msup></mfrac><mo>=</mo><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mi>z</mi></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mi>z</mi></mfrac></mrow></mrow><mo rspace=\"0.055em\">)</mo><mo rspace=\"0.222em\">⋅</mo><mfrac><mn>1</mn><msup><mi>ϵ</mi><mn>2</mn></msup></mfrac><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{\\alpha_{1}^{2}\\alpha_{2}^{2}}{z^{2}}=\\frac{\\alpha_{1}^{2}}{z}\\cdot\\mathopen{}\\mathclose{{\\left(1-\\frac{\\alpha_{1}^{2}}{z}}}\\right)\\cdot\\frac{1}{\\epsilon^{2}}.</annotation></semantics></math></p>\n</div>\n<div id=\"A3.SS2.p13\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Let <math id=\"A3.SS2.p13.m1\" class=\"ltx_Math\" alttext=\"\\alpha^{\\prime 2}_{1}=\\frac{\\alpha_{1}^{2}}{z}\" display=\"inline\"><semantics><mrow><msubsup><mi>α</mi><mn>1</mn><mrow><mo mathsize=\"1.420em\">′</mo><mo lspace=\"0em\">⁣</mo><mn>2</mn></mrow></msubsup><mo>=</mo><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mi>z</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">\\alpha^{\\prime 2}_{1}=\\frac{\\alpha_{1}^{2}}{z}</annotation></semantics></math>, then rewriting\n<a href=\"#A3.E56\" title=\"In C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">56</span></a>, we have picked <math id=\"A3.SS2.p13.m2\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics><mi>ϵ</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> such that by <a href=\"#A3.Thmtheorem3\" title=\"Claim C.3. ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C.3</span></a></p>\n<table id=\"A4.EGx38\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E57\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E57.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\alpha^{\\prime 2}_{1}\\cdot(1-\\alpha^{\\prime 2}_{1})\\frac{1}{\\epsilon^{2}}+\\alpha^{\\prime 4}_{1}}}\\right]&gt;1.\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msubsup><mi>α</mi><mn>1</mn><mrow><mo mathsize=\"1.420em\">′</mo><mo lspace=\"0em\">⁣</mo><mn>2</mn></mrow></msubsup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msubsup><mi>α</mi><mn>1</mn><mrow><mo mathsize=\"1.420em\">′</mo><mo lspace=\"0em\">⁣</mo><mn>2</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msup><mi>ϵ</mi><mn>2</mn></msup></mfrac></mstyle><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mrow><mo mathsize=\"1.420em\">′</mo><mo lspace=\"0em\">⁣</mo><mn>4</mn></mrow></msubsup></mrow></mrow><mo>]</mo><mo>&gt;</mo><mn>1</mn><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\alpha^{\\prime 2}_{1}\\cdot(1-\\alpha^{\\prime 2}_{1})\\frac{1}{\\epsilon^{2}}+\\alpha^{\\prime 4}_{1}}}\\right]&gt;1.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(57)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS2.p14\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We can then conclude that replay has increased average forgetting.</p>\n</div>\n<div id=\"A3.Thmtheorem3\" class=\"ltx_theorem ltx_theorem_claim\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Claim C.3</span></span><span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"A3.Thmtheorem3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Let <math id=\"A3.Thmtheorem3.p1.m1\" class=\"ltx_Math\" alttext=\"\\alpha_{1},\\alpha_{2}\\sim\\sf{N}(0,1)\" display=\"inline\"><semantics><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>,</mo><msub><mi>α</mi><mn>2</mn></msub></mrow><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mn>𝟣</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1},\\alpha_{2}\\sim\\sf{N}(0,1)</annotation></semantics></math>, and <math id=\"A3.Thmtheorem3.p1.m2\" class=\"ltx_Math\" alttext=\"{\\alpha^{\\prime}_{1}}^{2}=\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{2}^{2}}{63}+\\alpha_{1}^{2}}\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>=</mo><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mrow><mfrac><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mn>63</mn></mfrac><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">{\\alpha^{\\prime}_{1}}^{2}=\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{2}^{2}}{63}+\\alpha_{1}^{2}}</annotation></semantics></math>. Then</span></p>\n<table id=\"A4.EGx39\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E58\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E58.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[63{\\alpha^{\\prime}_{1}}^{2}-62{\\alpha^{\\prime}_{1}}^{4}}}\\right]\\geq 1.4.\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mn>63</mn><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>−</mo><mn>62</mn><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>4</mn></mmultiscripts></mrow></mrow><mo>]</mo><mo>≥</mo><mn>1.4</mn><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[63{\\alpha^{\\prime}_{1}}^{2}-62{\\alpha^{\\prime}_{1}}^{4}}}\\right]\\geq 1.4.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(58)</span></td>\n</tr></tbody>\n</table>\n</div>\n</div>\n<div id=\"A3.SS2.p15\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Proof of this claim is given in <a href=\"#A3.SS4\" title=\"C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.</p>\n</div>\n<div id=\"A3.SS2.p16\" class=\"ltx_para\">\n<p class=\"ltx_p\">∎</p>\n</div>\n</section>\n<section id=\"A3.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.3 </span>Proof of <a href=\"#S3.Thmtheorem5\" title=\"Theorem 3.5. ‣ 3.2.2 Average Case Results ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">3.5</span></a>\n</h3>\n\n<div id=\"A3.SS3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#S3.Thmtheorem5\" title=\"Theorem 3.5. ‣ 3.2.2 Average Case Results ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">3.5</span></a></span>\n  \nThe construction for this higher dimensional case is an extension of the lower dimensional one. We start by describing the construction, and then calculate forgetting with and without replay. Calculating expected forgetting with replay is more involved.</p>\n</div>\n<section id=\"A3.SS3.SSS0.Px1\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Construction</h5>\n\n<div id=\"A3.SS3.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Let <math id=\"A3.SS3.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> be the input dimension.\nThe first task is a <math id=\"A3.SS3.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"d-1\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d-1</annotation></semantics></math> dimensional subspace spanned by <math id=\"A3.SS3.SSS0.Px1.p1.m3\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{u}},{\\bm{v}}_{1},{\\bm{v}}_{3},\\dots,{\\bm{v}}_{d-1}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><mi>𝒖</mi><mo>,</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{u}},{\\bm{v}}_{1},{\\bm{v}}_{3},\\dots,{\\bm{v}}_{d-1}}}\\right\\}</annotation></semantics></math>, where <math id=\"A3.SS3.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"{\\bm{u}}=\\epsilon{\\bm{v}}_{2}+\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{d}\" display=\"inline\"><semantics><mrow><mi>𝒖</mi><mo>=</mo><mrow><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}=\\epsilon{\\bm{v}}_{2}+\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{d}</annotation></semantics></math>, for some <math id=\"A3.SS3.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"0&lt;\\epsilon&lt;\\frac{1}{2}\" display=\"inline\"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>ϵ</mi><mo>&lt;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">0&lt;\\epsilon&lt;\\frac{1}{2}</annotation></semantics></math>. So columns of <math id=\"A3.SS3.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{1}\" display=\"inline\"><semantics><msub><mi>𝑾</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{W}}_{1}</annotation></semantics></math> consist of <math id=\"A3.SS3.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"{\\bm{u}},{\\bm{v}}_{1},{\\bm{v}}_{3},\\dots,{\\bm{v}}_{d-1}\" display=\"inline\"><semantics><mrow><mi>𝒖</mi><mo>,</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒗</mi><mn>3</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}},{\\bm{v}}_{1},{\\bm{v}}_{3},\\dots,{\\bm{v}}_{d-1}</annotation></semantics></math>. The subspace for the second task is spanned by <math id=\"A3.SS3.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{d}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{d}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"A3.SS3.SSS0.Px2\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Simplifying the forgetting expression</h5>\n\n<div id=\"A3.SS3.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We can check that <math id=\"A3.SS3.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{d}\" display=\"inline\"><semantics><mrow><msub><mi>𝒖</mi><mo>⟂</mo></msub><mo>=</mo><mrow><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{d}</annotation></semantics></math> spans the null space of the first task, that is <math id=\"A3.SS3.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1}={\\bm{u}}_{\\perp}{\\bm{u}}_{\\perp}^{\\top}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo>=</mo><mrow><msub><mi>𝒖</mi><mo>⟂</mo></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒖</mi><mo>⟂</mo><mo>⊤</mo></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1}={\\bm{u}}_{\\perp}{\\bm{u}}_{\\perp}^{\\top}</annotation></semantics></math>.\nPlugging this in the forgetting expression (<a href=\"#S3.E7\" title=\"In Proposition 3.4. ‣ 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>) we get</p>\n<table id=\"A4.EGx40\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E59\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E59.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E59.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=\\mathopen{}\\mathclose{{\\left\\|{(\\mathbf{I}-{\\bm{P}}_{1}){\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mo>=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐈</mi><mo>−</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\">−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathopen{}\\mathclose{{\\left\\|{(\\mathbf{I}-{\\bm{P}}_{1}){\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(59)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E60\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E60.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=a^{2}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}-a^{2}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}_{\\perp}{\\bm{u}}_{\\perp}^{\\top}{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mo>=</mo><msup><mi>a</mi><mn>2</mn></msup><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\">−</mo><msup><mi>a</mi><mn>2</mn></msup><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝒖</mi><mo>⟂</mo></msub><msubsup><mi>𝒖</mi><mo>⟂</mo><mo>⊤</mo></msubsup><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=a^{2}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}-a^{2}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}_{\\perp}{\\bm{u}}_{\\perp}^{\\top}{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(60)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E61\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E61.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=a^{2}\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{4}}}\\right),\" display=\"inline\"><semantics><mrow><mo>=</mo><msup><mi>a</mi><mn>2</mn></msup><mrow><mi></mi><mrow><mo>(</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\">−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>4</mn></msubsup></mrow></mrow><mo>)</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=a^{2}\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{4}}}\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(61)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">where <math id=\"A3.SS3.SSS0.Px2.p2.m1\" class=\"ltx_Math\" alttext=\"a={\\bm{u}}_{\\perp}^{\\top}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msubsup><mi>𝒖</mi><mo>⟂</mo><mo>⊤</mo></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">a={\\bm{u}}_{\\perp}^{\\top}{\\bm{w}}^{*}</annotation></semantics></math>, and last equality follows from <math id=\"A3.SS3.SSS0.Px2.p2.m2\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}_{\\perp}{\\bm{u}}_{\\perp}^{\\top}{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=({\\bm{u}}_{\\perp}^{\\top}{\\bm{P}}_{2}{\\bm{u}}_{\\perp})^{2}=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{4}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝒖</mi><mo>⟂</mo></msub><msubsup><mi>𝒖</mi><mo>⟂</mo><mo>⊤</mo></msubsup><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><msup><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>𝒖</mi><mo>⟂</mo><mo>⊤</mo></msubsup><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>4</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{u}}_{\\perp}{\\bm{u}}_{\\perp}^{\\top}{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=({\\bm{u}}_{\\perp}^{\\top}{\\bm{P}}_{2}{\\bm{u}}_{\\perp})^{2}=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{4}</annotation></semantics></math>.\nIt is then easy to see that forgetting is maximized when <math id=\"A3.SS3.SSS0.Px2.p2.m3\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=\\frac{1}{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=\\frac{1}{2}</annotation></semantics></math>, as shown in <cite class=\"ltx_cite ltx_citemacro_cite\">Evron et al. (<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</p>\n</div>\n</section>\n<section id=\"A3.SS3.SSS0.Px3\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Forgetting without replay</h5>\n\n<div id=\"A3.SS3.SSS0.Px3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">To find forgetting without replay, it would suffice to compute <a href=\"#A3.E61\" title=\"In Simplifying the forgetting expression ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">61</span></a>.\nThe null space of the second task is spanned by <math id=\"A3.SS3.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d-1}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d-1}</annotation></semantics></math>, so projection of <math id=\"A3.SS3.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{d}\" display=\"inline\"><semantics><mrow><msub><mi>𝒖</mi><mo>⟂</mo></msub><mo>=</mo><mrow><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>−</mo><mrow><mi>ϵ</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{d}</annotation></semantics></math> into this null space would just be <math id=\"A3.SS3.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{2}{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow><mo>=</mo><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{2}{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}</annotation></semantics></math>.\nThen forgetting without replay would be</p>\n<table id=\"A4.EGx41\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E62\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E62.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=a^{2}\\cdot\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})-(1-\\epsilon^{2})^{2}}}\\right)=a^{2}\\cdot\\epsilon^{2}\\cdot(1-\\epsilon^{2}).\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><msup><mi>a</mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mo>−</mo><msup><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>)</mo><mo>=</mo><mi>a</mi><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>ϵ</mi><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=a^{2}\\cdot\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})-(1-\\epsilon^{2})^{2}}}\\right)=a^{2}\\cdot\\epsilon^{2}\\cdot(1-\\epsilon^{2}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(62)</span></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"A3.SS3.SSS0.Px4\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Forgetting with replay</h5>\n\n<div id=\"A3.SS3.SSS0.Px4.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">By <a href=\"#A3.Thmtheorem2\" title=\"Claim C.2. ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a>, to find expected forgetting with replay, we need to find <math id=\"A3.SS3.SSS0.Px4.p1.m1\" class=\"ltx_math_unparsed\" alttext=\"\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]</annotation></semantics></math>, where the expectation is over the randomness in <math id=\"A3.SS3.SSS0.Px4.p1.m2\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math>.\nWe can use the same steps as in <a href=\"#A3.E59\" title=\"In Simplifying the forgetting expression ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">59</span></a> to get</p>\n<table id=\"A4.EGx42\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E63\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E63.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=a^{2}\\cdot\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\\cdot\\mathopen{}\\mathclose{{\\left(1-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><msup><mi>a</mi><mn>2</mn></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}=a^{2}\\cdot\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\\cdot\\mathopen{}\\mathclose{{\\left(1-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(63)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">To understand <math id=\"A3.SS3.SSS0.Px4.p2.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}</annotation></semantics></math>, we first focus on the span of the second tasks samples combined with replay samples. Note that the <math id=\"A3.SS3.SSS0.Px4.p2.m2\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> replay samples from the first task have the form <math id=\"A3.SS3.SSS0.Px4.p2.m3\" class=\"ltx_Math\" alttext=\"\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}+\\dots+\\alpha_{d-1}{\\bm{v}}_{d-1}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><msub><mi>α</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒖</mi></mrow><mo>+</mo><mi mathvariant=\"normal\">⋯</mi><mo>+</mo><mrow><msub><mi>α</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1}{\\bm{v}}_{1}+\\alpha_{2}{\\bm{u}}+\\dots+\\alpha_{d-1}{\\bm{v}}_{d-1}</annotation></semantics></math>, where <math id=\"A3.SS3.SSS0.Px4.p2.m4\" class=\"ltx_Math\" alttext=\"\\alpha_{i}\\sim\\sf{N}(0,\\frac{1}{d-1})\" display=\"inline\"><semantics><mrow><msub><mi>α</mi><mi>i</mi></msub><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mfrac><mn>𝟣</mn><mrow><mi>𝖽</mi><mo>−</mo><mn>𝟣</mn></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{i}\\sim\\sf{N}(0,\\frac{1}{d-1})</annotation></semantics></math>. Since the second task is only spanned by <math id=\"A3.SS3.SSS0.Px4.p2.m5\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{d}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{d}</annotation></semantics></math>, we can make the replay samples orthogonal to <math id=\"A3.SS3.SSS0.Px4.p2.m6\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{2}\" display=\"inline\"><semantics><msub><mi>𝚷</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{2}</annotation></semantics></math> by subtracting this component from each sample. Then span of the second task, including the <math id=\"A3.SS3.SSS0.Px4.p2.m7\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> replay samples is</p>\n<table id=\"A4.EGx43\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E64\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E64.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{d},\\tilde{{\\mathbf{Z}}}_{11},\\dots,\\tilde{{\\mathbf{Z}}}_{1m}}}\\right\\}\" display=\"inline\"><semantics><mrow><mi>span</mi><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mi>d</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mn>11</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{d},\\tilde{{\\mathbf{Z}}}_{11},\\dots,\\tilde{{\\mathbf{Z}}}_{1m}}}\\right\\}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(64)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">where <math id=\"A3.SS3.SSS0.Px4.p3.m1\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}_{1i}=\\alpha_{1}^{(i)}\\cdot{\\bm{v}}_{1}+\\alpha_{2}^{(i)}\\cdot\\epsilon{\\bm{v}}_{2}+\\dots+\\alpha_{d-1}^{(i)}{\\bm{v}}_{d-1}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><mrow><msubsup><mi>α</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><mrow><msubsup><mi>α</mi><mn>2</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mi>ϵ</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>+</mo><mi mathvariant=\"normal\">⋯</mi><mo>+</mo><mrow><msubsup><mi>α</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}_{1i}=\\alpha_{1}^{(i)}\\cdot{\\bm{v}}_{1}+\\alpha_{2}^{(i)}\\cdot\\epsilon{\\bm{v}}_{2}+\\dots+\\alpha_{d-1}^{(i)}{\\bm{v}}_{d-1}</annotation></semantics></math>, and <math id=\"A3.SS3.SSS0.Px4.p3.m2\" class=\"ltx_Math\" alttext=\"\\alpha_{j}^{(i)}\\sim\\sf{N}(0,1)\" display=\"inline\"><semantics><mrow><msubsup><mi>α</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mn>𝟣</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{j}^{(i)}\\sim\\sf{N}(0,1)</annotation></semantics></math>.\nThat is, <math id=\"A3.SS3.SSS0.Px4.p3.m3\" class=\"ltx_Math\" alttext=\"{\\bm{u}}\" display=\"inline\"><semantics><mi>𝒖</mi><annotation encoding=\"application/x-tex\">{\\bm{u}}</annotation></semantics></math> is replaced by <math id=\"A3.SS3.SSS0.Px4.p3.m4\" class=\"ltx_Math\" alttext=\"\\epsilon{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\epsilon{\\bm{v}}_{2}</annotation></semantics></math>.\nWe have ignored the multiplicative factor of <math id=\"A3.SS3.SSS0.Px4.p3.m5\" class=\"ltx_Math\" alttext=\"\\frac{1}{d-1}\" display=\"inline\"><semantics><mfrac><mn>1</mn><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{d-1}</annotation></semantics></math>, since that would not affect the span. Null space of <math id=\"A3.SS3.SSS0.Px4.p3.m6\" class=\"ltx_math_unparsed\" alttext=\"\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{d},\\tilde{{\\mathbf{Z}}}_{11},\\dots,\\tilde{{\\mathbf{Z}}}_{1m}}}\\right\\}\" display=\"inline\"><semantics><mrow><mi>span</mi><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mi>d</mi></msub><mo>,</mo><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mn>11</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{span}\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{d},\\tilde{{\\mathbf{Z}}}_{11},\\dots,\\tilde{{\\mathbf{Z}}}_{1m}}}\\right\\}</annotation></semantics></math> is orthogonal to <math id=\"A3.SS3.SSS0.Px4.p3.m7\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{d}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{d}</annotation></semantics></math>, so <math id=\"A3.SS3.SSS0.Px4.p3.m8\" class=\"ltx_math_unparsed\" alttext=\"\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}=\\tilde{{\\bm{P}}}_{2}\\mathopen{}\\mathclose{{\\left(\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{d}}}\\right)=\\sqrt{1-\\epsilon^{2}}\\cdot\\tilde{{\\bm{P}}}_{2}{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub><mo>=</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mrow><mi></mi><mrow><mo>(</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mn>2</mn></msub><mo>−</mo><mi>ϵ</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow><mo>)</mo><mo>=</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><msub><mi></mi><mn>2</mn></msub><mi>𝒗</mi><msub><mi></mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}=\\tilde{{\\bm{P}}}_{2}\\mathopen{}\\mathclose{{\\left(\\sqrt{1-\\epsilon^{2}}\\cdot{\\bm{v}}_{2}-\\epsilon\\cdot{\\bm{v}}_{d}}}\\right)=\\sqrt{1-\\epsilon^{2}}\\cdot\\tilde{{\\bm{P}}}_{2}{\\bm{v}}_{2}</annotation></semantics></math>.\nLet <math id=\"A3.SS3.SSS0.Px4.p3.m9\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{\\Pi}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{\\Pi}}}_{2}</annotation></semantics></math> be the orthogonal projection into the span of <math id=\"A3.SS3.SSS0.Px4.p3.m10\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{\\tilde{{\\mathbf{Z}}}_{11},\\dots,\\tilde{{\\mathbf{Z}}}_{1m}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mn>11</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{\\tilde{{\\mathbf{Z}}}_{11},\\dots,\\tilde{{\\mathbf{Z}}}_{1m}}}\\right\\}</annotation></semantics></math>, then we can write <math id=\"A3.SS3.SSS0.Px4.p3.m11\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}=\\mathbf{I}-({\\bm{v}}_{d}{\\bm{v}}_{d}^{\\top}+\\tilde{{\\bm{\\Pi}}}_{2})\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo>=</mo><mrow><mi>𝐈</mi><mo>−</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>𝒗</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>𝒗</mi><mi>d</mi><mo>⊤</mo></msubsup></mrow><mo>+</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}=\\mathbf{I}-({\\bm{v}}_{d}{\\bm{v}}_{d}^{\\top}+\\tilde{{\\bm{\\Pi}}}_{2})</annotation></semantics></math>. Next, we focus on <math id=\"A3.SS3.SSS0.Px4.p3.m12\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}</annotation></semantics></math>.\nWithout loss of generality, we can assume that <math id=\"A3.SS3.SSS0.Px4.p3.m13\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d-1}\" display=\"inline\"><semantics><mrow><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d-1}</annotation></semantics></math> are canonical basis vectors, and <math id=\"A3.SS3.SSS0.Px4.p3.m14\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}_{1i}\\sim\\sf{N}(0,\\Sigma)\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mi class=\"ltx_mathvariant_sans-serif\" mathvariant=\"sans-serif\">Σ</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}_{1i}\\sim\\sf{N}(0,\\Sigma)</annotation></semantics></math> where <math id=\"A3.SS3.SSS0.Px4.p3.m15\" class=\"ltx_Math\" alttext=\"\\Sigma\" display=\"inline\"><semantics><mi mathvariant=\"normal\">Σ</mi><annotation encoding=\"application/x-tex\">\\Sigma</annotation></semantics></math> is a <math id=\"A3.SS3.SSS0.Px4.p3.m16\" class=\"ltx_Math\" alttext=\"d-1\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d-1</annotation></semantics></math> dimensional diagonal matrix,</p>\n<table id=\"A4.EGx44\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E65\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E65.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\Sigma=\\begin{bmatrix}1&amp;&amp;\\\\\n&amp;\\epsilon^{2}&amp;\\\\\n&amp;&amp;\\mathbf{I}_{d-3}\\end{bmatrix}.\" display=\"inline\"><semantics><mrow><mrow><mi mathvariant=\"normal\">Σ</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing=\"5pt\" rowspacing=\"0pt\"><mtr><mtd><mn>1</mn></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd><msup><mi>ϵ</mi><mn>2</mn></msup></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd></mtd><mtd><msub><mi>𝐈</mi><mrow><mi>d</mi><mo>−</mo><mn>3</mn></mrow></msub></mtd></mtr></mtable><mo>]</mo></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Sigma=\\begin{bmatrix}1&amp;&amp;\\\\\n&amp;\\epsilon^{2}&amp;\\\\\n&amp;&amp;\\mathbf{I}_{d-3}\\end{bmatrix}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(65)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Equivalently, we could write <math id=\"A3.SS3.SSS0.Px4.p3.m17\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}_{1i}=\\Sigma^{1/2}{{\\mathbf{Z}}^{\\prime}}_{1i}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><msup><mi mathvariant=\"normal\">Σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow><mrow></mrow></mmultiscripts></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}_{1i}=\\Sigma^{1/2}{{\\mathbf{Z}}^{\\prime}}_{1i}</annotation></semantics></math>, where <math id=\"A3.SS3.SSS0.Px4.p3.m18\" class=\"ltx_Math\" alttext=\"{{\\mathbf{Z}}^{\\prime}}_{1i}\\sim\\sf{N}(0,\\mathbf{I}_{d-1})\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow><mrow></mrow></mmultiscripts><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><msub><mi>𝐈</mi><mrow><mi>𝖽</mi><mo>−</mo><mn>𝟣</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{{\\mathbf{Z}}^{\\prime}}_{1i}\\sim\\sf{N}(0,\\mathbf{I}_{d-1})</annotation></semantics></math>. Collecting what we have so far we get</p>\n<table id=\"A4.EGx45\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E66\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E66.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\mathopen{}\\mathclose{{\\left(\\mathbf{I}-{\\bm{v}}_{d}{\\bm{v}}_{d}^{\\top}-\\tilde{{\\bm{\\Pi}}}_{2}}}\\right){\\bm{v}}_{2}=\\sqrt{1-\\epsilon^{2}}\\mathopen{}\\mathclose{{\\left(\\mathbf{I}-\\tilde{{\\bm{\\Pi}}}_{2}}}\\right){\\bm{v}}_{2},\" display=\"inline\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub><mo>=</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mrow><mi></mi><mrow><mo>(</mo><mi>𝐈</mi><mo>−</mo><msub><mi>𝒗</mi><mi>d</mi></msub><msubsup><mi>𝒗</mi><mi>d</mi><mo>⊤</mo></msubsup><mo>−</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub></mrow></mrow><mo>)</mo></mrow><msub><mi>𝒗</mi><mn>2</mn></msub><mo>=</mo><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mrow><mi></mi><mrow><mo>(</mo><mi>𝐈</mi><mo>−</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub></mrow></mrow><mo>)</mo><mi>𝒗</mi><msub><mi></mi><mn>2</mn></msub><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}=\\sqrt{1-\\epsilon^{2}}\\mathopen{}\\mathclose{{\\left(\\mathbf{I}-{\\bm{v}}_{d}{\\bm{v}}_{d}^{\\top}-\\tilde{{\\bm{\\Pi}}}_{2}}}\\right){\\bm{v}}_{2}=\\sqrt{1-\\epsilon^{2}}\\mathopen{}\\mathclose{{\\left(\\mathbf{I}-\\tilde{{\\bm{\\Pi}}}_{2}}}\\right){\\bm{v}}_{2},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(66)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">since <math id=\"A3.SS3.SSS0.Px4.p3.m19\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{d}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{d}</annotation></semantics></math> is orthogonal to <math id=\"A3.SS3.SSS0.Px4.p3.m20\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{2}</annotation></semantics></math>.\nWe can write</p>\n<table id=\"A4.EGx46\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E67\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E67.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}}}\\right)=(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\">−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}}}\\right)=(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(67)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p4\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">To bound expectation of the term given in <a href=\"#A3.E63\" title=\"In Forgetting with replay ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">63</span></a>, we first give high probability upper and lower bounds for <math id=\"A3.SS3.SSS0.Px4.p4.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p5\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Let <math id=\"A3.SS3.SSS0.Px4.p5.m1\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}\" display=\"inline\"><semantics><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}</annotation></semantics></math> be the <math id=\"A3.SS3.SSS0.Px4.p5.m2\" class=\"ltx_Math\" alttext=\"m\\times d-1\" display=\"inline\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">m\\times d-1</annotation></semantics></math> dimensional matrix that has <math id=\"A3.SS3.SSS0.Px4.p5.m3\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}_{1i}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}_{1i}</annotation></semantics></math> as rows, and similarly, let <math id=\"A3.SS3.SSS0.Px4.p5.m4\" class=\"ltx_Math\" alttext=\"{{\\mathbf{Z}}^{\\prime}}\" display=\"inline\"><semantics><msup><mi>𝐙</mi><mo>′</mo></msup><annotation encoding=\"application/x-tex\">{{\\mathbf{Z}}^{\\prime}}</annotation></semantics></math> be the <math id=\"A3.SS3.SSS0.Px4.p5.m5\" class=\"ltx_Math\" alttext=\"m\\times d-1\" display=\"inline\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">m\\times d-1</annotation></semantics></math> dimensional matrix with <math id=\"A3.SS3.SSS0.Px4.p5.m6\" class=\"ltx_Math\" alttext=\"{{\\mathbf{Z}}^{\\prime}}_{1i}\" display=\"inline\"><semantics><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>i</mi></mrow><mrow></mrow></mmultiscripts><annotation encoding=\"application/x-tex\">{{\\mathbf{Z}}^{\\prime}}_{1i}</annotation></semantics></math> in the rows. Then we can write <math id=\"A3.SS3.SSS0.Px4.p5.m7\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}={{\\mathbf{Z}}^{\\prime}}\\Sigma^{1/2}\" display=\"inline\"><semantics><mrow><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mo>=</mo><mrow><msup><mi>𝐙</mi><mo>′</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi mathvariant=\"normal\">Σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}={{\\mathbf{Z}}^{\\prime}}\\Sigma^{1/2}</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p6\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">One way to write the projection is <math id=\"A3.SS3.SSS0.Px4.p6.m1\" class=\"ltx_math_unparsed\" alttext=\"\\tilde{{\\bm{\\Pi}}}_{2}=\\tilde{{\\mathbf{Z}}}^{\\top}\\mathopen{}\\mathclose{{\\left(\\tilde{{\\mathbf{Z}}}\\tilde{{\\mathbf{Z}}}^{\\top}}}\\right)^{-1}\\tilde{{\\mathbf{Z}}}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><mo>=</mo><msup><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mo>⊤</mo></msup><mrow><mi></mi><mrow><mo>(</mo><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><msup><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mo>⊤</mo></msup></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{\\Pi}}}_{2}=\\tilde{{\\mathbf{Z}}}^{\\top}\\mathopen{}\\mathclose{{\\left(\\tilde{{\\mathbf{Z}}}\\tilde{{\\mathbf{Z}}}^{\\top}}}\\right)^{-1}\\tilde{{\\mathbf{Z}}}</annotation></semantics></math>, this can be checked by writing the singular value decomposition of <math id=\"A3.SS3.SSS0.Px4.p6.m2\" class=\"ltx_Math\" alttext=\"\\tilde{{\\mathbf{Z}}}\" display=\"inline\"><semantics><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{{\\mathbf{Z}}}</annotation></semantics></math>. Going back to calculating\n<math id=\"A3.SS3.SSS0.Px4.p6.m3\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}</annotation></semantics></math>,</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p7\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx47\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E68\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E68.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}={\\bm{v}}_{2}^{\\top}\\tilde{{\\mathbf{Z}}}^{\\top}\\mathopen{}\\mathclose{{\\left(\\tilde{{\\mathbf{Z}}}\\tilde{{\\mathbf{Z}}}^{\\top}}}\\right)^{-1}\\tilde{{\\mathbf{Z}}}{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><msup><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mo>⊤</mo></msup><mrow><mi></mi><mrow><mo>(</mo><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><msup><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mo>⊤</mo></msup></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mover accent=\"true\"><mi>𝐙</mi><mo>~</mo></mover><mi>𝒗</mi><msub><mi></mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}={\\bm{v}}_{2}^{\\top}\\tilde{{\\mathbf{Z}}}^{\\top}\\mathopen{}\\mathclose{{\\left(\\tilde{{\\mathbf{Z}}}\\tilde{{\\mathbf{Z}}}^{\\top}}}\\right)^{-1}\\tilde{{\\mathbf{Z}}}{\\bm{v}}_{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E68.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle={\\bm{v}}_{2}^{\\top}\\Sigma^{1/2}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}\\Sigma^{1/2}{\\bm{v}}_{2}.\" display=\"inline\"><semantics><mrow><mo>=</mo><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><msup><mi mathvariant=\"normal\">Σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mi mathvariant=\"normal\">Σ</mi><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐙</mi><msup><mi></mi><mo>′</mo></msup><mi mathvariant=\"normal\">Σ</mi><msup><mi></mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mi>𝒗</mi><msub><mi></mi><mn>2</mn></msub><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle={\\bm{v}}_{2}^{\\top}\\Sigma^{1/2}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}\\Sigma^{1/2}{\\bm{v}}_{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(68)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p8\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Note that <math id=\"A3.SS3.SSS0.Px4.p8.m1\" class=\"ltx_Math\" alttext=\"\\Sigma^{1/2}{\\bm{v}}_{2}=\\epsilon{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><mrow><msup><mi mathvariant=\"normal\">Σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>=</mo><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\Sigma^{1/2}{\\bm{v}}_{2}=\\epsilon{\\bm{v}}_{2}</annotation></semantics></math>, and since <math id=\"A3.SS3.SSS0.Px4.p8.m2\" class=\"ltx_Math\" alttext=\"\\Sigma\\succeq\\epsilon^{2}\\mathbf{I}_{d-1}\" display=\"inline\"><semantics><mrow><mi mathvariant=\"normal\">Σ</mi><mo>⪰</mo><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝐈</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\Sigma\\succeq\\epsilon^{2}\\mathbf{I}_{d-1}</annotation></semantics></math>, <math id=\"A3.SS3.SSS0.Px4.p8.m3\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}\\preceq\\frac{1}{\\epsilon^{2}}\\cdot\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}\" display=\"inline\"><semantics><mrow><msup><mrow><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mi mathvariant=\"normal\">Σ</mi><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⪯</mo><mfrac><mn>1</mn><msup><mi>ϵ</mi><mn>2</mn></msup></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}\\preceq\\frac{1}{\\epsilon^{2}}\\cdot\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}</annotation></semantics></math>. Then</p>\n<table id=\"A4.EGx48\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E69\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E69.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq{\\bm{v}}_{2}^{\\top}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}{\\bm{v}}_{2}.\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐙</mi><msup><mi></mi><mo>′</mo></msup><mi>𝒗</mi><msub><mi></mi><mn>2</mn></msub><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq{\\bm{v}}_{2}^{\\top}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}{\\bm{v}}_{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(69)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p9\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">On the other hand, since <math id=\"A3.SS3.SSS0.Px4.p9.m1\" class=\"ltx_Math\" alttext=\"\\Sigma\\preceq\\mathbf{I}_{d-1}\" display=\"inline\"><semantics><mrow><mi mathvariant=\"normal\">Σ</mi><mo>⪯</mo><msub><mi>𝐈</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\Sigma\\preceq\\mathbf{I}_{d-1}</annotation></semantics></math>, <math id=\"A3.SS3.SSS0.Px4.p9.m2\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}\\preceq\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}\" display=\"inline\"><semantics><mrow><msup><mrow><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⪯</mo><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mi mathvariant=\"normal\">Σ</mi><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}\\preceq\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}</annotation></semantics></math>, so</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p10\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx49\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E70\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E70.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}={\\bm{v}}_{2}^{\\top}\\Sigma^{1/2}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}\\Sigma^{1/2}{\\bm{v}}_{2}\\geq\\epsilon^{2}{\\bm{v}}_{2}^{\\top}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><msup><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><msup><mi mathvariant=\"normal\">Σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mi mathvariant=\"normal\">Σ</mi><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>𝐙</mi><mo>′</mo></msup><msup><mi mathvariant=\"normal\">Σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><msub><mi>𝒗</mi><mn>2</mn></msub><mo>≥</mo><msup><mi>ϵ</mi><mn>2</mn></msup><msubsup><mi>𝒗</mi><mn>2</mn><mo>⊤</mo></msubsup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐙</mi><msup><mi></mi><mo>′</mo></msup><mi>𝒗</mi><msub><mi></mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}={\\bm{v}}_{2}^{\\top}\\Sigma^{1/2}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}\\Sigma{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}\\Sigma^{1/2}{\\bm{v}}_{2}\\geq\\epsilon^{2}{\\bm{v}}_{2}^{\\top}{{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}^{\\top}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}{\\bm{v}}_{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(70)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p11\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Recall that <math id=\"A3.SS3.SSS0.Px4.p11.m1\" class=\"ltx_Math\" alttext=\"{{\\mathbf{Z}}^{\\prime}}\" display=\"inline\"><semantics><msup><mi>𝐙</mi><mo>′</mo></msup><annotation encoding=\"application/x-tex\">{{\\mathbf{Z}}^{\\prime}}</annotation></semantics></math> is a <math id=\"A3.SS3.SSS0.Px4.p11.m2\" class=\"ltx_Math\" alttext=\"m\\times(d-1)\" display=\"inline\"><semantics><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m\\times(d-1)</annotation></semantics></math> dimensional matrix whose entries are from a standard normal distribution, so <math id=\"A3.SS3.SSS0.Px4.p11.m3\" class=\"ltx_math_unparsed\" alttext=\"\\hat{{\\bm{P}}}\\vcentcolon={{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}\" display=\"inline\"><semantics><mrow><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><mo>:=</mo><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝐙</mi><mo>′</mo></msup><msup><mi>𝐙</mi><mo>′</mo></msup></mrow></mrow><mo>)</mo><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>𝐙</mi><msup><mi></mi><mo>′</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{P}}}\\vcentcolon={{\\mathbf{Z}}^{\\prime}}^{\\top}\\mathopen{}\\mathclose{{\\left({{\\mathbf{Z}}^{\\prime}}{{\\mathbf{Z}}^{\\prime}}}}\\right)^{-1}{{\\mathbf{Z}}^{\\prime}}</annotation></semantics></math>, is a random projection into a <math id=\"A3.SS3.SSS0.Px4.p11.m4\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> dimensional subspace of <math id=\"A3.SS3.SSS0.Px4.p11.m5\" class=\"ltx_Math\" alttext=\"d-1\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d-1</annotation></semantics></math> dimensional space.</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p12\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Calculations above show that</p>\n<table id=\"A4.EGx50\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E71\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E71.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\epsilon^{2}\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\epsilon^{2}\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(71)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p13\" class=\"ltx_para\">\n<p class=\"ltx_p\">As described in <cite class=\"ltx_cite ltx_citemacro_citet\">Dasgupta &amp; Gupta (<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2003</a>)</cite>, <math id=\"A3.SS3.SSS0.Px4.p13.m1\" class=\"ltx_Math\" alttext=\"\\hat{{\\bm{P}}}{\\bm{v}}_{2}\" display=\"inline\"><semantics><mrow><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{P}}}{\\bm{v}}_{2}</annotation></semantics></math> has the same distribution as a fixed projection (into an <math id=\"A3.SS3.SSS0.Px4.p13.m2\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> dimensional subspace) of a vector <math id=\"A3.SS3.SSS0.Px4.p13.m3\" class=\"ltx_Math\" alttext=\"\\hat{{\\bm{u}}}\" display=\"inline\"><semantics><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{{\\bm{u}}}</annotation></semantics></math> picked uniformly at random from the sphere. This fixed subspace can be chosen to be the first <math id=\"A3.SS3.SSS0.Px4.p13.m4\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> coordinates. Lemma 2.2 of <cite class=\"ltx_cite ltx_citemacro_citet\">Dasgupta &amp; Gupta (<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2003</a>)</cite>, gives concentration bounds on <math id=\"A3.SS3.SSS0.Px4.p13.m5\" class=\"ltx_Math\" alttext=\"\\hat{{\\bm{u}}}_{1}^{2}+\\hat{{\\bm{u}}}_{2}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}\" display=\"inline\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant=\"normal\">⋯</mi><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mi>m</mi><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{u}}}_{1}^{2}+\\hat{{\\bm{u}}}_{2}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}</annotation></semantics></math>. We include this lemma here using our notation for convenience.</p>\n</div>\n<div id=\"A3.Thmtheorem4\" class=\"ltx_theorem ltx_theorem_lemma\">\n<h6 class=\"ltx_title ltx_runin ltx_title_theorem\">\n<span class=\"ltx_tag ltx_tag_theorem\"><span class=\"ltx_text ltx_font_bold\">Lemma C.4</span></span><span class=\"ltx_text ltx_font_bold\"> </span>(Lemma 2.2, <cite class=\"ltx_cite ltx_citemacro_citet\">Dasgupta &amp; Gupta (<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2003</a>)</cite>)<span class=\"ltx_text ltx_font_bold\">.</span>\n</h6>\n<div id=\"A3.Thmtheorem4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Assume <math id=\"A3.Thmtheorem4.p1.m1\" class=\"ltx_Math\" alttext=\"m&lt;d-1\" display=\"inline\"><semantics><mrow><mi>m</mi><mo>&lt;</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">m&lt;d-1</annotation></semantics></math>. Then for <math id=\"A3.Thmtheorem4.p1.m2\" class=\"ltx_Math\" alttext=\"t&lt;1\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t&lt;1</annotation></semantics></math></span></p>\n<table id=\"A4.EGx51\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E72\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E72.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\hat{{\\bm{u}}}_{1}^{2}+\\hat{{\\bm{u}}}_{2}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}\\leq\\frac{tm}{d-1}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(\\frac{m}{2}\\mathopen{}\\mathclose{{\\left(1-t+\\ln t}}\\right)}}\\right),\" display=\"inline\"><semantics><mrow><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant=\"normal\">⋯</mi><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mi>m</mi><mn>2</mn></msubsup><mo>≤</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><mo>≤</mo><mi>exp</mi><mrow><mi></mi><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mn>2</mn></mfrac></mstyle><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>t</mi><mo>+</mo><mi>ln</mi><mi>t</mi></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\hat{{\\bm{u}}}_{1}^{2}+\\hat{{\\bm{u}}}_{2}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}\\leq\\frac{tm}{d-1}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(\\frac{m}{2}\\mathopen{}\\mathclose{{\\left(1-t+\\ln t}}\\right)}}\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(72)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">and for <math id=\"A3.Thmtheorem4.p1.m3\" class=\"ltx_Math\" alttext=\"t&gt;1\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t&gt;1</annotation></semantics></math></span></p>\n<table id=\"A4.EGx52\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E73\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E73.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\hat{{\\bm{u}}}_{1}^{2}+\\hat{{\\bm{u}}}_{2}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}\\geq\\frac{tm}{d-1}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(\\frac{m}{2}\\mathopen{}\\mathclose{{\\left(1-t+\\ln t}}\\right)}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant=\"normal\">⋯</mi><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mi>m</mi><mn>2</mn></msubsup><mo>≥</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><mo>≤</mo><mi>exp</mi><mrow><mi></mi><mrow><mo>(</mo><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mn>2</mn></mfrac></mstyle><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>t</mi><mo>+</mo><mi>ln</mi><mi>t</mi></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\hat{{\\bm{u}}}_{1}^{2}+\\hat{{\\bm{u}}}_{2}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}\\geq\\frac{tm}{d-1}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(\\frac{m}{2}\\mathopen{}\\mathclose{{\\left(1-t+\\ln t}}\\right)}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(73)</span></td>\n</tr></tbody>\n</table>\n</div>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p14\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Since <math id=\"A3.SS3.SSS0.Px4.p14.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}</annotation></semantics></math> has the same distribution as <math id=\"A3.SS3.SSS0.Px4.p14.m2\" class=\"ltx_Math\" alttext=\"\\hat{{\\bm{u}}}_{1}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}\" display=\"inline\"><semantics><mrow><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><mi mathvariant=\"normal\">⋯</mi><mo>+</mo><msubsup><mover accent=\"true\"><mi>𝒖</mi><mo>^</mo></mover><mi>m</mi><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{u}}}_{1}^{2}+\\dots+\\hat{{\\bm{u}}}_{m}^{2}</annotation></semantics></math>, we can apply the lemma above to get that for <math id=\"A3.SS3.SSS0.Px4.p14.m3\" class=\"ltx_Math\" alttext=\"t=\\frac{1}{30}\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>=</mo><mfrac><mn>1</mn><mn>30</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">t=\\frac{1}{30}</annotation></semantics></math></p>\n<table id=\"A4.EGx53\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E74\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E74.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\frac{m}{30(d-1)}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(-m}}\\right),\" display=\"inline\"><semantics><mrow><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><mo>≤</mo><mi>exp</mi><mrow><mi></mi><mrow><mo>(</mo><mo lspace=\"0em\">−</mo><mi>m</mi></mrow></mrow><mo>)</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\frac{m}{30(d-1)}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(-m}}\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(74)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">and for <math id=\"A3.SS3.SSS0.Px4.p14.m4\" class=\"ltx_Math\" alttext=\"t=5\" display=\"inline\"><semantics><mrow><mi>t</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">t=5</annotation></semantics></math>,</p>\n<table id=\"A4.EGx54\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E75\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E75.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\geq\\frac{5m}{d-1}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(-m}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mover accent=\"true\"><mi>𝑷</mi><mo>^</mo></mover><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≥</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><mo>≤</mo><mi>exp</mi><mrow><mi></mi><mrow><mo>(</mo><mo lspace=\"0em\">−</mo><mi>m</mi></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{\\hat{{\\bm{P}}}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\geq\\frac{5m}{d-1}}}\\right]\\leq\\exp\\mathopen{}\\mathclose{{\\left(-m}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(75)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Using <a href=\"#A3.E71\" title=\"In Forgetting with replay ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">71</span></a>, we have that with probability of at least <math id=\"A3.SS3.SSS0.Px4.p14.m5\" class=\"ltx_Math\" alttext=\"1-2\\exp(-m)\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">1-2\\exp(-m)</annotation></semantics></math></p>\n<table id=\"A4.EGx55\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E76\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E76.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\epsilon^{2}\\frac{m}{30(d-1)}\\leq\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\frac{5m}{d-1}.\" display=\"inline\"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle><mo>≤</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\epsilon^{2}\\frac{m}{30(d-1)}\\leq\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}\\leq\\frac{5m}{d-1}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(76)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Recall from <a href=\"#A3.E67\" title=\"In Forgetting with replay ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">67</span></a> that <math id=\"A3.SS3.SSS0.Px4.p14.m6\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}}}\\right)\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">=</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝚷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒗</mi><mn>2</mn></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}=(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{\\Pi}}}_{2}{\\bm{v}}_{2}}}}\\right\\|_{2}^{2}}}\\right)</annotation></semantics></math>, so with probability of at least <math id=\"A3.SS3.SSS0.Px4.p14.m7\" class=\"ltx_Math\" alttext=\"1-2\\exp(-m)\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">1-2\\exp(-m)</annotation></semantics></math>,</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p15\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx56\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E77\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E77.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\frac{5m}{d-1}}}\\right)\\leq\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\\leq(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow><mo>≤</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\frac{5m}{d-1}}}\\right)\\leq\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\\leq(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(77)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Note that, we always have that <math id=\"A3.SS3.SSS0.Px4.p15.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\\leq 1-\\epsilon^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}\\leq 1-\\epsilon^{2}</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p16\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Define <math id=\"A3.SS3.SSS0.Px4.p16.m1\" class=\"ltx_Math\" alttext=\"f(x)=x(1-x)\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f(x)=x(1-x)</annotation></semantics></math>, from <a href=\"#A3.E63\" title=\"In Forgetting with replay ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">63</span></a> we can see that expected forgetting with replay is</p>\n<table id=\"A4.EGx57\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E78\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E78.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=a^{2}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[f\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right)}}\\right],\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow><mo>=</mo><msup><mi>a</mi><mn>2</mn></msup><mo lspace=\"0.167em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=a^{2}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[f\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right)}}\\right],</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(78)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">while expected forgetting without replay is <math id=\"A3.SS3.SSS0.Px4.p16.m2\" class=\"ltx_Math\" alttext=\"a^{2}f(1-\\epsilon^{2})\" display=\"inline\"><semantics><mrow><msup><mi>a</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">a^{2}f(1-\\epsilon^{2})</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p17\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Since <math id=\"A3.SS3.SSS0.Px4.p17.m1\" class=\"ltx_Math\" alttext=\"\\epsilon&lt;1/2\" display=\"inline\"><semantics><mrow><mi>ϵ</mi><mo>&lt;</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon&lt;1/2</annotation></semantics></math>, <math id=\"A3.SS3.SSS0.Px4.p17.m2\" class=\"ltx_Math\" alttext=\"1-\\epsilon^{2}&gt;3/4\" display=\"inline\"><semantics><mrow><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo>&gt;</mo><mrow><mn>3</mn><mo>/</mo><mn>4</mn></mrow></mrow><annotation encoding=\"application/x-tex\">1-\\epsilon^{2}&gt;3/4</annotation></semantics></math>. Assume that <math id=\"A3.SS3.SSS0.Px4.p17.m3\" class=\"ltx_Math\" alttext=\"\\frac{m}{d-1}&lt;\\frac{1}{15}\" display=\"inline\"><semantics><mrow><mfrac><mi>m</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>&lt;</mo><mfrac><mn>1</mn><mn>15</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{m}{d-1}&lt;\\frac{1}{15}</annotation></semantics></math> so that <math id=\"A3.SS3.SSS0.Px4.p17.m4\" class=\"ltx_math_unparsed\" alttext=\"\\frac{1}{2}&lt;(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\frac{5m}{d-1}}}\\right)\" display=\"inline\"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>&lt;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>m</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{1}{2}&lt;(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\frac{5m}{d-1}}}\\right)</annotation></semantics></math>.\nSince <math id=\"A3.SS3.SSS0.Px4.p17.m5\" class=\"ltx_Math\" alttext=\"f(x)\" display=\"inline\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(x)</annotation></semantics></math> is a concave function that is maximized at <math id=\"A3.SS3.SSS0.Px4.p17.m6\" class=\"ltx_Math\" alttext=\"1/2\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1/2</annotation></semantics></math>, it is monotonically decreasing in the interval <math id=\"A3.SS3.SSS0.Px4.p17.m7\" class=\"ltx_Math\" alttext=\"[1/2,1]\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1/2,1]</annotation></semantics></math>.\nThis means that if the bounds in <a href=\"#A3.E77\" title=\"In Forgetting with replay ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">77</span></a> hold,</p>\n<table id=\"A4.EGx58\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E79\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E79.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle f\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right)\\geq f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow><mo>≥</mo><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle f\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right)\\geq f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(79)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We can lower bound</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p18\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx59\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E80\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E80.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[f\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right)}}\\right]\\geq(1-2\\exp(-m))f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)+\\exp(-m)f\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}}}\\right).\" display=\"inline\"><semantics><mrow><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝒖</mi><mo>⟂</mo></msub></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mo>≥</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mn>2</mn><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\">−</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo>+</mo><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\">−</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mrow><mo>)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[f\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{u}}_{\\perp}}}}\\right\\|_{2}^{2}}}\\right)}}\\right]\\geq(1-2\\exp(-m))f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)+\\exp(-m)f\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}}}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(80)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">To show that expected forgetting would increase with replay, it would suffice to show that</p>\n<table id=\"A4.EGx60\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E81\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E81.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle(1-2\\exp(-m))f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)+\\exp(-m)f\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}}}\\right)&gt;f(1-\\epsilon^{2}),\" display=\"inline\"><semantics><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mn>2</mn><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\">−</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo>+</mo><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\">−</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></mrow><mo>)</mo><mo>&gt;</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle(1-2\\exp(-m))f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)+\\exp(-m)f\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}}}\\right)&gt;f(1-\\epsilon^{2}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(81)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">or equivalently,</p>\n<table id=\"A4.EGx61\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E82\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E82.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle(1-2\\exp(-m))f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)&gt;(1-\\exp(-m))f(1-\\epsilon^{2}).\" display=\"inline\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mn>2</mn><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\">−</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mstyle displaystyle=\"true\"><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo><mo>&gt;</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>exp</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\">−</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle(1-2\\exp(-m))f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)&gt;(1-\\exp(-m))f(1-\\epsilon^{2}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(82)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">To show this, we will argue that</p>\n<table id=\"A4.EGx62\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E83\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E83.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{f(1-\\epsilon^{2})}{f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)}&lt;\\frac{1-2\\exp(-m)}{1-\\exp(-m)}=1-\\frac{\\exp(-m)}{1-\\exp(-m)}\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mstyle><mo>&lt;</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo lspace=\"0.167em\" rspace=\"0em\">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mrow><mn>1</mn><mo>−</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>1</mn><mo>−</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{f(1-\\epsilon^{2})}{f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)}&lt;\\frac{1-2\\exp(-m)}{1-\\exp(-m)}=1-\\frac{\\exp(-m)}{1-\\exp(-m)}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(83)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p19\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Let <math id=\"A3.SS3.SSS0.Px4.p19.m1\" class=\"ltx_Math\" alttext=\"\\gamma\\vcentcolon=\\frac{m}{30(d-1)}\" display=\"inline\"><semantics><mrow><mi>γ</mi><mo>:=</mo><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\gamma\\vcentcolon=\\frac{m}{30(d-1)}</annotation></semantics></math>, then</p>\n<table id=\"A4.EGx63\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E84\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E84.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{f(1-\\epsilon^{2})}{f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)}=\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mfrac><mi>m</mi><mrow><mn>30</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mstyle><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{f(1-\\epsilon^{2})}{f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\frac{m}{30(d-1)}}}\\right)}}\\right)}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E84.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{f(1-\\epsilon^{2})}{f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)}}\\right)}\" display=\"inline\"><semantics><mstyle displaystyle=\"true\"><mfrac><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mi>f</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mfrac></mstyle><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{f(1-\\epsilon^{2})}{f\\mathopen{}\\mathclose{{\\left((1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)}}\\right)}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(84)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E85\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E85.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E85.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{\\epsilon^{2}(1-\\epsilon^{2})}{(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)\\epsilon^{2}(1+\\gamma-\\epsilon^{2}\\gamma)}\" display=\"inline\"><semantics><mstyle displaystyle=\"true\"><mfrac><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi></mrow></mrow><mo>)</mo><mi>ϵ</mi><msup><mi></mi><mn>2</mn></msup><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>γ</mi><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mstyle><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{\\epsilon^{2}(1-\\epsilon^{2})}{(1-\\epsilon^{2})\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)\\epsilon^{2}(1+\\gamma-\\epsilon^{2}\\gamma)}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(85)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E86\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E86.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle=\" display=\"inline\"><semantics><mo>=</mo><annotation encoding=\"application/x-tex\">\\displaystyle=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E86.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)(1+\\gamma-\\epsilon^{2}\\gamma)}.\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi></mrow></mrow><mo>)</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>γ</mi><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mstyle><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)(1+\\gamma-\\epsilon^{2}\\gamma)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(86)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p20\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Since <math id=\"A3.SS3.SSS0.Px4.p20.m1\" class=\"ltx_Math\" alttext=\"\\epsilon^{2}&lt;1/4\" display=\"inline\"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo>&lt;</mo><mrow><mn>1</mn><mo>/</mo><mn>4</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon^{2}&lt;1/4</annotation></semantics></math>, we have</p>\n<table id=\"A4.EGx64\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E87\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E87.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)(1+\\gamma-\\epsilon^{2}\\gamma)&gt;\\mathopen{}\\mathclose{{\\left(1-\\frac{\\gamma}{4}}}\\right)(1+\\frac{3\\gamma}{4})&gt;1+\\frac{\\gamma}{2}-\\frac{3\\gamma}{16}=1+\\frac{5\\gamma}{16},\" display=\"inline\"><semantics><mrow><mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi></mrow></mrow><mo>)</mo></mrow><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>γ</mi><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi><mo stretchy=\"false\">)</mo></mrow><mo>&gt;</mo><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mstyle displaystyle=\"true\"><mfrac><mi>γ</mi><mn>4</mn></mfrac></mstyle></mrow></mrow><mo>)</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>4</mn></mfrac></mstyle><mo stretchy=\"false\">)</mo><mo>&gt;</mo><mn>1</mn><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mi>γ</mi><mn>2</mn></mfrac></mstyle><mo>−</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac></mstyle><mo>=</mo><mn>1</mn><mo>+</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac></mstyle><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)(1+\\gamma-\\epsilon^{2}\\gamma)&gt;\\mathopen{}\\mathclose{{\\left(1-\\frac{\\gamma}{4}}}\\right)(1+\\frac{3\\gamma}{4})&gt;1+\\frac{\\gamma}{2}-\\frac{3\\gamma}{16}=1+\\frac{5\\gamma}{16},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(87)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">so</p>\n<table id=\"A4.EGx65\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E88\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E88.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)(1+\\gamma-\\epsilon^{2}\\gamma)}&lt;\\frac{1}{1+\\frac{5\\gamma}{16}}=1-\\frac{\\frac{5\\gamma}{16}}{1+\\frac{5\\gamma}{16}}.\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mrow><mi></mi><mrow><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi></mrow></mrow><mo>)</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>γ</mi><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup><mi>γ</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mstyle><mo>&lt;</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac></mrow></mfrac></mstyle><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mstyle displaystyle=\"true\"><mfrac><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac></mrow></mfrac></mstyle></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{\\mathopen{}\\mathclose{{\\left(1-\\epsilon^{2}\\gamma}}\\right)(1+\\gamma-\\epsilon^{2}\\gamma)}&lt;\\frac{1}{1+\\frac{5\\gamma}{16}}=1-\\frac{\\frac{5\\gamma}{16}}{1+\\frac{5\\gamma}{16}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(88)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p21\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">To show <a href=\"#A3.E83\" title=\"In Forgetting with replay ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">83</span></a>, it would suffice to argue that</p>\n<table id=\"A4.EGx66\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E89\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E89.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\frac{5\\gamma}{16}}{1+\\frac{5\\gamma}{16}}&gt;\\frac{\\exp(-m)}{1-\\exp(-m)}\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow><mn>16</mn></mfrac></mrow></mfrac></mstyle><mo>&gt;</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mn>1</mn><mo>−</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>−</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{\\frac{5\\gamma}{16}}{1+\\frac{5\\gamma}{16}}&gt;\\frac{\\exp(-m)}{1-\\exp(-m)}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(89)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p22\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">for some values of <math id=\"A3.SS3.SSS0.Px4.p22.m1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math id=\"A3.SS3.SSS0.Px4.p22.m2\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. Simplifying the expressions above, this would be equivalent to</p>\n<table id=\"A4.EGx67\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E90\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E90.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{16}{5\\gamma}+1&lt;\\exp(m)-1.\" display=\"inline\"><semantics><mrow><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>16</mn><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>γ</mi></mrow></mfrac></mstyle><mo>+</mo><mn>1</mn></mrow><mo>&lt;</mo><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>−</mo><mn>1</mn></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{16}{5\\gamma}+1&lt;\\exp(m)-1.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(90)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p23\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Plugging in <math id=\"A3.SS3.SSS0.Px4.p23.m1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics><mi>γ</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, we can see that this inequality would be satisfied as long as</p>\n<table id=\"A4.EGx68\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E91\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E91.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle 96\\cdot\\frac{d-1}{m}+2&lt;\\exp(m).\" display=\"inline\"><semantics><mrow><mrow><mrow><mrow><mn>96</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><mi>m</mi></mfrac></mstyle></mrow><mo>+</mo><mn>2</mn></mrow><mo>&lt;</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle 96\\cdot\\frac{d-1}{m}+2&lt;\\exp(m).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(91)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p24\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Setting <math id=\"A3.SS3.SSS0.Px4.p24.m1\" class=\"ltx_Math\" alttext=\"c_{1}=120,c_{2}=15\" display=\"inline\"><semantics><mrow><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>=</mo><mn>120</mn></mrow><mo>,</mo><mrow><msub><mi>c</mi><mn>2</mn></msub><mo>=</mo><mn>15</mn></mrow></mrow><annotation encoding=\"application/x-tex\">c_{1}=120,c_{2}=15</annotation></semantics></math> and <math id=\"A3.SS3.SSS0.Px4.p24.m2\" class=\"ltx_Math\" alttext=\"c_{3}=97\" display=\"inline\"><semantics><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>=</mo><mn>97</mn></mrow><annotation encoding=\"application/x-tex\">c_{3}=97</annotation></semantics></math> gives the stated results.</p>\n</div>\n<div id=\"A3.SS3.SSS0.Px4.p25\" class=\"ltx_para\">\n<p class=\"ltx_p\">∎</p>\n</div>\n</section>\n</section>\n<section id=\"A3.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.4 </span>Proofs of claims and propositions</h3>\n\n<div id=\"A3.SS4.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#S3.Thmtheorem4\" title=\"Proposition 3.4. ‣ 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Proposition</span> <span class=\"ltx_text ltx_ref_tag\">3.4</span></a></span></p>\n</div>\n<div id=\"A3.SS4.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Note that</p>\n<table id=\"A4.EGx69\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E92\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E92.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{tj}{{\\mathbf{X}}^{\\prime}}_{tj}^{\\top}}}\\right]=\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mrow></mrow></mmultiscripts><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>]</mo><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{tj}{{\\mathbf{X}}^{\\prime}}_{tj}^{\\top}}}\\right]=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E92.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{\\bm{W}}_{t}{{\\mathbf{Z}}^{\\prime}}_{tj}{{\\mathbf{Z}}^{\\prime}}_{tj}^{\\top}{\\bm{W}}_{t}^{\\top}}}\\right]={\\bm{W}}_{t}\\frac{\\mathbf{I}_{k}}{k_{t}}{\\bm{W}}_{t}^{\\top}=\\frac{1}{k_{t}}\\;{\\bm{W}}_{t}{\\bm{W}}_{t}^{\\top}=\\frac{1}{k_{t}}{\\bm{\\Pi}}_{t},\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>𝑾</mi><mi>t</mi></msub><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mrow></mrow></mmultiscripts><mmultiscripts><mi>𝐙</mi><mrow></mrow><mo>′</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mo>⊤</mo></mmultiscripts><msubsup><mi>𝑾</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow></mrow><mo>]</mo><mo>=</mo><mi>𝑾</mi><msub><mi></mi><mi>t</mi></msub><mstyle displaystyle=\"true\"><mfrac><msub><mi>𝐈</mi><mi>k</mi></msub><msub><mi>k</mi><mi>t</mi></msub></mfrac></mstyle><mi>𝑾</mi><msub><mi></mi><mi>t</mi></msub><msup><mi></mi><mo>⊤</mo></msup><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>k</mi><mi>t</mi></msub></mfrac></mstyle><mi>𝑾</mi><msub><mi></mi><mi>t</mi></msub><mi>𝑾</mi><msub><mi></mi><mi>t</mi></msub><msup><mi></mi><mo>⊤</mo></msup><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><msub><mi>k</mi><mi>t</mi></msub></mfrac></mstyle><mi>𝚷</mi><msub><mi></mi><mi>t</mi></msub><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{\\bm{W}}_{t}{{\\mathbf{Z}}^{\\prime}}_{tj}{{\\mathbf{Z}}^{\\prime}}_{tj}^{\\top}{\\bm{W}}_{t}^{\\top}}}\\right]={\\bm{W}}_{t}\\frac{\\mathbf{I}_{k}}{k_{t}}{\\bm{W}}_{t}^{\\top}=\\frac{1}{k_{t}}\\;{\\bm{W}}_{t}{\\bm{W}}_{t}^{\\top}=\\frac{1}{k_{t}}{\\bm{\\Pi}}_{t},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(92)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"A3.SS4.p2.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{i}\" display=\"inline\"><semantics><msub><mi>𝚷</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{i}</annotation></semantics></math> was the projection matrix into the subspace spanned by samples of task <math id=\"A3.SS4.p2.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>.\nAdditionally, we can write</p>\n<table id=\"A4.EGx70\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E93\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E93.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{t}^{\\top}{{\\mathbf{X}}^{\\prime}}_{t}}}\\right]=\\sum_{j\\in[k_{t}]}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{tj}{{\\mathbf{X}}^{\\prime}}_{tj}^{\\top}}}\\right]={\\bm{\\Pi}}_{t},\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mi>t</mi><mo>⊤</mo></mmultiscripts><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mi>t</mi><mrow></mrow></mmultiscripts></mrow></mrow><mo>]</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munder><mo movablelimits=\"false\">∑</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow></munder></mstyle><mo lspace=\"0.167em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mrow></mrow></mmultiscripts><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>]</mo><mo>=</mo><mi>𝚷</mi><msub><mi></mi><mi>t</mi></msub><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{t}^{\\top}{{\\mathbf{X}}^{\\prime}}_{t}}}\\right]=\\sum_{j\\in[k_{t}]}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{tj}{{\\mathbf{X}}^{\\prime}}_{tj}^{\\top}}}\\right]={\\bm{\\Pi}}_{t},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(93)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">which will be useful when we compute expected forgetting below.</p>\n</div>\n<div id=\"A3.SS4.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Recall that <math id=\"A3.SS4.p3.m1\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{tj}</annotation></semantics></math> were used to generate training samples for the task (<a href=\"#S3.E5\" title=\"In 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>).\nSince any <math id=\"A3.SS4.p3.m2\" class=\"ltx_Math\" alttext=\"k_{t}\" display=\"inline\"><semantics><msub><mi>k</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">k_{t}</annotation></semantics></math> independent <math id=\"A3.SS4.p3.m3\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{tj}</annotation></semantics></math> samples are going to be linearly independent, we are guaranteed that <math id=\"A3.SS4.p3.m4\" class=\"ltx_Math\" alttext=\"{\\mathbf{Z}}_{tj}\" display=\"inline\"><semantics><msub><mi>𝐙</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\mathbf{Z}}_{tj}</annotation></semantics></math> will have the same span as <math id=\"A3.SS4.p3.m5\" class=\"ltx_Math\" alttext=\"{\\bm{W}}_{t}\" display=\"inline\"><semantics><msub><mi>𝑾</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{W}}_{t}</annotation></semantics></math> under <a href=\"#S3.Thmtheorem3\" title=\"Assumption 3.3. ‣ 3.2.1 Average Case Setup ‣ 3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, which states that <math id=\"A3.SS4.p3.m6\" class=\"ltx_Math\" alttext=\"n_{t}\\geq k_{t}\" display=\"inline\"><semantics><mrow><msub><mi>n</mi><mi>t</mi></msub><mo>≥</mo><msub><mi>k</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">n_{t}\\geq k_{t}</annotation></semantics></math>.\nConsequently, the null space of each task <math id=\"A3.SS4.p3.m7\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is <math id=\"A3.SS4.p3.m8\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{t}=\\mathbf{I}-{\\bm{\\Pi}}_{t}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>𝐈</mi><mo>−</mo><msub><mi>𝚷</mi><mi>t</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{t}=\\mathbf{I}-{\\bm{\\Pi}}_{t}</annotation></semantics></math>.\nThen similar to <a href=\"#S2.E4\" title=\"In Learning Procedure. ‣ 2.2 General Setup ‣ 2 Background and Setup ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>, we can write each term in the expected forgetting (with respect to test samples) as</p>\n<table id=\"A4.EGx71\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E94\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E94.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{T})}}\\right]=\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{T})}}\\right]=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E94.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{{\\mathbf{X}}^{\\prime}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}}}\\right]=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{{\\mathbf{X}}^{\\prime}}_{t}{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right],\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mo lspace=\"0.167em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mi>t</mi><mrow></mrow></mmultiscripts><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo>−</mo><msup><mi>𝒘</mi><mo>∗</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mo lspace=\"0.167em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mi>t</mi><mrow></mrow></mmultiscripts><msub><mi>𝑷</mi><mi>T</mi></msub><msub><mi>𝑷</mi><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{{\\mathbf{X}}^{\\prime}}_{t}({\\bm{w}}_{T}-{\\bm{w}}^{*})}}}\\right\\|_{2}^{2}}}\\right]=\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{{\\mathbf{X}}^{\\prime}}_{t}{\\bm{P}}_{T}{\\bm{P}}_{T-1}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right],</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(94)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where now the expectation is only over the randomness in <math id=\"A3.SS4.p3.m9\" class=\"ltx_Math\" alttext=\"{X^{\\prime}}_{t}\" display=\"inline\"><semantics><mmultiscripts><mi>X</mi><mrow></mrow><mo>′</mo><mi>t</mi><mrow></mrow></mmultiscripts><annotation encoding=\"application/x-tex\">{X^{\\prime}}_{t}</annotation></semantics></math>.\nExpanding the square inside the expectations in <a href=\"#A3.E94\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">94</span></a> and applying <a href=\"#A3.E93\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">93</span></a> we get</p>\n</div>\n<div id=\"A3.SS4.p4\" class=\"ltx_para\">\n<table id=\"A4.EGx72\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E95\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E95.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\dots\\bm{P}_{T}{\\bm{X}}_{t}^{\\top}{\\bm{X}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}\\right]\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">=</mo><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><msub><mi>𝑷</mi><mn>1</mn></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mi>T</mi></msub><msubsup><mi>𝑿</mi><mi>t</mi><mo>⊤</mo></msubsup><msub><mi>𝑿</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{X}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\dots\\bm{P}_{T}{\\bm{X}}_{t}^{\\top}{\\bm{X}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}\\right]</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E95.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle={{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\dots\\bm{P}_{T}{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}\" display=\"inline\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝚷</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">…</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle={{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\dots\\bm{P}_{T}{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(95)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E96\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E96.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2},\" display=\"inline\"><semantics><mrow><mo>=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(96)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where the last equality follows from the fact that <math id=\"A3.SS4.p4.m1\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{t}\" display=\"inline\"><semantics><msub><mi>𝚷</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{t}</annotation></semantics></math> is an orthonormal projection and <math id=\"A3.SS4.p4.m2\" class=\"ltx_Math\" alttext=\"{\\bm{\\Pi}}_{t}={\\bm{\\Pi}}_{t}^{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝚷</mi><mi>t</mi></msub><mo>=</mo><msubsup><mi>𝚷</mi><mi>t</mi><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">{\\bm{\\Pi}}_{t}={\\bm{\\Pi}}_{t}^{2}</annotation></semantics></math>.\nPlugging this back into <a href=\"#A3.E94\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">94</span></a> we can write the expected forgetting as</p>\n<table id=\"A4.EGx73\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E97\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E97.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{T})}}\\right]=\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}({\\bm{w}}_{T})}}\\right]=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E97.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.\" display=\"inline\"><semantics><mrow><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mstyle><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mi>t</mi></msub><msub><mi>𝑷</mi><mi>T</mi></msub><mi mathvariant=\"normal\">…</mi><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{1}{T-1}\\sum_{t=1}^{T-1}\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{t}{\\bm{P}}_{T}\\dots\\bm{P}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(97)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">∎</p>\n</div>\n<div id=\"A3.SS4.p5\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#A3.Thmtheorem2\" title=\"Claim C.2. ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a></span>\n  \nSuppose that <math id=\"A3.SS4.p5.m1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> samples are randomly (without replacement) selected from the <math id=\"A3.SS4.p5.m2\" class=\"ltx_Math\" alttext=\"n_{1}\" display=\"inline\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math>\nsamples for task one. Alternatively, we can think of them as being fixed before seeing any samples. Let <math id=\"A3.SS4.p5.m3\" class=\"ltx_Math\" alttext=\"S_{1}\\subseteq[n_{1}]\" display=\"inline\"><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub><mo>⊆</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>n</mi><mn>1</mn></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">S_{1}\\subseteq[n_{1}]</annotation></semantics></math> be a randomly chosen set of indices of samples that were selected for replay. Let <math id=\"A3.SS4.p5.m4\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math> be the projection into the null space of the combined samples for the second task.\nThen <math id=\"A3.SS4.p5.m5\" class=\"ltx_math_unparsed\" alttext=\"\\tilde{{\\bm{P}}}_{2}=\\tilde{{\\bm{P}}}_{2}(\\mathopen{}\\mathclose{{\\left\\{{\\mathbf{X}}_{1,s}}}\\right\\}_{s\\in S_{1}})\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo>=</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mrow><mo stretchy=\"false\">(</mo><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝐗</mi><mrow><mn>1</mn><mo>,</mo><mi>s</mi></mrow></msub></mrow></mrow><mo>}</mo></mrow><mrow><mi>s</mi><mo>∈</mo><msub><mi>S</mi><mn>1</mn></msub></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}=\\tilde{{\\bm{P}}}_{2}(\\mathopen{}\\mathclose{{\\left\\{{\\mathbf{X}}_{1,s}}}\\right\\}_{s\\in S_{1}})</annotation></semantics></math> is\nrandom, unlike <math id=\"A3.SS4.p5.m6\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{2}\" display=\"inline\"><semantics><msub><mi>𝑷</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{P}}_{2}</annotation></semantics></math>. The expected forgetting is</p>\n</div>\n<div id=\"A3.SS4.p6\" class=\"ltx_para ltx_noindent\">\n<table id=\"A4.EGx74\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E98\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E98.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}(\\tilde{{\\bm{w}}}_{2})}}\\right]=\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><msub><mi>F</mi><msup><mi>S</mi><mo>′</mo></msup></msub><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo><mo>=</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[F_{S^{\\prime}}(\\tilde{{\\bm{w}}}_{2})}}\\right]=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E98.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{{\\mathbf{X}}^{\\prime}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\tilde{{\\bm{P}}}_{2}{{\\mathbf{X}}^{\\prime}}_{1}^{\\top}{{\\mathbf{X}}^{\\prime}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}\\right].\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mn>1</mn><mrow></mrow></mmultiscripts><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">=</mo><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><msub><mi>𝑷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mn>1</mn><mo>⊤</mo></mmultiscripts><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mn>1</mn><mrow></mrow></mmultiscripts><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo>]</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{{\\mathbf{X}}^{\\prime}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\tilde{{\\bm{P}}}_{2}{{\\mathbf{X}}^{\\prime}}_{1}^{\\top}{{\\mathbf{X}}^{\\prime}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}\\right].</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(98)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Since <math id=\"A3.SS4.p6.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\mathbf{X}}_{1,s}}}\\right\\}_{s\\in S_{1}}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝐗</mi><mrow><mn>1</mn><mo>,</mo><mi>s</mi></mrow></msub></mrow></mrow><mo>}</mo><msub><mi></mi><mrow><mi>s</mi><mo>∈</mo><msub><mi>S</mi><mn>1</mn></msub></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\mathbf{X}}_{1,s}}}\\right\\}_{s\\in S_{1}}</annotation></semantics></math> is independent from <math id=\"A3.SS4.p6.m2\" class=\"ltx_Math\" alttext=\"X^{\\prime}_{1}\" display=\"inline\"><semantics><msubsup><mi>X</mi><mn>1</mn><mo>′</mo></msubsup><annotation encoding=\"application/x-tex\">X^{\\prime}_{1}</annotation></semantics></math> and <math id=\"A3.SS4.p6.m3\" class=\"ltx_math_unparsed\" alttext=\"\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{1}^{\\top}{{\\mathbf{X}}^{\\prime}}_{1}}}\\right]={\\bm{\\Pi}}_{1}\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mn>1</mn><mo>⊤</mo></mmultiscripts><mmultiscripts><mi>𝐗</mi><mrow></mrow><mo>′</mo><mn>1</mn><mrow></mrow></mmultiscripts></mrow></mrow><mo>]</mo><mo>=</mo><mi>𝚷</mi><msub><mi></mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\mathbf{X}}^{\\prime}}_{1}^{\\top}{{\\mathbf{X}}^{\\prime}}_{1}}}\\right]={\\bm{\\Pi}}_{1}</annotation></semantics></math> (see <a href=\"#A3.E93\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">93</span></a>), we can write the expectation above as</p>\n<table id=\"A4.EGx75\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E99\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E99.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right],\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><msub><mi>𝑷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">=</mo><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝚷</mi><mn>1</mn></msub><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}^{\\top}{\\bm{P}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}\\right]=\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[\\mathopen{}\\mathclose{{\\left\\|{{\\bm{\\Pi}}_{1}\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}^{*}}}}\\right\\|_{2}^{2}}}\\right],</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(99)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where now the expectation is only over the randomness in <math id=\"A3.SS4.p6.m4\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS4.p7\" class=\"ltx_para\">\n<p class=\"ltx_p\">∎</p>\n</div>\n<div id=\"A3.SS4.p8\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#A3.Thmtheorem3\" title=\"Claim C.3. ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C.3</span></a></span>\n  \nWithout loss of generality we can assume that <math id=\"A3.SS4.p8.m1\" class=\"ltx_Math\" alttext=\"\\alpha_{1},\\alpha_{2}\\sim\\sf{N}(0,1)\" display=\"inline\"><semantics><mrow><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>,</mo><msub><mi>α</mi><mn>2</mn></msub></mrow><mo>∼</mo><mrow><mi>𝖭</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>𝟢</mn><mo>,</mo><mn>𝟣</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1},\\alpha_{2}\\sim\\sf{N}(0,1)</annotation></semantics></math>, as this would not change the distribution of <math id=\"A3.SS4.p8.m2\" class=\"ltx_Math\" alttext=\"{\\alpha^{\\prime}_{1}}^{2}\" display=\"inline\"><semantics><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><annotation encoding=\"application/x-tex\">{\\alpha^{\\prime}_{1}}^{2}</annotation></semantics></math>. Define <math id=\"A3.SS4.p8.m3\" class=\"ltx_Math\" alttext=\"f(\\alpha_{1}^{\\prime})=63{\\alpha^{\\prime}_{1}}^{2}-62{\\alpha^{\\prime}_{1}}^{4}={\\alpha^{\\prime}_{1}}^{2}(63-62{\\alpha^{\\prime}_{1}}^{2})\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>α</mi><mn>1</mn><mo>′</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mn>63</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts></mrow><mo>−</mo><mrow><mn>62</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>4</mn></mmultiscripts></mrow></mrow><mo>=</mo><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>63</mn><mo>−</mo><mrow><mn>62</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\alpha_{1}^{\\prime})=63{\\alpha^{\\prime}_{1}}^{2}-62{\\alpha^{\\prime}_{1}}^{4}={\\alpha^{\\prime}_{1}}^{2}(63-62{\\alpha^{\\prime}_{1}}^{2})</annotation></semantics></math>.\nWe can lower bound the expectation by considering the following three events:</p>\n<ol id=\"A3.I1\" class=\"ltx_enumerate\">\n<li id=\"A3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span> \n<div id=\"A3.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><math id=\"A3.I1.i1.p1.m1\" class=\"ltx_Math\" alttext=\"{\\alpha^{\\prime}_{1}}^{2}&lt;1/31\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>&lt;</mo><mrow><mn>1</mn><mo>/</mo><mn>31</mn></mrow></mrow><annotation encoding=\"application/x-tex\">{\\alpha^{\\prime}_{1}}^{2}&lt;1/31</annotation></semantics></math>: under this event we use the trivial lower bound <math id=\"A3.I1.i1.p1.m2\" class=\"ltx_Math\" alttext=\"f(\\alpha^{\\prime}_{1})\\geq 0\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>α</mi><mn>1</mn><mo>′</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">f(\\alpha^{\\prime}_{1})\\geq 0</annotation></semantics></math>.</p>\n</div>\n</li>\n<li id=\"A3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span> \n<div id=\"A3.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><math id=\"A3.I1.i2.p1.m1\" class=\"ltx_Math\" alttext=\"1/31\\leq{\\alpha^{\\prime}_{1}}^{2}\\leq 63/64\" display=\"inline\"><semantics><mrow><mrow><mn>1</mn><mo>/</mo><mn>31</mn></mrow><mo>≤</mo><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>≤</mo><mrow><mn>63</mn><mo>/</mo><mn>64</mn></mrow></mrow><annotation encoding=\"application/x-tex\">1/31\\leq{\\alpha^{\\prime}_{1}}^{2}\\leq 63/64</annotation></semantics></math>: then <math id=\"A3.I1.i2.p1.m2\" class=\"ltx_Math\" alttext=\"f(\\alpha^{\\prime}_{1})\\geq 1.9\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>α</mi><mn>1</mn><mo>′</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>≥</mo><mn>1.9</mn></mrow><annotation encoding=\"application/x-tex\">f(\\alpha^{\\prime}_{1})\\geq 1.9</annotation></semantics></math></p>\n</div>\n</li>\n<li id=\"A3.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span> \n<div id=\"A3.I1.i3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><math id=\"A3.I1.i3.p1.m1\" class=\"ltx_Math\" alttext=\"{\\alpha^{\\prime}_{1}}^{2}&gt;63/64\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>&gt;</mo><mrow><mn>63</mn><mo>/</mo><mn>64</mn></mrow></mrow><annotation encoding=\"application/x-tex\">{\\alpha^{\\prime}_{1}}^{2}&gt;63/64</annotation></semantics></math>: since we always have <math id=\"A3.I1.i3.p1.m2\" class=\"ltx_Math\" alttext=\"{\\alpha^{\\prime}_{1}}^{2}\\leq 1\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>≤</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha^{\\prime}_{1}}^{2}\\leq 1</annotation></semantics></math>, we will use the bound <math id=\"A3.I1.i3.p1.m3\" class=\"ltx_Math\" alttext=\"f(\\alpha^{\\prime}_{1})\\geq 1\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>α</mi><mn>1</mn><mo>′</mo></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>≥</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">f(\\alpha^{\\prime}_{1})\\geq 1</annotation></semantics></math>.</p>\n</div>\n</li>\n</ol>\n<p class=\"ltx_p\">Now we bound the probability of these events.\nNote that by symmetry, <math id=\"A3.SS4.p8.m4\" class=\"ltx_Math\" alttext=\"\\alpha_{2}^{2}\\leq\\alpha_{1}^{2}\" display=\"inline\"><semantics><mrow><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mo>≤</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\alpha_{2}^{2}\\leq\\alpha_{1}^{2}</annotation></semantics></math> with probability of <math id=\"A3.SS4.p8.m5\" class=\"ltx_Math\" alttext=\"1/2\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1/2</annotation></semantics></math>, then with would have</p>\n<table id=\"A4.EGx76\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E100\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E100.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle{\\alpha^{\\prime}_{1}}^{2}=\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>=</mo><mi></mi></mrow><annotation encoding=\"application/x-tex\">\\displaystyle{\\alpha^{\\prime}_{1}}^{2}=</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E100.m2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{2}^{2}}{63}+\\alpha_{1}^{2}}\\geq\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{1}^{2}}{63}+\\alpha_{1}^{2}}=\\frac{63}{64}.\" display=\"inline\"><semantics><mrow><mrow><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mrow><mfrac><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mn>63</mn></mfrac><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>≥</mo><mstyle displaystyle=\"true\"><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mrow><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mn>63</mn></mfrac><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>63</mn><mn>64</mn></mfrac></mstyle></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{2}^{2}}{63}+\\alpha_{1}^{2}}\\geq\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{1}^{2}}{63}+\\alpha_{1}^{2}}=\\frac{63}{64}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(100)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS4.p9\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">So the event in <a href=\"#A3.I1.i3\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Item</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> happens with probability <math id=\"A3.SS4.p9.m1\" class=\"ltx_Math\" alttext=\"1/2\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1/2</annotation></semantics></math>. Next, we argue that probability of the event in <a href=\"#A3.I1.i1\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Item</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a> is very small.\nIf <math id=\"A3.SS4.p9.m2\" class=\"ltx_Math\" alttext=\"{\\alpha^{\\prime}_{1}}^{2}=\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{2}^{2}}{63}+\\alpha_{1}^{2}}&lt;1/31\" display=\"inline\"><semantics><mrow><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>=</mo><mfrac><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mrow><mfrac><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mn>63</mn></mfrac><mo>+</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac><mo>&lt;</mo><mrow><mn>1</mn><mo>/</mo><mn>31</mn></mrow></mrow><annotation encoding=\"application/x-tex\">{\\alpha^{\\prime}_{1}}^{2}=\\frac{\\alpha_{1}^{2}}{\\frac{\\alpha_{2}^{2}}{63}+\\alpha_{1}^{2}}&lt;1/31</annotation></semantics></math>, then it must be that\n<math id=\"A3.SS4.p9.m3\" class=\"ltx_Math\" alttext=\"30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>63</mn></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup></mrow><mo>&lt;</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}</annotation></semantics></math>. We first argue that with high probability <math id=\"A3.SS4.p9.m4\" class=\"ltx_Math\" alttext=\"\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}\" display=\"inline\"><semantics><mrow><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>≥</mo><mfrac><mn>4</mn><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}</annotation></semantics></math>.\nNote that the pdf of normal distribution is upper bounded by <math id=\"A3.SS4.p9.m5\" class=\"ltx_Math\" alttext=\"\\frac{1}{\\sqrt{2\\pi}}\" display=\"inline\"><semantics><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>π</mi></mrow></msqrt></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{\\sqrt{2\\pi}}</annotation></semantics></math>, so</p>\n<table id=\"A4.EGx77\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E101\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E101.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\alpha_{1}^{2}&lt;\\frac{4}{30\\cdot 61}}}\\right]\\leq\\sqrt{\\frac{2\\cdot 4}{2\\pi\\cdot 30\\cdot 61}}\\leq 0.018.\" display=\"inline\"><semantics><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>&lt;</mo><mstyle displaystyle=\"true\"><mfrac><mn>4</mn><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo><mo>≤</mo><msqrt><mstyle displaystyle=\"true\"><mfrac><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>4</mn></mrow><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>π</mi></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mstyle></msqrt><mo>≤</mo><mn>0.018</mn><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\alpha_{1}^{2}&lt;\\frac{4}{30\\cdot 61}}}\\right]\\leq\\sqrt{\\frac{2\\cdot 4}{2\\pi\\cdot 30\\cdot 61}}\\leq 0.018.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(101)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Then we have</p>\n<table id=\"A4.EGx78\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E102\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E102.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[{\\alpha^{\\prime}_{1}}^{2}&lt;1/31}}\\right]\\leq\" display=\"inline\"><semantics><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>&lt;</mo><mn>1</mn><mo>/</mo><mn>31</mn></mrow></mrow><mo>]</mo><mo>≤</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[{\\alpha^{\\prime}_{1}}^{2}&lt;1/31}}\\right]\\leq</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E102.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\alpha_{1}^{2}&lt;\\frac{4}{30\\cdot 61}}}\\right]+\\Pr\\mathopen{}\\mathclose{{\\left[30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\\mid\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}}}\\right]\" display=\"inline\"><semantics><mrow><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>&lt;</mo><mstyle displaystyle=\"true\"><mfrac><mn>4</mn><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><mo>+</mo><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>63</mn><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>&lt;</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0.167em\">∣</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>≥</mo><mstyle displaystyle=\"true\"><mfrac><mn>4</mn><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[\\alpha_{1}^{2}&lt;\\frac{4}{30\\cdot 61}}}\\right]+\\Pr\\mathopen{}\\mathclose{{\\left[30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\\mid\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}}}\\right]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(102)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E103\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E103.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\leq\" display=\"inline\"><semantics><mo>≤</mo><annotation encoding=\"application/x-tex\">\\displaystyle\\leq</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E103.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\;0.018+\\Pr\\mathopen{}\\mathclose{{\\left[30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\\mid\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}}}\\right].\" display=\"inline\"><semantics><mrow><mn> 0.018</mn><mo>+</mo><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>63</mn><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>&lt;</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0.167em\">∣</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>≥</mo><mstyle displaystyle=\"true\"><mfrac><mn>4</mn><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\;0.018+\\Pr\\mathopen{}\\mathclose{{\\left[30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\\mid\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}}}\\right].</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(103)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Note that</p>\n<table id=\"A4.EGx79\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E104\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E104.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\\mid\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}}}\\right]\\leq\\Pr\\mathopen{}\\mathclose{{\\left[4&lt;\\alpha_{2}^{2}}}\\right]\\leq 0.0001.\" display=\"inline\"><semantics><mrow><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>63</mn><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>&lt;</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\" rspace=\"0.167em\">∣</mo><msubsup><mi>α</mi><mn>1</mn><mn>2</mn></msubsup><mo>≥</mo><mstyle displaystyle=\"true\"><mfrac><mn>4</mn><mrow><mn>30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>61</mn></mrow></mfrac></mstyle></mrow></mrow><mo>]</mo></mrow><mo>≤</mo><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mn>4</mn><mo>&lt;</mo><msubsup><mi>α</mi><mn>2</mn><mn>2</mn></msubsup></mrow></mrow><mo>]</mo><mo>≤</mo><mn>0.0001</mn><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Pr\\mathopen{}\\mathclose{{\\left[30\\cdot 63\\alpha_{1}^{2}&lt;\\alpha_{2}^{2}\\mid\\alpha_{1}^{2}\\geq\\frac{4}{30\\cdot 61}}}\\right]\\leq\\Pr\\mathopen{}\\mathclose{{\\left[4&lt;\\alpha_{2}^{2}}}\\right]\\leq 0.0001.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(104)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Now we have that <math id=\"A3.SS4.p9.m6\" class=\"ltx_math_unparsed\" alttext=\"\\Pr\\mathopen{}\\mathclose{{\\left[{\\alpha^{\\prime}_{1}}^{2}&lt;1/31}}\\right]\\leq 0.019\" display=\"inline\"><semantics><mrow><mi>Pr</mi><mrow><mi></mi><mrow><mo>[</mo><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>&lt;</mo><mn>1</mn><mo>/</mo><mn>31</mn></mrow></mrow><mo>]</mo><mo>≤</mo><mn>0.019</mn></mrow><annotation encoding=\"application/x-tex\">\\Pr\\mathopen{}\\mathclose{{\\left[{\\alpha^{\\prime}_{1}}^{2}&lt;1/31}}\\right]\\leq 0.019</annotation></semantics></math>. This lets us lower bound the probability of the event in <a href=\"#A3.I1.i2\" title=\"In C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Item</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a> by <math id=\"A3.SS4.p9.m7\" class=\"ltx_Math\" alttext=\"0.5-0.019=0.481\" display=\"inline\"><semantics><mrow><mrow><mn>0.5</mn><mo>−</mo><mn>0.019</mn></mrow><mo>=</mo><mn>0.481</mn></mrow><annotation encoding=\"application/x-tex\">0.5-0.019=0.481</annotation></semantics></math>.\nCollecting these three bounds together we get</p>\n<table id=\"A4.EGx80\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E105\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E105.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[63{\\alpha^{\\prime}_{1}}^{2}-62{\\alpha^{\\prime}_{1}}^{4}}}\\right]\\geq 0.481\\cdot 1.9+0.5\\cdot 1\\geq 1.4.\" display=\"inline\"><semantics><mrow><mo rspace=\"0em\">𝔼</mo><mrow><mi></mi><mrow><mo>[</mo><mn>63</mn><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>2</mn></mmultiscripts><mo>−</mo><mn>62</mn><mmultiscripts><mi>α</mi><mn>1</mn><mo>′</mo><mrow></mrow><mn>4</mn></mmultiscripts></mrow></mrow><mo>]</mo><mo>≥</mo><mn>0.481</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>1.9</mn><mo>+</mo><mn>0.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">⋅</mo><mn>1</mn><mo>≥</mo><mn>1.4</mn><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}\\mathopen{}\\mathclose{{\\left[63{\\alpha^{\\prime}_{1}}^{2}-62{\\alpha^{\\prime}_{1}}^{4}}}\\right]\\geq 0.481\\cdot 1.9+0.5\\cdot 1\\geq 1.4.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(105)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"A3.SS4.p10\" class=\"ltx_para\">\n<p class=\"ltx_p\">∎</p>\n</div>\n<div id=\"A3.SS4.p11\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proof of <a href=\"#S3.Thmtheorem6\" title=\"Proposition 3.6.In 3.3 When Replay Cannot Increase Forgetting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Proposition</span> <span class=\"ltx_text ltx_ref_tag\">3.6</span></a></span>\n \n\nNote that <math id=\"A3.SS4.p11.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{span}(\\tilde{{\\bm{P}}}_{2})\\subseteq\\mathrm{span}({\\bm{P}}_{2})\" display=\"inline\"><semantics><mrow><mrow><mi>span</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>⊆</mo><mrow><mi>span</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{span}(\\tilde{{\\bm{P}}}_{2})\\subseteq\\mathrm{span}({\\bm{P}}_{2})</annotation></semantics></math>, since combining replay samples with samples of the second task would only reduce the size of the null space.\nWe can decompose <math id=\"A3.SS4.p11.m2\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{2}=\\tilde{{\\bm{P}}}_{2}+\\bar{{\\bm{P}}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo>+</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><mn>2</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{2}=\\tilde{{\\bm{P}}}_{2}+\\bar{{\\bm{P}}}_{2}</annotation></semantics></math> where <math id=\"A3.SS4.p11.m3\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{P}}}_{2}\\bar{{\\bm{P}}}_{2}=0\" display=\"inline\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>¯</mo></mover><mn>2</mn></msub></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{P}}}_{2}\\bar{{\\bm{P}}}_{2}=0</annotation></semantics></math>.\nThen it is easy to see that for any vector <math id=\"A3.SS4.p11.m4\" class=\"ltx_Math\" alttext=\"{\\bm{w}}\" display=\"inline\"><semantics><mi>𝒘</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math>, <math id=\"A3.SS4.p11.m5\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}}}}\\right\\|_{2}^{2}\\leq\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><mi>𝒘</mi></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0.0835em\">≤</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><mi>𝒘</mi></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}{\\bm{w}}}}}\\right\\|_{2}^{2}\\leq\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}{\\bm{w}}}}}\\right\\|_{2}^{2}</annotation></semantics></math>.\nDefine <math id=\"A3.SS4.p11.m6\" class=\"ltx_Math\" alttext=\"{\\bm{A}}\\vcentcolon={\\bm{P}}_{2}{\\bm{P}}_{1}\" display=\"inline\"><semantics><mrow><mi>𝑨</mi><mo>:=</mo><mrow><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{A}}\\vcentcolon={\\bm{P}}_{2}{\\bm{P}}_{1}</annotation></semantics></math> and <math id=\"A3.SS4.p11.m7\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{A}}}\\vcentcolon=\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}\" display=\"inline\"><semantics><mrow><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>:=</mo><mrow><msub><mover accent=\"true\"><mi>𝑷</mi><mo>~</mo></mover><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{A}}}\\vcentcolon=\\tilde{{\\bm{P}}}_{2}{\\bm{P}}_{1}</annotation></semantics></math>.\nThis implies that</p>\n<table id=\"A4.EGx81\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E106\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E106.m1\" class=\"ltx_Math\" alttext=\"\\displaystyle 0\\leq{\\bm{w}}^{\\top}{\\bm{A}}^{\\top}{\\bm{A}}{\\bm{w}}^{\\top}-{\\bm{w}}^{\\top}\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}{\\bm{w}}\" display=\"inline\"><semantics><mrow><mn>0</mn><mo>≤</mo><mrow><mrow><msup><mi>𝒘</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝑨</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>𝒘</mi><mo>⊤</mo></msup></mrow><mo>−</mo><mrow><msup><mi>𝒘</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝒘</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle 0\\leq{\\bm{w}}^{\\top}{\\bm{A}}^{\\top}{\\bm{A}}{\\bm{w}}^{\\top}-{\\bm{w}}^{\\top}\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}{\\bm{w}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(106)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">for every <math id=\"A3.SS4.p11.m8\" class=\"ltx_Math\" alttext=\"{\\bm{w}}\" display=\"inline\"><semantics><mi>𝒘</mi><annotation encoding=\"application/x-tex\">{\\bm{w}}</annotation></semantics></math>. Since <math id=\"A3.SS4.p11.m9\" class=\"ltx_Math\" alttext=\"{\\bm{A}}^{\\top}{\\bm{A}}\" display=\"inline\"><semantics><mrow><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝑨</mi></mrow><annotation encoding=\"application/x-tex\">{\\bm{A}}^{\\top}{\\bm{A}}</annotation></semantics></math> and <math id=\"A3.SS4.p11.m10\" class=\"ltx_Math\" alttext=\"\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}\" display=\"inline\"><semantics><mrow><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}</annotation></semantics></math> are symmetric positive semidefinite matrices, for <math id=\"A3.SS4.p11.m11\" class=\"ltx_Math\" alttext=\"i\\in[d]\" display=\"inline\"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><mi>d</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in[d]</annotation></semantics></math>, the eigenvalues <math id=\"A3.SS4.p11.m12\" class=\"ltx_math_unparsed\" alttext=\"\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)\\leq\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)\" display=\"inline\"><semantics><mrow><mrow><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>)</mo></mrow><mo>≤</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)\\leq\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)</annotation></semantics></math>, see for example Corollary 7.7.4 of <cite class=\"ltx_cite ltx_citemacro_citet\">Horn &amp; Johnson (<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">1985</a>)</cite> .</p>\n</div>\n<div id=\"A3.SS4.p12\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">From <a href=\"#A3.E59\" title=\"In Simplifying the forgetting expression ‣ C.3 Proof of Theorem 3.5 ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span> <span class=\"ltx_text ltx_ref_tag\">59</span></a>, and using the fact that <math id=\"A3.SS4.p12.m1\" class=\"ltx_Math\" alttext=\"{\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}={\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{2}{\\bm{P}}_{1}\" display=\"inline\"><semantics><mrow><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>1</mn></msub></mrow><mo>=</mo><mrow><msub><mi>𝑷</mi><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝑷</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>��</mi><mn>1</mn></msub></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}={\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{2}{\\bm{P}}_{1}</annotation></semantics></math>, forgetting for a fixed <math id=\"A3.SS4.p12.m2\" class=\"ltx_Math\" alttext=\"{{\\bm{w}}^{*}}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{{\\bm{w}}^{*}}</annotation></semantics></math> is</p>\n<table id=\"A4.EGx82\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E107\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E107.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle F({\\bm{w}}_{2})=\\mathopen{}\\mathclose{{\\left\\|{(\\mathbf{I}-{\\bm{P}}_{1}){\\bm{P}}_{2}{\\bm{P}}_{1}{{\\bm{w}}^{*}}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><mrow><mo stretchy=\"false\">(</mo><mi>𝐈</mi><mo>−</mo><msub><mi>𝑷</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle F({\\bm{w}}_{2})=\\mathopen{}\\mathclose{{\\left\\|{(\\mathbf{I}-{\\bm{P}}_{1}){\\bm{P}}_{2}{\\bm{P}}_{1}{{\\bm{w}}^{*}}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E107.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}{{\\bm{w}}^{*}}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{{\\bm{w}}^{*}}}}}\\right\\|_{2}^{2}\" display=\"inline\"><semantics><mrow><mo>=</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup><mo lspace=\"0em\">−</mo><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>1</mn></msub><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><msubsup><mo lspace=\"0em\" stretchy=\"true\">∥</mo><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}{{\\bm{w}}^{*}}}}}\\right\\|_{2}^{2}-\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{1}{\\bm{P}}_{2}{\\bm{P}}_{1}{{\\bm{w}}^{*}}}}}\\right\\|_{2}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(107)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E108\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E108.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle={{{\\bm{w}}^{*}}}^{\\top}{\\bm{A}}^{\\top}{\\bm{A}}{{\\bm{w}}^{*}}-{{\\bm{w}}^{*}}^{\\top}\\mathopen{}\\mathclose{{\\left({\\bm{A}}^{\\top}{\\bm{A}}}}\\right)^{2}{{\\bm{w}}^{*}}.\" display=\"inline\"><semantics><mrow><mo>=</mo><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi><msup><mi>𝒘</mi><mo>∗</mo></msup><mo>−</mo><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo><msup><mi></mi><mn>2</mn></msup><mi>𝒘</mi><msup><mi></mi><mo>∗</mo></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle={{{\\bm{w}}^{*}}}^{\\top}{\\bm{A}}^{\\top}{\\bm{A}}{{\\bm{w}}^{*}}-{{\\bm{w}}^{*}}^{\\top}\\mathopen{}\\mathclose{{\\left({\\bm{A}}^{\\top}{\\bm{A}}}}\\right)^{2}{{\\bm{w}}^{*}}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(108)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Since <math id=\"A3.SS4.p12.m3\" class=\"ltx_math_unparsed\" alttext=\"\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}{{\\bm{w}}^{*}}^{\\top}}}\\right]=\\mathbf{I}\" display=\"inline\"><semantics><mrow><msub><mo>𝔼</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></msub><mrow><mi></mi><mrow><mo>[</mo><msup><mi>𝒘</mi><mo>∗</mo></msup><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts></mrow></mrow><mo>]</mo><mo>=</mo><mi>𝐈</mi></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[{{\\bm{w}}^{*}}{{\\bm{w}}^{*}}^{\\top}}}\\right]=\\mathbf{I}</annotation></semantics></math>, taking the expectation over <math id=\"A3.SS4.p12.m4\" class=\"ltx_Math\" alttext=\"{{\\bm{w}}^{*}}\" display=\"inline\"><semantics><msup><mi>𝒘</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">{{\\bm{w}}^{*}}</annotation></semantics></math> gives</p>\n<table id=\"A4.EGx83\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E109\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E109.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F({\\bm{w}}_{2})}}\\right]\" display=\"inline\"><semantics><mrow><munder><mo>𝔼</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></munder><mrow><mi></mi><mrow><mo>[</mo><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F({\\bm{w}}_{2})}}\\right]</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E109.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[\\operatorname{Tr}\\mathopen{}\\mathclose{{\\left({{\\bm{w}}^{*}}^{\\top}({\\bm{A}}^{\\top}{\\bm{A}}-({\\bm{A}}^{\\top}{\\bm{A}})^{2}){{\\bm{w}}^{*}}}}\\right)}}\\right]=\\operatorname{Tr}\\mathopen{}\\mathclose{{\\left[{\\bm{A}}^{\\top}{\\bm{A}}-({\\bm{A}}^{\\top}{\\bm{A}})^{2}}}\\right]\" display=\"inline\"><semantics><mrow><mrow><mo rspace=\"0.1389em\">=</mo><munder><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></munder><mrow><mi></mi><mrow><mo>[</mo><mi>Tr</mi><mrow><mi></mi><mrow><mo>(</mo><mmultiscripts><mi>𝒘</mi><mrow></mrow><mo>∗</mo><mrow></mrow><mo>⊤</mo></mmultiscripts><mrow><mo stretchy=\"false\">(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi><mo>−</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><msup><mi>𝒘</mi><mo>∗</mo></msup></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mo>=</mo><mi>Tr</mi><mrow><mi></mi><mrow><mo>[</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi><mo>−</mo><msup><mrow><mo stretchy=\"false\">(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[\\operatorname{Tr}\\mathopen{}\\mathclose{{\\left({{\\bm{w}}^{*}}^{\\top}({\\bm{A}}^{\\top}{\\bm{A}}-({\\bm{A}}^{\\top}{\\bm{A}})^{2}){{\\bm{w}}^{*}}}}\\right)}}\\right]=\\operatorname{Tr}\\mathopen{}\\mathclose{{\\left[{\\bm{A}}^{\\top}{\\bm{A}}-({\\bm{A}}^{\\top}{\\bm{A}})^{2}}}\\right]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(109)</span></td>\n</tr></tbody>\n<tbody id=\"A3.E110\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"A3.E110.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle=\\sum_{i=1}^{d}\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)^{2}.\" display=\"inline\"><semantics><mrow><mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo></mrow><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\sum_{i=1}^{d}\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(110)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Similar calculations would show that with replay</p>\n<table id=\"A4.EGx84\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E111\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E111.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F(\\tilde{{\\bm{w}}}_{2})}}\\right]=\\sum_{i=1}^{d}\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)^{2}.\" display=\"inline\"><semantics><mrow><mrow><mrow><munder><mo>𝔼</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></munder><mrow><mi></mi><mrow><mo>[</mo><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo>=</mo><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover></mstyle><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>)</mo></mrow><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>)</mo><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F(\\tilde{{\\bm{w}}}_{2})}}\\right]=\\sum_{i=1}^{d}\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(111)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Since <math id=\"A3.SS4.p12.m5\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}}}}\\right\\|_{\\text{op}}\\leq\\frac{\\sqrt{2}}{2}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo lspace=\"0em\" rspace=\"0.167em\" stretchy=\"true\">∥</mo><msub><mi>𝑷</mi><mn>2</mn></msub><msub><mi>𝑷</mi><mn>1</mn></msub></mrow></mrow><msub><mo lspace=\"0em\" rspace=\"0.0835em\" stretchy=\"true\">∥</mo><mtext>op</mtext></msub><mo lspace=\"0.0835em\">≤</mo><mfrac><msqrt><mn>2</mn></msqrt><mn>2</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\|{{\\bm{P}}_{2}{\\bm{P}}_{1}}}}\\right\\|_{\\text{op}}\\leq\\frac{\\sqrt{2}}{2}</annotation></semantics></math>, all the eigenvalues of both <math id=\"A3.SS4.p12.m6\" class=\"ltx_Math\" alttext=\"{\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}\" display=\"inline\"><semantics><mrow><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">{\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}</annotation></semantics></math> and <math id=\"A3.SS4.p12.m7\" class=\"ltx_Math\" alttext=\"{\\bm{A}}^{\\top}{\\bm{A}}\" display=\"inline\"><semantics><mrow><msup><mi>𝑨</mi><mo>⊤</mo></msup><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>𝑨</mi></mrow><annotation encoding=\"application/x-tex\">{\\bm{A}}^{\\top}{\\bm{A}}</annotation></semantics></math> are less than <math id=\"A3.SS4.p12.m8\" class=\"ltx_Math\" alttext=\"1/2\" display=\"inline\"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">1/2</annotation></semantics></math>.\nThe function <math id=\"A3.SS4.p12.m9\" class=\"ltx_Math\" alttext=\"f(x)=x-x^{2}\" display=\"inline\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>x</mi><mo>−</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">f(x)=x-x^{2}</annotation></semantics></math> is monotonically increasing in the interval <math id=\"A3.SS4.p12.m10\" class=\"ltx_Math\" alttext=\"[0,1/2]\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1/2]</annotation></semantics></math>, so we have that for each <math id=\"A3.SS4.p12.m11\" class=\"ltx_Math\" alttext=\"i\\in[d]\" display=\"inline\"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy=\"false\">[</mo><mi>d</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in[d]</annotation></semantics></math>, since <math id=\"A3.SS4.p12.m12\" class=\"ltx_math_unparsed\" alttext=\"\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)\\leq\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)\" display=\"inline\"><semantics><mrow><mrow><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>)</mo></mrow><mo>≤</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)\\leq\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)</annotation></semantics></math></p>\n<table id=\"A4.EGx85\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"A3.E112\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"A3.E112.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)^{2}\\leq\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)^{2}.\" display=\"inline\"><semantics><mrow><mrow><msup><mrow><mrow><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>)</mo></mrow><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover><mo>⊤</mo></msup><mover accent=\"true\"><mi>𝑨</mi><mo>~</mo></mover></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo>≤</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo></mrow><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mrow><mi></mi><mrow><mo>(</mo><msup><mi>𝑨</mi><mo>⊤</mo></msup><mi>𝑨</mi></mrow></mrow><mo>)</mo><msup><mi></mi><mn>2</mn></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({\\tilde{{\\bm{A}}}^{\\top}\\tilde{{\\bm{A}}}}}}\\right)^{2}\\leq\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)-\\lambda_{i}\\mathopen{}\\mathclose{{\\left({{\\bm{A}}^{\\top}{\\bm{A}}}}}\\right)^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(112)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">The equation above combined with Equations <a href=\"#A3.E111\" title=\"Equation 111 ‣ C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">111</span></a> and <a href=\"#A3.E108\" title=\"Equation 108 ‣ C.4 Proofs of claims and propositions ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">108</span></a> implies that <math id=\"A3.SS4.p12.m13\" class=\"ltx_math_unparsed\" alttext=\"\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F(\\tilde{{\\bm{w}}}_{2})}}\\right]\\leq\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F({\\bm{w}}_{2})}}\\right]\" display=\"inline\"><semantics><mrow><mrow><msub><mo>𝔼</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></msub><mrow><mi></mi><mrow><mo>[</mo><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>𝒘</mi><mo>~</mo></mover><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mo rspace=\"0.1389em\">≤</mo><msub><mo lspace=\"0.1389em\" rspace=\"0em\">𝔼</mo><msup><mi>𝒘</mi><mo>∗</mo></msup></msub><mrow><mi></mi><mrow><mo>[</mo><mi>F</mi><mrow><mo stretchy=\"false\">(</mo><msub><mi>𝒘</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F(\\tilde{{\\bm{w}}}_{2})}}\\right]\\leq\\operatorname*{\\mathbb{E}}_{{{\\bm{w}}^{*}}}\\mathopen{}\\mathclose{{\\left[F({\\bm{w}}_{2})}}\\right]</annotation></semantics></math>.</p>\n</div>\n<div id=\"A3.SS4.p13\" class=\"ltx_para\">\n<p class=\"ltx_p\">∎</p>\n</div>\n</section>\n</section>\n<section id=\"A4\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Details of the Experiments</h2>\n\n<section id=\"A4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">D.1 </span>Continual Linear Regression Experiments</h3>\n\n<div id=\"A4.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The linear models were trained starting from <math id=\"A4.SS1.p1.m1\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn>0</mn></math> using SGD while the neural nets were trained with Adam and randomly initialized (Glorot uniform).\nThe models are trained until convergence. Unless explicitly specified otherwise, the MLPs have one hidden layer of width <math id=\"A4.SS1.p1.m2\" class=\"ltx_Math\" alttext=\"128d\" display=\"inline\"><semantics><mrow><mn>128</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">128d</annotation></semantics></math> where <math id=\"A4.SS1.p1.m3\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the input dimension.\nThe number of samples per task was also <math id=\"A4.SS1.p1.m4\" class=\"ltx_Math\" alttext=\"10\" display=\"inline\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> and <math id=\"A4.SS1.p1.m5\" class=\"ltx_Math\" alttext=\"100\" display=\"inline\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> for the <math id=\"A4.SS1.p1.m6\" class=\"ltx_Math\" alttext=\"3\" display=\"inline\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> and <math id=\"A4.SS1.p1.m7\" class=\"ltx_Math\" alttext=\"50\" display=\"inline\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> dimensional case respectively.</p>\n</div>\n<section id=\"A4.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Training details</h5>\n\n<div id=\"A4.SS1.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Both of the linear and MLP models were trained for <math id=\"A4.SS1.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"7000\" display=\"inline\"><semantics><mn>7000</mn><annotation encoding=\"application/x-tex\">7000</annotation></semantics></math> epochs on each task to produce <a href=\"#S4.F1.sf1\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>, and <math id=\"A4.SS1.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"5000\" display=\"inline\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> epochs in <a href=\"#S4.F1.sf2\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. The batch sizes used for experiments with <math id=\"A4.SS1.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"3\" display=\"inline\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> and <math id=\"A4.SS1.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"50\" display=\"inline\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> dimensions are <math id=\"A4.SS1.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> and <math id=\"A4.SS1.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"32\" display=\"inline\"><semantics><mn>32</mn><annotation encoding=\"application/x-tex\">32</annotation></semantics></math> respectively.\nThe linear model in <a href=\"#S4.F1.sf1\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> was trained with plain SGD with learning rate <math id=\"A4.SS1.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. The linear model for the higher dimensional case in <a href=\"#S4.F1.sf2\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> was also trained with SGD with learning rate <math id=\"A4.SS1.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> on the first task, while for the second task, the learning rate was <math id=\"A4.SS1.SSS0.Px1.p1.m9\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math> with exponential decay rate <math id=\"A4.SS1.SSS0.Px1.p1.m10\" class=\"ltx_Math\" alttext=\"0.8\" display=\"inline\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math>.</p>\n</div>\n<div id=\"A4.SS1.SSS0.Px1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In the three dimensional case (<a href=\"#S4.F1.sf1\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>), the MLP was trained on the first task using Adam with learning rate <math id=\"A4.SS1.SSS0.Px1.p2.m1\" class=\"ltx_Math\" alttext=\"8\\mathrm{e}{-4}\" display=\"inline\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">e</mi></mrow><mo>−</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">8\\mathrm{e}{-4}</annotation></semantics></math> and exponential decay rate <math id=\"A4.SS1.SSS0.Px1.p2.m2\" class=\"ltx_Math\" alttext=\"0.7\" display=\"inline\"><semantics><mn>0.7</mn><annotation encoding=\"application/x-tex\">0.7</annotation></semantics></math>. On the second task, the learning rate was <math id=\"A4.SS1.SSS0.Px1.p2.m3\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics><mn>0.001</mn><annotation encoding=\"application/x-tex\">0.001</annotation></semantics></math> with exponential decay rate <math id=\"A4.SS1.SSS0.Px1.p2.m4\" class=\"ltx_Math\" alttext=\"0.6\" display=\"inline\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math>.\nIn the <math id=\"A4.SS1.SSS0.Px1.p2.m5\" class=\"ltx_Math\" alttext=\"50\" display=\"inline\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> dimensional case, the MLP was trained on the first task starting with learning rate <math id=\"A4.SS1.SSS0.Px1.p2.m6\" class=\"ltx_Math\" alttext=\"1e{-4}\" display=\"inline\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>e</mi></mrow><mo>−</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">1e{-4}</annotation></semantics></math> and exponential decay rate <math id=\"A4.SS1.SSS0.Px1.p2.m7\" class=\"ltx_Math\" alttext=\"0.6\" display=\"inline\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math>. The starting learning rate on the second task was <math id=\"A4.SS1.SSS0.Px1.p2.m8\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics><mn>0.001</mn><annotation encoding=\"application/x-tex\">0.001</annotation></semantics></math> and the exponential decay rate was <math id=\"A4.SS1.SSS0.Px1.p2.m9\" class=\"ltx_Math\" alttext=\"0.6\" display=\"inline\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math>.</p>\n</div>\n<div id=\"A4.SS1.SSS0.Px1.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">These parameters were picked such that the training converges and training error is minimized.\nWe have plotted the forgetting with higher number of independent runs, since the variance in error is quite high. Note that the statement of the average case result is on the expectation, and hence the error bars show standard mean error. The construction of the input distribution is the same as the one given in the proof of the theorem with <math id=\"A4.SS1.SSS0.Px1.p3.m1\" class=\"ltx_Math\" alttext=\"\\epsilon=0.2\" display=\"inline\"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.2</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"A4.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Replay Implementation</h5>\n\n<div id=\"A4.SS1.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Let <math id=\"A4.SS1.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> be the batch size for training on the second task, during each training step of the second task, a random batch of <math id=\"A4.SS1.SSS0.Px2.p1.m2\" class=\"ltx_math_unparsed\" alttext=\"b^{\\prime}=\\min\\mathopen{}\\mathclose{{\\left\\{b,m}}\\right\\}\" display=\"inline\"><semantics><mrow><msup><mi>b</mi><mo>′</mo></msup><mo>=</mo><mi>min</mi><mrow><mi></mi><mrow><mo>{</mo><mi>b</mi><mo>,</mo><mi>m</mi></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">b^{\\prime}=\\min\\mathopen{}\\mathclose{{\\left\\{b,m}}\\right\\}</annotation></semantics></math>\nmany samples from the second task are combined with a random batch of <math id=\"A4.SS1.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> samples from <math id=\"A4.SS1.SSS0.Px2.p1.m4\" class=\"ltx_Math\" alttext=\"({\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}})\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>𝑿</mi><mtext>mem</mtext></msup><mo>,</mo><msup><mi>𝒚</mi><mtext>mem</mtext></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{X}}^{\\text{mem}},{\\bm{y}}^{\\text{mem}})</annotation></semantics></math>, and they are weighted by <math id=\"A4.SS1.SSS0.Px2.p1.m5\" class=\"ltx_Math\" alttext=\"\\frac{b}{b^{\\prime}}\" display=\"inline\"><semantics><mfrac><mi>b</mi><msup><mi>b</mi><mo>′</mo></msup></mfrac><annotation encoding=\"application/x-tex\">\\frac{b}{b^{\\prime}}</annotation></semantics></math> so that their total weight is equal to that of task two samples.</p>\n</div>\n</section>\n<section id=\"A4.SS1.SSS0.Px3\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Extension of the three dimensional construction in <a href=\"#A3.Thmtheorem1\" title=\"Theorem C.1 (Average case replay). ‣ C.2 Lower dimensional average case results ‣ Appendix C Proofs ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Theorem</span> <span class=\"ltx_text ltx_ref_tag\">C.1</span></a> to higher dimensions.</h5>\n\n<div id=\"A4.SS1.SSS0.Px3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Fix an arbitrary orthonormal basis <math id=\"A4.SS1.SSS0.Px3.p1.m1\" class=\"ltx_math_unparsed\" alttext=\"\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d}}}\\right\\}\" display=\"inline\"><semantics><mrow><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow><mo>}</mo></mrow><annotation encoding=\"application/x-tex\">\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},\\dots,{\\bm{v}}_{d}}}\\right\\}</annotation></semantics></math> and <math id=\"A4.SS1.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"\\epsilon=0.4\" display=\"inline\"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.4</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.4</annotation></semantics></math>.\nSet <math id=\"A4.SS1.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{u}}=\\epsilon{\\bm{v}}_{2}+\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}\" display=\"inline\"><semantics><mrow><mi>𝒖</mi><mo>=</mo><mrow><mrow><mi>ϵ</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><msup><mi>ϵ</mi><mn>2</mn></msup></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>𝒗</mi><mn>3</mn></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}=\\epsilon{\\bm{v}}_{2}+\\sqrt{1-\\epsilon^{2}}{\\bm{v}}_{3}</annotation></semantics></math> like the three dimensional construction.\nThe first task spans the <math id=\"A4.SS1.SSS0.Px3.p1.m4\" class=\"ltx_Math\" alttext=\"d-1\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d-1</annotation></semantics></math> dimensional subspace <math id=\"A4.SS1.SSS0.Px3.p1.m5\" class=\"ltx_math_unparsed\" alttext=\"\\mathrm{span}\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{u}},{\\bm{v}}_{4},\\dots,{\\bm{v}}_{d}}}\\right\\}}}\\right)\" display=\"inline\"><semantics><mrow><mi>span</mi><mrow><mi></mi><mrow><mo>(</mo><mrow><mi></mi><mrow><mo>{</mo><msub><mi>𝒗</mi><mn>1</mn></msub><mo>,</mo><mi>𝒖</mi><mo>,</mo><msub><mi>𝒗</mi><mn>4</mn></msub><mo>,</mo><mi mathvariant=\"normal\">…</mi><mo>,</mo><msub><mi>𝒗</mi><mi>d</mi></msub></mrow></mrow><mo>}</mo></mrow></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{span}\\mathopen{}\\mathclose{{\\left(\\mathopen{}\\mathclose{{\\left\\{{\\bm{v}}_{1},{\\bm{u}},{\\bm{v}}_{4},\\dots,{\\bm{v}}_{d}}}\\right\\}}}\\right)</annotation></semantics></math>.\nAs in the three dimensional construction, the second task is spanned by <math id=\"A4.SS1.SSS0.Px3.p1.m6\" class=\"ltx_Math\" alttext=\"{\\bm{v}}_{3}\" display=\"inline\"><semantics><msub><mi>𝒗</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{v}}_{3}</annotation></semantics></math> only.</p>\n</div>\n<div id=\"A4.SS1.SSS0.Px3.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In both the three dimensional and higher dimensional case <math id=\"A4.SS1.SSS0.Px3.p2.m1\" class=\"ltx_Math\" alttext=\"d-1\" display=\"inline\"><semantics><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d-1</annotation></semantics></math> samples from the first task would information theoretically be sufficient to learn <math id=\"A4.SS1.SSS0.Px3.p2.m2\" class=\"ltx_Math\" alttext=\"w^{*}\" display=\"inline\"><semantics><msup><mi>w</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">w^{*}</annotation></semantics></math>, but training until close to zero error might be challenging especially in the linear case. We experimentally verify this by directly solving the linear system and observing that replaying a few samples increases forgetting, while replaying <math id=\"A4.SS1.SSS0.Px3.p2.m3\" class=\"ltx_Math\" alttext=\"50\" display=\"inline\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> samples will result in zero forgetting.</p>\n</div>\n</section>\n<section id=\"A4.SS1.SSS0.Px4\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Narrower Network</h5>\n\n<div id=\"A4.SS1.SSS0.Px4.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We also include <a href=\"#A4.F8\" title=\"In Narrower Network ‣ D.1 Continual Linear Regression Experiments ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">8</span></a>,\nwhich shows forgetting against the number of replay samples for a smaller network, where the width of the hidden layer is <math id=\"A4.SS1.SSS0.Px4.p1.m1\" class=\"ltx_Math\" alttext=\"4d=200\" display=\"inline\"><semantics><mrow><mrow><mn>4</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow><mo>=</mo><mn>200</mn></mrow><annotation encoding=\"application/x-tex\">4d=200</annotation></semantics></math>. The input data is generated with the same distributional parameters as in <a href=\"#S4.F1.sf2\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. The training parameters for the second task were slightly different here.\nSpecifically, the exponential learning decay rate used on the second task was <math id=\"A4.SS1.SSS0.Px4.p1.m2\" class=\"ltx_Math\" alttext=\"0.9\" display=\"inline\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>.</p>\n</div>\n<div id=\"A4.SS1.SSS0.Px4.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We note that it is possible that regularization, and training with small learning rate affect the observed pattern, especially with narrower networks.\nHowever, studying the effect of regularization and hyper parameters on forgetting with replay is outside the scope of this paper.</p>\n</div>\n<figure id=\"A4.F8\" class=\"ltx_figure\"><img src=\"./assets/x13.png\" id=\"A4.F8.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"498\" height=\"271\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>Same experiment as in <a href=\"#S4.F1.sf2\" title=\"In Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> with a network of width <math id=\"A4.F8.m2\" class=\"ltx_Math\" alttext=\"4d\" display=\"inline\"><semantics><mrow><mn>4</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">4d</annotation></semantics></math> instead</figcaption>\n</figure>\n</section>\n<section id=\"A4.SS1.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">D.1.1 </span>The effect of the angle between tasks while training with a MLP</h4>\n\n<div id=\"A4.SS1.SSS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">We discussed in <a href=\"#S3.SS2\" title=\"3.2 Average Case: Forgetting in a Random Sample Setting ‣ 3 Replay Can Provably Increase Forgetting in Continual Linear Regression ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">3.2</span></a> how replay changes the angle between the two tasks in a way that increases forgetting on average. To understand whether a similar mechanism is responsible for the increase in forgetting due to replay in the nonlinear case,\nwe also look at the effect of the angle between two (linear) tasks on forgetting while using a nonlinear model for training. To do this, we pick the two tasks to be spanned by two <math id=\"A4.SS1.SSS1.p1.m1\" class=\"ltx_Math\" alttext=\"9\" display=\"inline\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> dimensional subspaces in <math id=\"A4.SS1.SSS1.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathbb{R}^{10}\" display=\"inline\"><semantics><msup><mi>ℝ</mi><mn>10</mn></msup><annotation encoding=\"application/x-tex\">\\mathbb{R}^{10}</annotation></semantics></math>, so that their null spaces are essentially given by two vectors <math id=\"A4.SS1.SSS1.p1.m3\" class=\"ltx_Math\" alttext=\"{\\bm{u}}_{1},{\\bm{u}}_{2}\" display=\"inline\"><semantics><mrow><msub><mi>𝒖</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒖</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">{\\bm{u}}_{1},{\\bm{u}}_{2}</annotation></semantics></math>.\nWe vary the angle between <math id=\"A4.SS1.SSS1.p1.m4\" class=\"ltx_Math\" alttext=\"{\\bm{u}}_{1}\" display=\"inline\"><semantics><msub><mi>𝒖</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{u}}_{1}</annotation></semantics></math> and <math id=\"A4.SS1.SSS1.p1.m5\" class=\"ltx_Math\" alttext=\"{\\bm{u}}_{2}\" display=\"inline\"><semantics><msub><mi>𝒖</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">{\\bm{u}}_{2}</annotation></semantics></math> and for each angle measure forgetting on the first task after training on the second task, see <a href=\"#A4.F9\" title=\"In D.1.1 The effect of the angle between tasks while training with a MLP ‣ D.1 Continual Linear Regression Experiments ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nWe can see that forgetting of the MLP and linear model behave differently around angels that are close to <math id=\"A4.SS1.SSS1.p1.m6\" class=\"ltx_Math\" alttext=\"\\pi/2\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/2</annotation></semantics></math>.\nIn our three dimensional average case construction this won’t matter, since initially without replay, the angle between the subspaces is close to zero, while with replay, the angle increases slightly but not a lot with high probability.\nSpecifically, the construction is such that probability of replay leading to the angle being close to <math id=\"A4.SS1.SSS1.p1.m7\" class=\"ltx_Math\" alttext=\"\\pi/2\" display=\"inline\"><semantics><mrow><mi>π</mi><mo>/</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\pi/2</annotation></semantics></math> is very small.</p>\n</div>\n<figure id=\"A4.F9\" class=\"ltx_figure\"><img src=\"./assets/x14.png\" id=\"A4.F9.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"415\" height=\"311\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>When training using a MLP, the angle between task null spaces mostly has a similar effect on forgetting as the linear case, with the exception of near orthogonal angles. Each point is averaged over 50 runs and the error bars here show standard deviation.</figcaption>\n</figure>\n<div id=\"A4.SS1.SSS1.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Here we give more details on the experiments used to get <a href=\"#A4.F9\" title=\"In D.1.1 The effect of the angle between tasks while training with a MLP ‣ D.1 Continual Linear Regression Experiments ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">9</span></a>.\nThe input distributions and what we referred to as the angle between two tasks has been already discussed in the last paragraph of <a href=\"#S4\" title=\"4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>. The number of samples used per task is <math id=\"A4.SS1.SSS1.p2.m1\" class=\"ltx_Math\" alttext=\"100\" display=\"inline\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>. The linear model was trained using SGD with learning rate <math id=\"A4.SS1.SSS1.p2.m2\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>. During training on the second task there was exponential decay rate of <math id=\"A4.SS1.SSS1.p2.m3\" class=\"ltx_Math\" alttext=\"0.95\" display=\"inline\"><semantics><mn>0.95</mn><annotation encoding=\"application/x-tex\">0.95</annotation></semantics></math>.\nThe MLP had one hidden layer of width <math id=\"A4.SS1.SSS1.p2.m4\" class=\"ltx_Math\" alttext=\"128d\" display=\"inline\"><semantics><mrow><mn>128</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">128d</annotation></semantics></math>, and it was trained on the first task with starting learning rate <math id=\"A4.SS1.SSS1.p2.m5\" class=\"ltx_Math\" alttext=\"1\\mathrm{e}-4\" display=\"inline\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">e</mi></mrow><mo>−</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">1\\mathrm{e}-4</annotation></semantics></math> and on the second task with initial learning rate <math id=\"A4.SS1.SSS1.p2.m6\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics><mn>0.001</mn><annotation encoding=\"application/x-tex\">0.001</annotation></semantics></math>. in both cases (MLP training on task one and two), there was an exponential decay rate <math id=\"A4.SS1.SSS1.p2.m7\" class=\"ltx_Math\" alttext=\"0.6\" display=\"inline\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math>.\nAll the models for this experiment were trained for <math id=\"A4.SS1.SSS1.p2.m8\" class=\"ltx_Math\" alttext=\"5000\" display=\"inline\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> epochs with the batch size <math id=\"A4.SS1.SSS1.p2.m9\" class=\"ltx_Math\" alttext=\"32\" display=\"inline\"><semantics><mn>32</mn><annotation encoding=\"application/x-tex\">32</annotation></semantics></math>.</p>\n</div>\n</section>\n</section>\n<section id=\"A4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">D.2 </span>Experiments on MNIST</h3>\n\n<div id=\"A4.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">In all the experiments, a fully connected network with two hidden layers of size <math id=\"A4.SS2.p1.m1\" class=\"ltx_Math\" alttext=\"256\" display=\"inline\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> was used. In all cases, training on each task was for <math id=\"A4.SS2.p1.m2\" class=\"ltx_Math\" alttext=\"3\" display=\"inline\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> epochs, with batch size of <math id=\"A4.SS2.p1.m3\" class=\"ltx_Math\" alttext=\"32\" display=\"inline\"><semantics><mn>32</mn><annotation encoding=\"application/x-tex\">32</annotation></semantics></math>, and using Adam <cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma &amp; Ba, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2014</a>)</cite> with learning rate of <math id=\"A4.SS2.p1.m4\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics><mn>0.001</mn><annotation encoding=\"application/x-tex\">0.001</annotation></semantics></math>.</p>\n</div>\n<section id=\"A4.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Statistical tests.</h5>\n\n<div id=\"A4.SS2.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">When we compared the means, we used Welch’s t-test, which is similar to a student t-test while allowing the populations to have different variances.</p>\n</div>\n</section>\n<section id=\"A4.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Rotated MNIST.</h5>\n\n<div id=\"A4.SS2.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Rotated MNIST experiments are in a task incremental setting and use the training data for all the <math id=\"A4.SS2.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"10\" display=\"inline\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> digits. The training data on the second task is the same as the training data on the first task, except that it is rotated.\nForgetting is measure on test samples.\nThe replay is done the same as the regression experiments. That is, for each class, a random sample is combined with the samples in the each batch and the replay samples are up-weighted such that the replay sample has equal weight to the rest of the samples.\nThe no replay baseline is what the literature might call the fine tuning baseline. The network is sequentially trained on the two tasks. The optimizer is reset after training on the first task.</p>\n</div>\n</section>\n<section id=\"A4.SS2.SSS0.Px3\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Split MNIST.</h5>\n\n<div id=\"A4.SS2.SSS0.Px3.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">These experiments are in a class incremental setting, so the network had <math id=\"A4.SS2.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> output heads. During evaluation on the first task, only the logits corresponding to the classes in the first task were taken into account. This is the case with or without replay.\nAgain, the replay implementation here is similar to the regression experiments.</p>\n</div>\n</section>\n</section>\n<section id=\"A4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">D.3 </span>Compute Resources</h3>\n\n<section id=\"A4.SS3.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">D.3.1 </span>Regression Experiments</h4>\n\n<div id=\"A4.SS3.SSS1.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The experiment in <a href=\"#A4.F9\" title=\"In D.1.1 The effect of the angle between tasks while training with a MLP ‣ D.1 Continual Linear Regression Experiments ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">9</span></a> took about 20 hours on a machine with single NVIDIA GeForce RTX 4080 GPU.\nEach run of the experiments in figures <a href=\"#S4.F1\" title=\"Figure 1 ‣ 4.1 Empirical Evaluation of the Theoretical Results ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a href=\"#A4.F8\" title=\"Figure 8 ‣ Narrower Network ‣ D.1 Continual Linear Regression Experiments ‣ Appendix D Details of the Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> would take about <math id=\"A4.SS3.SSS1.p1.m1\" class=\"ltx_Math\" alttext=\"0.5-1\" display=\"inline\"><semantics><mrow><mn>0.5</mn><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0.5-1</annotation></semantics></math> hour on a single NVIDIA A100-SXM4-80GB GPU.\nAll the experiments did not use a significant amount of memory, since the input data was at most <math id=\"A4.SS3.SSS1.p1.m2\" class=\"ltx_Math\" alttext=\"50\" display=\"inline\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math> dimensional.</p>\n</div>\n</section>\n</section>\n<section id=\"A4.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">D.4 </span>MNIST Experiments</h3>\n\n<div id=\"A4.SS4.p1\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">The experiments in <a href=\"#S4.F2\" title=\"In 4.2.1 Rotated MNIST ‣ 4.2 Experiments on MNIST ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a> took about <math id=\"A4.SS4.p1.m1\" class=\"ltx_Math\" alttext=\"6\" display=\"inline\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math> hours on a machine with a single NVIDIA GeForce RTX 4080 GPU for each rotation.\nThe experiment in <a href=\"#S4.F4\" title=\"In 4.2.2 Split MNIST ‣ 4.2 Experiments on MNIST ‣ 4 Experiments ‣ Replay can provably increase forgetting\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a> took about <math id=\"A4.SS4.p1.m2\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> hours on the same machine.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section><div class=\"ltx_rdf\" about=\"\" property=\"dcterms:title\" content=\"Overleaf Example\"></div>",
  "css": "",
  "arxiv_id": "2506.04377",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:27.925Z"
}
