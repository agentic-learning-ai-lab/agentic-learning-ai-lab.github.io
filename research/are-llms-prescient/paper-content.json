{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">Traditional Large Language Model (LLM) benchmarks are often static, and do not reflect real-world information that evolves over time. This presents two significant challenges. First, as LLMs are updated, there is a risk that static benchmarks become outdated and more vulnerable to data leakage, where their content might end up in the training data of newer models. This undermines the reliability of performance assessments on these benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et al., <a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">2023</a>; Xu et al., <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">2024</a>; McIntosh et al., <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">2025</a>; Li &amp; Flanigan, <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2024</a>)</cite>. Second, static benchmarks often lack temporal information, making it difficult to track models’ performance variations over time <cite class=\"ltx_cite ltx_citemacro_citep\">(McIntosh et al., <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">2025</a>)</cite>. This creates a need for evaluation methods that always remain relevant and incorporate temporal dynamics.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">Daily news provides a natural setting for continuous evaluation of LLMs. Since the world is constantly changing, a benchmark designed around forecasting the next day’s news will never be out of date by construction. In addition to enabling continuous evaluation, forecasting is itself a longstanding challenge with significant implications across various domains, including healthcare, finance, and policymaking <cite class=\"ltx_cite ltx_citemacro_citep\">(Tetlock &amp; Gardner, <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">2016</a>; Dempsey et al., <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2017</a>; Gillingham et al., <a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">2018</a>; Lopez-Lira &amp; Tang, <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">2023</a>)</cite>. While human experts have traditionally made such forecasts, machine learning models, particularly LLMs, have emerged as promising alternatives due to their ability to learn from vast and diverse corpora <cite class=\"ltx_cite ltx_citemacro_citep\">(Halawi et al., <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>; Ye et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2024</a>; Yan et al., <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">2024</a>)</cite>. Several recent forecasting question-answer (QA) datasets have been developed <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2021</a>; Zou et al., <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>, however, they are limited in either size, scope, or they do not continuously keep pace with the rapidly changing world. More critically, the extent to which LLMs’ predictive abilities change over time remains understudied.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">In this work, we propose Daily Oracle—a continuous evaluation benchmark that uses automatically generated QA pairs from daily news to assess how the future prediction capabilities of LLMs evolve over time. The QA pairs are generated on a daily basis, consisting of True/False (TF) and Multiple Choice (MC) questions across various categories such as business, politics, and arts. Unlike traditional reading comprehension tasks, these QA pairs are designed to challenge LLMs to predict future events based on their own existing knowledge, effectively evaluating their temporal generalization and forecasting abilities.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">We continuously evaluate various LLMs, both with and without access to a limited archive of news articles. Our experiments reveal that LLMs experience <span class=\"ltx_text ltx_font_bold\" id=\"S1.p4.1.1\">significant performance degradation</span> between January 2020 and December 2024, with degradation becoming more pronounced before and after the models’ knowledge cutoff dates. On average, performance drops by 21.55% on TF questions and 11.33% on MC questions. While model performance can be improved with more recent news articles using Retrieval Augmented Generation (RAG) <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2020</a>)</cite>, the <span class=\"ltx_text ltx_font_bold\" id=\"S1.p4.1.2\">downward</span> trend persists, suggesting the challenge in maintaining its prediction ability over time.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.1\">To summarize, our key contributions are two-fold:</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i1.p1.1.1\">Continuous Forecasting Evaluation Benchmark</span>: We present Daily Oracle, the largest and most up-to-date forecasting dataset, composed of automatically generated QA pairs. This benchmark continuously evaluates LLMs’ temporal generalization and future prediction abilities using daily news, ensuring relevance over time and offering a challenging evaluation framework.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i2.p1.1.1\">Empirical Findings on Performance Degradation</span>: Since our benchmark provides new questions each day, we can study how model performance changes along the temporal axis. Our work effectively reveals a clear performance degradation pattern in LLMs’ forecasting accuracy over time. Additionally, we study how this pattern changes as the LLMs are given access to updated knowledge up to different times. Surprisingly, we find that, <span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i2.p1.1.2\">even when the model has access to recent information in an “open-book” setting,</span> it still experiences performance degradation. Moreover, the sheer degree of decline, along with its smoothness over time, was unexpected. On the one hand, this highlights the problems with outdated LLM pre-training data and on the other hand underscores the need for continuous model updating.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n<figure class=\"ltx_table\" id=\"S2.T1\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.12\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.12.13.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.12.13.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.12.13.1.1.1\" style=\"font-size:80%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.12.13.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.12.13.1.2.1\" style=\"font-size:80%;\">Continuous?</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.12.13.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.12.13.1.3.1\" style=\"font-size:80%;\">Interval</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.12.13.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.12.13.1.4.1\" style=\"font-size:80%;\">Forecast?</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.12.13.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.12.13.1.5.1\" style=\"font-size:80%;\">Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.12.13.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.12.13.1.6.1\" style=\"font-size:80%;\">Latest Update</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S2.T1.2.2.3\">\n<span class=\"ltx_text\" id=\"S2.T1.2.2.3.1\" style=\"font-size:80%;\">TimeQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.2.2.3.2.1\" style=\"font-size:80%;\">(</span>Chen et al.<span class=\"ltx_text\" id=\"S2.T1.2.2.3.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2021</a><span class=\"ltx_text\" id=\"S2.T1.2.2.3.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.1\">\n<span class=\"ltx_text\" id=\"S2.T1.1.1.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.1.1.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.2.2.4\"><span class=\"ltx_text\" id=\"S2.T1.2.2.4.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.2.2.2\">\n<span class=\"ltx_text\" id=\"S2.T1.2.2.2.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.2.2.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.2.2.5\"><span class=\"ltx_text\" id=\"S2.T1.2.2.5.1\" style=\"font-size:80%;\">20,000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.2.2.6\"><span class=\"ltx_text\" id=\"S2.T1.2.2.6.1\" style=\"font-size:80%;\">2021</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.4.4.3\">\n<span class=\"ltx_text\" id=\"S2.T1.4.4.3.1\" style=\"font-size:80%;\">SituatedQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.4.4.3.2.1\" style=\"font-size:80%;\">(</span>Zhang &amp; Choi<span class=\"ltx_text\" id=\"S2.T1.4.4.3.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">2021</a><span class=\"ltx_text\" id=\"S2.T1.4.4.3.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.3.3.1\">\n<span class=\"ltx_text\" id=\"S2.T1.3.3.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.3.3.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.4.4\"><span class=\"ltx_text\" id=\"S2.T1.4.4.4.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.4.2\">\n<span class=\"ltx_text\" id=\"S2.T1.4.4.2.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.4.4.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.4.4.5\"><span class=\"ltx_text\" id=\"S2.T1.4.4.5.1\" style=\"font-size:80%;\">4,757</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.4.4.6\"><span class=\"ltx_text\" id=\"S2.T1.4.4.6.1\" style=\"font-size:80%;\">2021</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.6.6.3\">\n<span class=\"ltx_text\" id=\"S2.T1.6.6.3.1\" style=\"font-size:80%;\">StreamingQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.6.6.3.2.1\" style=\"font-size:80%;\">(</span>Liska et al.<span class=\"ltx_text\" id=\"S2.T1.6.6.3.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2022</a><span class=\"ltx_text\" id=\"S2.T1.6.6.3.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.5.5.1\">\n<span class=\"ltx_text\" id=\"S2.T1.5.5.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.5.5.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.6.6.4\"><span class=\"ltx_text\" id=\"S2.T1.6.6.4.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.6.6.2\">\n<span class=\"ltx_text\" id=\"S2.T1.6.6.2.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.6.6.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.6.6.5\"><span class=\"ltx_text\" id=\"S2.T1.6.6.5.1\" style=\"font-size:80%;\">36,800</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.6.6.6\"><span class=\"ltx_text\" id=\"S2.T1.6.6.6.1\" style=\"font-size:80%;\">2021</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.8.8.3\">\n<span class=\"ltx_text\" id=\"S2.T1.8.8.3.1\" style=\"font-size:80%;\">RealTimeQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.8.8.3.2.1\" style=\"font-size:80%;\">(</span>Kasai et al.<span class=\"ltx_text\" id=\"S2.T1.8.8.3.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">2024</a><span class=\"ltx_text\" id=\"S2.T1.8.8.3.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.7.7.1\">\n<span class=\"ltx_text\" id=\"S2.T1.7.7.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.7.7.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.8.8.4\"><span class=\"ltx_text\" id=\"S2.T1.8.8.4.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.8.8.2\">\n<span class=\"ltx_text\" id=\"S2.T1.8.8.2.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.8.8.2.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.8.8.5\"><span class=\"ltx_text\" id=\"S2.T1.8.8.5.1\" style=\"font-size:80%;\">1,470</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.8.8.6\"><span class=\"ltx_text\" id=\"S2.T1.8.8.6.1\" style=\"font-size:80%;\">2023</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.9.9.2\">\n<span class=\"ltx_text\" id=\"S2.T1.9.9.2.1\" style=\"font-size:80%;\">FreshQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.9.9.2.2.1\" style=\"font-size:80%;\">(</span>Vu et al.<span class=\"ltx_text\" id=\"S2.T1.9.9.2.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2024</a><span class=\"ltx_text\" id=\"S2.T1.9.9.2.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.9.9.3\"><span class=\"ltx_text\" id=\"S2.T1.9.9.3.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.9.9.4\"><span class=\"ltx_text\" id=\"S2.T1.9.9.4.1\" style=\"font-size:80%;\">Weekly</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.9.9.1\">\n<span class=\"ltx_text\" id=\"S2.T1.9.9.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.9.9.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.9.9.5\"><span class=\"ltx_text\" id=\"S2.T1.9.9.5.1\" style=\"font-size:80%;\">600</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.9.9.6\"><span class=\"ltx_text\" id=\"S2.T1.9.9.6.1\" style=\"font-size:80%;\">2024</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.10.10.2\">\n<span class=\"ltx_text\" id=\"S2.T1.10.10.2.1\" style=\"font-size:80%;\">ForecastQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.10.10.2.2.1\" style=\"font-size:80%;\">(</span>Jin et al.<span class=\"ltx_text\" id=\"S2.T1.10.10.2.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2021</a><span class=\"ltx_text\" id=\"S2.T1.10.10.2.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.10.10.1\">\n<span class=\"ltx_text\" id=\"S2.T1.10.10.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.10.10.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.10.10.3\"><span class=\"ltx_text\" id=\"S2.T1.10.10.3.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.10.10.4\"><span class=\"ltx_text\" id=\"S2.T1.10.10.4.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.10.10.5\"><span class=\"ltx_text\" id=\"S2.T1.10.10.5.1\" style=\"font-size:80%;\">10,382</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.10.10.6\"><span class=\"ltx_text\" id=\"S2.T1.10.10.6.1\" style=\"font-size:80%;\">2019</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.11.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.11.11.2\">\n<span class=\"ltx_text\" id=\"S2.T1.11.11.2.1\" style=\"font-size:80%;\">AutoCast </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.11.11.2.2.1\" style=\"font-size:80%;\">(</span>Zou et al.<span class=\"ltx_text\" id=\"S2.T1.11.11.2.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a><span class=\"ltx_text\" id=\"S2.T1.11.11.2.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.11.11.1\">\n<span class=\"ltx_text\" id=\"S2.T1.11.11.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.11.11.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.11.11.3\"><span class=\"ltx_text\" id=\"S2.T1.11.11.3.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.11.11.4\"><span class=\"ltx_text\" id=\"S2.T1.11.11.4.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.11.11.5\"><span class=\"ltx_text\" id=\"S2.T1.11.11.5.1\" style=\"font-size:80%;\">6,707</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.11.11.6\"><span class=\"ltx_text\" id=\"S2.T1.11.11.6.1\" style=\"font-size:80%;\">2022</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.12.14.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.12.14.1.1\">\n<span class=\"ltx_text\" id=\"S2.T1.12.14.1.1.1\" style=\"font-size:80%;\">ForecastBench </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.12.14.1.1.2.1\" style=\"font-size:80%;\">(</span>Karger et al.<span class=\"ltx_text\" id=\"S2.T1.12.14.1.1.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2025</a><span class=\"ltx_text\" id=\"S2.T1.12.14.1.1.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.14.1.2\"><span class=\"ltx_text\" id=\"S2.T1.12.14.1.2.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.14.1.3\"><span class=\"ltx_text\" id=\"S2.T1.12.14.1.3.1\" style=\"font-size:80%;\">Biweekly</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.14.1.4\"><span class=\"ltx_text\" id=\"S2.T1.12.14.1.4.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.14.1.5\"><span class=\"ltx_text\" id=\"S2.T1.12.14.1.5.1\" style=\"font-size:80%;\">1,000</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.12.14.1.6\"><span class=\"ltx_text\" id=\"S2.T1.12.14.1.6.1\" style=\"font-size:80%;\">2024</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.12.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.12.12.2\">\n<span class=\"ltx_text\" id=\"S2.T1.12.12.2.1\" style=\"font-size:80%;\">TLB-forecast </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.12.12.2.2.1\" style=\"font-size:80%;\">(</span>Zhang et al.<span class=\"ltx_text\" id=\"S2.T1.12.12.2.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a><span class=\"ltx_text\" id=\"S2.T1.12.12.2.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.12.1\">\n<span class=\"ltx_text\" id=\"S2.T1.12.12.1.1\" style=\"font-size:80%;\"></span><svg class=\"ltx_picture\" height=\"10.02\" id=\"S2.T1.12.12.1.pic1\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g fill=\"#000000\" stroke=\"#000000\" transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\"><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-linecap=\"round\" stroke-width=\"0.7pt\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.12.3\"><span class=\"ltx_text\" id=\"S2.T1.12.12.3.1\" style=\"font-size:80%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.12.4\"><span class=\"ltx_text\" id=\"S2.T1.12.12.4.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.12.5\"><span class=\"ltx_text\" id=\"S2.T1.12.12.5.1\" style=\"font-size:80%;\">6,604</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.12.12.6\"><span class=\"ltx_text\" id=\"S2.T1.12.12.6.1\" style=\"font-size:80%;\">2022</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.12.15.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S2.T1.12.15.2.1\">\n<span class=\"ltx_text\" id=\"S2.T1.12.15.2.1.1\" style=\"font-size:80%;\">FreshBench </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" id=\"S2.T1.12.15.2.1.2.1\" style=\"font-size:80%;\">(</span>Zhu et al.<span class=\"ltx_text\" id=\"S2.T1.12.15.2.1.3.2.1.1\" style=\"font-size:80%;\">, </span><a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">2025</a><span class=\"ltx_text\" id=\"S2.T1.12.15.2.1.4.3\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.15.2.2\"><span class=\"ltx_text\" id=\"S2.T1.12.15.2.2.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.15.2.3\"><span class=\"ltx_text\" id=\"S2.T1.12.15.2.3.1\" style=\"font-size:80%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.15.2.4\"><span class=\"ltx_text\" id=\"S2.T1.12.15.2.4.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.12.15.2.5\"><span class=\"ltx_text\" id=\"S2.T1.12.15.2.5.1\" style=\"font-size:80%;\">2,769</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.12.15.2.6\"><span class=\"ltx_text\" id=\"S2.T1.12.15.2.6.1\" style=\"font-size:80%;\">2024</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.12.16.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.12.16.3.1\"><span class=\"ltx_text\" id=\"S2.T1.12.16.3.1.1\" style=\"font-size:80%;\">Daily Oracle (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.12.16.3.2\"><span class=\"ltx_text\" id=\"S2.T1.12.16.3.2.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.12.16.3.3\"><span class=\"ltx_text\" id=\"S2.T1.12.16.3.3.1\" style=\"font-size:80%;\">Daily</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.12.16.3.4\"><span class=\"ltx_text\" id=\"S2.T1.12.16.3.4.1\" style=\"font-size:80%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S2.T1.12.16.3.5\"><span class=\"ltx_text\" id=\"S2.T1.12.16.3.5.1\" style=\"font-size:80%;\">31,510</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.12.16.3.6\">\n<span class=\"ltx_text\" id=\"S2.T1.12.16.3.6.1\" style=\"font-size:80%;\">2024</span><sup class=\"ltx_sup\" id=\"S2.T1.12.16.3.6.2\"><span class=\"ltx_text\" id=\"S2.T1.12.16.3.6.2.1\" style=\"font-size:80%;\">*</span></sup>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<ul class=\"ltx_itemize ltx_centering ltx_figure_panel\" id=\"S2.I1\">\n<li class=\"ltx_item\" id=\"S2.I1.ix1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">*</span>\n<div class=\"ltx_para\" id=\"S2.I1.ix1.p1\">\n<p class=\"ltx_p\" id=\"S2.I1.ix1.p1.1\"><span class=\"ltx_text\" id=\"S2.I1.ix1.p1.1.1\" style=\"font-size:80%;\">Our experiments use the subset generated until December 2024. Daily Oracle itself remains active, continuing to generate new questions daily from 2025 onward.</span></p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_flex_break\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>We compare Daily Oracle with existing benchmarks in the literature. For continuously updated datasets (e.g. Daily Oracle, FreshQA, FreshBench, and ForecastBench), “Interval” refers to the dataset update interval, and “Size” and “Latest Update” refer to the fixed data currently available. Our Daily Oracle benchmark is the only forecasting benchmark which is <span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.14.1\">continuously updated every day</span> using questions generated from daily news.\n</figcaption>\n</figure>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Generalization of LLMs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px1.p1.1\"><cite class=\"ltx_cite ltx_citemacro_citet\">Lazaridou et al. (<a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2021</a>)</cite> define temporal generalization as the ability of Language Models to generalize well to future data from beyond their training period. They demonstrate that Transformer-XL’s performance deteriorates over time, evidenced by increasing perplexity when evaluated on post-training data. However, perplexity-based metrics have two main limitations: they cannot be applied to closed-source models lacking accessible logits, and increased perplexity does not necessarily indicate degraded performance on downstream tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Röttger &amp; Pierrehumbert, <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">2021</a>; Agarwal &amp; Nenkova, <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">2022</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et al. (<a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">2025</a>)</cite> investigate temporal generalization using the Bits Per Character (BPC) metric. Similar to perplexity, BPC fails to capture higher-level performance on downstream tasks. In contrast, our work focuses on the downstream forecasting task, evaluating how well models understand world knowledge and make predictions. This approach offers a more reliable evaluation of temporal generalization with direct relevance to real-world applications and public interest.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Dynamic QA Datasets.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px2.p1.1\">While static QA datasets evaluate models on fixed knowledge snapshots, dynamic QA datasets incorporate a temporal dimension, allowing assessment of how models adapt to evolving information. Several dynamic QA datasets are proposed. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et al. (<a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2021</a>)</cite> construct TimeQA by using time-sensitive facts in WikiData with aligned Wikipedia passages to synthesize QA pairs. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang &amp; Choi (<a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">2021</a>)</cite> introduce SituatedQA by manually annotating temporally and geographically dependent questions. StreamingQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Liska et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2022</a>)</cite> and RealtimeQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Kasai et al., <a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">2024</a>)</cite> are both dynamic benchmarks with QA pairs answerable from news articles. StreamingQA, however, does not provide continuous evaluation with always-relevant data. RealTimeQA does not address forecasting and is more like a plugin for a search engine, in the sense that it tests whether a model has updated its knowledge as facts change, rather than testing whether it can predict what will change given its knowledge of the past. FreshQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Vu et al., <a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2024</a>)</cite> contains a fixed set of human-written open-ended questions whose answers by nature can change based on new developments in the world, but is smaller and does not address forecasting. It is also updated weekly rather than daily. While all these datasets have some form of time-sensitivity like the Daily Oracle, they either do not provide continuous evaluation or do not evaluate forecasting capabilities, or neither.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Forecasting Datasets.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px3.p1.1\">Forecasting questions aim to assess a model’s ability to predict the outcomes of future events based on its existing knowledge. Several datasets in the event forecasting field have been introduced. ForecastQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2021</a>)</cite> used crowdworkers to collect 10,392 QA pairs from news articles. <cite class=\"ltx_cite ltx_citemacro_citet\">Zou et al. (<a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a>)</cite> argue that the QA pairs from ForecastQA are often nonsensical or ambiguous since they are written by humans without forecasting expertise. They further introduce AutoCast, a forecasting dataset from popular human forecasting tournaments containing 6,707 QA pairs. While ForecastQA and AutoCast remain static, ForecastBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Karger et al., <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2025</a>)</cite> regularly updates a set of 1,000 forecasting questions either sourced from forecasting markets or generated via fixed templates based on real-world event datasets. However, it still depends on users actively submitting new forecasting questions or maintaining the underlying datasets. In contrast, our Daily Oracle dataset is generated automatically from daily news articles, which means that it is never out of date, can easily grow its size automatically without additional inputs from human forecasters, and provides more comprehensive event coverage than human forecasting tournaments.</p>\n</div>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p2\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px3.p2.1\">Similar to our generation method, TLB-forecast <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite> has an automatic forecasting QA generation framework using news articles. However, their dataset is constrained both temporally and topically, only containing cooperation and conflict events in Middle-Eastern countries from 2015 to 2022. This restricts the dataset from evaluating more general event-prediction abilities. Furthermore, considering most of the powerful LLMs have been developed after 2020, the portions of the dataset covering earlier years may contain answers already seen during training. This prior exposure compromises the dataset’s effectiveness as a forecasting benchmark. In contrast, our dataset spans a broader timeframe and covers more topics, offering a more comprehensive forecasting benchmark.</p>\n</div>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p3\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px3.p3.1\">Note that none of the aforementioned datasets provide insights into how prediction ability changes over time. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et al. (<a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">2025</a>)</cite> introduce FreshBench, a forecasting dataset scraped from the Good Judgment Open platform, and also study temporal generalization. However, they report accuracy in a relatively short time window (from January 2023 to August 2024) with only 2,769 questions. While we observe a gradual performance decline in our dataset, they report significant fluctuations in model accuracy shortly after release. A closer look reveals key limitations of forecasting market–based questions for studying temporal generalization: they suffer from limited early coverage, inconsistent distribution over time, and reduced dataset size after filtering due to a high proportion of low-quality questions, making it difficult to reliably analyze temporal performance trends. In contrast, our automatically generated dataset has broad event coverage, consistent growth, and more uniform question quality over time.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>See Appendix <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Comparing LLM-Generated and Forecasting Market Datasets ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details on comparing LLM-generated and forecasting market datasets.</span></span></span></p>\n</div>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p4\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px3.p4.1\">In order to clearly showcase the differences between our dataset and prior work, we highlight a few key features in Table <a class=\"ltx_ref\" href=\"#S2.T1\" title=\"Table 1 ‣ 2 Related Work ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The Daily Oracle is the only benchmark which is <span class=\"ltx_text ltx_font_bold\" id=\"S2.SS0.SSS0.Px3.p4.1.1\">continuously updated</span> on a daily basis and evaluates forecasting ability. Additionally, at the fixed size we use for analysis we provide significantly more evaluation examples than the other automatically updated benchmarks.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>The Daily Oracle Dataset</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\" id=\"S3.p1.1\">In this section, we present Daily Oracle, a continuously updated QA benchmark of forecasting questions that are automatically generated from daily news. For our current analysis of LLM performance, we utilize a subset of the data consisting of 16,783 TF questions and 14,727 MC questions, covering a diverse range of forecasting topics, which are generated using daily news articles from January 2020 up until December 2024. However, our QA generation framework is continuous and updates daily. In Section <a class=\"ltx_ref\" href=\"#S3.SS1\" title=\"3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we describe our LLM-based dataset construction pipeline, detailing the data sources and the four-step construction process. Section <a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> provides an analysis and general overview of the dataset. Lastly, in Section <a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Human Evaluation ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, we conduct a human evaluation, similar to our QA filtering process, to verify the quality of the generated QA pairs.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Dataset Construction</h3>\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data Source.</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px1.p1.1\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Zou et al. (<a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a>)</cite>, we collect a large corpus of news articles from the daily-updated Common Crawl News Dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Nagel, <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2016</a>)</cite> with the <code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.SS1.SSS0.Px1.p1.1.1\">news-please</code> package <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamborg et al., <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">2017</a>)</cite>. To further enrich our news dataset, we supplement it with daily scraped news using the <code class=\"ltx_verbatim ltx_font_typewriter\" id=\"S3.SS1.SSS0.Px1.p1.1.2\">Newspaper3k</code> package.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://newspaper.readthedocs.io/en/latest/\" title=\"\">https://newspaper.readthedocs.io/en/latest/</a></span></span></span> We filter for mainstream sources—CBS News, CNBC, CNN, Forbes, and NPR. While our data collection and evaluation are performed daily, for this study we utilize a static news corpus with 1,246,973 English articles spanning January 2019 to December 2024. This corpus is also used for the constrained open-book evaluation setting in Section <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Experimental Setup ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM-based Construction Process.</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px2.p1.1\">QA pairs are generated from articles published between January 2020 and December 2024.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For news articles in 2019, we use them as the corpus for the constrained open-book setting.</span></span></span> For each day, we select six articles for QA generation: three are chosen randomly, and three are selected from hot topics.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>See Appendix <a class=\"ltx_ref\" href=\"#A1.SS2\" title=\"A.2 Details for Article Selection ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a> for hot-topic selection details.</span></span></span> For each selected article, we then use LLM to generate two TF QA pairs and two MC QA pairs with the few-shot prompting technique.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>See Appendix <a class=\"ltx_ref\" href=\"#A4\" title=\"Appendix D Prompts ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for all the prompts we use.</span></span></span></p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px2.p2.1\">We adopt the methodology of <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>, as their prompt design largely suits our setting. To further filter the QA pairs, we establish seven key criteria to ensure they qualify as valid forecasting questions, incorporating these into a <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS1.SSS0.Px2.p2.1.1\">QA Filtering step</span>. The QA construction follows four steps, as illustrated in Figure <a class=\"ltx_ref\" href=\"#A1.F7\" title=\"Figure 7 ‣ A.1 Illustration of the dataset construction process. ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>:</p>\n<ol class=\"ltx_enumerate\" id=\"S3.I1\">\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(1)</span>\n<div class=\"ltx_para\" id=\"S3.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.I1.i1.p1.1.1\">Article Summary.</span> We generate a summary for each article, focusing on new events from the publishing date, instead of opinion articles discussing events from the past. This approach allows us to use the publication date as the resolution date of the generated question. Questions can then be regarded as valid forecasting questions since they are prior to the resolution date.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(2)</span>\n<div class=\"ltx_para\" id=\"S3.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.I1.i2.p1.1.1\">QA Generation.</span> After filtering out the articles that do not introduce new events, two TF questions and two MC questions are generated together with the answers per article. To ensure balance in the TF questions, we instruct the LLM to generate the first question with a “Yes” answer and the second with a “No.”</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(3)</span>\n<div class=\"ltx_para\" id=\"S3.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.I1.i3.p1.1.1\">Misleading Choices Generation.</span> For MC, we provide the article, its publishing date, and the QA pair to the LLM, which then generates three misleading choices.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(4)</span>\n<div class=\"ltx_para\" id=\"S3.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i4.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.I1.i4.p1.1.1\">QA Filtering.</span> We prompt the LLM to check seven principles: correctness of answers, non-answerability before the publication date, absence of information leakage, objectivity, inclusion of a clear temporal element, public interest, and non-obviousness of the answer. Each principle is scored with 0, 1, or 2 points, and we selected the questions that received at least 13 points in total. These principles are detailed in Appendix <a class=\"ltx_ref\" href=\"#A1.SS3\" title=\"A.3 QA Filtering Principles ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.</p>\n</div>\n</li>\n</ol>\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px2.p2.2\">We use GPT-3.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2024a</a>)</cite> or GPT-4o-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2024b</a>)</cite> for steps (1) and (4), while GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">2023</a>)</cite> or GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2024b</a>)</cite> is utilized for steps (2) and (3) to ensure high data quality.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>We use GPT-3.5 and GPT-4 until September 2024. After October 2024, we switch to GPT-4o-mini and GPT-4o, as they are both more cost-effective and more powerful.</span></span></span></p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"313\" id=\"S3.F1.g1\" src=\"./assets/x1.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Pie charts showing (a) MC question type distribution and (b) question category distribution in Daily Oracle.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"292\" id=\"S3.F2.g1\" src=\"./assets/x2.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Following <cite class=\"ltx_cite ltx_citemacro_citet\">Anderson &amp; Schooler (<a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">1991</a>)</cite>, we plot the probability of a word occurring in an <span class=\"ltx_text ltx_font_bold\" id=\"S3.F2.10.1\">(left)</span> article title, <span class=\"ltx_text ltx_font_bold\" id=\"S3.F2.11.2\">(middle)</span> True/False question, or <span class=\"ltx_text ltx_font_bold\" id=\"S3.F2.12.3\">(right)</span> Multiple Choice question given how frequently it had appeared in one over the past 100 days, computed over our entire dataset. We fit a linear regression and show a linear relationship in each case (<math alttext=\"R^{2}=0.843\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.4.m1.1\"><semantics id=\"S3.F2.4.m1.1b\"><mrow id=\"S3.F2.4.m1.1.1\" xref=\"S3.F2.4.m1.1.1.cmml\"><msup id=\"S3.F2.4.m1.1.1.2\" xref=\"S3.F2.4.m1.1.1.2.cmml\"><mi id=\"S3.F2.4.m1.1.1.2.2\" xref=\"S3.F2.4.m1.1.1.2.2.cmml\">R</mi><mn id=\"S3.F2.4.m1.1.1.2.3\" xref=\"S3.F2.4.m1.1.1.2.3.cmml\">2</mn></msup><mo id=\"S3.F2.4.m1.1.1.1\" xref=\"S3.F2.4.m1.1.1.1.cmml\">=</mo><mn id=\"S3.F2.4.m1.1.1.3\" xref=\"S3.F2.4.m1.1.1.3.cmml\">0.843</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.F2.4.m1.1c\"><apply id=\"S3.F2.4.m1.1.1.cmml\" xref=\"S3.F2.4.m1.1.1\"><eq id=\"S3.F2.4.m1.1.1.1.cmml\" xref=\"S3.F2.4.m1.1.1.1\"></eq><apply id=\"S3.F2.4.m1.1.1.2.cmml\" xref=\"S3.F2.4.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.F2.4.m1.1.1.2.1.cmml\" xref=\"S3.F2.4.m1.1.1.2\">superscript</csymbol><ci id=\"S3.F2.4.m1.1.1.2.2.cmml\" xref=\"S3.F2.4.m1.1.1.2.2\">𝑅</ci><cn id=\"S3.F2.4.m1.1.1.2.3.cmml\" type=\"integer\" xref=\"S3.F2.4.m1.1.1.2.3\">2</cn></apply><cn id=\"S3.F2.4.m1.1.1.3.cmml\" type=\"float\" xref=\"S3.F2.4.m1.1.1.3\">0.843</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F2.4.m1.1d\">R^{2}=0.843</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.F2.4.m1.1e\">italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.843</annotation></semantics></math>, <math alttext=\"0.982\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.5.m2.1\"><semantics id=\"S3.F2.5.m2.1b\"><mn id=\"S3.F2.5.m2.1.1\" xref=\"S3.F2.5.m2.1.1.cmml\">0.982</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F2.5.m2.1c\"><cn id=\"S3.F2.5.m2.1.1.cmml\" type=\"float\" xref=\"S3.F2.5.m2.1.1\">0.982</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F2.5.m2.1d\">0.982</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.F2.5.m2.1e\">0.982</annotation></semantics></math>, and <math alttext=\"0.990\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.6.m3.1\"><semantics id=\"S3.F2.6.m3.1b\"><mn id=\"S3.F2.6.m3.1.1\" xref=\"S3.F2.6.m3.1.1.cmml\">0.990</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F2.6.m3.1c\"><cn id=\"S3.F2.6.m3.1.1.cmml\" type=\"float\" xref=\"S3.F2.6.m3.1.1\">0.990</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F2.6.m3.1d\">0.990</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.F2.6.m3.1e\">0.990</annotation></semantics></math> for left, middle, and right respectively).</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Dataset Analysis</h3>\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Summary Statistics.</h4>\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px1.p1.1\">At the time of writing this paper, the subset dataset we use from Daily Oracle consists of 16,783 TF and 14,727 MC QA pairs, covering the period from January 1st, 2020, to December 31st, 2024, with an average of 17.2 questions per day. Figure <a class=\"ltx_ref\" href=\"#S3.F1\" title=\"Figure 1 ‣ LLM-based Construction Process. ‣ 3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a) shows that our dataset covers various MC question types, mainly starting with “What will” (27.3%), “Who will” (21.2%), and “Which … will” (18.4%). Figure <a class=\"ltx_ref\" href=\"#S3.F1\" title=\"Figure 1 ‣ LLM-based Construction Process. ‣ 3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b) provides a breakdown of the categories, highlighting our dataset’s broad coverage. The categorization of each question is determined using GPT-3.5, based on the prompt from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al. (<a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>)</cite>. Examples of QA pairs are shown in Table <a class=\"ltx_ref\" href=\"#S3.T2\" title=\"Table 2 ‣ Summary Statistics. ‣ 3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S3.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Daily Oracle Example Questions and Answers.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1\" style=\"font-size:70%;\">Type</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.1.1.1.2\"></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.3.1.1.1\" style=\"font-size:70%;\">Category</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.4.1.1.1\" style=\"font-size:70%;\">Question and Answer</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.2.1.1\"><span class=\"ltx_text\" id=\"S3.T2.1.2.1.1.1\" style=\"font-size:70%;\">TF</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.2.1.2\"></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T2.1.2.1.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.2.1.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.2.1.3.1.1.1\" style=\"font-size:70%;\">Politics &amp; Governance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T2.1.2.1.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.2.1.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.2.1.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.2.1.4.1.1.1\" style=\"font-size:70%;\">– Will the prosecution’s key witness in the New York hush money trial in April 2024 be someone other than Michael Cohen? –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.1.4.1.1.2\" style=\"font-size:70%;\">No.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.3.2.1\"><span class=\"ltx_text\" id=\"S3.T2.1.3.2.1.1\" style=\"font-size:70%;\">TF</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T2.1.3.2.2\"></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.3.2.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.3.2.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.3.2.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.3.2.3.1.1.1\" style=\"font-size:70%;\">Politics &amp; Governance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.3.2.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.3.2.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.3.2.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.3.2.4.1.1.1\" style=\"font-size:70%;\">– Will the House Energy and Commerce Committee vote unanimously to advance a bill that could potentially ban TikTok if ByteDance does not sell the app by March 2024? –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.2.4.1.1.2\" style=\"font-size:70%;\">Yes.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.4.3.1\"><span class=\"ltx_text\" id=\"S3.T2.1.4.3.1.1\" style=\"font-size:70%;\">MC</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.4.3.2\"><span class=\"ltx_text\" id=\"S3.T2.1.4.3.2.1\" style=\"font-size:70%;\">What</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.4.3.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.4.3.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.4.3.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.4.3.3.1.1.1\" style=\"font-size:70%;\">Science &amp; Tech</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.4.3.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.4.3.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.4.3.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.4.3.4.1.1.1\" style=\"font-size:70%;\">– What will be the starting price range for the Google Pixel 8a as of May 2024? A.$599–$649 B. $199–$249 C. $750–$800, D. $499–$559. –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.4.3.4.1.1.2\" style=\"font-size:70%;\">D.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.5.4.1\"><span class=\"ltx_text\" id=\"S3.T2.1.5.4.1.1\" style=\"font-size:70%;\">MC</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.5.4.2\"><span class=\"ltx_text\" id=\"S3.T2.1.5.4.2.1\" style=\"font-size:70%;\">Who</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.5.4.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.5.4.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.5.4.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.5.4.3.1.1.1\" style=\"font-size:70%;\">Sports</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.5.4.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.5.4.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.5.4.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.5.4.4.1.1.1\" style=\"font-size:70%;\">– Who will go on the injured list before the New York Mets’ game on May 29, 2024? A. Pete Alonso B. Edwin Diaz C. Jeff McNeil D. Francisco Lindor –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.5.4.4.1.1.2\" style=\"font-size:70%;\">B.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.6.5.1\"><span class=\"ltx_text\" id=\"S3.T2.1.6.5.1.1\" style=\"font-size:70%;\">MC</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.6.5.2\"><span class=\"ltx_text\" id=\"S3.T2.1.6.5.2.1\" style=\"font-size:70%;\">Which</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.6.5.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.6.5.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.6.5.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.6.5.3.1.1.1\" style=\"font-size:70%;\">Arts &amp; Recreation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.6.5.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.6.5.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.6.5.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.6.5.4.1.1.1\" style=\"font-size:70%;\">– By May 2024, on which streaming service will “The First Omen” become available for subscribers? A. Disney+, B. Hulu, C. Amazon Prime Video, D. Netflix –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.6.5.4.1.1.2\" style=\"font-size:70%;\">B.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.7.6.1\"><span class=\"ltx_text\" id=\"S3.T2.1.7.6.1.1\" style=\"font-size:70%;\">MC</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.7.6.2\"><span class=\"ltx_text\" id=\"S3.T2.1.7.6.2.1\" style=\"font-size:70%;\">How many</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.7.6.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.7.6.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.7.6.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.7.6.3.1.1.1\" style=\"font-size:70%;\">Science &amp; Tech</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.7.6.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.7.6.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.7.6.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.7.6.4.1.1.1\" style=\"font-size:70%;\">– How many U.S. states will the path of totality cross during the total solar eclipse on April 8, as reported by February 2024? A. 15 B. 10 C. 20 D. 6 –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.7.6.4.1.1.2\" style=\"font-size:70%;\">A.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.8.7.1\"><span class=\"ltx_text\" id=\"S3.T2.1.8.7.1.1\" style=\"font-size:70%;\">MC</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.8.7.2\"><span class=\"ltx_text\" id=\"S3.T2.1.8.7.2.1\" style=\"font-size:70%;\">Where</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.8.7.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.8.7.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.8.7.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.8.7.3.1.1.1\" style=\"font-size:70%;\">Healthcare &amp; Biology</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.8.7.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.8.7.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.8.7.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.8.7.4.1.1.1\" style=\"font-size:70%;\">– Where will the second known U.S. case of bird flu in a human be reported by March 2024? A. California, B. Texas, C. New York, D. Florida –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.8.7.4.1.1.2\" style=\"font-size:70%;\">B.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.1.9.8.1\"><span class=\"ltx_text\" id=\"S3.T2.1.9.8.1.1\" style=\"font-size:70%;\">MC</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T2.1.9.8.2\"><span class=\"ltx_text\" id=\"S3.T2.1.9.8.2.1\" style=\"font-size:70%;\">How much</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T2.1.9.8.3\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.9.8.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.9.8.3.1.1\" style=\"width:65.4pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.9.8.3.1.1.1\" style=\"font-size:70%;\">Economics &amp; Business</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T2.1.9.8.4\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.9.8.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.9.8.4.1.1\" style=\"width:301.6pt;\"><span class=\"ltx_text\" id=\"S3.T2.1.9.8.4.1.1.1\" style=\"font-size:70%;\">– How much will Apple, Inc. (AAPL) be up year-to-date by the end of June 2024? A. Up 149.5% B. Just over 19% C. 9.7%. D. 27%. –</span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.9.8.4.1.1.2\" style=\"font-size:70%;\">C.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Past and Future Information Usage.</h4>\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px2.p1.1\">Each question in Daily Oracle implicitly requires the model to retrieve relevant knowledge. How do these requirements change day by day over the course of our benchmark? <cite class=\"ltx_cite ltx_citemacro_citet\">Anderson &amp; Schooler (<a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">1991</a>)</cite> explored similar patterns in human information environments. Inspired by their work, Figure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ LLM-based Construction Process. ‣ 3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> examines whether a word’s frequency over the past 100 days predicts its occurrence the next day—e.g., if many questions concern the unemployment rate, will this trend continue?</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px2.p2.1\">We analyze this relationship for words in the titles of the articles we use to generate questions as well as in the text of the TF and MC questions themselves. Past frequency is computed by checking, for each day in the 100 day window, if a word has occurred in any article title (so, the maximum frequency is 100). We find that there is a linear relationship between the frequency of usage in the past 100 days and the probability of occurrence on the 101st day in all cases, replicating Anderson &amp; Schooler’s findings for <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS2.SSS0.Px2.p2.1.1\">New York Times</span> headlines. This indicates that past information usage strongly predicts future retrieval needs, suggesting a temporal structure in the knowledge demands of our benchmark.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>Human Evaluation</h3>\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.p1.1\">To assess the quality of our LLM-based filtering method, we randomly sample 30 TF and 30 MC QA pairs and ask 4 human annotators to evaluate them using the seven principles outlined in the <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS3.p1.1.1\">QA Filtering</span> step in Section <a class=\"ltx_ref\" href=\"#S3.SS1\" title=\"3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>. We evaluate the consistency among annotators using Fleiss’ Kappa <cite class=\"ltx_cite ltx_citemacro_citep\">(Fleiss, <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">1971</a>)</cite>, which yields an average inter-rater agreement score of 0.26, indicating fair agreement. We then compute the human consensus score as the average of human scores and compare it to LLM-assigned scores, finding an average accuracy of 89.52% across the 7 principles. For final QA pair acceptance (i.e., threshold above 13 points), the LLM and human consensus scores demonstrated an accuracy of 85.00%, further supporting the reliability of our LLM-based filtering approach. A detailed breakdown of human evaluation metrics is provided in Appendix <a class=\"ltx_ref\" href=\"#A1.SS4\" title=\"A.4 Details for Human Evaluation ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\" id=\"S4.p1.1\">We first introduce three evaluation settings in Section <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Experimental Setup ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>: 1) no access to external information, 2) access to retrieved recent news articles, and 3) access to gold articles. Section <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> presents the results, and Section <a class=\"ltx_ref\" href=\"#S4.SS3\" title=\"4.3 Discussion ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a> provides deeper insights into the observed degradation patterns.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"251\" id=\"S4.F3.g1\" src=\"./assets/x3.png\" width=\"623\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>Results for the closed-book setting. We plot the 5-month moving average accuracy for TF and MC questions across various models, showing LLMs’ performance degradation in future event prediction.</figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>For evaluating various LLMs with different knowledge cutoffs (K-Cutoffs), we show the yearly average accuracy (calculated as the average across months) from 2020 to 2024, along with the average YoY accuracy change (%) before the knowledge cutoff date (Pre-Cutoff), after the knowledge cutoff date (Post-Cutoff), and the overall average YoY accuracy change across all months (Avg).</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T3.1.1.1.1\" rowspan=\"2\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T3.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.2.1\" style=\"font-size:70%;\">LLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T3.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.3.1\" style=\"font-size:70%;\">K-Cutoff</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"5\" id=\"S4.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.4.1\" style=\"font-size:70%;\">Average Yearly Accuracy (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T3.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.5.1\" style=\"font-size:70%;\">Average YoY Accuracy Change (%)</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.1.1\" style=\"font-size:70%;\">2020</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.2.1\" style=\"font-size:70%;\">2021</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.3.1\" style=\"font-size:70%;\">2022</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.4.1\" style=\"font-size:70%;\">2023</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T3.1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.5.1\" style=\"font-size:70%;\">2024</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.6.1\" style=\"font-size:70%;\">Pre-Cutoff</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.7.1\" style=\"font-size:70%;\">Post-Cutoff</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.2.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.8.1\" style=\"font-size:70%;\">Avg</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.1.1\" rowspan=\"8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.3.1.1.1\" style=\"font-size:70%;\">TF</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.1.2\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.2.1\" style=\"font-size:70%;\">Claude-3.5-Sonnet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.1.3\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.3.1\" style=\"font-size:70%;\">Apr 2024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.4\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.4.1\" style=\"font-size:70%;\">81.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.5\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.5.1\" style=\"font-size:70%;\">79.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.6\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.6.1\" style=\"font-size:70%;\">78.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.7\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.7.1\" style=\"font-size:70%;\">74.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.1.8\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.8.1\" style=\"font-size:70%;\">64.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.9\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.9.1\" style=\"font-size:70%;\">-4.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.10\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.10.1\" style=\"font-size:70%;\">-12.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1.11\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.11.1\" style=\"font-size:70%;\">-5.58</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.4.2.1\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.1.1\" style=\"font-size:70%;\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.2.2\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.2.1\" style=\"font-size:70%;\">Apr 2023</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.3\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.3.1\" style=\"font-size:70%;\">69.68</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.4\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.4.1\" style=\"font-size:70%;\">66.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.5\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.5.1\" style=\"font-size:70%;\">60.36</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.6\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.6.1\" style=\"font-size:70%;\">60.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.2.7\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.7.1\" style=\"font-size:70%;\">56.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.8\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.8.1\" style=\"font-size:70%;\">-5.83</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.9\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.9.1\" style=\"font-size:70%;\">-1.96</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2.10\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.10.1\" style=\"font-size:70%;\">-4.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.5.3.1\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.1.1\" style=\"font-size:70%;\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.3.2\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.2.1\" style=\"font-size:70%;\">Sept 2021</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.3\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.3.1\" style=\"font-size:70%;\">62.86</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.4\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.4.1\" style=\"font-size:70%;\">60.12</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.5\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.5.1\" style=\"font-size:70%;\">59.36</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.6\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.6.1\" style=\"font-size:70%;\">57.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.3.7\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.7.1\" style=\"font-size:70%;\">56.09</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.8\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.8.1\" style=\"font-size:70%;\">-4.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.9\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.9.1\" style=\"font-size:70%;\">-3.43</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.3.10\"><span class=\"ltx_text\" id=\"S4.T3.1.5.3.10.1\" style=\"font-size:70%;\">-2.84</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.6.4.1\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.1.1\" style=\"font-size:70%;\">Mixtral-8x7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.4.2\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.2.1\" style=\"font-size:70%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.3\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.3.1\" style=\"font-size:70%;\">57.83</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.4\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.4.1\" style=\"font-size:70%;\">52.69</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.5\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.5.1\" style=\"font-size:70%;\">43.09</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.6\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.6.1\" style=\"font-size:70%;\">39.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.4.7\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.7.1\" style=\"font-size:70%;\">36.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.8\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.8.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.9\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.9.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.4.10\"><span class=\"ltx_text\" id=\"S4.T3.1.6.4.10.1\" style=\"font-size:70%;\">-10.78</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.7.5.1\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.1.1\" style=\"font-size:70%;\">Mistral-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.5.2\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.2.1\" style=\"font-size:70%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.3\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.3.1\" style=\"font-size:70%;\">57.57</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.4\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.4.1\" style=\"font-size:70%;\">54.65</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.5\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.5.1\" style=\"font-size:70%;\">48.22</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.6\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.6.1\" style=\"font-size:70%;\">41.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.5.7\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.7.1\" style=\"font-size:70%;\">40.92</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.8\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.8.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.9\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.9.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.5.10\"><span class=\"ltx_text\" id=\"S4.T3.1.7.5.10.1\" style=\"font-size:70%;\">-7.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.8.6.1\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.1.1\" style=\"font-size:70%;\">Llama-3-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.8.6.2\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.2.1\" style=\"font-size:70%;\">Mar 2023</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.3\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.3.1\" style=\"font-size:70%;\">65.06</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.4\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.4.1\" style=\"font-size:70%;\">64.24</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.5\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.5.1\" style=\"font-size:70%;\">62.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.6\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.6.1\" style=\"font-size:70%;\">58.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.8.6.7\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.7.1\" style=\"font-size:70%;\">56.99</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.8\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.8.1\" style=\"font-size:70%;\">-1.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.9\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.9.1\" style=\"font-size:70%;\">-6.50</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.6.10\"><span class=\"ltx_text\" id=\"S4.T3.1.8.6.10.1\" style=\"font-size:70%;\">-2.97</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.9.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.9.7.1\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.1.1\" style=\"font-size:70%;\">Qwen-2-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.9.7.2\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.2.1\" style=\"font-size:70%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.3\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.3.1\" style=\"font-size:70%;\">62.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.4\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.4.1\" style=\"font-size:70%;\">60.18</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.5\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.5.1\" style=\"font-size:70%;\">57.67</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.6\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.6.1\" style=\"font-size:70%;\">53.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.9.7.7\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.7.1\" style=\"font-size:70%;\">53.04</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.8\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.8.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.9\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.9.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.7.10\"><span class=\"ltx_text\" id=\"S4.T3.1.9.7.10.1\" style=\"font-size:70%;\">-3.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.10.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.10.8.1\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.1.1\" style=\"font-size:70%;\">Gemma-2-2B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.10.8.2\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.2.1\" style=\"font-size:70%;\">Jul 2024</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.3\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.3.1\" style=\"font-size:70%;\">58.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.4\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.4.1\" style=\"font-size:70%;\">59.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.5\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.5.1\" style=\"font-size:70%;\">57.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.6\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.6.1\" style=\"font-size:70%;\">56.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.10.8.7\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.7.1\" style=\"font-size:70%;\">55.79</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.8\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.8.1\" style=\"font-size:70%;\">-1.41</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.9\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.9.1\" style=\"font-size:70%;\">-3.68</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.8.10\"><span class=\"ltx_text\" id=\"S4.T3.1.10.8.10.1\" style=\"font-size:70%;\">-1.04</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.11.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T3.1.11.9.1\" rowspan=\"8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.11.9.1.1\" style=\"font-size:70%;\">MC</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T3.1.11.9.2\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.2.1\" style=\"font-size:70%;\">Claude-3.5-Sonnet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.11.9.3\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.3.1\" style=\"font-size:70%;\">Apr 2024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.4\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.4.1\" style=\"font-size:70%;\">76.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.5\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.5.1\" style=\"font-size:70%;\">77.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.6\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.6.1\" style=\"font-size:70%;\">74.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.7\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.7.1\" style=\"font-size:70%;\">69.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.11.9.8\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.8.1\" style=\"font-size:70%;\">61.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.9\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.9.1\" style=\"font-size:70%;\">-6.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.10\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.10.1\" style=\"font-size:70%;\">-11.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.11.9.11\"><span class=\"ltx_text\" id=\"S4.T3.1.11.9.11.1\" style=\"font-size:70%;\">-5.03</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.12.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.12.10.1\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.1.1\" style=\"font-size:70%;\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.12.10.2\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.2.1\" style=\"font-size:70%;\">Apr 2023</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.3\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.3.1\" style=\"font-size:70%;\">70.60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.4\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.4.1\" style=\"font-size:70%;\">70.62</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.5\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.5.1\" style=\"font-size:70%;\">66.76</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.6\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.6.1\" style=\"font-size:70%;\">56.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.12.10.7\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.7.1\" style=\"font-size:70%;\">51.63</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.8\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.8.1\" style=\"font-size:70%;\">-4.23</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.9\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.9.1\" style=\"font-size:70%;\">-18.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.10.10\"><span class=\"ltx_text\" id=\"S4.T3.1.12.10.10.1\" style=\"font-size:70%;\">-7.04</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.13.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.13.11.1\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.1.1\" style=\"font-size:70%;\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.13.11.2\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.2.1\" style=\"font-size:70%;\">Sept 2021</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.3\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.3.1\" style=\"font-size:70%;\">50.27</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.4\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.4.1\" style=\"font-size:70%;\">50.40</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.5\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.5.1\" style=\"font-size:70%;\">44.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.6\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.6.1\" style=\"font-size:70%;\">41.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.13.11.7\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.7.1\" style=\"font-size:70%;\">43.09</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.8\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.8.1\" style=\"font-size:70%;\">0.14</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.9\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.9.1\" style=\"font-size:70%;\">-0.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.13.11.10\"><span class=\"ltx_text\" id=\"S4.T3.1.13.11.10.1\" style=\"font-size:70%;\">-3.08</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.14.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.14.12.1\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.1.1\" style=\"font-size:70%;\">Mixtral-8x7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.14.12.2\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.2.1\" style=\"font-size:70%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.3\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.3.1\" style=\"font-size:70%;\">57.38</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.4\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.4.1\" style=\"font-size:70%;\">56.97</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.5\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.5.1\" style=\"font-size:70%;\">50.76</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.6\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.6.1\" style=\"font-size:70%;\">47.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.14.12.7\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.7.1\" style=\"font-size:70%;\">46.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.8\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.8.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.9\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.9.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.12.10\"><span class=\"ltx_text\" id=\"S4.T3.1.14.12.10.1\" style=\"font-size:70%;\">-4.68</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.15.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.15.13.1\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.1.1\" style=\"font-size:70%;\">Mistral-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.15.13.2\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.2.1\" style=\"font-size:70%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.3\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.3.1\" style=\"font-size:70%;\">50.07</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.4\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.4.1\" style=\"font-size:70%;\">52.36</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.5\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.5.1\" style=\"font-size:70%;\">48.06</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.6\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.6.1\" style=\"font-size:70%;\">44.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.15.13.7\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.7.1\" style=\"font-size:70%;\">42.99</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.8\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.8.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.9\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.9.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.13.10\"><span class=\"ltx_text\" id=\"S4.T3.1.15.13.10.1\" style=\"font-size:70%;\">-2.82</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.16.14\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.16.14.1\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.1.1\" style=\"font-size:70%;\">Llama-3-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.16.14.2\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.2.1\" style=\"font-size:70%;\">Mar 2023</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.3\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.3.1\" style=\"font-size:70%;\">52.44</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.4\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.4.1\" style=\"font-size:70%;\">54.18</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.5\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.5.1\" style=\"font-size:70%;\">50.66</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.6\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.6.1\" style=\"font-size:70%;\">47.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.16.14.7\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.7.1\" style=\"font-size:70%;\">46.95</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.8\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.8.1\" style=\"font-size:70%;\">-2.21</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.9\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.9.1\" style=\"font-size:70%;\">-1.25</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.14.10\"><span class=\"ltx_text\" id=\"S4.T3.1.16.14.10.1\" style=\"font-size:70%;\">-2.30</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.17.15\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S4.T3.1.17.15.1\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.1.1\" style=\"font-size:70%;\">Qwen-2-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.17.15.2\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.2.1\" style=\"font-size:70%;\">Unknown</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.3\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.3.1\" style=\"font-size:70%;\">55.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.4\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.4.1\" style=\"font-size:70%;\">55.93</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.5\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.5.1\" style=\"font-size:70%;\">53.44</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.6\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.6.1\" style=\"font-size:70%;\">49.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.17.15.7\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.7.1\" style=\"font-size:70%;\">49.37</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.8\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.8.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.9\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.9.1\" style=\"font-size:70%;\">–</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.15.10\"><span class=\"ltx_text\" id=\"S4.T3.1.17.15.10.1\" style=\"font-size:70%;\">-2.35</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.18.16\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" id=\"S4.T3.1.18.16.1\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.1.1\" style=\"font-size:70%;\">Gemma-2-2B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.18.16.2\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.2.1\" style=\"font-size:70%;\">Jul 2024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.3\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.3.1\" style=\"font-size:70%;\">47.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.4\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.4.1\" style=\"font-size:70%;\">50.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.5\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.5.1\" style=\"font-size:70%;\">46.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.6\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.6.1\" style=\"font-size:70%;\">45.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.1.18.16.7\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.7.1\" style=\"font-size:70%;\">43.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.8\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.8.1\" style=\"font-size:70%;\">-4.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.9\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.9.1\" style=\"font-size:70%;\">-4.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.18.16.10\"><span class=\"ltx_text\" id=\"S4.T3.1.18.16.10.1\" style=\"font-size:70%;\">-1.98</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Experimental Setup</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Closed-Book Setting.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px1.p1.1\">We evaluate various LLMs on Daily Oracle to assess their understanding of real-world events and temporal generalization abilities, i.e., how accurately LLMs can answer forecasting questions based on the knowledge they learned from their training data. Our evaluation differentiates between two scenarios based on the question’s resolution date and model’s knowledge cutoff date: (1) <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.SSS0.Px1.p1.1.1\">Pre-Knowledge Cutoff Questions:</span> These questions have resolution dates before the model’s knowledge cutoff, testing the model’s understanding of past events. (2) <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.SSS0.Px1.p1.1.2\">Post-Knowledge Cutoff Questions:</span> These have resolution dates after the knowledge cutoff, requiring models to predict future events and test their forecasting and temporal generalization abilities.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Constrained Open-Book Setting.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px2.p1.3\">In addition to a closed-book evaluation, we explore the constrained open-book setting: how access to news articles up to different time cutoffs influences LLM performance using RAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2020</a>)</cite>. We introduce the concept of the RAG cutoff (<span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.SSS0.Px2.p1.3.1\">R-Cutoff</span>), which limits the latest accessible date for retrieving articles. To prevent the models from leveraging information beyond the resolution date, for any question with a resolution date (<math alttext=\"d_{\\text{res}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1\"><semantics id=\"S4.SS1.SSS0.Px2.p1.1.m1.1a\"><msub id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3a.cmml\">res</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1b\"><apply id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3\"><mtext id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml\" mathsize=\"70%\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3\">res</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1c\">d_{\\text{res}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1d\">italic_d start_POSTSUBSCRIPT res end_POSTSUBSCRIPT</annotation></semantics></math>), the accessible articles span from January 1st, 2019 (the start of our news corpus) up to whichever comes first between the day before the resolution date and the RAG cutoff date (<math alttext=\"d_{\\text{R-Cutoff}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1\"><semantics id=\"S4.SS1.SSS0.Px2.p1.2.m2.1a\"><msub id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3a.cmml\">R-Cutoff</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1b\"><apply id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3\"><mtext id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml\" mathsize=\"70%\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3\">R-Cutoff</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1c\">d_{\\text{R-Cutoff}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1d\">italic_d start_POSTSUBSCRIPT R-Cutoff end_POSTSUBSCRIPT</annotation></semantics></math>). Formally, the accessible date range is <math alttext=\"[01/01/2019,\\min(d_{\\text{res}}-1,d_{\\text{R-Cutoff}}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3\"><semantics id=\"S4.SS1.SSS0.Px2.p1.3.m3.3a\"><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\"><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.3\" stretchy=\"false\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\">[</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.cmml\"><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2.cmml\">01</mn><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml\">/</mo><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3.cmml\">01</mn><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1a\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml\">/</mo><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4.cmml\">2019</mn></mrow><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.4\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\">,</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml\">min</mi><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2a\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">⁡</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\"><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.3\" stretchy=\"false\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">(</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.cmml\"><msub id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3a.cmml\">res</mtext></msub><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1.cmml\">−</mo><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3.cmml\">1</mn></mrow><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.4\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">,</mo><msub id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3a.cmml\">R-Cutoff</mtext></msub><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.5\" stretchy=\"false\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.5\" stretchy=\"false\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3b\"><interval closure=\"closed-open\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2\"><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1\"><divide id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1\"></divide><cn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2.cmml\" type=\"integer\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2\">01</cn><cn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3\">01</cn><cn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4.cmml\" type=\"integer\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4\">2019</cn></apply><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2\"><min id=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1\"></min><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1\"><minus id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1\"></minus><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3\"><mtext id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3.cmml\" mathsize=\"70%\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3\">res</mtext></ci></apply><cn id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3\">1</cn></apply><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3\"><mtext id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3.cmml\" mathsize=\"70%\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3\">R-Cutoff</mtext></ci></apply></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3c\">[01/01/2019,\\min(d_{\\text{res}}-1,d_{\\text{R-Cutoff}}))</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3d\">[ 01 / 01 / 2019 , roman_min ( italic_d start_POSTSUBSCRIPT res end_POSTSUBSCRIPT - 1 , italic_d start_POSTSUBSCRIPT R-Cutoff end_POSTSUBSCRIPT ) )</annotation></semantics></math>. Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2021</a>; Zou et al., <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>, we employ BM25 <cite class=\"ltx_cite ltx_citemacro_citep\">(Robertson et al., <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">1995</a>)</cite> as the retriever and select the top 5 articles relevant to each question. We truncate each retrieved article to a maximum length of 512 words. These articles are then incorporated into the input prompts to serve as additional information.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Gold Article Setting.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px3.p1.1\">We further include a setting where models are provided direct access to the gold article, from which the question is generated.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>See Appendix <a class=\"ltx_ref\" href=\"#A2.SS5\" title=\"B.5 An Example of Evaluating LLMs Under Different Settings ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a> for a case example of evaluating LLMs under all three different settings.</span></span></span> This transforms the forecasting questions into reading comprehension ones, which can also access LLMs’ general question-answering capabilities. Achieving high accuracy here ensures that the questions from our Daily Oracle dataset are answerable.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Metrics.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px4.p1.1\">Accuracy score is used as the evaluation metric. Though LLMs are tested daily, to show clearer trends, we plot the monthly performance in Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, and apply a 5-month moving average to smooth the curve. We also report yearly averages and average year-over-year (YoY) accuracy change before and after models’ knowledge cutoff dates in Table <a class=\"ltx_ref\" href=\"#S4.T3\" title=\"Table 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Additionally, despite prompting the models to avoid responses like “I cannot predict the future” and instead provide definitive answers, there are cases where such refusals still occur. The refusal rates are provided in the Appendix <a class=\"ltx_ref\" href=\"#A2.SS2\" title=\"B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>, and these cases are counted as incorrect to ensure comparability across model results.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Main Results</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results for the Closed-Book Setting.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS0.Px1.p1.1\">Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Table <a class=\"ltx_ref\" href=\"#S4.T3\" title=\"Table 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> present our primary results for the closed-book setting. The “Avg” column in Table <a class=\"ltx_ref\" href=\"#S4.T3\" title=\"Table 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the average YoY accuracy change of all months, revealing a clear degradation in performance over time across all models on both TF and MC questions. When comparing accuracies from the beginning to the end of the evaluation period, we observe that, on average, the models’ performance declines by 21.55% on TF questions (from 64.68% to 50.74%) and by 11.33% on MC questions (from 58.30% to 51.69%). This indicates that while LLMs demonstrate certain abilities to understand real-world events and make predictions, they struggle to maintain these abilities.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p2\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS0.Px1.p2.1\">Notably, the average YoY accuracy declines provide further insight. Before the knowledge cutoff, the average YoY decline across all models was relatively moderate. However, post-knowledge cutoff, we observe steeper declines in many models, with GPT-4 showing the most drastic drop in MC performance, declining by 18.54%, compared to just 4.23% before the cutoff. This contrast highlights that while LLMs manage to retain a baseline of past knowledge with small degradation, their ability to forecast future events deteriorates much more rapidly as they move beyond their training data, struggling with temporal generalization.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p3\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS0.Px1.p3.1\">Among different models, Claude-3.5-Sonnet <cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic, <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">2024</a>)</cite> significantly outperforms all others, while GPT-4 excels in MC questions but its performance in TF is not as remarkable as in MC. GPT-3.5, Qwen-2-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2024</a>)</cite> and Llama-3-8B <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">2024</a>)</cite> show smaller temporal declines than GPT-4 in both TF and MC questions. Interestingly, Mistral-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2023</a>)</cite> and Mixtral-8x7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et al., <a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">2024</a>)</cite> show the most pronounced drops in TF accuracy, with scores falling below the random baseline 50% due to increased answer refusals, as shown in Figure <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"Figure 9 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. Gemma-2-2B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">2024</a>)</cite> exhibits the most consistent performance with the smallest average YoY decline, likely due to its more recent knowledge cutoff date.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results for the Constrained Open-Book Setting.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS0.Px2.p1.1\">In Figure <a class=\"ltx_ref\" href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Constrained Open-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we present the results of the constrained open-book setting, with Mixtral-8x7B on TF questions and Llama-3-8B on MC questions across different RAG cutoff dates.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>Refer to Appendix <a class=\"ltx_ref\" href=\"#A2.SS4\" title=\"B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a> for results of other models in the constrained open-book setting.</span></span></span> For Mixtral-8x7B, as the RAG cutoff dates extend to closer to the resolution dates, we observe a clear improvement in performance, indicating the model benefits from increasingly updated information retrieval. However, there are noticeable performance drops immediately after each RAG cutoff date when compared to providing information up to the day before the resolution date. This highlights the importance of keeping up-to-date information for optimal RAG performance. Interestingly, RAG does not uniformly enhance performance. Llama-3-8B may perform worse than the closed-book setting when the RAG cutoff is prior to the knowledge cutoff dates, suggesting outdated information may negatively impact performance. Conversely, for more recent RAG cutoff dates that extend beyond the knowledge cutoff, significant performance improvements are observed (as illustrated by the curves with cutoffs in September 2023 and March 2024). Notably, across all different RAG cutoffs, the overall performance decline pattern persists, likely due to outdated internal representations and the model’s inherent knowledge limitations.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"263\" id=\"S4.F4.g1\" src=\"./assets/x4.png\" width=\"623\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>Results for the constrained open-book setting, evaluating Mixtral-8x7B on TF questions and Llama-3-8B on MC questions with different RAG cutoff dates.\n</figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results for the Gold Article Setting.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS0.Px3.p1.1\">Figure <a class=\"ltx_ref\" href=\"#S4.F5\" title=\"Figure 5 ‣ Results for the Gold Article Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows that when given access to the gold articles from which the questions are generated, LLM performance can approach around 90%, demonstrating the answerability of Daily Oracle.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Results for GPT-3.5 are provided and discussed in Appendix <a class=\"ltx_ref\" href=\"#A2.SS3\" title=\"B.3 Results for GPT-3.5 in the Gold Article Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>, as this older model performs relatively poorly and including it on the same scale would obscure the trends of other models.</span></span></span> However, most of the models still show declining trends. This is noteworthy because, ideally, LLMs are expected to achieve consistent accuracy regardless of the article’s publication date when answers are directly accessible. However, the outdated representations hinder their ability to consistently generate correct answers, even in a reading comprehension setting.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"259\" id=\"S4.F5.g1\" src=\"./assets/x5.png\" width=\"622\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>Results for the gold article setting. Most of the models struggle with temporal generalization, even when provided with gold articles containing the answers.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Discussion</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLMs’ Performance Degradation Pattern Over Time.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px1.p1.1\">We observe LLMs’ performance evolution patterns in Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>: (1) <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS0.Px1.p1.1.1\">Gradual Decline in the Recent Past:</span> In the months before the knowledge cutoff date, which we call the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS0.Px1.p1.1.2\">recent past</span>, we observe a gradual decline in model performance, as seen in Llama-3-8B, GPT-4, and Claude-3.5-Sonnet, likely due to a lack of representation of recent news in the training data. (2) <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS0.Px1.p1.1.3\">Rapid Decline in the Near Future:</span> In the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS0.Px1.p1.1.4\">near future</span>, which we define as the months following a model’s knowledge cutoff date, sharp performance drops are observed in several models in MC questions. For instance, the decline in Claude-3.5-Sonnet and GPT-4 accelerates soon after their knowledge cutoffs. Most of the models, however, do not lose all the predictive power at once, as evidenced by the further decline into the farther future.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px1.p2\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px1.p2.1\">We explore this further by analyzing the slope of accuracy as a function of time. In Figure <a class=\"ltx_ref\" href=\"#S4.F6\" title=\"Figure 6 ‣ LLMs’ Performance Degradation Pattern Over Time. ‣ 4.3 Discussion ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we show how the slope changes as we fit a regression to an increasingly larger window of data, until we reach the full set of accuracies. Specifically, using the 5-month moving average of each model’s accuracy on MC questions (visualized in Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), we start by fitting a linear regression line on the first 10 months of data. We then add an additional month and compute a new regression on the larger window, repeating until we reach the final month, and applying an exponential decay weighting to past data to reduce the influence of distant observations. With this, we can analyze how the slope of our regression line changes as each month is added to the data. The slope in each case is negative after the cutoff data and for Claude-3.5-Sonnet, GPT-4, and Llama-3-8B, the slope eventually or immediately becomes more negative than it was at any point preceding the cutoff. Both Claude-3.5-Sonnet and Llama-3-8B have a crossover from positive to negative slope in late summer 2022, July and August, respectively, while GPT-4’s seems to occur slightly earlier, in March of 2022. For GPT-3.5, GPT-4, and Llama-3-8B, the slope becomes increasingly negative not long after the knowledge cutoff, giving evidence for a rapid decline in the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS0.Px1.p2.1.1\">near future</span>. For example, Llama-3-8B’s slope is around -0.06% per month near its cutoff in March 2023, but declines by more than 2 times to approximately -0.13% per month by the end of 2023. Likewise, the period preceding the cutoff shows a milder decline, with models like GPT-3.5 and Llama-3-8B exhibiting slightly negative but consistent slopes (approximately -0.01% and -0.06% per month respectively, over the four months leading up to the cutoff). This suggests a gradual decline in the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.SSS0.Px1.p2.1.2\">recent past</span>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"1642\" id=\"S4.F6.g1\" src=\"./assets/x6.png\" width=\"780\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>Coefficients for regressing accuracy on the MC questions against time, as the number of months grows. Using an initial window of 10 months, we progressively add data for additional months to our regression and plot the coefficient (slope) for the regression of accuracy against time. For our regression, we use the moving average of accuracy and apply exponentially decaying weights to older months (i.e., given a window of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F6.5.m1.1\"><semantics id=\"S4.F6.5.m1.1b\"><mi id=\"S4.F6.5.m1.1.1\" xref=\"S4.F6.5.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.F6.5.m1.1c\"><ci id=\"S4.F6.5.m1.1.1.cmml\" xref=\"S4.F6.5.m1.1.1\">𝑘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F6.5.m1.1d\">k</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F6.5.m1.1e\">italic_k</annotation></semantics></math> months, we weight <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F6.6.m2.1\"><semantics id=\"S4.F6.6.m2.1b\"><msub id=\"S4.F6.6.m2.1.1\" xref=\"S4.F6.6.m2.1.1.cmml\"><mi id=\"S4.F6.6.m2.1.1.2\" xref=\"S4.F6.6.m2.1.1.2.cmml\">x</mi><mi id=\"S4.F6.6.m2.1.1.3\" xref=\"S4.F6.6.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F6.6.m2.1c\"><apply id=\"S4.F6.6.m2.1.1.cmml\" xref=\"S4.F6.6.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F6.6.m2.1.1.1.cmml\" xref=\"S4.F6.6.m2.1.1\">subscript</csymbol><ci id=\"S4.F6.6.m2.1.1.2.cmml\" xref=\"S4.F6.6.m2.1.1.2\">𝑥</ci><ci id=\"S4.F6.6.m2.1.1.3.cmml\" xref=\"S4.F6.6.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F6.6.m2.1d\">x_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F6.6.m2.1e\">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext=\"\\lambda^{k-t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F6.7.m3.1\"><semantics id=\"S4.F6.7.m3.1b\"><msup id=\"S4.F6.7.m3.1.1\" xref=\"S4.F6.7.m3.1.1.cmml\"><mi id=\"S4.F6.7.m3.1.1.2\" xref=\"S4.F6.7.m3.1.1.2.cmml\">λ</mi><mrow id=\"S4.F6.7.m3.1.1.3\" xref=\"S4.F6.7.m3.1.1.3.cmml\"><mi id=\"S4.F6.7.m3.1.1.3.2\" xref=\"S4.F6.7.m3.1.1.3.2.cmml\">k</mi><mo id=\"S4.F6.7.m3.1.1.3.1\" xref=\"S4.F6.7.m3.1.1.3.1.cmml\">−</mo><mi id=\"S4.F6.7.m3.1.1.3.3\" xref=\"S4.F6.7.m3.1.1.3.3.cmml\">t</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.F6.7.m3.1c\"><apply id=\"S4.F6.7.m3.1.1.cmml\" xref=\"S4.F6.7.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F6.7.m3.1.1.1.cmml\" xref=\"S4.F6.7.m3.1.1\">superscript</csymbol><ci id=\"S4.F6.7.m3.1.1.2.cmml\" xref=\"S4.F6.7.m3.1.1.2\">𝜆</ci><apply id=\"S4.F6.7.m3.1.1.3.cmml\" xref=\"S4.F6.7.m3.1.1.3\"><minus id=\"S4.F6.7.m3.1.1.3.1.cmml\" xref=\"S4.F6.7.m3.1.1.3.1\"></minus><ci id=\"S4.F6.7.m3.1.1.3.2.cmml\" xref=\"S4.F6.7.m3.1.1.3.2\">𝑘</ci><ci id=\"S4.F6.7.m3.1.1.3.3.cmml\" xref=\"S4.F6.7.m3.1.1.3.3\">𝑡</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F6.7.m3.1d\">\\lambda^{k-t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F6.7.m3.1e\">italic_λ start_POSTSUPERSCRIPT italic_k - italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>; in this case <math alttext=\"\\lambda=0.995\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F6.8.m4.1\"><semantics id=\"S4.F6.8.m4.1b\"><mrow id=\"S4.F6.8.m4.1.1\" xref=\"S4.F6.8.m4.1.1.cmml\"><mi id=\"S4.F6.8.m4.1.1.2\" xref=\"S4.F6.8.m4.1.1.2.cmml\">λ</mi><mo id=\"S4.F6.8.m4.1.1.1\" xref=\"S4.F6.8.m4.1.1.1.cmml\">=</mo><mn id=\"S4.F6.8.m4.1.1.3\" xref=\"S4.F6.8.m4.1.1.3.cmml\">0.995</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F6.8.m4.1c\"><apply id=\"S4.F6.8.m4.1.1.cmml\" xref=\"S4.F6.8.m4.1.1\"><eq id=\"S4.F6.8.m4.1.1.1.cmml\" xref=\"S4.F6.8.m4.1.1.1\"></eq><ci id=\"S4.F6.8.m4.1.1.2.cmml\" xref=\"S4.F6.8.m4.1.1.2\">𝜆</ci><cn id=\"S4.F6.8.m4.1.1.3.cmml\" type=\"float\" xref=\"S4.F6.8.m4.1.1.3\">0.995</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F6.8.m4.1d\">\\lambda=0.995</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F6.8.m4.1e\">italic_λ = 0.995</annotation></semantics></math>).</figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Need for Continuous pre-training.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px2.p1.1\">The overall decline trend may come from two sources, the missing knowledge of future and a lack of up-to-date language representation. The absence of relevant future information can lead to two outcomes: either the model makes uninformed or incorrect predictions, or, in some cases, more likely to refuse to answer altogether. We observe this latter behavior notably in Mistral-7B and Mixtral-8x7B, where refusal rates are significantly higher compared to other models, as shown in Figure <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"Figure 9 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>See Appendix <a class=\"ltx_ref\" href=\"#A2.SS2\" title=\"B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a> for more discussion of the refusal behavior.</span></span></span> The lack of knowledge can be partially recovered with information retrieval, as seen in the constrained open-book and gold article settings. For instance, Figures <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"Figure 9 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, <a class=\"ltx_ref\" href=\"#A2.F10\" title=\"Figure 10 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, and <a class=\"ltx_ref\" href=\"#A2.F11\" title=\"Figure 11 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> show that Mixtral-8x7B’s refusal rate drops from 14–28% in closed-book to 3–15% with open-book retrieval, and further to 0.5–4.2% with gold articles. However, accuracy still declines over time. Notably, the gold article setting provides an “upper bound” of open-book retrieval. The remaining performance drop despite full access to relevant information suggests that the models’ internal representations are outdated. This indicates continuous pre-training of LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2022</a>; Jin et al., <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">2022</a>; Ke et al., <a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">2022a</a>, <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">b</a>; Yıldız et al., <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">2024</a>)</cite> is still needed in the context of news event forecasting.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">TF &amp; MC Comparison.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px3.p1.1\">All models except for Claude-3.5-Sonnet struggle with TF questions, where the degradation trends towards the random baseline accuracy of 50%, indicating that predicting if a future event will happen or not can be sometimes challenging for LLMs. In contrast, on MC questions, models tend to perform much better than the random baseline at 25%. There are two potential reasons that can explain the disparity. First, TF questions can be considered more open-ended than MC because the “No” answer contains other possible open-ended outcomes. Second, since the distractor choices are created by an LLM, they may not be as likely to happen as the true answer.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Consistent Performance Decline After September 2021.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px4.p1.1\">Interestingly, Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveals a higher rate of performance decline around September 2021, which is the knowledge cutoff date of GPT-3.5, across all models, particularly for MC questions. In contrast, performance remains relatively stable prior to this date. We hypothesize that this trend arises because the period up to September 2021 may be overrepresented in many pre-training corpora <cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2020</a>; Gao et al., <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">2020</a>; Kobayashi, <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">2018</a>; Gokaslan &amp; Cohen, <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2019</a>; Zhu et al., <a class=\"ltx_ref\" href=\"#bib.bib54\" title=\"\">2015</a>; Rae et al., <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">2020</a>; Tiedemann, <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">2016</a>; Saxton et al., <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">2019</a>)</cite>, compared to more recent periods. Another potential cause of this imbalance is an increasing number of websites restricting access to web crawlers after the rise of ChatGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Longpre et al., <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">2024</a>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px5.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px5.p1.1\">On the data generation side, the generated questions as well as the distractor answers could contain biases from an outdated LLM, making the benchmark less reliable in the long run unless we upgrade the models. Additionally, generating questions from news articles can introduce bias by focusing only on events that have definitively occurred, overlooking potential events that never occur and thus never appear in the news. On the evaluation side, our paper proposes the continuous evaluation benchmark but at the time of the writing there isn’t a long enough time horizon on each model, especially after the cutoff dates, for a thorough analysis. Ideally, we would like to analyze the relation between the effect of knowledge and RAG cutoff dates but the trend seems to be weak within the time horizon available.</p>\n</div>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion and Future Work</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\" id=\"S5.p1.1\">We introduce Daily Oracle, a continuously updated QA benchmark leveraging daily news to evaluate the temporal generalization and future prediction capabilities of LLMs. Our experiments reveal that while LLMs maintain a degree of predictive power over future events, their prediction accuracy exhibits a significant smooth decline over time. Although RAG mitigates the effect of outdated knowledge, a strong and noticeable decline remains. Our findings in the gold article setting further emphasize the importance of disentangling missing knowledge from the lack of up-to-date representations. In the future, alongside maintaining Daily Oracle, we plan to incorporate a broader range of models and explore how continuous pre-training and efficient adaptation can address the performance degradation challenges presented in our work.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Impact Statement</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\" id=\"Sx1.p1.1\">Daily Oracle serves as an up-to-date, continuous evaluation benchmark for assessing AI models’ forecasting accuracy and temporal generalization. These abilities have broad applications in areas such as finance, healthcare, and policy-making. The performance drop we observe highlights the risk of outdated knowledge and the importance of continuous model updates to keep AI systems reliable. In the future, by assessing continuous model update strategies in Daily Oracle, the broader ML community can gain valuable insights into how to maintain AI systems that are relevant and well-informed on recent and upcoming events.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx2\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgment</h2>\n<div class=\"ltx_para\" id=\"Sx2.p1\">\n<p class=\"ltx_p\" id=\"Sx2.p1.1\">This work was supported in part by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) with a grant funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research. (No. RS-2024-00469482 &amp; RS-2024-00509279)\nWe also thank the Microsoft Accelerating Foundation Models Research program for providing Azure cloud compute credits for the LLM APIs. The compute was also supported by the NYU High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Agarwal &amp; Nenkova (2022)</span>\n<span class=\"ltx_bibblock\">\nAgarwal, O. and Nenkova, A.\n\n</span>\n<span class=\"ltx_bibblock\">Temporal effects on pre-trained models for language processing tasks.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib1.1.1\">Transactions of the Association for Computational Linguistics</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Anderson &amp; Schooler (1991)</span>\n<span class=\"ltx_bibblock\">\nAnderson, J. R. and Schooler, L. J.\n\n</span>\n<span class=\"ltx_bibblock\">Reflections of the environment in memory.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib2.1.1\">Psychological Science</em>, 2(6):396–408, 1991.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Anthropic (2024)</span>\n<span class=\"ltx_bibblock\">\nAnthropic.\n\n</span>\n<span class=\"ltx_bibblock\">The claude 3 model family: Opus, sonnet, haiku.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf\" title=\"\">https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</a>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nChen, W., Wang, X., Wang, W. Y., and Wang, W. Y.\n\n</span>\n<span class=\"ltx_bibblock\">A dataset for answering time-sensitive questions.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib4.1.1\">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dempsey et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nDempsey, W. H., Moreno, A., Scott, C. K., Dennis, M. L., Gustafson, D. H., Murphy, S. A., and Rehg, J. M.\n\n</span>\n<span class=\"ltx_bibblock\">iSurvive: An interpretable, event-time prediction model for mHealth.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib5.1.1\">Proceedings of the 34th International Conference on Machine Learning</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dubey et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al.\n\n</span>\n<span class=\"ltx_bibblock\">The llama 3 herd of models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib6.1.1\">arXiv preprint arXiv:2407.21783</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ester et al. (1996)</span>\n<span class=\"ltx_bibblock\">\nEster, M., Kriegel, H.-P., Sander, J., Xu, X., et al.\n\n</span>\n<span class=\"ltx_bibblock\">A density-based algorithm for discovering clusters in large spatial databases with noise.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib7.1.1\">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</em>, 1996.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fleiss (1971)</span>\n<span class=\"ltx_bibblock\">\nFleiss, J. L.\n\n</span>\n<span class=\"ltx_bibblock\">Measuring nominal scale agreement among many raters.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib8.1.1\">Psychological bulletin</em>, 76(5):378, 1971.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gao et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.\n\n</span>\n<span class=\"ltx_bibblock\">The pile: An 800gb dataset of diverse text for language modeling.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib9.1.1\">arXiv preprint arXiv:2101.00027</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gillingham et al. (2018)</span>\n<span class=\"ltx_bibblock\">\nGillingham, K., Nordhaus, W., Anthoff, D., Blanford, G., Bosetti, V., Christensen, P., McJeon, H., and Reilly, J.\n\n</span>\n<span class=\"ltx_bibblock\">Modeling uncertainty in integrated assessment of climate change: A multimodel comparison.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib10.1.1\">Journal of the Association of Environmental and Resource Economists</em>, 5(4):791–826, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gokaslan &amp; Cohen (2019)</span>\n<span class=\"ltx_bibblock\">\nGokaslan, A. and Cohen, V.\n\n</span>\n<span class=\"ltx_bibblock\">Openwebtext corpus.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://Skylion007.github.io/OpenWebTextCorpus\" title=\"\">http://Skylion007.github.io/OpenWebTextCorpus</a>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Halawi et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nHalawi, D., Zhang, F., Yueh-Han, C., and Steinhardt, J.\n\n</span>\n<span class=\"ltx_bibblock\">Approaching human-level forecasting with language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib12.1.1\">Advances in Neural Information Processing Systems</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hamborg et al. (2017)</span>\n<span class=\"ltx_bibblock\">\nHamborg, F., Meuschke, N., Breitinger, C., and Gipp, B.\n\n</span>\n<span class=\"ltx_bibblock\">news-please: A generic news crawler and extractor.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib13.1.1\">Proceedings of the 15th International Symposium of Information Science</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jang et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nJang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S. J., and Seo, M.\n\n</span>\n<span class=\"ltx_bibblock\">Towards continual knowledge learning of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib14.1.1\">International Conference on Learning Representations</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Mistral 7b.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib15.1.1\">arXiv preprint arXiv:2310.06825</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Mixtral of experts.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib16.1.1\">arXiv preprint arXiv:2401.04088</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jin et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nJin, W., Khanna, R., Kim, S., Lee, D.-H., Morstatter, F., Galstyan, A., and Ren, X.\n\n</span>\n<span class=\"ltx_bibblock\">ForecastQA: A question answering challenge for event forecasting with temporal text data.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib17.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jin et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nJin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X., Arnold, A., and Ren, X.\n\n</span>\n<span class=\"ltx_bibblock\">Lifelong pretraining: Continually adapting language models to emerging corpora.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib18.1.1\">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Karger et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nKarger, E., Bastani, H., Yueh-Han, C., Jacobs, Z., Halawi, D., Zhang, F., and Tetlock, P. E.\n\n</span>\n<span class=\"ltx_bibblock\">Forecastbench: A dynamic benchmark of ai forecasting capabilities.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib19.1.1\">International Conference on Learning Representations</em>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kasai et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nKasai, J., Sakaguchi, K., Le Bras, R., Asai, A., Yu, X., Radev, D., Smith, N. A., Choi, Y., Inui, K., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Realtime qa: what’s the answer right now?\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib20.1.1\">Advances in Neural Information Processing Systems</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ke et al. (2022a)</span>\n<span class=\"ltx_bibblock\">\nKe, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B.\n\n</span>\n<span class=\"ltx_bibblock\">Continual pre-training of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib21.1.1\">International Conference on Learning Representations</em>, 2022a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ke et al. (2022b)</span>\n<span class=\"ltx_bibblock\">\nKe, Z., Shao, Y., Lin, H., Xu, H., Shu, L., and Liu, B.\n\n</span>\n<span class=\"ltx_bibblock\">Adapting a language model while preserving its general knowledge.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib22.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kobayashi (2018)</span>\n<span class=\"ltx_bibblock\">\nKobayashi, S.\n\n</span>\n<span class=\"ltx_bibblock\">Homemade bookcorpus.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/soskek/bookcorpus\" title=\"\">https://github.com/soskek/bookcorpus</a>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lazaridou et al. (2021)</span>\n<span class=\"ltx_bibblock\">\nLazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., de Masson d'Autume, C., Kocisky, T., Ruder, S., Yogatama, D., Cao, K., Young, S., and Blunsom, P.\n\n</span>\n<span class=\"ltx_bibblock\">Mind the gap: Assessing temporal generalization in neural language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib24.1.1\">Advances in Neural Information Processing Systems</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lewis et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Retrieval-augmented generation for knowledge-intensive nlp tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib25.1.1\">Advances in Neural Information Processing Systems</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li &amp; Flanigan (2024)</span>\n<span class=\"ltx_bibblock\">\nLi, C. and Flanigan, J.\n\n</span>\n<span class=\"ltx_bibblock\">Task contamination: Language models may not be few-shot anymore.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib26.1.1\">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liska et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nLiska, A., Kocisky, T., Gribovskaya, E., Terzi, T., Sezener, E., Agrawal, D., Cyprien De Masson, D., Scholtes, T., Zaheer, M., Young, S., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib27.1.1\">International Conference on Machine Learning</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Longpre et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nLongpre, S., Mahari, R., Lee, A., Lund, C., Oderinwale, H., Brannon, W., Saxena, N., Obeng-Marnu, N., South, T., Hunter, C., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Consent in crisis: The rapid decline of the ai data commons.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib28.1.1\">Advances in Neural Information Processing Systems</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lopez-Lira &amp; Tang (2023)</span>\n<span class=\"ltx_bibblock\">\nLopez-Lira, A. and Tang, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Can chatgpt forecast stock price movements? return predictability and large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib29.1.1\">arXiv preprint arXiv:2304.07619</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McIntosh et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nMcIntosh, T. R., Susnjak, T., Arachchilage, N., Liu, T., Xu, D., Watters, P., and Halgamuge, M. N.\n\n</span>\n<span class=\"ltx_bibblock\">Inadequacies of large language model benchmarks in the era of generative artificial intelligence.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib30.1.1\">IEEE Transactions on Artificial Intelligence</em>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nagel (2016)</span>\n<span class=\"ltx_bibblock\">\nNagel, S.\n\n</span>\n<span class=\"ltx_bibblock\">Common crawl news dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://data.commoncrawl.org/crawl-data/CC-NEWS/index.html\" title=\"\">https://data.commoncrawl.org/crawl-data/CC-NEWS/index.html</a>, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI (2023)</span>\n<span class=\"ltx_bibblock\">\nOpenAI.\n\n</span>\n<span class=\"ltx_bibblock\">GPT-4 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib32.1.1\">arXiv preprint arXiv:2303.08774</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI (2024a)</span>\n<span class=\"ltx_bibblock\">\nOpenAI.\n\n</span>\n<span class=\"ltx_bibblock\">New embedding models and api updates.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/new-embedding-models-and-api-updates/\" title=\"\">https://openai.com/index/new-embedding-models-and-api-updates/</a>, 2024a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI (2024b)</span>\n<span class=\"ltx_bibblock\">\nOpenAI.\n\n</span>\n<span class=\"ltx_bibblock\">Gpt-4o system card.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib34.1.1\">arXiv preprint arXiv:2410.21276</em>, 2024b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rae et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nRae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P.\n\n</span>\n<span class=\"ltx_bibblock\">Compressive transformers for long-range sequence modelling.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib35.1.1\">International Conference on Learning Representations</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Raffel et al. (2020)</span>\n<span class=\"ltx_bibblock\">\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.\n\n</span>\n<span class=\"ltx_bibblock\">Exploring the limits of transfer learning with a unified text-to-text transformer.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib36.1.1\">Journal of Machine Learning Research</em>, 21:1–67, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Robertson et al. (1995)</span>\n<span class=\"ltx_bibblock\">\nRobertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., Gatford, M., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Okapi at trec-3.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib37.1.1\">Nist Special Publication Sp</em>, 109:109, 1995.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Röttger &amp; Pierrehumbert (2021)</span>\n<span class=\"ltx_bibblock\">\nRöttger, P. and Pierrehumbert, J.\n\n</span>\n<span class=\"ltx_bibblock\">Temporal adaptation of BERT and performance on downstream document classification: Insights from social media.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib38.1.1\">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sainz et al. (2023)</span>\n<span class=\"ltx_bibblock\">\nSainz, O., Campos, J., García-Ferrero, I., Etxaniz, J., de Lacalle, O. L., and Agirre, E.\n\n</span>\n<span class=\"ltx_bibblock\">NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib39.1.1\">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Saxton et al. (2019)</span>\n<span class=\"ltx_bibblock\">\nSaxton, D., Grefenstette, E., Hill, F., and Kohli, P.\n\n</span>\n<span class=\"ltx_bibblock\">Analysing mathematical reasoning abilities of neural models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib40.1.1\">International Conference on Learning Representations</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Team et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nTeam, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Gemma 2: Improving open language models at a practical size.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib41.1.1\">arXiv preprint arXiv:2408.00118</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tetlock &amp; Gardner (2016)</span>\n<span class=\"ltx_bibblock\">\nTetlock, P. E. and Gardner, D.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib42.1.1\">Superforecasting: The art and science of prediction</em>.\n\n</span>\n<span class=\"ltx_bibblock\">Random House, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tiedemann (2016)</span>\n<span class=\"ltx_bibblock\">\nTiedemann, J.\n\n</span>\n<span class=\"ltx_bibblock\">Finding alternative translations in a large corpus of movie subtitle.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib43.1.1\">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</em>, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le, Q., and Luong, T.\n\n</span>\n<span class=\"ltx_bibblock\">FreshLLMs: Refreshing large language models with search engine augmentation.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib44.1.1\">Findings of the Association for Computational Linguistics ACL 2024</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D.\n\n</span>\n<span class=\"ltx_bibblock\">Chain-of-thought prompting elicits reasoning in large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib45.1.1\">Advances in neural information processing systems</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nXu, C., Guan, S., Greene, D., and Kechadi, M.-T.\n\n</span>\n<span class=\"ltx_bibblock\">Benchmark data contamination of large language models: A survey.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib46.1.1\">arXiv preprint arXiv:2406.04244</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yan et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYan, Q., Seraj, R., He, J., Meng, L., and Sylvain, T.\n\n</span>\n<span class=\"ltx_bibblock\">Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib47.1.1\">The Twelfth International Conference on Learning Representations</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib48.1.1\">arXiv preprint arXiv:2407.10671</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYe, C., Hu, Z., Deng, Y., Huang, Z., Ma, M. D., Zhu, Y., and Wang, W.\n\n</span>\n<span class=\"ltx_bibblock\">Mirai: Evaluating llm agents for event forecasting.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib49.1.1\">arXiv preprint arXiv:2407.01231</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yıldız et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYıldız, Ç., Ravichandran, N. K., Punia, P., Bethge, M., and Ermis, B.\n\n</span>\n<span class=\"ltx_bibblock\">Investigating continual pretraining in large language models: Insights and implications.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib50.1.1\">arXiv preprint arXiv:2402.17400</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang &amp; Choi (2021)</span>\n<span class=\"ltx_bibblock\">\nZhang, M. and Choi, E.\n\n</span>\n<span class=\"ltx_bibblock\">SituatedQA: Incorporating extra-linguistic contexts into QA.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib51.1.1\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. (2024)</span>\n<span class=\"ltx_bibblock\">\nZhang, Z., Cao, Y., Ye, C., Ma, Y., Liao, L., and Chua, T.-S.\n\n</span>\n<span class=\"ltx_bibblock\">Analyzing temporal complex events with large language models? a benchmark towards temporal, long context understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib52.1.1\">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu et al. (2025)</span>\n<span class=\"ltx_bibblock\">\nZhu, C., Chen, N., Gao, Y., Zhang, Y., Tiwari, P., and Wang, B.\n\n</span>\n<span class=\"ltx_bibblock\">Is your LLM outdated? a deep look at temporal generalization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib53.1.1\">Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu et al. (2015)</span>\n<span class=\"ltx_bibblock\">\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S.\n\n</span>\n<span class=\"ltx_bibblock\">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib54.1.1\">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</em>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zou et al. (2022)</span>\n<span class=\"ltx_bibblock\">\nZou, A., Xiao, T., Jia, R., Kwon, J., Mazeika, M., Li, R., Song, D., Steinhardt, J., Evans, O., and Hendrycks, D.\n\n</span>\n<span class=\"ltx_bibblock\">Forecasting future world events with neural networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib55.1.1\">Advances in Neural Information Processing Systems</em>, 2022.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_appendix\" id=\"Ax1\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n</section>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Dataset Details</h2>\n<section class=\"ltx_subsection\" id=\"A1.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Illustration of the dataset construction process.</h3>\n<div class=\"ltx_para\" id=\"A1.SS1.p1\">\n<p class=\"ltx_p\" id=\"A1.SS1.p1.1\">Figure <a class=\"ltx_ref\" href=\"#A1.F7\" title=\"Figure 7 ‣ A.1 Illustration of the dataset construction process. ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows how Daily Oracle is automatically generated as discussed in Section <a class=\"ltx_ref\" href=\"#S3.SS1\" title=\"3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A1.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"266\" id=\"A1.F7.g1\" src=\"./assets/x7.png\" width=\"415\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>Data Construction Process of Daily Oracle.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Details for Article Selection</h3>\n<div class=\"ltx_para\" id=\"A1.SS2.p1\">\n<p class=\"ltx_p\" id=\"A1.SS2.p1.1\">We select daily articles that generate the QA pairs in two ways: (1) <span class=\"ltx_text ltx_font_italic\" id=\"A1.SS2.p1.1.1\">Random Selection:</span> We randomly sample three articles each day. (2) <span class=\"ltx_text ltx_font_italic\" id=\"A1.SS2.p1.1.2\">Hot Topic Selection:</span> To better capture daily events and reduce noise, we select three articles from the top three hot topics of the day. We identify these hot topics by applying the density-based clustering algorithm DBSCAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Ester et al., <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">1996</a>)</cite> to the new articles based on TF-IDF (Term Frequency-Inverse Document Frequency) representations, forming clusters of news articles for each day. We filter out chaotic clusters by removing those with low average in-cluster cosine similarity scores, which typically correspond to clusters containing a large number of diverse articles. The top three clusters, determined by size, are assumed to represent the most discussed events, i.e. hot topics, since larger clusters indicate more articles covering the same event. One article is picked randomly from each of the top three clusters.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>QA Filtering Principles</h3>\n<div class=\"ltx_para\" id=\"A1.SS3.p1\">\n<p class=\"ltx_p\" id=\"A1.SS3.p1.1\">During the design stage of QA pair generation, we manually review the questions and identify seven key criteria to ensure the QA pairs qualify as valid forecasting questions. These principles guide the QA filtering step in the data construction process:</p>\n</div>\n<div class=\"ltx_para\" id=\"A1.SS3.p2\">\n<ol class=\"ltx_enumerate\" id=\"A1.I1\">\n<li class=\"ltx_item\" id=\"A1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(1)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i1.p1.1.1\">Correctness of Answers:</span> The answer must be factually accurate and fully aligned with the information in the given article.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(2)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i2.p1.1.1\">Non-answerability Before the Publication Date:</span> Since we treat the article’s publication date as the question’s resolution date, the question should not be definitively answerable based on information available before the article’s publication.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(3)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i3.p1.1.1\">Absence of Information Leakage:</span> Questions must avoid revealing information that became known only after the article’s publication, maintaining fairness for pre-publication evaluation.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(4)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i4.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i4.p1.1.1\">Objectivity:</span> Both questions and answers must rely on objective facts, avoiding subjective ideas from the authors.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(5)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i5.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i5.p1.1.1\">Inclusion of a Clear Temporal Element: </span> Questions must contain a specific and clear reference to time, avoiding vague phrases like “in the future” or “soon.”</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(6)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i6.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i6.p1.1.1\">Public Interest:</span> The questions should address topics of broad public concern.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(7)</span>\n<div class=\"ltx_para\" id=\"A1.I1.i7.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i7.p1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A1.I1.i7.p1.1.1\">Non-obviousness of the Answer:</span> The answer should not be immediately predictable from the question and must provide new or non-trivial insights.</p>\n</div>\n</li>\n</ol>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.4 </span>Details for Human Evaluation</h3>\n<div class=\"ltx_para\" id=\"A1.SS4.p1\">\n<p class=\"ltx_p\" id=\"A1.SS4.p1.1\">We assess the quality of our dataset by evaluating the effectiveness of our LLM-based evaluator in the <span class=\"ltx_text ltx_font_italic\" id=\"A1.SS4.p1.1.1\">QA Filtering</span> step. Four human annotators independently review a randomly sampled subset of Daily Oracle, consisting of 30 TF and 30 MC QA pairs. They follow the same instructions used to prompt the LLM and evaluate each QA pair based on the seven filtering principles listed in Appendix <a class=\"ltx_ref\" href=\"#A1.SS3\" title=\"A.3 QA Filtering Principles ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"A1.SS4.p2\">\n<p class=\"ltx_p\" id=\"A1.SS4.p2.1\">Table <a class=\"ltx_ref\" href=\"#A1.T4\" title=\"Table 4 ‣ A.4 Details for Human Evaluation ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the inter-rater agreement among human annotators and the agreement between human and LLM evaluators. The average Fleiss’ Kappa of 0.26 indicates fair agreement among annotators. Among the seven principles, <span class=\"ltx_text ltx_font_italic\" id=\"A1.SS4.p2.1.1\">Objectivity</span> exhibits the highest agreement (0.66), while <span class=\"ltx_text ltx_font_italic\" id=\"A1.SS4.p2.1.2\">Non-Answerability Before the Publication Date</span> has the lowest (0.02).</p>\n</div>\n<div class=\"ltx_para\" id=\"A1.SS4.p3\">\n<p class=\"ltx_p\" id=\"A1.SS4.p3.1\">Comparing human-assigned and LLM-assigned scores, the exact-match accuracy between the human consensus and LLM evaluations averages 89.52% across the seven principles, showing the effectiveness of our LLM-based filtering method. <span class=\"ltx_text ltx_font_italic\" id=\"A1.SS4.p3.1.1\">Non-Answerability Before the Publication Date</span> shows the lowest agreement (83.33% accuracy), suggesting it is the most challenging principle for both humans and the LLM to evaluate consistently.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A1.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>The inter-rater agreement among four human annotators is evaluated using Fleiss’ Kappa, while the agreement between human and LLM evaluators is measured through accuracy scores. We report metrics across seven <span class=\"ltx_text ltx_font_italic\" id=\"A1.T4.2.1\">QA Filtering</span> principles using a sample of 60 randomly selected QA pairs.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T4.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T4.3.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"A1.T4.3.1.1.1\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A1.T4.3.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.3.1.1.2.1\" style=\"font-size:80%;\">Human Agreement</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T4.3.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.3.1.1.3.1\" style=\"font-size:80%;\">Human vs. LLM</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.2.2.1\"><span class=\"ltx_text\" id=\"A1.T4.3.2.2.1.1\" style=\"font-size:80%;\">Metric</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.2.2.2\"><span class=\"ltx_text\" id=\"A1.T4.3.2.2.2.1\" style=\"font-size:80%;\">Fleiss’ Kappa</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.2.2.3\"><span class=\"ltx_text\" id=\"A1.T4.3.2.2.3.1\" style=\"font-size:80%;\">Accuracy (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A1.T4.3.3.3.1\"><span class=\"ltx_text\" id=\"A1.T4.3.3.3.1.1\" style=\"font-size:80%;\">Correctness of Answers</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A1.T4.3.3.3.2\"><span class=\"ltx_text\" id=\"A1.T4.3.3.3.2.1\" style=\"font-size:80%;\">0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T4.3.3.3.3\"><span class=\"ltx_text\" id=\"A1.T4.3.3.3.3.1\" style=\"font-size:80%;\">96.67</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.4.4.1\"><span class=\"ltx_text\" id=\"A1.T4.3.4.4.1.1\" style=\"font-size:80%;\">Non-answerability Before the Publication Date</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.4.4.2\"><span class=\"ltx_text\" id=\"A1.T4.3.4.4.2.1\" style=\"font-size:80%;\">0.02</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.4.4.3\"><span class=\"ltx_text\" id=\"A1.T4.3.4.4.3.1\" style=\"font-size:80%;\">83.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.5.5.1\"><span class=\"ltx_text\" id=\"A1.T4.3.5.5.1.1\" style=\"font-size:80%;\">Absence of Information Leakage</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.5.5.2\"><span class=\"ltx_text\" id=\"A1.T4.3.5.5.2.1\" style=\"font-size:80%;\">0.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.5.5.3\"><span class=\"ltx_text\" id=\"A1.T4.3.5.5.3.1\" style=\"font-size:80%;\">86.67</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.6.6.1\"><span class=\"ltx_text\" id=\"A1.T4.3.6.6.1.1\" style=\"font-size:80%;\">Objectivity</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.6.6.2\"><span class=\"ltx_text\" id=\"A1.T4.3.6.6.2.1\" style=\"font-size:80%;\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.6.6.3\"><span class=\"ltx_text\" id=\"A1.T4.3.6.6.3.1\" style=\"font-size:80%;\">98.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.7.7.1\"><span class=\"ltx_text\" id=\"A1.T4.3.7.7.1.1\" style=\"font-size:80%;\">Inclusion of a Clear Temporal Element</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.7.7.2\"><span class=\"ltx_text\" id=\"A1.T4.3.7.7.2.1\" style=\"font-size:80%;\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.7.7.3\"><span class=\"ltx_text\" id=\"A1.T4.3.7.7.3.1\" style=\"font-size:80%;\">90.00</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.8.8.1\"><span class=\"ltx_text\" id=\"A1.T4.3.8.8.1.1\" style=\"font-size:80%;\">Public Interest</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.8.8.2\"><span class=\"ltx_text\" id=\"A1.T4.3.8.8.2.1\" style=\"font-size:80%;\">0.18</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.8.8.3\"><span class=\"ltx_text\" id=\"A1.T4.3.8.8.3.1\" style=\"font-size:80%;\">88.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A1.T4.3.9.9.1\"><span class=\"ltx_text\" id=\"A1.T4.3.9.9.1.1\" style=\"font-size:80%;\">Non-obviousness of the Answer</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A1.T4.3.9.9.2\"><span class=\"ltx_text\" id=\"A1.T4.3.9.9.2.1\" style=\"font-size:80%;\">0.29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A1.T4.3.9.9.3\"><span class=\"ltx_text\" id=\"A1.T4.3.9.9.3.1\" style=\"font-size:80%;\">83.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.3.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T4.3.10.10.1\"><span class=\"ltx_text\" id=\"A1.T4.3.10.10.1.1\" style=\"font-size:80%;\">Average</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T4.3.10.10.2\"><span class=\"ltx_text\" id=\"A1.T4.3.10.10.2.1\" style=\"font-size:80%;\">0.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T4.3.10.10.3\"><span class=\"ltx_text\" id=\"A1.T4.3.10.10.3.1\" style=\"font-size:80%;\">89.52</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.5 </span>Distribution of Question Categories Over Time</h3>\n<div class=\"ltx_para\" id=\"A1.SS5.p1\">\n<p class=\"ltx_p\" id=\"A1.SS5.p1.1\">In Figure <a class=\"ltx_ref\" href=\"#A1.F8\" title=\"Figure 8 ‣ A.5 Distribution of Question Categories Over Time ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we provide the distributions of question categories for both TF and MC questions.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A1.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"240\" id=\"A1.F8.g1\" src=\"./assets/x8.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>Question category distribution over time.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Experiment Details</h2>\n<section class=\"ltx_subsection\" id=\"A2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Baseline Models Information</h3>\n<div class=\"ltx_para\" id=\"A2.SS1.p1\">\n<p class=\"ltx_p\" id=\"A2.SS1.p1.1\">Table <a class=\"ltx_ref\" href=\"#A2.T5\" title=\"Table 5 ‣ B.1 Baseline Models Information ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> lists the LLM model versions used in our experiments.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A2.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Baseline model versions.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A2.T5.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T5.1.1.1.2\">Model Version</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T5.1.2.1.1\">Claude-3.5-Sonnet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T5.1.2.1.2\">claude-3-5-sonnet-20240620</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.3.2.1\">GPT-4</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.3.2.2\">gpt-4-1106-preview</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.4.3.1\">GPT-3.5</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.4.3.2\">gpt-3.5-turbo-0125</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.5.4.1\">Mixtral-8x7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.5.4.2\">Mixtral-8x7B-Instruct-v0.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.6.5.1\">Mistral-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.6.5.2\">Mistral-7B-Instruct-v0.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.7.6.1\">Llama-3-8B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.7.6.2\">Meta-Llama-3-8B-Instruct</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T5.1.8.7.1\">Qwen-2-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T5.1.8.7.2\">Qwen2-7B-Instruct</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T5.1.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T5.1.9.8.1\">Gemma-2-2B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T5.1.9.8.2\">gemma-2-2b-it</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Refusal Rates</h3>\n<div class=\"ltx_para\" id=\"A2.SS2.p1\">\n<p class=\"ltx_p\" id=\"A2.SS2.p1.3\">Although the models are prompted to provide definitive answers rather than responding like “I cannot predict the future,” some models still occasionally refuse to do so. Figures <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"Figure 9 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b), <a class=\"ltx_ref\" href=\"#A2.F10\" title=\"Figure 10 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(b), <a class=\"ltx_ref\" href=\"#A2.F11\" title=\"Figure 11 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(b) show the refusal rates for the closed-book, constraint open-book, and gold article settings. In closed-book evaluation (Figure <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"Figure 9 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(b)), we can see that the refusal rates increase throughout the time for Mistral-7B in TF questions and Mixtral-8x7B in both TF and MC questions.\nAdditionally, these two models exhibit notable refusal rates, with approximately 10–30% on TF questions and 1.5–8% on MC questions, resulting in their closed-book performances dropping below the random baseline of 50% in certain months, as shown in Figure <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In comparison, Qwen-2-7B and Gemma-2-2B show relatively low refusal rates—<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.1.m1.1\"><semantics id=\"A2.SS2.p1.1.m1.1a\"><mo id=\"A2.SS2.p1.1.m1.1.1\" xref=\"A2.SS2.p1.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS2.p1.1.m1.1b\"><lt id=\"A2.SS2.p1.1.m1.1.1.cmml\" xref=\"A2.SS2.p1.1.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS2.p1.1.m1.1c\">&lt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS2.p1.1.m1.1d\">&lt;</annotation></semantics></math>5% for TF and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.2.m2.1\"><semantics id=\"A2.SS2.p1.2.m2.1a\"><mo id=\"A2.SS2.p1.2.m2.1.1\" xref=\"A2.SS2.p1.2.m2.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS2.p1.2.m2.1b\"><lt id=\"A2.SS2.p1.2.m2.1.1.cmml\" xref=\"A2.SS2.p1.2.m2.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS2.p1.2.m2.1c\">&lt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS2.p1.2.m2.1d\">&lt;</annotation></semantics></math>2% for MC—while all other models have near-zero refusal rates for TF and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.3.m3.1\"><semantics id=\"A2.SS2.p1.3.m3.1a\"><mo id=\"A2.SS2.p1.3.m3.1.1\" xref=\"A2.SS2.p1.3.m3.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS2.p1.3.m3.1b\"><lt id=\"A2.SS2.p1.3.m3.1.1.cmml\" xref=\"A2.SS2.p1.3.m3.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS2.p1.3.m3.1c\">&lt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS2.p1.3.m3.1d\">&lt;</annotation></semantics></math>1% for MC.</p>\n</div>\n<div class=\"ltx_para\" id=\"A2.SS2.p2\">\n<p class=\"ltx_p\" id=\"A2.SS2.p2.1\">The refusal behavior is likely influenced by alignment techniques, which discourage uncertain responses in the post-training stage. Although refusal rates contribute to lower accuracies for certain models, our results show that performance degradation trends persist even when refusals are excluded (Figures <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"Figure 9 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>(a), <a class=\"ltx_ref\" href=\"#A2.F10\" title=\"Figure 10 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(a), <a class=\"ltx_ref\" href=\"#A2.F11\" title=\"Figure 11 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>(a)). We consider refusal to answer an indicator of performance limitations in forecasting tasks, as it reflects the model’s lack of actionable knowledge. When models are supplied with more up-to-date and relevant information, their refusal rates decrease (Figure <a class=\"ltx_ref\" href=\"#A2.F10\" title=\"Figure 10 ‣ B.2 Refusal Rates ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(b)). This suggests that refusal is one example of the broader challenge of temporal generalization and reinforces the need for continual model updates or improved external knowledge integration.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"561\" id=\"A2.F9.g1\" src=\"./assets/x9.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>Accuracy excluding refusal rates and refusal rates under the closed-book setting. We plot the 5-month moving average refusal rates for TF and MC questions across different LLMs. We count refusal cases as incorrect both to maintain comparability across models and because failing to provide an answer when a prediction is expected represents an unsatisfactory outcome from the user’s perspective.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"559\" id=\"A2.F10.g1\" src=\"./assets/x10.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>Accuracy excluding refusal rates and refusal rates for Mixtral-8x7B under the constrained open‑book setting. (b) shows that the open-book refusal rate (blue curves) is lower than in the closed-book setting (cyan curve), indicating that access to more up-to-date and relevant information reduces model refusals.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"556\" id=\"A2.F11.g1\" src=\"./assets/x11.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>Accuracy excluding refusal rates and refusal rates under the gold article setting.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Results for GPT-3.5 in the Gold Article Setting</h3>\n<div class=\"ltx_para\" id=\"A2.SS3.p1\">\n<p class=\"ltx_p\" id=\"A2.SS3.p1.1\">To more effectively illustrate the trends of other models at a suitable scale, we display GPT-3.5’s performance in the gold article setting separately. As shown in Figure <a class=\"ltx_ref\" href=\"#A2.F12\" title=\"Figure 12 ‣ B.3 Results for GPT-3.5 in the Gold Article Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, this outdated model performs relatively poorly throughout. While its accuracy could improve with chain-of-thought prompting <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2022</a>)</cite>, we report its performance using the same prompt format as the other models for consistency in comparison. Nevertheless, the degrading trend can still be observed.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F12\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"257\" id=\"A2.F12.g1\" src=\"./assets/x12.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>Results for GPT-3.5 in the gold article setting. Compared to other models achieving around 0.9 accuracy, GPT-3.5 performs worse in both MC questions and, more notably, in TF questions.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>More Results in the Constraint Open-Book Setting</h3>\n<div class=\"ltx_para\" id=\"A2.SS4.p1\">\n<p class=\"ltx_p\" id=\"A2.SS4.p1.1\">Figures <a class=\"ltx_ref\" href=\"#A2.F13\" title=\"Figure 13 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"#A2.F14\" title=\"Figure 14 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, <a class=\"ltx_ref\" href=\"#A2.F15\" title=\"Figure 15 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, <a class=\"ltx_ref\" href=\"#A2.F16\" title=\"Figure 16 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>, <a class=\"ltx_ref\" href=\"#A2.F17\" title=\"Figure 17 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>, <a class=\"ltx_ref\" href=\"#A2.F18\" title=\"Figure 18 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, and <a class=\"ltx_ref\" href=\"#A2.F19\" title=\"Figure 19 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> show the constrained open-book evaluation results for more models. Similar patterns are observed as discussed in Section <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. Specifically, for Claude-3.5-Sonnet, the constrained open-book performance lags behind its closed-book performance, likely because it already has robust representations of world events, suggesting that irrelevant or confounding retrieved information may degrade performance. This highlights the need for more careful RAG design in models that already possess robust world knowledge. GPT-3.5 is not included in the constrained open-book setting due to its unexpectedly poor performance in the gold article setting (Figure <a class=\"ltx_ref\" href=\"#A2.F12\" title=\"Figure 12 ‣ B.3 Results for GPT-3.5 in the Gold Article Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>) and budget limitations. Additionally, due to budget constraints, open-book evaluations of proprietary LLMs (Claude-3.5-Sonnet, GPT-3.5, GPT-4) are conducted only up to September 2024, whereas other LLMs are evaluated through December 2024.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F13\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"279\" id=\"A2.F13.g1\" src=\"./assets/x13.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 13: </span>Results for Claude-3.5-Sonnet in the constrained open-book setting.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F14\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"277\" id=\"A2.F14.g1\" src=\"./assets/x14.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 14: </span>Results for GPT-4 in the constrained open-book setting.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F15\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"270\" id=\"A2.F15.g1\" src=\"./assets/x15.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 15: </span>Results for Mixtral-8x7B in the constrained open-book setting.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F16\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"267\" id=\"A2.F16.g1\" src=\"./assets/x16.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 16: </span>Results for Mistral-7B in the constrained open-book setting.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F17\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"275\" id=\"A2.F17.g1\" src=\"./assets/x17.png\" width=\"705\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 17: </span>Results for Llama-3-8B in the constrained open-book setting.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F18\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"264\" id=\"A2.F18.g1\" src=\"./assets/x18.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 18: </span>Results for Qwen-2-7B in the constrained open-book setting.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F19\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"273\" id=\"A2.F19.g1\" src=\"./assets/x19.png\" width=\"705\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 19: </span>Results for Gemma-2-2B in the constrained open-book setting.\n</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.5 </span>An Example of Evaluating LLMs Under Different Settings</h3>\n<div class=\"ltx_para\" id=\"A2.SS5.p1\">\n<p class=\"ltx_p\" id=\"A2.SS5.p1.1\">Figure <a class=\"ltx_ref\" href=\"#A2.F20\" title=\"Figure 20 ‣ B.5 An Example of Evaluating LLMs Under Different Settings ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> presents a case study demonstrating how Mixtral-8x7B responds to a question under different experimental settings. The model provides an incorrect answer in the closed-book setting. However, when supplemented with retrieved relevant articles or the gold article, it produces the correct answer.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F20\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A2.F20.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.2.1\" style=\"font-size:90%;\">[News Article]\n<br class=\"ltx_break\"/>Title:<span class=\"ltx_text ltx_font_medium\" id=\"A2.F20.2.1.1\"> DFL leaders, Minneapolis announce deal on rideshare pay, but Lyft and Uber say they will leave\n<br class=\"ltx_break\"/></span>Publishing Date:<span class=\"ltx_text ltx_font_medium\" id=\"A2.F20.2.1.2\"> 2024-05-06\n<br class=\"ltx_break\"/></span>Text:<span class=\"ltx_text ltx_font_medium\" id=\"A2.F20.2.1.3\"> ST. PAUL, Minn. — Key lawmakers and DFL legislative leaders on Monday announced that they have a compromise on statewide rideshare regulations in Minnesota, but Uber and Lyft quickly rejected the plan and said that they will still exit the market if the proposal becomes law.\n<br class=\"ltx_break\"/></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.3\"><span class=\"ltx_text\" id=\"A2.F20.3.1\" style=\"font-size:90%;\">The amendment, which will be heard in a House committee Tuesday, includes a minimum wage pay rate of $1.27 per mile and 49 cents per minute. Leaders said they worked with the Minneapolis City Council on finding compromise, but the companies were not included in the negotiations.\n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.4\"><span class=\"ltx_text\" id=\"A2.F20.4.1\" style=\"font-size:90%;\">The proposal is lower than the Minneapolis rate of $1.40 per mile and $0.49 per minute, but higher than study done by the Minnesota Department of Labor and Industry, which determined $0.89 mile and $0.49 for the Twin Cities metro would bring driver pay up to the cityś minimum wage of $15.57 an hour.\n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.5\"><span class=\"ltx_text\" id=\"A2.F20.5.1\" style=\"font-size:90%;\">“This was a really big move for the Minneapolis City Council. This is coming down 13 cents from the rate that they passed. And this is consistent with the rates that Uber and Lyft are operating with and other states and other jurisdictions,” House Majority Leader Jamie Long, DFL-Minneapolis, who was a key negotiator, told reporters. “We know that the companies have said the same thing in other states. They’ve said that they were going to leave at certain rates, and they’ve stayed and been able to operate profitably.”\n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.6\"><span class=\"ltx_text\" id=\"A2.F20.6.1\" style=\"font-size:90%;\">Uber and Lyft quickly responded that they will still plan to leave Minneapolis — as they said would happen on July 1, when the cityś ordinance is effective — and the state if this becomes law.\n<br class=\"ltx_break\"/>…\n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.7\"><span class=\"ltx_rule\" style=\"width:100%;height:1px;background:black;display:inline-block;\"> </span><span class=\"ltx_text\" id=\"A2.F20.7.1\" style=\"font-size:90%;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.7.1.1\">[Generated QA Pairs]\n<br class=\"ltx_break\"/>Question:</span> What will be the response from Uber and Lyft if the proposed rideshare regulations become law in Minnesota by May 2024?\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.7.1.2\">Answer:</span> (a) They will plan to leave the market.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.7.1.3\">Misleading Choices:\n<br class=\"ltx_break\"/></span>(b) They will stay and adjust their pricing to meet regulations.\n<br class=\"ltx_break\"/>(c) They will collaborate to develop an alternative service model\n<br class=\"ltx_break\"/>(d) They will reduce their services rather than exit.\n<br class=\"ltx_break\"/><span class=\"ltx_rule\" style=\"width:100%;height:1px;background:black;display:inline-block;\"> </span>\n<span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.7.1.4\">[Closed-Book Setting]</span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<ul class=\"ltx_itemize ltx_centering ltx_figure_panel\" id=\"A2.I1\">\n<li class=\"ltx_item\" id=\"A2.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A2.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"A2.I1.i1.p1.1\"><span class=\"ltx_text\" id=\"A2.I1.i1.p1.1.1\" style=\"font-size:90%;\">The question is asked directly to Mixtral-8x7B.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A2.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"A2.I1.i2.p1.1\"><span class=\"ltx_text\" id=\"A2.I1.i2.p1.1.1\" style=\"font-size:90%;\">Model response: </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.I1.i2.p1.1.2\" style=\"font-size:90%;\">(wrong)</span><span class=\"ltx_text\" id=\"A2.I1.i2.p1.1.3\" style=\"font-size:90%;\"> “(d) They will reduce their services rather than exit. This is because leaving the market entirely can be a drastic move and may not be the first response to new regulations. Reducing services is a more likely response, as it allows rideshare companies to comply with the new laws while still maintaining a presence in the market. Additionally, they may also try to negotiate or influence the regulations before they become law.”</span></p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.8\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.8.1\" style=\"font-size:90%;\">[Constraint-Open Book Setting]<span class=\"ltx_text ltx_font_medium\" id=\"A2.F20.8.1.1\"></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<ul class=\"ltx_itemize ltx_centering ltx_figure_panel\" id=\"A2.I2\">\n<li class=\"ltx_item\" id=\"A2.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i1.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i1.p1.1.1\" style=\"font-size:90%;\">The RAG-cutoff refers to the latest accessible date for retrieving articles. With RAG-cutoff = 2024-03-01, we extracted the top 5 relevant articles (here, only the titles are displayed, but the articles are included in the evaluation prompt):</span></p>\n<ul class=\"ltx_itemize\" id=\"A2.I2.i1.I1\">\n<li class=\"ltx_item\" id=\"A2.I2.i1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.I2.i1.I1.i1.1.1.1\">–</span></span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i1.I1.i1.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i1.I1.i1.p1.1.1\" style=\"font-size:90%;\">(2023-05-25) Uber Threatens To Cut Most Service In Minnesota If Minimum Pay Law Takes Effect</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.I2.i1.I1.i2.1.1.1\">–</span></span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i1.I1.i2.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i1.I1.i2.p1.1.1\" style=\"font-size:90%;\">(2023-05-25) Minnesota Governor Vetoes Rideshare Minimum Pay Bill After Uber Threatens To Leave State</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.I2.i1.I1.i3.1.1.1\">–</span></span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i1.I1.i3.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i1.I1.i3.p1.1.1\" style=\"font-size:90%;\">(2023-08-09) Minneapolis City Council moving towards minimum wage for rideshare drivers</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i1.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.I2.i1.I1.i4.1.1.1\">–</span></span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i1.I1.i4.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i1.I1.i4.p1.1.1\" style=\"font-size:90%;\">(2024-02-27) Minneapolis City Council to consider rideshare minimum wage ordinance Tuesday</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i1.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.I2.i1.I1.i5.1.1.1\">–</span></span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i1.I1.i5.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i1.I1.i5.p1.1.1\" style=\"font-size:90%;\">(2024-01-11) Some Lyft, Uber drivers to strike Thursday at MSP Airport</span></p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A2.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"A2.I2.i2.p1.1\"><span class=\"ltx_text\" id=\"A2.I2.i2.p1.1.1\" style=\"font-size:90%;\">Model response: </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.I2.i2.p1.1.2\" style=\"font-size:90%;\">(correct)</span><span class=\"ltx_text\" id=\"A2.I2.i2.p1.1.3\" style=\"font-size:90%;\"> “(a) They will plan to leave the market.”</span></p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A2.F20.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.F20.9.1\" style=\"font-size:90%;\">[Gold Article Setting]<span class=\"ltx_text ltx_font_medium\" id=\"A2.F20.9.1.1\"></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<ul class=\"ltx_itemize ltx_centering ltx_figure_panel\" id=\"A2.I3\">\n<li class=\"ltx_item\" id=\"A2.I3.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A2.I3.i1.p1\">\n<p class=\"ltx_p\" id=\"A2.I3.i1.p1.1\"><span class=\"ltx_text\" id=\"A2.I3.i1.p1.1.1\" style=\"font-size:90%;\">In the gold article setting, models are provided direct access to the gold article, from which the question is generated. The question then is treated as a reading comprehensive question rather than a forecasting question.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I3.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A2.I3.i2.p1\">\n<p class=\"ltx_p\" id=\"A2.I3.i2.p1.1\"><span class=\"ltx_text\" id=\"A2.I3.i2.p1.1.1\" style=\"font-size:90%;\">Model response: </span><span class=\"ltx_text ltx_font_bold\" id=\"A2.I3.i2.p1.1.2\" style=\"font-size:90%;\">(correct)</span><span class=\"ltx_text\" id=\"A2.I3.i2.p1.1.3\" style=\"font-size:90%;\"> “(a) They will plan to leave the market.”</span></p>\n</div>\n</li>\n</ul>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 20: </span>An example of evaluating Mixtral-8x7B across different settings.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Comparing LLM-Generated and Forecasting Market Datasets</h2>\n<div class=\"ltx_para\" id=\"A3.p1\">\n<p class=\"ltx_p\" id=\"A3.p1.1\">Online forecasting markets such as Metaculus and Polymarket allow users to submit questions and predict the outcomes of future events. A natural question arises: why do we focus on LLM-generated questions rather than sourcing from these markets, as the choice in prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Zou et al., <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2022</a>; Halawi et al., <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>; Karger et al., <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2025</a>)</cite>?</p>\n</div>\n<div class=\"ltx_para\" id=\"A3.p2\">\n<p class=\"ltx_p\" id=\"A3.p2.1\">To answer this, we analyze the dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al. (<a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>)</cite>, which compiled 50,343 raw questions from five forecasting platforms, of which 21,149 were resolved. Of these, 82.64% are TF questions, 13.36% are MC questions, and the rest are free-response or numerical. After their quality filtering, only 5,516 TF questions remained. We observe that, due to a high proportion of low-quality questions in the raw data, sparse coverage in earlier years, and inconsistent distribution over time, performance trends derived from this dataset are substantially more volatile and harder to interpret than those based on our LLM-generated dataset.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"A3.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Lower Quality in Raw Questions.</h4>\n<div class=\"ltx_para\" id=\"A3.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"A3.SS0.SSS0.Px1.p1.1\">Manual inspection confirms that a substantial portion of the raw dataset consists of low-quality questions, as also noted by the original authors. Examples include: <span class=\"ltx_text ltx_font_italic\" id=\"A3.SS0.SSS0.Px1.p1.1.1\">“Will I have a chess.com rating of <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1\"><semantics id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1a\"><mo id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1.1\" xref=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml\">&gt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1b\"><gt id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml\" xref=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1.1\"></gt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1c\">&gt;</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS0.SSS0.Px1.p1.1.1.m1.1d\">&gt;</annotation></semantics></math>1300 …?”</span> (personal), <span class=\"ltx_text ltx_font_italic\" id=\"A3.SS0.SSS0.Px1.p1.1.2\">“Will Jamaica beat Mexico?”</span> (missing a time frame), and <span class=\"ltx_text ltx_font_italic\" id=\"A3.SS0.SSS0.Px1.p1.1.3\">“Are there more disadvantages in AI than advantages?”</span> (ill-defined). From a random sample of 50 questions, only 28% were well-defined. Specifically, 26% lacked a clear time element, 20% were overly personal, and 26% were ill-defined. Importantly, only 5,516 out of 17,477 resolved TF questions were retained after their filtering—an acceptance rate of just 32%, which aligns with our own quality assessments.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A3.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limited Early-Year Coverage.</h4>\n<div class=\"ltx_para\" id=\"A3.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"A3.SS0.SSS0.Px2.p1.1\">Figure <a class=\"ltx_ref\" href=\"#A3.F21\" title=\"Figure 21 ‣ Limited Early-Year Coverage. ‣ Appendix C Comparing LLM-Generated and Forecasting Market Datasets ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> (left) shows that the coverage before October 2022 is sparse, averaging only  40 raw and  26 filtered questions per month. This scarcity limits the feasibility of longitudinal trend analysis, especially across earlier model pre-training cutoffs. In contrast, our method supports high scalability and retrospective generation, allowing for uniform coverage across the full time range.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F21\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"747\" id=\"A3.F21.g1\" src=\"./assets/x20.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 21: </span>Dataset size and closed-book evaluation accuracy over time on the forecasting market dataset.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"A3.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Harder-to-Discern Trends.</h4>\n<div class=\"ltx_para\" id=\"A3.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"A3.SS0.SSS0.Px3.p1.1\">To evaluate the impact of using forecasting market questions in our study, we run a closed-book evaluation on TF questions from both the raw dataset (16,089 questions) and the filtered subset (4,572 questions), starting from 2020-01 (the same start date as our dataset). Notably, the original data is imbalanced, with 61.03% “No” answers in the raw set and 64.28% in the filtered set. After balancing, we retain 12,438 questions from the raw data and 3,232 from the filtered set.\nAs shown in Figure <a class=\"ltx_ref\" href=\"#A3.F21\" title=\"Figure 21 ‣ Limited Early-Year Coverage. ‣ Appendix C Comparing LLM-Generated and Forecasting Market Datasets ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> (right), neither the raw nor filtered datasets reveal a clear performance trend—model accuracy fluctuates significantly over time. We believe this is due to several factors:</p>\n</div>\n<div class=\"ltx_para\" id=\"A3.SS0.SSS0.Px3.p2\">\n<ul class=\"ltx_itemize\" id=\"A3.I1\">\n<li class=\"ltx_item\" id=\"A3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A3.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"A3.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.I1.i1.p1.1.1\">Lower data quality</span>: Approximately 70% of raw questions exhibit quality issues. While the overall dataset sizes are comparable (13,744 in ours vs. 12,438 in the raw market dataset), the difference in quality introduces additional noise, making trends harder to detect.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A3.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"A3.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.I1.i2.p1.1.1\">Limited early coverage</span>: Even within the filtered dataset, sparse early coverage and inconsistent monthly volume increase variance and reduce the reliability of time-based trends.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A3.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A3.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"A3.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.I1.i3.p1.1.1\">Confounding factors</span>: We argue that human-submitted questions introduce more confounding factors than automatically generated ones. Figure <a class=\"ltx_ref\" href=\"#A3.F22\" title=\"Figure 22 ‣ Harder-to-Discern Trends. ‣ Appendix C Comparing LLM-Generated and Forecasting Market Datasets ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">22</span></a> shows the distribution of data sources and question categories varies significantly across time (e.g. more sports-related questions in later periods). Human-written questions also may differ widely in style and difficulty, making them harder to control for consistency. In contrast, as shown in Figure <a class=\"ltx_ref\" href=\"#A1.F8\" title=\"Figure 8 ‣ A.5 Distribution of Question Categories Over Time ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, our dataset maintains relatively stable distributions over time.</p>\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F22\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"572\" id=\"A3.F22.g1\" src=\"./assets/x21.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 22: </span>Distribution shift of the filtered forecasting market dataset.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"A3.SS0.SSS0.Px3.p3\">\n<p class=\"ltx_p\" id=\"A3.SS0.SSS0.Px3.p3.1\">Thus, while we do not claim that LLM-generated questions are of inherently higher quality, we argue that our dataset is better suited for analyzing performance trends over time, due to its scalability, stylistic uniformity, stable category distribution, and reduced susceptibility to human-authored confounders. Moreover, if one sources questions from forecasting markets, the dataset update frequency is dependent on whether people are still actively submitting high-quality forecasting questions to the platform. In contrast, our approach enables daily updates and more comprehensive event coverage, making it a valuable complement to human-curated forecasting benchmarks.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Prompts</h2>\n<div class=\"ltx_para\" id=\"A4.p1\">\n<p class=\"ltx_p\" id=\"A4.p1.1\">All the prompts we use are shown in this section. The QA generation prompts and evaluation prompts are adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>, and the prompt to categorize our generated questions is taken from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al. (<a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>)</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A4.F23\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F23.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F23.2\"><span class=\"ltx_text\" id=\"A4.F23.2.1\" style=\"font-size:90%;\">You are an expert in extracting summary and keypoint from articles.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Rules\n<br class=\"ltx_break\"/>1. Provide a comprehensive summary of the entire article in one paragraph, ensuring that all essential aspects are addressed. Your summary should include key statistics, notable dates, and any significant statements to fully convey the context and content of the news story.\n<br class=\"ltx_break\"/>2. Please provide one keypoint that summarizes the new event from the article with the following rules:\n<br class=\"ltx_break\"/>- Focus specifically on events that are newly occurring on the publication date of the article. If the article does not introduce a new event but instead discusses ongoing topics or is about non-news content like advertisements, state ‘No new event reported.’\n<br class=\"ltx_break\"/>- The point should be concise, accurate and complete, especially for numbers, names and dates.\n<br class=\"ltx_break\"/>- Basically NO “he, she, they, it, them, etc” are allowed. Please clearly write out the entity you are referencing in the point.\n<br class=\"ltx_break\"/>- You are not allowed to start with any of the phrases: the article discusses, the article shows, the article emphasizes, the article discusses, the speaker says, the speaker discusses, the author mentions, etc.\n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F23.3\"><span class=\"ltx_text\" id=\"A4.F23.3.1\" style=\"font-size:90%;\"># Examples\n<br class=\"ltx_break\"/>Here are several examples of extracting keypoints from articles. Note that the articles in different examples are irrelevant.\n<br class=\"ltx_break\"/>## Example 1:\n<br class=\"ltx_break\"/>Article: Professional golfer Lexi Thompson has announced her retirement from professional golf at the end of the 2024 season at the age of 29. Thompson, an 11-time LPGA Tour champion, made the announcement ahead of her 18th consecutive US Women’s Open appearance. She turned professional in 2010 and won her first major at the 2014 Kraft Nabisco Championship. Despite enduring injuries that led to a drop in her world ranking, Thompson continued to compete at a high level. In her retirement announcement, Thompson expressed gratitude for the opportunities golf provided her and highlighted her excitement for the next chapter of her life.\n<br class=\"ltx_break\"/>Publishing Date: 2024-05-29, Wednesday\n<br class=\"ltx_break\"/>Keypoint: Lexi Thompson announced her retirement from professional golf at the end of the 2024 season at the age of 29.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## Example 2:\n<br class=\"ltx_break\"/>Article: India’s capital territory of Delhi sweltered to its highest-ever temperature of 49.9 degrees Celsius (121.8 degrees Fahrenheit) on Tuesday, as an oppressive heat wave forced authorities to impose water rationing.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>The Indian Meteorological Department (IMD) said the new record was measured in the suburb of Mungeshpur, surpassing Delhi’s previous high of 49.2 degrees Celsius (120.5 degrees Fahrenheit), observed in May 2022.\n<br class=\"ltx_break\"/>Publishing Date: 2024-05-29, Wednesday\n<br class=\"ltx_break\"/>Keypoint: Delhi experienced its highest-ever temperature of 49.9 degrees Celsius on 2024-05-28, Tuesday.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># New Article\n<br class=\"ltx_break\"/>Based on the provided rules and examples, please summarize the article and identify one key point that concludes the new event on the publishing date.\n<br class=\"ltx_break\"/>Article: {}\n<br class=\"ltx_break\"/>Publishing Date: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Output: Output should follow the format of\n<br class=\"ltx_break\"/>Summary:\n<br class=\"ltx_break\"/>Keypoint:\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 23: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F23.5.1\">Article Summary</span> step, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F24\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F24.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F24.2\"><span class=\"ltx_text\" id=\"A4.F24.2.1\" style=\"font-size:90%;\">[Setup:] Today is {}. You will be testing people who are from the past, i.e. a week or a month ago. A recently published article will serve as the basis for your questioning. Your objective is to ask them questions that assess the accuracy and plausibility of their predictions about events.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>You will write question-answer pairs:\n<br class=\"ltx_break\"/>1. The question should challenge the person to predict an outcome or development related to the article’s content as if they were being asked one week or one month ago. Please provide the question that can be answered on {}, but only guessable not answerable before {}.\n<br class=\"ltx_break\"/>2. The answer MUST be based on factual information from the article. Ensure that the answers do not predict outcomes that have not been explicitly stated in the article. \n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>[Rules:] \n<br class=\"ltx_break\"/>Article: {}. \n<br class=\"ltx_break\"/>Publishing date: {} \n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F24.3\"><span class=\"ltx_text\" id=\"A4.F24.3.1\" style=\"font-size:90%;\">Please generate four questions about the above article, along with answers. You should follow the instructions below: \n<br class=\"ltx_break\"/>1. Please turn the key point “{}” into the question, with focusing more on whether the event will happen.\n<br class=\"ltx_break\"/>2. The question should NOT be designed for reading comprehension. Please focus more on what happened rather than the implications after the event.\n<br class=\"ltx_break\"/>3. The question MUST be in future tense. \n<br class=\"ltx_break\"/>- Start the first question with “Will”, with the answer as “Yes”. \n<br class=\"ltx_break\"/>- Start the second question with “Will”, with the answer as “No”. \n<br class=\"ltx_break\"/>- Start the third and fourth questions with a phrase like “What will”, “Who will”, “Where will”, “Which xxx will”, “How much will”, or “How many will”. \n<br class=\"ltx_break\"/>4. There must be a time element in the question. It can be phrases like “In {} …”, “By {}, …”, “… in {}?”.\n<br class=\"ltx_break\"/>5. You MUST NOT use unclear implicit time element phrases like “in the future” or “in the upcoming weeks”.\n<br class=\"ltx_break\"/>6. You should avoid: questions that require numerical reasoning; questions that require substantial world knowledge.\n<br class=\"ltx_break\"/>7. The answer MUST be short and concise, avoiding using redundant words or repeating the information in the question.\n<br class=\"ltx_break\"/>8. The question must be grammatically correct and contain the information required to answer. NO “he, she, they, it, them, etc” allowed. Please clearly write out the entity you are referencing in the question.\n<br class=\"ltx_break\"/>9. The question MUST be able to be answered by the article.\n<br class=\"ltx_break\"/>10. The question MUST NOT include the information that came out just now. It should be understandable to people from the past. Avoid using “How will” or “Why will” questions, as they imply that the event has already occurred.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>[Suggested questions and questions to avoid are detailed below:]\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>- Keypoint: Delhi experienced its highest-ever temperature of 49.9 degrees Celsius on Tuesday, leading to water rationing due to the oppressive heat wave.\n<br class=\"ltx_break\"/>- Suggested Question: Will Delhi break the highest temperature record again by May 2024?\n<br class=\"ltx_break\"/>- Avoid This Question: Will extreme heat events continue to pose a threat to India’s development in the upcoming years?\n<br class=\"ltx_break\"/>- Reason to Avoid: The time constraint “in the upcoming years” is vague and the question can not be answered based on today’s knowledge.\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 24: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F24.5.1\">QA Generation</span> step (part 1), adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F25\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F25.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F25.2\"><span class=\"ltx_text\" id=\"A4.F25.2.1\" style=\"font-size:90%;\">- Keypoint: Owners of nearly 84,000 older Nissan vehicles in the United States equipped with recalled, unrepaired Takata air bags, including models such as the 2002-2006 Nissan Sentra, are advised by NHTSA to immediately stop driving them due to safety concerns.\n<br class=\"ltx_break\"/>- Suggested Question: Will the older Nissan vehicles such as the 2002-2006 Nissan Sentra exhibit quality issues by May 2024?\n<br class=\"ltx_break\"/>- Avoid This Question: Will owners of the 2002-2006 Nissan Sentra, 2002-2004 Nissan Pathfinder, and 2002-2003 Infiniti QX4 heed the NHTSA’s advice to immediately stop driving their vehicles in late May 2024?\n<br class=\"ltx_break\"/>- Reason to Avoid: This question is overly specific. People from the past would not have known the “NHTSA’s advice”.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>- Keypoint: Children’s sketches of violent scenes, likely made by children aged 5-7 before the eruption of Mt. Vesuvius in 79 AD, have been uncovered at the archaeological park of Pompeii.\n<br class=\"ltx_break\"/>- Suggested Question: Will children’s sketches of violent scenes be discovered at the archaeological park of Pompeii by May 2024?\n<br class=\"ltx_break\"/>- Avoid This Question: Will the newly discovered children’s sketches at the archaeological park of Pompeii be available for public viewing by May 2024?\n<br class=\"ltx_break\"/>- Reason to Avoid: This question includes future events about newly discovered children’s sketches in Pompeii, which wouldn’t be known to a past audience.\n<br class=\"ltx_break\"/></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F25.3\"><span class=\"ltx_text\" id=\"A4.F25.3.1\" style=\"font-size:90%;\">- Keypoint: North Korea has been sending “filth and garbage” across the border to South Korea using giant balloons as a new strategy, prompting South Korean authorities to warn of the objects landing in residential areas. The move, according to North Korean state media KCNA, was to retaliate against South Korean activists who often send materials to the North.\n<br class=\"ltx_break\"/>- Suggested Question: What will North Korea do to retaliate against South Korean activists who often send materials to the North by May 2024?\n<br class=\"ltx_break\"/>- Avoid This Question: Will North Korea continue using balloons to send items across the border to South Korea by May 2024?\n<br class=\"ltx_break\"/>- Reason to Avoid: The word “continue” should not be used here. The question MUST NOT include the information that came out just now.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>[Output:] Now please write four clear and concise question-answer pairs following the instructions and examples above. Once again the question should NOT be designed for reading comprehension but of forecasting interests. Also, vague and implicit time elements like “in the future”, “in the upcoming weeks” or “in the coming years” should NOT be used. The question should be able to answer on {}, but only guessable not answerable before {}. You should output the question along with its answer, in the format of \n<br class=\"ltx_break\"/>’”\n<br class=\"ltx_break\"/>Question 1: “Will xxx?”\n<br class=\"ltx_break\"/>Answer 1: Yes.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question 2: “Will xxx?”\n<br class=\"ltx_break\"/>Answer 2: No.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question 3: Either “What will xxx?”, “Who will xxx?”, “‘Where will xxx?”, “Which xxx will”, “How much will xxx?”, or “How many will xxx?”\n<br class=\"ltx_break\"/>Answer 3: xxx.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question 4: Either “What will xxx?”, “Who will xxx?”, “Where will xxx?”, “Which xxx will”, “How much will xxx?”, or “How many will xxx?”\n<br class=\"ltx_break\"/>Answer 4: xxx.\n<br class=\"ltx_break\"/>’”\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 25: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F25.5.1\">QA Generation</span> step (part 2), adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F26\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F26.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F26.2\"><span class=\"ltx_text\" id=\"A4.F26.2.1\" style=\"font-size:90%;\"># Rules\n<br class=\"ltx_break\"/>Article: {}\n<br class=\"ltx_break\"/>Given the article, please generate three noising answers to the given questions, whose correct answers can be obtained from the article. Name the three noising answers as (b), (c) and (d) respectively. While (b), (c) and (d) should all be unambiguously incorrect, they should also make sense and be plausible.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Examples\n<br class=\"ltx_break\"/>Here are examples showing the output format. This example is NOT related to the noising answers you will generate.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: What will be the annual change in the UK’s Consumer Prices Index (CPI) for November 2021?\n<br class=\"ltx_break\"/>Correct Answer: ‘Less than 1.7%’\n<br class=\"ltx_break\"/>Noising Answers: \n<br class=\"ltx_break\"/>(b) ‘Between 1.7% and 2.2%, inclusive’\n<br class=\"ltx_break\"/>(c) ‘More than 2.2% but less than 2.9%’\n<br class=\"ltx_break\"/>(d) ‘2.9% or more’\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: Who will win the 2020 Georgia Democratic primary?\n<br class=\"ltx_break\"/>Correct Answer: ‘Joe Biden’\n<br class=\"ltx_break\"/>Noising Answers:\n<br class=\"ltx_break\"/>(b) ‘Michael Bloomberg’\n<br class=\"ltx_break\"/>(c) ‘Pete Buttigieg’\n<br class=\"ltx_break\"/>(d) ‘Someone else’\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: Before July 2020, will it be officially announced that the Tokyo 2020 Summer Olympics and/or Paralympics will be postponed, canceled, and/or relocated?\n<br class=\"ltx_break\"/>Correct Answer: Yes, the Olympic Games only\n<br class=\"ltx_break\"/>Noising Answers: \n<br class=\"ltx_break\"/>(b) ‘Yes, the Paralympic Games only’\n<br class=\"ltx_break\"/>(c) ‘Yes, both’\n<br class=\"ltx_break\"/>(d) ‘No’\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Input:\n<br class=\"ltx_break\"/>Question 1: {}\n<br class=\"ltx_break\"/>Correct Answer 1: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question 2: {}\n<br class=\"ltx_break\"/>Correct Answer 2: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Output: Now please generate three noising answers to the question, given the above article, instructions and examples. DO NOT output the backgrounds, the question or any other explanations.\n<br class=\"ltx_break\"/>Noising Answers 1:\n<br class=\"ltx_break\"/>(b) xxx.\n<br class=\"ltx_break\"/>(c) xxx.\n<br class=\"ltx_break\"/>(d) xxx.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Noising Answers 2:\n<br class=\"ltx_break\"/>(b) xxx.\n<br class=\"ltx_break\"/>(c) xxx.\n<br class=\"ltx_break\"/>(d) xxx.\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 26: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F26.4.1\">Misleading Choices Generation</span> step, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F27\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F27.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F27.2\"><span class=\"ltx_text\" id=\"A4.F27.2.1\" style=\"font-size:90%;\"># Task\n<br class=\"ltx_break\"/>Please help evaluate the quality of question-answer pairs derived from the given news article. The questions will be presented to someone who has not seen the corresponding news article, in order to evaluate the accuracy and plausibility of the event prediction ability.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Inputs\n<br class=\"ltx_break\"/>Article: {}\n<br class=\"ltx_break\"/>Publishing Date: {}\n<br class=\"ltx_break\"/>Question 1: {}\n<br class=\"ltx_break\"/>Answer 1: {}\n<br class=\"ltx_break\"/>Question 2: {}\n<br class=\"ltx_break\"/>Answer 2: {}\n<br class=\"ltx_break\"/>Question 3: {}\n<br class=\"ltx_break\"/>Answer 3: {}\n<br class=\"ltx_break\"/>Question 4: {}\n<br class=\"ltx_break\"/>Answer 4: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Scoring Categories\n<br class=\"ltx_break\"/>## Correctness: Given the above article, please check if the answer is correct to the question with 100% certainty.\n<br class=\"ltx_break\"/>- 2 points: There is evidence in the article that the answer is correct with 100% certainty.\n<br class=\"ltx_break\"/>- 1 point: The answer generally aligns with the news facts but has minor inaccuracies or missing details.\n<br class=\"ltx_break\"/>- 0 point: Significantly misaligned with the news facts.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## Only Answerable on Publishing Date: Imagine traveling back in time to one week before the article’s publishing date ({}). At that time, you are asked the question without having seen this specific article, but you do have access to all earlier news articles. The question should ideally be only guessable—not definitively answerable—based on the information available at that time. That is, the answer should be able to be found in the given article, but it should not be obtainable from earlier articles. Note that past tense descriptions in the article DO NOT INFLUENCE this assessment.\n<br class=\"ltx_break\"/>- 2 points: The question is answerable on {}, but only guessable not answerable before {}.\n<br class=\"ltx_break\"/>- 1 point: Could be somewhat predicted before {}, but not with complete certainty.\n<br class=\"ltx_break\"/>- 0 point: A person (could be anyone, even an expert in the field) would be able to find an article (or many) published before {} that answers the question with 100% certainty.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>### 0 point examples\n<br class=\"ltx_break\"/>Example 1:\n<br class=\"ltx_break\"/>Question: What will be one of Lexi Thompson’s career highlights in professional golf?\n<br class=\"ltx_break\"/>Answer: Winning 11 LPGA Tour titles.\n<br class=\"ltx_break\"/>Reasoning: This question is answerable with prior knowledge and does not test predictive ability related to the publishing date.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## No New Information: Ensure the question does not include new information that only became known on the publishing date, making it understandable for a past audience.\n<br class=\"ltx_break\"/>- 2 points: No new information from the publishing date are included.\n<br class=\"ltx_break\"/>- 1 point: Minor new information from the publishing date might be inferred but are not explicitly stated.\n<br class=\"ltx_break\"/>- 0 point: Includes clear new information from the publishing date, unsuitable for past understanding.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>### 0 point examples\n<br class=\"ltx_break\"/>Example 1:\n<br class=\"ltx_break\"/>Question: Will owners of the 2002-2006 Nissan Sentra, 2002-2004 Nissan Pathfinder, and 2002-2003 Infiniti QX4 heed the NHTSA’s advice to immediately stop driving their vehicles in late May 2024?\n<br class=\"ltx_break\"/>Reasoning: This question contains new information on the publishing date. People from the past would not have known the “NHTSA’s advice”.\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 27: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F27.4.1\">QA Filtering</span> step (part 1).</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F28\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F28.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F28.2\"><span class=\"ltx_text\" id=\"A4.F28.2.1\" style=\"font-size:90%;\">Example 2:\n<br class=\"ltx_break\"/>Question: “What will Lexi Thompson’s ranking be at the time of her retirement announcement in May 2024?”\n<br class=\"ltx_break\"/>Reasoning: This question contains the information that Lexi will annouce her retirement, which is not known to the people from the past.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Example 3:\n<br class=\"ltx_break\"/>Question: “Will the newly discovered children’s sketches at the archaeological park of Pompeii be available for public viewing by May 2024?”\n<br class=\"ltx_break\"/>Reasoning: This question includes future events about newly discovered children’s sketches in Pompeii, which wouldn’t be known to a past audience\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## Objectiveness: The answer should not rely more on the author’s personal views than on objective facts.\n<br class=\"ltx_break\"/>- 2 points: Completely objective, based strictly on reported facts.\n<br class=\"ltx_break\"/>- 1 point: Primarily objective, with minor subjective interpretations.\n<br class=\"ltx_break\"/>- 0 point: Largely subjective or opinion-based, lacking a factual basis.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## Clear Time Element: This category checks if the question has a clear element in it, without having vague phrases like “in the future” or “in the upcoming weeks”.\n<br class=\"ltx_break\"/>- 2 points: The question has clear time elements, like “by May 2024” or “in July 2023”.\n<br class=\"ltx_break\"/>- 1 point: The question includes a general timeframe, like “next month” or “this winter,” which allows for some estimation but lacks precise dates.\n<br class=\"ltx_break\"/>- 0 point: The question includes vague time phrases like “in the future” or “in the upcoming weeks,” which do not specify a clear or precise timeframe.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>### 0 point examples\n<br class=\"ltx_break\"/>Example 1:\n<br class=\"ltx_break\"/>Question: Will extreme heat events continue to pose a threat to India’s development in the upcoming years?\n<br class=\"ltx_break\"/>Reasoning: The time constraint “in the upcoming years” is vague.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Example 2: \n<br class=\"ltx_break\"/>Question: “What will Illinois require from parents who monetize their children’s online activities starting in July?”\n<br class=\"ltx_break\"/>Reasoning: The mention of “July” specifies only the month and lacks the necessary detail of the year.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## Public Interest: Determine if the question addresses a topic of public concern.\n<br class=\"ltx_break\"/>- 2 points: The question covers a topic that widely affects or interests the public.\n<br class=\"ltx_break\"/>- 1 point: The question is of moderate interest, relevant to specific groups.\n<br class=\"ltx_break\"/>- 0 point: The topic is overly personal or localized, lacking relevance to the broader public.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>### 0 point examples\n<br class=\"ltx_break\"/>Example 1:\n<br class=\"ltx_break\"/>Question: Will the exhibition ‘Fragile Beauty’ at London’s Victoria &amp; Albert Museum include both midcentury and contemporary works in May 2024?\n<br class=\"ltx_break\"/>Reasoning: The specific details of an personal art exhibition’s contents are generally of limited public interest.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>## Answer Not Too Obvious: This category evaluates whether the answer to a question is too predictable or straightforward based on the question itself.\n<br class=\"ltx_break\"/>- 2 points: The answer provides new or non-obvious insights, requiring additional context or understanding not explicit in the question.\n<br class=\"ltx_break\"/>- 1 point: The answer is somewhat predictable but includes minor additional information or a slight twist.\n<br class=\"ltx_break\"/>- 0 point: The answer directly restates or closely mirrors the question, offering no new details or insights.\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 28: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F28.4.1\">QA Filtering</span> step (part 2).</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F29\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F29.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F29.2\"><span class=\"ltx_text\" id=\"A4.F29.2.1\" style=\"font-size:90%;\">### 0 point examples\n<br class=\"ltx_break\"/>Example 1:\n<br class=\"ltx_break\"/>Question: What will New York officials do to ensure safety for the ICC Men’s T20 Cricket World Cup following global threats from ISIS-K?\n<br class=\"ltx_break\"/>Answer: New York officials will implement increased safety precautions for the event.\n<br class=\"ltx_break\"/>Reasoning: The answer is straightforward and expected, as it directly restates the premise of the question without providing any new or specific details on how the safety precautions will be implemented or what they might entail.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Instructions\n<br class=\"ltx_break\"/>Evaluate each question-answer pair by assigning points in each of the categories based on the criteria provided. Please be strict on giving points. If the requirements of a category are not fulfilled, assign a point of 0.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/># Please strictly follow this output template:\n<br class=\"ltx_break\"/>*Question 1*\n<br class=\"ltx_break\"/>## Correctness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Only Answerable on Publishing Date\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## No New Information\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Objectiveness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Clear Time Element\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Public Interest\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Answer Not Too Obvious\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*Question 2*\n<br class=\"ltx_break\"/>## Correctness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n## Only Answerable on Publishing Date\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## No New Information\n<br class=\"ltx_break\"/>- Reasoning:\n- Point:\n<br class=\"ltx_break\"/>## Objectiveness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Clear Time Element\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 29: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F29.4.1\">QA Filtering</span> step (part 3).</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F30\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F30.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F30.2\"><span class=\"ltx_text\" id=\"A4.F30.2.1\" style=\"font-size:90%;\">## Public Interest\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Answer Not Too Obvious\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*Question 3*\n<br class=\"ltx_break\"/>## Correctness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Only Answerable on Publishing Date\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## No New Information\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Objectiveness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Clear Time Element\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Public Interest\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Answer Not Too Obvious\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>*Question 4*\n<br class=\"ltx_break\"/>## Correctness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Only Answerable on Publishing Date\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## No New Information\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Objectiveness\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Clear Time Element\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Public Interest\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n<br class=\"ltx_break\"/>## Answer Not Too Obvious\n<br class=\"ltx_break\"/>- Reasoning:\n<br class=\"ltx_break\"/>- Point:\n</span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 30: </span>Prompt in the <span class=\"ltx_text ltx_font_italic\" id=\"A4.F30.4.1\">QA Filtering</span> step (part 4).</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F31\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F31.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F31.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F31.2.1\" style=\"font-size:90%;\">System Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F31.2.1.1\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"/></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F31.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F31.3.1\" style=\"font-size:90%;\">User Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F31.3.1.1\"> You should output your answer as either ‘Yes’ or ‘No’ WITHOUT anything else.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: {}\n<br class=\"ltx_break\"/>Choices: ‘Yes’ or ‘No’\n<br class=\"ltx_break\"/>[Output:] Your answer:\n</span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 31: </span>Closed-book evaluation prompt for TF questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F32\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F32.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F32.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F32.2.1\" style=\"font-size:90%;\">System Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F32.2.1.1\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"/></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F32.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F32.3.1\" style=\"font-size:90%;\">User Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F32.3.1.1\"> You should output your answer as either ‘(a)’, ‘(b)’, ‘(c)’ or ‘(d)’ WITHOUT anything else.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: {}\n<br class=\"ltx_break\"/>Choices: \n<br class=\"ltx_break\"/>(a) {}\n<br class=\"ltx_break\"/>(b) {}\n<br class=\"ltx_break\"/>(c) {}\n<br class=\"ltx_break\"/>(d) {}\n<br class=\"ltx_break\"/>[Output:] Your answer:\n</span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 32: </span>Closed-book evaluation prompt for MC questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F33\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F33.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F33.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F33.2.1\" style=\"font-size:90%;\">System Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F33.2.1.1\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"/></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F33.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F33.3.1\" style=\"font-size:90%;\">User Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F33.3.1.1\"> You should output your answer as either ‘Yes’ or ‘No’ WITHOUT anything else. Below are the top 5 relevant news article fragments retrieved for the question, which may or may not assist you in making a forecast.\n<br class=\"ltx_break\"/>Article 1: {} \n<br class=\"ltx_break\"/>Article 2: {} \n<br class=\"ltx_break\"/>Article 3: {}\n<br class=\"ltx_break\"/>Article 4: {}\n<br class=\"ltx_break\"/>Article 5: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: {}\n<br class=\"ltx_break\"/>Choices: ‘Yes’ or ‘No’\n<br class=\"ltx_break\"/>[Output:] Your answer:\n</span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 33: </span>Constrained open-book evaluation prompt for TF questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F34\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F34.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F34.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F34.2.1\" style=\"font-size:90%;\">System Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F34.2.1.1\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"/></span></span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F34.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F34.3.1\" style=\"font-size:90%;\">User Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F34.3.1.1\"> You should output your answer as either ‘(a)’, ‘(b)’, ‘(c)’ or ‘(d)’ WITHOUT anything else. Below are the top 5 relevant news article fragments retrieved for the question, which may or may not assist you in making a forecast.\n<br class=\"ltx_break\"/>Article 1: {}\n<br class=\"ltx_break\"/>Article 2: {}\n<br class=\"ltx_break\"/>Article 3: {}\n<br class=\"ltx_break\"/>Article 4: {}\n<br class=\"ltx_break\"/>Article 5: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: {}\n<br class=\"ltx_break\"/>Choices: \n<br class=\"ltx_break\"/>(a) {}\n<br class=\"ltx_break\"/>(b) {}\n<br class=\"ltx_break\"/>(c) {}\n<br class=\"ltx_break\"/>(d) {}\n<br class=\"ltx_break\"/>[Output:] Your answer:\n</span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 34: </span>Constrained open-book evaluation prompt for MC questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F35\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F35.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F35.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F35.2.1\" style=\"font-size:90%;\">System Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F35.2.1.1\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/></span>User Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F35.2.1.2\"> You should output your answer as either ‘Yes’ or ‘No’ WITHOUT anything else. Below is the updated news article relevant to the question, which may help you in providing an answer.\n<br class=\"ltx_break\"/>Article: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: {}\n<br class=\"ltx_break\"/>Choices: ‘Yes’ or ‘No’\n<br class=\"ltx_break\"/>[Output:] Your answer:\n</span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 35: </span>Gold article evaluation prompt for TF questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F36\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F36.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F36.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.F36.2.1\" style=\"font-size:90%;\">System Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F36.2.1.1\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/></span>User Prompt:<span class=\"ltx_text ltx_font_medium\" id=\"A4.F36.2.1.2\"> You should output your answer as either ‘(a)’, ‘(b)’, ‘(c)’ or ‘(d)’ WITHOUT anything else. Below is the updated news article relevant to the question, which may help you in providing an answer.\n<br class=\"ltx_break\"/>Article: {}\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Question: {}\n<br class=\"ltx_break\"/>Choices: \n<br class=\"ltx_break\"/>(a) {}\n<br class=\"ltx_break\"/>(b) {}\n<br class=\"ltx_break\"/>(c) {}\n<br class=\"ltx_break\"/>(d) {}\n<br class=\"ltx_break\"/>[Output:] Your answer:\n</span></span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 36: </span>Gold article evaluation prompt for MC questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. (<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A4.F37\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_ERROR ltx_centering ltx_figure_panel undefined\" id=\"A4.F37.1\">{mdframed}</span></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\" id=\"A4.F37.2\"><span class=\"ltx_text\" id=\"A4.F37.2.1\" style=\"font-size:90%;\">Question: {}\n<br class=\"ltx_break\"/>Options:\n<br class=\"ltx_break\"/>- Science &amp; Tech\n<br class=\"ltx_break\"/>- Healthcare &amp; Biology\n<br class=\"ltx_break\"/>- Economics &amp; Business\n<br class=\"ltx_break\"/>- Environment &amp; Energy\n<br class=\"ltx_break\"/>- Politics &amp; Governance\n<br class=\"ltx_break\"/>- Education &amp; Research\n<br class=\"ltx_break\"/>- Arts &amp; Recreation\n<br class=\"ltx_break\"/>- Security &amp; Defense\n<br class=\"ltx_break\"/>- Social Sciences\n<br class=\"ltx_break\"/>- Sports\n<br class=\"ltx_break\"/>- Other\n<br class=\"ltx_break\"/>Instruction: Assign a category for the given question.\n<br class=\"ltx_break\"/>Rules:\n<br class=\"ltx_break\"/>1. Make sure you only return one of the options from the option list.\n<br class=\"ltx_break\"/>2. Only output the category, and do not output any other words in your response.\n<br class=\"ltx_break\"/>3. You have to pick a string from the above categories.\n<br class=\"ltx_break\"/>Answer:\n</span></p>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 37: </span>Prompt to categorize the generated questions, taken from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al. (<a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>)</cite>.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2411.08324",
  "source": "arxiv-experimental",
  "generated": "2025-10-18T01:15:21.218Z"
}
