{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p id=\"S1.p1.1\" class=\"ltx_p\">Traditional Large Language Model (LLM) benchmarks are often static, and do not reflect real-world information that evolves over time. This presents two significant challenges. First, as LLMs are updated, there is a risk that the static benchmarks become outdated and more vulnerable to data leakage, where their content might end up in the training data of newer models. This undermines the reliability of performance assessments on these benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et al.,, <a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">2023</a>; Xu et al.,, <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2024</a>; McIntosh et al.,, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2024</a>; Li and Flanigan,, <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>. Second, the static benchmarks often lack the temporal information to track the model’s performance variations over time <cite class=\"ltx_cite ltx_citemacro_citep\">(McIntosh et al.,, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>. This creates a need for evaluation methods that stay relevant over time and incorporate temporal dynamics.</p>\n</div>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p id=\"S1.p2.1\" class=\"ltx_p\">Daily news provides a natural setting for continuous evaluation of LLMs. Since the world is constantly changing, a benchmark designed around forecasting the next day’s news will never be out of date by construction. In addition to enabling continuous evaluation, forecasting is itself a longstanding challenge with significant implications across various domains, including healthcare, finance, and policymaking <cite class=\"ltx_cite ltx_citemacro_citep\">(Tetlock and Gardner,, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2016</a>; Dempsey et al.,, <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2017</a>; Gillingham et al.,, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2018</a>; Lopez-Lira and Tang,, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. While human experts have traditionally made such forecasts, machine learning models, particularly LLMs, have emerged as promising alternatives due to their capability to learn from vast and diverse corpora <cite class=\"ltx_cite ltx_citemacro_citep\">(Halawi et al.,, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2024</a>; Ye et al.,, <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2024</a>; Yan et al.,, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. Several recent forecasting question-answer (QA) datasets have been developed <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al.,, <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2021</a>; Zou et al.,, <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2022</a>; Zhang et al.,, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, however, they are limited in either size, scope, or do not continuously keep pace with the rapidly changing world. More critically, the extent to which LLMs’ predictive abilities change over time remains underexplored.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p id=\"S1.p3.1\" class=\"ltx_p\">In this work, we propose Daily Oracle—a continuous evaluation benchmark that uses automatically generated QA pairs from daily news to assess how the future prediction capabilities of LLMs evolve over time. The QA pairs are generated on a daily basis, consisting of True/False (TF) and Multiple Choice (MC) questions across various categories such as business, politics, and arts. Unlike traditional reading comprehension tasks, these QA pairs are designed to challenge LLMs to predict future events based on their own existing knowledge, effectively evaluating their temporal generalization and forecasting abilities.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p id=\"S1.p4.1\" class=\"ltx_p\">We continuously evaluate various LLMs, both with and without access to a limited archive of news articles. Our experiments reveal that LLMs experience an average performance decline of 20.14% on True/False (TF) questions and 23.26% on Multiple Choice (MC) questions between January 2020 and September 2024, with degradation becoming more pronounced before and after the models’ knowledge cutoff dates. Although models utilizing Retrieval Augmented Generation (RAG) <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et al.,, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> can demonstrate improved prediction performance, the downward trend persists, suggesting an ongoing challenge in maintaining temporal generalization. Overall, our benchmark highlights the challenges posed by outdated pertaining data in LLMs, and underscores the necessity for continuous model updating to keep up with the constantly evolving stream of real-world information.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para\">\n<p id=\"S1.p5.1\" class=\"ltx_p\">To summarize, our key contributions are two-fold:</p>\n<ul id=\"S1.I1\" class=\"ltx_itemize\">\n<li id=\"S1.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S1.I1.i1.p1\" class=\"ltx_para\">\n<p id=\"S1.I1.i1.p1.1\" class=\"ltx_p\"><span id=\"S1.I1.i1.p1.1.1\" class=\"ltx_text ltx_font_bold\">Continuous Forecasting Evaluation Benchmark</span>: We present Daily Oracle, the largest and most up-to-date forecasting dataset, composed of automatically generated QA pairs. This benchmark continuously evaluates LLMs’ temporal generalization and future prediction abilities using daily news, ensuring relevance over time and offering a challenging evaluation framework.</p>\n</div>\n</li>\n<li id=\"S1.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S1.I1.i2.p1\" class=\"ltx_para\">\n<p id=\"S1.I1.i2.p1.1\" class=\"ltx_p\"><span id=\"S1.I1.i2.p1.1.1\" class=\"ltx_text ltx_font_bold\">Empirical Findings on Performance Degradation</span>: Our experiments demonstrate a consistent decline in LLM prediction accuracy over time, even when supplemented with recent “open-book” information. The persistent degradation highlights the challenges posed by outdated pretraining data in LLMs and underscores the necessity for continuous model updating.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n\n<section id=\"S2.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Temporal Generalization of LLMs.</h4>\n\n<div id=\"S2.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px1.p1.1\" class=\"ltx_p\"><cite class=\"ltx_cite ltx_citemacro_citet\">Lazaridou et al., (<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> define temporal generalization as the ability of Language Models to generalize well to future data from beyond their training period. They demonstrate that Transformer-XL’s performance deteriorates over time, evidenced by increasing perplexity when evaluated on post-training data. However, perplexity-based metrics have two main limitations: they cannot be applied to closed-source models lacking accessible logits, and increased perplexity does not necessarily indicate degraded performance on downstream tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Röttger and Pierrehumbert,, <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2021</a>; Agarwal and Nenkova,, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et al., (<a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> investigate temporal generation using the Bits Per Character (BPC) metric. Similar to perplexity, BPC fails to capture higher-level performance on downstream tasks. In contrast, our work focuses on evaluating how well models acquire and utilize real-world event knowledge in downstream forecasting tasks, providing a more nuanced assessment of temporal generalization.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Dynamic QA Datasets.</h4>\n\n<div id=\"S2.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px2.p1.1\" class=\"ltx_p\">While static QA datasets evaluate models on fixed knowledge snapshots, dynamic QA datasets incorporate a temporal dimension, allowing assessment of how models adapt to evolving information. Several dynamic QA datasets are proposed. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et al., (<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> construct TimeQA by using time-sensitive facts in WikiData with aligned Wikipedia passages to synthesize QA pairs. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang and Choi, (<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> introduce SituatedQA by manually annotating temporally and geographically dependent questions. StreamingQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Liska et al.,, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> and RealtimeQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Kasai et al.,, <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> are both dynamic benchmarks with QA pairs answerable from news articles. StreamingQA, however, does not provide continuous evaluation with always-relevant data. RealTimeQA does not address forecasting and is more like a plugin for a search engine, in the sense that it tests whether a model has updated its knowledge as facts change, rather than testing whether it can predict what will change given its knowledge of the past. FreshQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Vu et al.,, <a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> contains a fixed set of human-written open-ended questions whose answers by nature can change based on new developments in the world, but is smaller and does not address forecasting. It is also updated weekly rather than daily. While all these datasets have some form of time-sensitivity like the Daily Oracle, they either do not provide continuous evaluation or do not evaluate forecasting capabilities, or neither.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Forecasting Datasets.</h4>\n\n<div id=\"S2.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px3.p1.1\" class=\"ltx_p\">Forecasting questions aim to assess a model’s ability to predict the outcomes of future events based on its existing knowledge. Several datasets in the event forecasting field have been introduced. ForecastQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al.,, <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> used crowdworkers to collect 10,392 QA pairs from news articles. <cite class=\"ltx_cite ltx_citemacro_citet\">Zou et al., (<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> argue that the QA pairs from ForecastQA are often nonsensical or ambiguous since they are written by humans without forecasting expertise. They further introduce AutoCast, a forecasting dataset from popular human forecasting tournaments containing 6,707 QA pairs. In contrast, our Daily Oracle dataset is generated automatically from daily news articles, which means that it is never out of date, can easily grow its size automatically without additional inputs from human forecasters, and provides more comprehensive event coverage than human forecasting tournaments.</p>\n</div>\n<div id=\"S2.SS0.SSS0.Px3.p2\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px3.p2.1\" class=\"ltx_p\">Similar to our generation method, TLB-forecast <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al.,, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> has an automatic forecasting QA generation framework using news articles. However, their dataset is constrained both temporally and topically, and only contains cooperation and conflict events in Middle-Eastern countries from 2015 to 2022. This restricts the dataset from evaluating more general event-prediction abilities. Furthermore, considering most of the powerful LLMs have been developed after 2020, the portions of the dataset covering earlier years may contain answers already seen during training. This prior exposure compromises the dataset’s effectiveness as a forecasting benchmark. In contrast, our dataset spans a broader timeframe and covers more topics, offering a more comprehensive and reliable forecasting benchmark.</p>\n</div>\n<div id=\"S2.SS0.SSS0.Px3.p3\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px3.p3.1\" class=\"ltx_p\">Note that none of the aforementioned datasets provide insights into how prediction ability changes over time. <cite class=\"ltx_cite ltx_citemacro_citet\">Zhu et al., (<a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> introduce FreshBench, a forecasting dataset scraped from the GoodJudgmentOpen platform, and also study temporal generalization. However, they report accuracy in a relatively short time window (from January 2023 to April 2024) with only 2,532 questions. While we observe a gradual performance decline in our dataset, they report significant fluctuations in model accuracy shortly after release. We argue that our dataset presents a more challenging task, as evidenced by the continuous degradation in model performance over time, making it a robust benchmark for assessing LLMs’ temporal generalization.</p>\n</div>\n<div id=\"S2.SS0.SSS0.Px3.p4\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px3.p4.1\" class=\"ltx_p\">In order to clearly showcase the differences between our dataset and prior work, we highlight a few key features in Table <a href=\"#S2.T1\" title=\"Table 1 ‣ Forecasting Datasets. ‣ 2 Related Work ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The Daily Oracle is the only benchmark which is continuously updated on a daily basis and evaluates forecasting ability. Additionally, at the fixed size we use for analysis we provide significantly more evaluation examples than the other automatically updated benchmarks.</p>\n</div>\n<figure id=\"S2.T1\" class=\"ltx_table\">\n<table id=\"S2.T1.12\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.12.13.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.12.13.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S2.T1.12.13.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th id=\"S2.T1.12.13.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S2.T1.12.13.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Continuous?</span></th>\n<th id=\"S2.T1.12.13.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S2.T1.12.13.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Interval</span></th>\n<th id=\"S2.T1.12.13.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S2.T1.12.13.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Forecast?</span></th>\n<th id=\"S2.T1.12.13.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S2.T1.12.13.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Size</span></th>\n<th id=\"S2.T1.12.13.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S2.T1.12.13.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Latest Update</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S2.T1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">TimeQA</span></td>\n<td id=\"S2.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.1.1.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.2.2.2.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">20,000</span></td>\n<td id=\"S2.T1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2021</span></td>\n</tr>\n<tr id=\"S2.T1.4.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">SituatedQA</span></td>\n<td id=\"S2.T1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.3.3.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.4.4.2.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">4,757</span></td>\n<td id=\"S2.T1.4.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.4.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2021</span></td>\n</tr>\n<tr id=\"S2.T1.6.6\" class=\"ltx_tr\">\n<td id=\"S2.T1.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.6.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">StreamingQA</span></td>\n<td id=\"S2.T1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.5.5.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.6.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.6.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.6.6.2.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.6.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">36,800</span></td>\n<td id=\"S2.T1.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.6.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2021</span></td>\n</tr>\n<tr id=\"S2.T1.8.8\" class=\"ltx_tr\">\n<td id=\"S2.T1.8.8.3\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.8.8.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">RealTimeQA</span></td>\n<td id=\"S2.T1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.7.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.7.7.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.8.8.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.8.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.8.8.2.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.8.8.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">1,470</span></td>\n<td id=\"S2.T1.8.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.8.8.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2023</span></td>\n</tr>\n<tr id=\"S2.T1.9.9\" class=\"ltx_tr\">\n<td id=\"S2.T1.9.9.2\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.9.9.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FreshQA</span></td>\n<td id=\"S2.T1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.9.9.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.9.9.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Weekly</span></td>\n<td id=\"S2.T1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.9.9.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.9.9.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.9.9.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">600</span></td>\n<td id=\"S2.T1.9.9.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.9.9.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2024</span></td>\n</tr>\n<tr id=\"S2.T1.10.10\" class=\"ltx_tr\">\n<td id=\"S2.T1.10.10.2\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.10.10.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">ForecastQA</span></td>\n<td id=\"S2.T1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.10.10.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.10.10.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.10.10.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.10.10.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.10.10.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10,382</span></td>\n<td id=\"S2.T1.10.10.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.10.10.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2019</span></td>\n</tr>\n<tr id=\"S2.T1.11.11\" class=\"ltx_tr\">\n<td id=\"S2.T1.11.11.2\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.11.11.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">AutoCast</span></td>\n<td id=\"S2.T1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.11.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.11.11.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.11.11.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.11.11.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.11.11.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">6,707</span></td>\n<td id=\"S2.T1.11.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.11.11.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2022</span></td>\n</tr>\n<tr id=\"S2.T1.12.12\" class=\"ltx_tr\">\n<td id=\"S2.T1.12.12.2\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.12.12.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">TLB-forecast</span></td>\n<td id=\"S2.T1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S2.T1.12.12.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span><svg id=\"S2.T1.12.12.1.pic1\" class=\"ltx_picture\" height=\"10.02\" overflow=\"visible\" version=\"1.1\" width=\"10.02\"><g transform=\"translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)\" fill=\"#000000\" stroke=\"#000000\"><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05\" style=\"fill:none\"></path></g><g stroke-width=\"0.7pt\" stroke-linecap=\"round\"><path d=\"M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45\" style=\"fill:none\"></path></g></g></svg>\n</td>\n<td id=\"S2.T1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.12.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S2.T1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.12.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.12.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">6,604</span></td>\n<td id=\"S2.T1.12.12.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.12.12.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2022</span></td>\n</tr>\n<tr id=\"S2.T1.12.14.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.12.14.1.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S2.T1.12.14.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FreshBench</span></td>\n<td id=\"S2.T1.12.14.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.14.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.12.14.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.14.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S2.T1.12.14.1.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.14.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.12.14.1.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S2.T1.12.14.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">2,532</span></td>\n<td id=\"S2.T1.12.14.1.6\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.12.14.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2024</span></td>\n</tr>\n<tr id=\"S2.T1.12.15.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.12.15.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S2.T1.12.15.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Daily Oracle (Ours)</span></td>\n<td id=\"S2.T1.12.15.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S2.T1.12.15.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.12.15.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S2.T1.12.15.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Daily</span></td>\n<td id=\"S2.T1.12.15.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S2.T1.12.15.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S2.T1.12.15.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S2.T1.12.15.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">29,988</span></td>\n<td id=\"S2.T1.12.15.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S2.T1.12.15.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2024</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>We compare Daily Oracle with existing benchmarks in the literature. For continuously updated datasets (e.g. Daily Oracle, FreshBench, and FreshQA), “Interval” refers to the dataset update interval, and “Size” and “Latest Update” refer to the fixed data currently available. Our Daily Oracle benchmark is the only forecasting benchmark which is continuously updated every day using questions generated from daily news.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>The Daily Oracle Dataset</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p id=\"S3.p1.1\" class=\"ltx_p\">In this section, we present Daily Oracle, a continuously updated QA benchmark of forecasting questions that are automatically generated from daily news. For our current analysis of LLM performance, we utilize a subset of the data consisting of 16,082 TF questions and 13,906 MC questions, covering a diverse range of forecasting topics, which are generated using daily news articles from January 2020 up until September 2024. However, our QA generation framework is continuous and updates daily. In section <a href=\"#S3.SS1\" title=\"3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we describe our LLM-based dataset construction pipeline, detailing the data sources and the four-step construction process. Section <a href=\"#S3.SS2\" title=\"3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> provides an analysis and general overview of the dataset.</p>\n</div>\n<section id=\"S3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Dataset Construction</h3>\n\n<figure id=\"S3.F1\" class=\"ltx_figure\"><img src=\"./assets/x1.png\" id=\"S3.F1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"368\" height=\"225\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span id=\"S3.F1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Data Construction Process of Daily Oracle.</span></figcaption>\n</figure>\n<section id=\"S3.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data Source.</h4>\n\n<div id=\"S3.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Zou et al., (<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, we collect a large corpus of news articles from the daily-updated Common Crawl News Dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Nagel,, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2016</a>)</cite> with the <code id=\"S3.SS1.SSS0.Px1.p1.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">news-please</code> package <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamborg et al.,, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>. We filter for mainstream sources—CBS News, CNBC, CNN, Forbes, and NPR. While our data collection and evaluation are performed daily, for this study we utilize a static news corpus with 1,216,925 English articles spanning January 2019 to September 2024. This corpus is also used for the constrained open-book evaluation setting in section <a href=\"#S4.SS1\" title=\"4.1 Experimental Setup ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>.</p>\n</div>\n</section>\n<section id=\"S3.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM-based Construction Process.</h4>\n\n<div id=\"S3.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">QA pairs are generated from articles published between January 2020 and September 2024.<span id=\"footnote1\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>For news articles in 2019, we use them as the corpus for the constrained open-book setting.</span></span></span> Due to budget constraints, for each day, we select six articles for QA generation: three are chosen randomly, and three are selected from hot topics. Details for how hot topics are chosen can be found in Appendix <a href=\"#A1.SS1\" title=\"A.1 Details for Article Selection ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>. For each selected article, we then use LLM to generate two TF QA pairs and two MC QA pairs with the few-shot prompting technique.<span id=\"footnote2\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>See Appendix <a href=\"#A3\" title=\"Appendix C Prompts ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for all the prompts we use.</span></span></span></p>\n</div>\n<div id=\"S3.SS1.SSS0.Px2.p2\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px2.p2.1\" class=\"ltx_p\">Inspired by <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, our QA construction follows four steps, as illustrated in Figure <a href=\"#S3.F1\" title=\"Figure 1 ‣ 3.1 Dataset Construction ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n<ol id=\"S3.I1\" class=\"ltx_enumerate\">\n<li id=\"S3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(1)</span> \n<div id=\"S3.I1.i1.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i1.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i1.p1.1.1\" class=\"ltx_text ltx_font_italic\">Article Summary.</span> We generate a summary for each article, focusing on new events from the publishing date, instead of opinion articles discussing events from the past. This approach allows us to use the publication date as the resolution date of the generated question. Questions can then be regarded as valid forecasting questions since they are prior to the resolution date.</p>\n</div>\n</li>\n<li id=\"S3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(2)</span> \n<div id=\"S3.I1.i2.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i2.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i2.p1.1.1\" class=\"ltx_text ltx_font_italic\">QA Generation.</span> After filtering out the articles that do not introduce new events, two TF questions and two MC questions are generated together with the answers per article. To ensure balance in the TF questions, we instruct the LLM to generate the first question with a “Yes” answer and the second with a “No.”</p>\n</div>\n</li>\n<li id=\"S3.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(3)</span> \n<div id=\"S3.I1.i3.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i3.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i3.p1.1.1\" class=\"ltx_text ltx_font_italic\">Misleading Choices Generation.</span> For MC, we provide the article, its publishing date, and the QA pair to the LLM, which then generates three misleading choices.</p>\n</div>\n</li>\n<li id=\"S3.I1.i4\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(4)</span> \n<div id=\"S3.I1.i4.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i4.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i4.p1.1.1\" class=\"ltx_text ltx_font_italic\">QA Filtering.</span> In the final step, we prompt the LLM to check seven principles: correctness of answers, non-answerability before the publication date, absence of information leakage, objectivity, inclusion of a clear temporal element, public interest, and non-obviousness of the answer. Each principle is scored with 0, 1, or 2 points, and we selected the questions that received at least 13 points in total.</p>\n</div>\n</li>\n</ol>\n<p id=\"S3.SS1.SSS0.Px2.p2.2\" class=\"ltx_p\">These principles are detailed in Appendix <a href=\"#A1.SS2\" title=\"A.2 QA Filtering Principles ‣ Appendix A Dataset Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We use GPT-3.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI,, <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> for steps (1) and (4), while GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI,, <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> is utilized for steps (2) and (3) to ensure high data quality, as GPT-4 is currently the best-performing LLM available.</p>\n</div>\n</section>\n</section>\n<section id=\"S3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Dataset Analysis</h3>\n\n<section id=\"S3.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Summary Statistics.</h4>\n\n<div id=\"S3.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.SSS0.Px1.p1.1\" class=\"ltx_p\">At the time of writing this paper, the subset dataset we use from Daily Oracle consists of 16,082 TF and 13,906 MC QA pairs, covering the period from January 1st, 2020, to September 30th, 2024, with an average of 17.3 questions per day. Figure <a href=\"#S3.F2.sf1\" title=\"Figure 2(a) ‣ Figure 2 ‣ Summary Statistics. ‣ 3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> shows that our dataset covers various MC question types, mainly starting with “What will” (26.9%), “Who will” (21.0%), and “Which … will” (18.4%). Figure <a href=\"#S3.F2.sf2\" title=\"Figure 2(b) ‣ Figure 2 ‣ Summary Statistics. ‣ 3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> provides a breakdown of the categories, highlighting our dataset’s broad coverage. The categorization of each question is determined using GPT-3.5, based on the prompt from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al., (<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>. Examples of QA pairs are shown in Table <a href=\"#S3.T2\" title=\"Table 2 ‣ Summary Statistics. ‣ 3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n</div>\n<figure id=\"S3.F2\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F2.sf1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x2.png\" id=\"S3.F2.sf1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"297\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F2.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S3.F2.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">MC question types for Daily Oracle.</span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F2.sf2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x3.png\" id=\"S3.F2.sf2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"307\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F2.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S3.F2.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Category Breakdown for Daily Oracle.</span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span id=\"S3.F2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Comparison of question types and categories.</span></figcaption>\n</figure>\n<figure id=\"S3.T2\" class=\"ltx_table\">\n<table id=\"S3.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S3.T2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Type</span></th>\n<th id=\"S3.T2.2.1.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt\"></th>\n<th id=\"S3.T2.2.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S3.T2.2.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.1.1.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Category</span></span>\n</span>\n</th>\n<th id=\"S3.T2.2.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S3.T2.2.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.1.1.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Question and Answer</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.2.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S3.T2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">TF</span></th>\n<th id=\"S3.T2.2.2.1.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"></th>\n<td id=\"S3.T2.2.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.2.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.2.1.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.2.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Politics &amp; Governance</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.2.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.2.2.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.2.1.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.2.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Will the prosecution’s key witness in the New York hush money trial in April 2024 be someone other than Michael Cohen? –</span><span id=\"S3.T2.2.2.1.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">No.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">TF</span></th>\n<th id=\"S3.T2.2.3.2.2\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<td id=\"S3.T2.2.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.3.2.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.3.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Politics &amp; Governance</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.3.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.3.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.3.2.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.3.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Will the House Energy and Commerce Committee vote unanimously to advance a bill that could potentially ban TikTok if ByteDance does not sell the app by March 2024? –</span><span id=\"S3.T2.2.3.2.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Yes.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC</span></th>\n<th id=\"S3.T2.2.4.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">What</span></th>\n<td id=\"S3.T2.2.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.4.3.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.4.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Science &amp; Tech</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.4.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.4.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.4.3.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.4.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">What will be the starting price range for the Google Pixel 8a as of May 2024? A.$599–$649 B. $199–$249 C. $750–$800, D. $499–$559. –</span><span id=\"S3.T2.2.4.3.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">D.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC</span></th>\n<th id=\"S3.T2.2.5.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Who</span></th>\n<td id=\"S3.T2.2.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.5.4.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.5.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sports</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.5.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.5.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.5.4.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.5.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Who will go on the injured list before the New York Mets’ game on May 29, 2024? A. Pete Alonso B. Edwin Diaz C. Jeff McNeil D. Francisco Lindor –</span><span id=\"S3.T2.2.5.4.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">B.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC</span></th>\n<th id=\"S3.T2.2.6.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Which</span></th>\n<td id=\"S3.T2.2.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.6.5.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.6.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Arts &amp; Recreation</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.6.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.6.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.6.5.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.6.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">By May 2024, on which streaming service will “The First Omen” become available for subscribers? A. Disney+, B. Hulu, C. Amazon Prime Video, D. Netflix –</span><span id=\"S3.T2.2.6.5.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">B.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.7.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC</span></th>\n<th id=\"S3.T2.2.7.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.7.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">How many</span></th>\n<td id=\"S3.T2.2.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.7.6.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.7.6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Science &amp; Tech</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.7.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.7.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.7.6.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.7.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">How many U.S. states will the path of totality cross during the total solar eclipse on April 8, as reported by February 2024? A. 15 B. 10 C. 20 D. 6 –</span><span id=\"S3.T2.2.7.6.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.8.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC</span></th>\n<th id=\"S3.T2.2.8.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S3.T2.2.8.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Where</span></th>\n<td id=\"S3.T2.2.8.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.8.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.8.7.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.8.7.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Healthcare &amp; Biology</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.8.7.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S3.T2.2.8.7.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.8.7.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.8.7.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Where will the second known U.S. case of bird flu in a human be reported by March 2024? A. California, B. Texas, C. New York, D. Florida –</span><span id=\"S3.T2.2.8.7.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">B.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S3.T2.2.9.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC</span></th>\n<th id=\"S3.T2.2.9.8.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S3.T2.2.9.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">How much</span></th>\n<td id=\"S3.T2.2.9.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S3.T2.2.9.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.9.8.3.1.1\" class=\"ltx_p\" style=\"width:99.6pt;\"><span id=\"S3.T2.2.9.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Economics &amp; Business</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.9.8.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S3.T2.2.9.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.9.8.4.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\"><span id=\"S3.T2.2.9.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">How much will Apple, Inc. (AAPL) be up year-to-date by the end of June 2024? A. Up 149.5% B. Just over 19% C. 9.7%. D. 27%. –</span><span id=\"S3.T2.2.9.8.4.1.1.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">C.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S3.T2.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span id=\"S3.T2.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Daily Oracle Example Questions and Answers.</span></figcaption>\n</figure>\n</section>\n<section id=\"S3.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Past and Future Information Usage.</h4>\n\n<div id=\"S3.SS2.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.SSS0.Px2.p1.1\" class=\"ltx_p\">Each question in Daily Oracle implicitly requires the model to retrieve relevant knowledge. How do these requirements change day by day over the course of our benchmark? <cite class=\"ltx_cite ltx_citemacro_cite\">Anderson and Schooler, (<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">1991</a>)</cite> study a similar relationship in human information environments, specifically in <span id=\"S3.SS2.SSS0.Px2.p1.1.1\" class=\"ltx_text ltx_font_italic\">New York Times</span> headlines, children’s verbal interactions, and emails. In Figure <a href=\"#S3.F3\" title=\"Figure 3 ‣ Past and Future Information Usage. ‣ 3.2 Dataset Analysis ‣ 3 The Daily Oracle Dataset ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we take inspiration from their work and analyze whether a word’s frequency of occurrence in the past 100 days predicts its occurrence on the next day. In other words, if over the past 100 days we have frequently required the model to retrieve specific knowledge, e.g. if there are many questions about the unemployment rate, is it likely it will have to retrieve this knowledge in the future?</p>\n</div>\n<div id=\"S3.SS2.SSS0.Px2.p2\" class=\"ltx_para\">\n<p id=\"S3.SS2.SSS0.Px2.p2.1\" class=\"ltx_p\">We analyze this relationship for words in the titles of the articles we use to generate questions as well as in the text of the TF and MC questions themselves. Past frequency is computed by checking, for each day in the 100 day window, if a word has occurred in any article title (so, the maximum frequency is 100). We find that there is a linear relationship between the frequency of usage in the past 100 days and the probability of occurrence on the 101st day in all cases, replicating Anderson &amp; Schooler’s findings for <span id=\"S3.SS2.SSS0.Px2.p2.1.1\" class=\"ltx_text ltx_font_italic\">New York Times</span> headlines. Interestingly, there is a drop in probability for both TF and MC questions for words with a frequency of 40, though it is unclear why. There is also some clustering at lower frequencies, particularly in the TF and MC question plots. Many words appear less than 20 times during the 100 day window. The temporal structure exhibited in the daily news stream may be of a future point of interest from a modeling perspective.</p>\n</div>\n<figure id=\"S3.F3\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_3\">\n<figure id=\"S3.F3.1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/figs/anderson_repo_title.png\" id=\"S3.F3.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"598\" height=\"598\" alt=\"Refer to caption\">\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\">\n<figure id=\"S3.F3.2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/figs/anderson_repo_tf_questions.png\" id=\"S3.F3.2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"598\" height=\"598\" alt=\"Refer to caption\">\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\">\n<figure id=\"S3.F3.3\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/figs/anderson_repo_mc_questions.png\" id=\"S3.F3.3.g1\" class=\"ltx_graphics ltx_img_square\" width=\"598\" height=\"598\" alt=\"Refer to caption\">\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F3.14.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span id=\"S3.F3.9.3\" class=\"ltx_text\" style=\"font-size:90%;\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Anderson and Schooler, (<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">1991</a>)</cite>, we plot the probability of a word occurring in an <span id=\"S3.F3.9.3.1\" class=\"ltx_text ltx_font_bold\">(left)</span> article title, <span id=\"S3.F3.9.3.2\" class=\"ltx_text ltx_font_bold\">(middle)</span> True/False question, or <span id=\"S3.F3.9.3.3\" class=\"ltx_text ltx_font_bold\">(right)</span> Multiple Choice question given how frequently it had appeared in one over the past 100 days, computed over our entire dataset. We fit a linear regression in each case and show a linear relationship in each case (<math id=\"S3.F3.7.1.m1.1\" class=\"ltx_Math\" alttext=\"R^{2}=0.978\" display=\"inline\"><semantics id=\"S3.F3.7.1.m1.1b\"><mrow id=\"S3.F3.7.1.m1.1.1\" xref=\"S3.F3.7.1.m1.1.1.cmml\"><msup id=\"S3.F3.7.1.m1.1.1.2\" xref=\"S3.F3.7.1.m1.1.1.2.cmml\"><mi id=\"S3.F3.7.1.m1.1.1.2.2\" xref=\"S3.F3.7.1.m1.1.1.2.2.cmml\">R</mi><mn id=\"S3.F3.7.1.m1.1.1.2.3\" xref=\"S3.F3.7.1.m1.1.1.2.3.cmml\">2</mn></msup><mo id=\"S3.F3.7.1.m1.1.1.1\" xref=\"S3.F3.7.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.F3.7.1.m1.1.1.3\" xref=\"S3.F3.7.1.m1.1.1.3.cmml\">0.978</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.F3.7.1.m1.1c\"><apply id=\"S3.F3.7.1.m1.1.1.cmml\" xref=\"S3.F3.7.1.m1.1.1\"><eq id=\"S3.F3.7.1.m1.1.1.1.cmml\" xref=\"S3.F3.7.1.m1.1.1.1\"></eq><apply id=\"S3.F3.7.1.m1.1.1.2.cmml\" xref=\"S3.F3.7.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.F3.7.1.m1.1.1.2.1.cmml\" xref=\"S3.F3.7.1.m1.1.1.2\">superscript</csymbol><ci id=\"S3.F3.7.1.m1.1.1.2.2.cmml\" xref=\"S3.F3.7.1.m1.1.1.2.2\">𝑅</ci><cn type=\"integer\" id=\"S3.F3.7.1.m1.1.1.2.3.cmml\" xref=\"S3.F3.7.1.m1.1.1.2.3\">2</cn></apply><cn type=\"float\" id=\"S3.F3.7.1.m1.1.1.3.cmml\" xref=\"S3.F3.7.1.m1.1.1.3\">0.978</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F3.7.1.m1.1d\">R^{2}=0.978</annotation></semantics></math>, <math id=\"S3.F3.8.2.m2.1\" class=\"ltx_Math\" alttext=\"0.986\" display=\"inline\"><semantics id=\"S3.F3.8.2.m2.1b\"><mn id=\"S3.F3.8.2.m2.1.1\" xref=\"S3.F3.8.2.m2.1.1.cmml\">0.986</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F3.8.2.m2.1c\"><cn type=\"float\" id=\"S3.F3.8.2.m2.1.1.cmml\" xref=\"S3.F3.8.2.m2.1.1\">0.986</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F3.8.2.m2.1d\">0.986</annotation></semantics></math>, and <math id=\"S3.F3.9.3.m3.1\" class=\"ltx_Math\" alttext=\"0.985\" display=\"inline\"><semantics id=\"S3.F3.9.3.m3.1b\"><mn id=\"S3.F3.9.3.m3.1.1\" xref=\"S3.F3.9.3.m3.1.1.cmml\">0.985</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F3.9.3.m3.1c\"><cn type=\"float\" id=\"S3.F3.9.3.m3.1.1.cmml\" xref=\"S3.F3.9.3.m3.1.1\">0.985</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F3.9.3.m3.1d\">0.985</annotation></semantics></math> for left, middle, and right respectively).</span></figcaption>\n</figure>\n</section>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Experimental Setup</h3>\n\n<section id=\"S4.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Closed-Book Setting.</h4>\n\n<div id=\"S4.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">We evaluate various LLMs on Daily Oracle to assess their understanding of real-world events and temporal generalization abilities, i.e., how accurately LLMs can answer forecasting questions based on the knowledge they learned from their training data. Our evaluation differentiates between two scenarios based on the question’s resolution date and model’s knowledge cutoff date: (1) <span id=\"S4.SS1.SSS0.Px1.p1.1.1\" class=\"ltx_text ltx_font_italic\">Pre-Knowledge Cutoff Questions:</span> These questions have resolution dates before the model’s knowledge cutoff, testing the model’s understanding of past events. (2) <span id=\"S4.SS1.SSS0.Px1.p1.1.2\" class=\"ltx_text ltx_font_italic\">Post-Knowledge Cutoff Questions:</span> These have resolution dates after the knowledge cutoff, requiring models to predict future events and test their forecasting and temporal generalization abilities.</p>\n</div>\n</section>\n<section id=\"S4.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Constrained Open-Book Setting.</h4>\n\n<div id=\"S4.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px2.p1.3\" class=\"ltx_p\">In addition to a closed-book evaluation, we explore the constrained open-book setting: how access to news articles up to different time cutoffs influences LLM performance using RAG <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et al.,, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. We introduce the concept of the RAG cutoff (<span id=\"S4.SS1.SSS0.Px2.p1.3.1\" class=\"ltx_text ltx_font_italic\">R-Cutoff</span>), which limits the latest accessible date for retrieving articles. To prevent the models from leveraging information beyond the resolution date, for any question with a resolution date (<math id=\"S4.SS1.SSS0.Px2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"d_{\\text{res}}\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px2.p1.1.m1.1a\"><msub id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3a.cmml\">res</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1b\"><apply id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3\"><mtext mathsize=\"70%\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.1.m1.1.1.3\">res</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px2.p1.1.m1.1c\">d_{\\text{res}}</annotation></semantics></math>), the accessible articles span from January 1st, 2019 (the start of our news corpus) up to whichever comes first between the day before the resolution date and the RAG cutoff date (<math id=\"S4.SS1.SSS0.Px2.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"d_{\\text{R-Cutoff}}\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px2.p1.2.m2.1a\"><msub id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3a.cmml\">R-Cutoff</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1b\"><apply id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3\"><mtext mathsize=\"70%\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.2.m2.1.1.3\">R-Cutoff</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px2.p1.2.m2.1c\">d_{\\text{R-Cutoff}}</annotation></semantics></math>). Formally, the accessible date range is <math id=\"S4.SS1.SSS0.Px2.p1.3.m3.3\" class=\"ltx_Math\" alttext=\"[01/01/2019,\\min(d_{\\text{res}}-1,d_{\\text{R-Cutoff}}))\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px2.p1.3.m3.3a\"><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\">[</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.cmml\"><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2.cmml\">01</mn><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml\">/</mo><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3.cmml\">01</mn><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1a\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml\">/</mo><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4.cmml\">2019</mn></mrow><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.4\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\">,</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml\">min</mi><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2a\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">⁡</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">(</mo><mrow id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.cmml\"><msub id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3a.cmml\">res</mtext></msub><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1.cmml\">−</mo><mn id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3.cmml\">1</mn></mrow><mo id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.4\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">,</mo><msub id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.cmml\"><mi id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2.cmml\">d</mi><mtext id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3a.cmml\">R-Cutoff</mtext></msub><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.5\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\">)</mo></mrow></mrow><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.5\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3b\"><interval closure=\"closed-open\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2\"><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1\"><divide id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1\"></divide><cn type=\"integer\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2\">01</cn><cn type=\"integer\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3\">01</cn><cn type=\"integer\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.4\">2019</cn></apply><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2\"><min id=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.1.1\"></min><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1\"><minus id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.1\"></minus><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3\"><mtext mathsize=\"70%\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.2.3\">res</mtext></ci></apply><cn type=\"integer\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.1.1.3\">1</cn></apply><apply id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.1.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2\">subscript</csymbol><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.2\">𝑑</ci><ci id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3a.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3\"><mtext mathsize=\"70%\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3.cmml\" xref=\"S4.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.2.2.3\">R-Cutoff</mtext></ci></apply></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px2.p1.3.m3.3c\">[01/01/2019,\\min(d_{\\text{res}}-1,d_{\\text{R-Cutoff}}))</annotation></semantics></math>. Following prior work <cite class=\"ltx_cite ltx_citemacro_citep\">(Jin et al.,, <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2021</a>; Zou et al.,, <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2022</a>; Zhang et al.,, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, we employ BM25 <cite class=\"ltx_cite ltx_citemacro_citep\">(Robertson et al.,, <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">1995</a>)</cite> as the retriever and select the top 5 articles relevant to each question. We truncate each retrieved article to a maximum length of 512 words. These articles are then incorporated into the input prompts to serve as additional information.</p>\n</div>\n</section>\n<section id=\"S4.SS1.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Gold Article Setting.</h4>\n\n<div id=\"S4.SS1.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px3.p1.1\" class=\"ltx_p\">We further include a setting where models are provided direct access to the gold article, from which the question is generated. This transforms the forecasting questions into reading comprehension ones, which can also access LLMs’ general question-answering capabilities. Achieving high accuracy here ensures that the questions from our Daily Oracle dataset are answerable.</p>\n</div>\n</section>\n<section id=\"S4.SS1.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Metrics.</h4>\n\n<div id=\"S4.SS1.SSS0.Px4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px4.p1.1\" class=\"ltx_p\">Accuracy score is used as the evaluation metric. Though LLMs are tested daily, to show clearer trends, we plot the monthly performance in Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, and apply a 5-month moving average to smooth the curve. We also report yearly averages and average year-over-year (YoY) accuracy change before and after models’ knowledge cutoff dates in Table <a href=\"#S4.T3\" title=\"Table 3 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Additionally, despite prompting the models to avoid responses like “I cannot predict the future” and instead provide definitive answers, there are cases where such refusals still occur. The rejection rates are provided in the Appendix <a href=\"#A2.SS2\" title=\"B.2 Rejection Rates in the Closed-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>, and these cases are counted as incorrect to ensure comparability across model results.</p>\n</div>\n</section>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Main Results</h3>\n\n<section id=\"S4.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results for the Closed-Book Setting.</h4>\n\n<div id=\"S4.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px1.p1.1\" class=\"ltx_p\">Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Table <a href=\"#S4.T3\" title=\"Table 3 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> present our primary results for the closed-book setting. The “Avg” column in Table <a href=\"#S4.T3\" title=\"Table 3 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the average YoY accuracy change of all months, revealing a clear degradation in performance over time across all models on both TF and MC questions. When comparing accuracies from the beginning to the end of the evaluation period, we observe that, on average, the models’ performance declines by 20.14% on TF questions (from 64.68% to 51.65%) and by 23.26% on MC questions (from 58.30% to 44.74%). This indicates that while LLMs demonstrate certain abilities to understand real-world events and make predictions, they struggle to maintain these abilities.</p>\n</div>\n<figure id=\"S4.F4\" class=\"ltx_figure\"><img src=\"./assets/x4.png\" id=\"S4.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"415\" height=\"168\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span id=\"S4.F4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for the closed-book setting. We plot the 5-month moving average accuracy for True/False (TF) and Multiple Choice (MC) questions across different LLMs.</span></figcaption>\n</figure>\n<div id=\"S4.SS2.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px1.p2.1\" class=\"ltx_p\">Notably, the average YoY accuracy declines provide further insight. Before the knowledge cutoff, the average YoY decline across all models was relatively moderate. However, post-knowledge cutoff, we observe steeper declines in many models, with GPT-4 showing the most drastic drop in MC performance, declining by 18.47%, compared to just 4.23% before the cutoff. This contrast highlights that while LLMs manage to retain a baseline of past knowledge with small degradation, their ability to forecast future events deteriorates much more rapidly as they move beyond their training data, struggling with temporal generalization.</p>\n</div>\n<div id=\"S4.SS2.SSS0.Px1.p3\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px1.p3.1\" class=\"ltx_p\">Among different models, Claude-3.5-Sonnet <cite class=\"ltx_cite ltx_citemacro_citep\">(Anthropic,, <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> significantly outperforms all others, while GPT-4 excels in MC questions but its performance in TF is not as remarkable as in MC. GPT-3.5, Qwen-2-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al.,, <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> and Llama-3-8B <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al.,, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> show smaller temporal declines than GPT-4 in both TF and MC questions. Interestingly, Mistral-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et al.,, <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> and Mixtral-8x7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et al.,, <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> show the most pronounced drops in TF accuracy, with scores falling below the random baseline 50% due to increased answer refusals, as shown in the Appendix <a href=\"#A2.SS2\" title=\"B.2 Rejection Rates in the Closed-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>. Gemma-2-2B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al.,, <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> exhibits the most consistent performance with the smallest average YoY decline, likely due to its more recent knowledge cutoff date.</p>\n</div>\n<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>For various LLMs with different knowledge cutoffs (K-Cutoffs), we show the yearly average accuracy (calculated as the average across months) from 2020 to 2024, along with the average year-over-year (YoY) accuracy change (%) before the knowledge cutoff date (Pre-Cutoff), after the knowledge cutoff date (Post-Cutoff), and the overall average YoY accuracy change across all months (Avg) on the Daily Oracle dataset.</figcaption>\n<div id=\"S4.T3.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:297.7pt;vertical-align:-0.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-19.9pt,13.6pt) scale(0.916113062064096,0.916113062064096) ;\">\n<table id=\"S4.T3.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"></th>\n<th id=\"S4.T3.4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLM</span></th>\n<th id=\"S4.T3.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">K-Cutoff</span></th>\n<th id=\"S4.T3.4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"5\"><span id=\"S4.T3.4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Average Yearly Accuracy (%)</span></th>\n<th id=\"S4.T3.4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span id=\"S4.T3.4.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Average YoY Accuracy Change (%)</span></th>\n</tr>\n<tr id=\"S4.T3.4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2020</span></th>\n<th id=\"S4.T3.4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2021</span></th>\n<th id=\"S4.T3.4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2022</span></th>\n<th id=\"S4.T3.4.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2023</span></th>\n<th id=\"S4.T3.4.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.2.2.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2024</span></th>\n<th id=\"S4.T3.4.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Pre-Cutoff</span></th>\n<th id=\"S4.T3.4.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Post-Cutoff</span></th>\n<th id=\"S4.T3.4.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.4.1.2.2.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Avg</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"8\"><span id=\"S4.T3.4.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TF</span></td>\n<td id=\"S4.T3.4.1.3.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Claude-3.5-Sonnet</span></td>\n<td id=\"S4.T3.4.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.3.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Apr 2024</span></td>\n<td id=\"S4.T3.4.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.21</span></td>\n<td id=\"S4.T3.4.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.88</span></td>\n<td id=\"S4.T3.4.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.05</span></td>\n<td id=\"S4.T3.4.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.38</span></td>\n<td id=\"S4.T3.4.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.3.1.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">66.25</span></td>\n<td id=\"S4.T3.4.1.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.77</span></td>\n<td id=\"S4.T3.4.1.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-11.97</span></td>\n<td id=\"S4.T3.4.1.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.3.1.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.47</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">GPT-4</span></td>\n<td id=\"S4.T3.4.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.4.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Apr 2023</span></td>\n<td id=\"S4.T3.4.1.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.68</span></td>\n<td id=\"S4.T3.4.1.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">66.41</span></td>\n<td id=\"S4.T3.4.1.4.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.36</span></td>\n<td id=\"S4.T3.4.1.4.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.54</span></td>\n<td id=\"S4.T3.4.1.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.4.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.88</span></td>\n<td id=\"S4.T3.4.1.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-5.83</span></td>\n<td id=\"S4.T3.4.1.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.96</span></td>\n<td id=\"S4.T3.4.1.4.2.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.4.2.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.21</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.5.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">GPT-3.5</span></td>\n<td id=\"S4.T3.4.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.5.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sept 2021</span></td>\n<td id=\"S4.T3.4.1.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.86</span></td>\n<td id=\"S4.T3.4.1.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.12</span></td>\n<td id=\"S4.T3.4.1.5.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">59.36</span></td>\n<td id=\"S4.T3.4.1.5.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.11</span></td>\n<td id=\"S4.T3.4.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.5.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.80</span></td>\n<td id=\"S4.T3.4.1.5.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.33</span></td>\n<td id=\"S4.T3.4.1.5.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-3.43</span></td>\n<td id=\"S4.T3.4.1.5.3.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.5.3.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-2.11</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.6.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mixtral-8x7B</span></td>\n<td id=\"S4.T3.4.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.6.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S4.T3.4.1.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.83</span></td>\n<td id=\"S4.T3.4.1.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.69</span></td>\n<td id=\"S4.T3.4.1.6.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.09</span></td>\n<td id=\"S4.T3.4.1.6.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">39.34</span></td>\n<td id=\"S4.T3.4.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.6.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">36.29</span></td>\n<td id=\"S4.T3.4.1.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.6.4.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.6.4.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-10.93</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.7.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mistral-7B</span></td>\n<td id=\"S4.T3.4.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.7.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S4.T3.4.1.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.57</span></td>\n<td id=\"S4.T3.4.1.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">54.65</span></td>\n<td id=\"S4.T3.4.1.7.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">48.22</span></td>\n<td id=\"S4.T3.4.1.7.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">41.35</span></td>\n<td id=\"S4.T3.4.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.7.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">41.89</span></td>\n<td id=\"S4.T3.4.1.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.7.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.7.5.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-7.67</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.8.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Llama-3-8B</span></td>\n<td id=\"S4.T3.4.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.8.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mar 2023</span></td>\n<td id=\"S4.T3.4.1.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.06</span></td>\n<td id=\"S4.T3.4.1.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.24</span></td>\n<td id=\"S4.T3.4.1.8.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.35</span></td>\n<td id=\"S4.T3.4.1.8.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.68</span></td>\n<td id=\"S4.T3.4.1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.8.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.44</span></td>\n<td id=\"S4.T3.4.1.8.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.95</span></td>\n<td id=\"S4.T3.4.1.8.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-6.5</span></td>\n<td id=\"S4.T3.4.1.8.6.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.8.6.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-3.23</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.9.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Qwen-2-7B</span></td>\n<td id=\"S4.T3.4.1.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.9.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S4.T3.4.1.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.39</span></td>\n<td id=\"S4.T3.4.1.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.15</span></td>\n<td id=\"S4.T3.4.1.9.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.67</span></td>\n<td id=\"S4.T3.4.1.9.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">53.39</span></td>\n<td id=\"S4.T3.4.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.9.7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">53.14</span></td>\n<td id=\"S4.T3.4.1.9.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.9.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.9.7.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.9.7.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-3.86</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.10.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Gemma-2-2B</span></td>\n<td id=\"S4.T3.4.1.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.10.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Jul 2024</span></td>\n<td id=\"S4.T3.4.1.10.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.71</span></td>\n<td id=\"S4.T3.4.1.10.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">59.31</span></td>\n<td id=\"S4.T3.4.1.10.8.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.64</span></td>\n<td id=\"S4.T3.4.1.10.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.61</span></td>\n<td id=\"S4.T3.4.1.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.10.8.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">55.87</span></td>\n<td id=\"S4.T3.4.1.10.8.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.41</span></td>\n<td id=\"S4.T3.4.1.10.8.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-5.28</span></td>\n<td id=\"S4.T3.4.1.10.8.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.10.8.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-0.97</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.11.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"8\"><span id=\"S4.T3.4.1.11.9.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MC</span></td>\n<td id=\"S4.T3.4.1.11.9.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.11.9.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Claude-3.5-Sonnet</span></td>\n<td id=\"S4.T3.4.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.11.9.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Apr 2024</span></td>\n<td id=\"S4.T3.4.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.86</span></td>\n<td id=\"S4.T3.4.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">77.67</span></td>\n<td id=\"S4.T3.4.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.32</span></td>\n<td id=\"S4.T3.4.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.37</span></td>\n<td id=\"S4.T3.4.1.11.9.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.1.11.9.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.79</span></td>\n<td id=\"S4.T3.4.1.11.9.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-6.26</span></td>\n<td id=\"S4.T3.4.1.11.9.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-12.82</span></td>\n<td id=\"S4.T3.4.1.11.9.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.1.11.9.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.83</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.12.10\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.12.10.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.12.10.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">GPT-4</span></td>\n<td id=\"S4.T3.4.1.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.12.10.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Apr 2023</span></td>\n<td id=\"S4.T3.4.1.12.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.59</span></td>\n<td id=\"S4.T3.4.1.12.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.62</span></td>\n<td id=\"S4.T3.4.1.12.10.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">66.75</span></td>\n<td id=\"S4.T3.4.1.12.10.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.40</span></td>\n<td id=\"S4.T3.4.1.12.10.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.12.10.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.96</span></td>\n<td id=\"S4.T3.4.1.12.10.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.23</span></td>\n<td id=\"S4.T3.4.1.12.10.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-18.47</span></td>\n<td id=\"S4.T3.4.1.12.10.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.12.10.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-7.48</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.13.11\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.13.11.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.13.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">GPT-3.5</span></td>\n<td id=\"S4.T3.4.1.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.13.11.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sept 2021</span></td>\n<td id=\"S4.T3.4.1.13.11.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.27</span></td>\n<td id=\"S4.T3.4.1.13.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.36</span></td>\n<td id=\"S4.T3.4.1.13.11.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.43</span></td>\n<td id=\"S4.T3.4.1.13.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">41.43</span></td>\n<td id=\"S4.T3.4.1.13.11.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.13.11.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">42.32</span></td>\n<td id=\"S4.T3.4.1.13.11.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.14</span></td>\n<td id=\"S4.T3.4.1.13.11.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-0.46</span></td>\n<td id=\"S4.T3.4.1.13.11.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.13.11.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.25</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.14.12\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.14.12.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.14.12.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mixtral-8x7B</span></td>\n<td id=\"S4.T3.4.1.14.12.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.14.12.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S4.T3.4.1.14.12.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.38</span></td>\n<td id=\"S4.T3.4.1.14.12.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.97</span></td>\n<td id=\"S4.T3.4.1.14.12.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.76</span></td>\n<td id=\"S4.T3.4.1.14.12.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">47.10</span></td>\n<td id=\"S4.T3.4.1.14.12.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.14.12.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">45.09</span></td>\n<td id=\"S4.T3.4.1.14.12.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.14.12.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.14.12.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.14.12.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-5.37</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.15.13\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.15.13.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.15.13.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mistral-7B</span></td>\n<td id=\"S4.T3.4.1.15.13.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.15.13.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S4.T3.4.1.15.13.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.07</span></td>\n<td id=\"S4.T3.4.1.15.13.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.36</span></td>\n<td id=\"S4.T3.4.1.15.13.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">48.06</span></td>\n<td id=\"S4.T3.4.1.15.13.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.40</span></td>\n<td id=\"S4.T3.4.1.15.13.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.15.13.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.08</span></td>\n<td id=\"S4.T3.4.1.15.13.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.15.13.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.15.13.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.15.13.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-2.56</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.16.14\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.16.14.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.16.14.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Llama-3-8B</span></td>\n<td id=\"S4.T3.4.1.16.14.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.16.14.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mar 2023</span></td>\n<td id=\"S4.T3.4.1.16.14.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.44</span></td>\n<td id=\"S4.T3.4.1.16.14.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">54.18</span></td>\n<td id=\"S4.T3.4.1.16.14.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.66</span></td>\n<td id=\"S4.T3.4.1.16.14.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">47.94</span></td>\n<td id=\"S4.T3.4.1.16.14.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.16.14.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">45.97</span></td>\n<td id=\"S4.T3.4.1.16.14.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-2.21</span></td>\n<td id=\"S4.T3.4.1.16.14.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.25</span></td>\n<td id=\"S4.T3.4.1.16.14.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.16.14.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-3.01</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.17.15\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.17.15.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T3.4.1.17.15.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Qwen-2-7B</span></td>\n<td id=\"S4.T3.4.1.17.15.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.17.15.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unknown</span></td>\n<td id=\"S4.T3.4.1.17.15.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">55.28</span></td>\n<td id=\"S4.T3.4.1.17.15.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">55.93</span></td>\n<td id=\"S4.T3.4.1.17.15.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">53.44</span></td>\n<td id=\"S4.T3.4.1.17.15.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">49.77</span></td>\n<td id=\"S4.T3.4.1.17.15.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.1.17.15.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">47.94</span></td>\n<td id=\"S4.T3.4.1.17.15.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.17.15.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">–</span></td>\n<td id=\"S4.T3.4.1.17.15.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.1.17.15.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-3.04</span></td>\n</tr>\n<tr id=\"S4.T3.4.1.18.16\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.18.16.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"S4.T3.4.1.18.16.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Gemma-2-2B</span></td>\n<td id=\"S4.T3.4.1.18.16.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T3.4.1.18.16.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Jul 2024</span></td>\n<td id=\"S4.T3.4.1.18.16.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">47.87</span></td>\n<td id=\"S4.T3.4.1.18.16.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.71</span></td>\n<td id=\"S4.T3.4.1.18.16.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">46.81</span></td>\n<td id=\"S4.T3.4.1.18.16.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">45.20</span></td>\n<td id=\"S4.T3.4.1.18.16.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T3.4.1.18.16.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.65</span></td>\n<td id=\"S4.T3.4.1.18.16.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">-4.46</span></td>\n<td id=\"S4.T3.4.1.18.16.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">-2.33</span></td>\n<td id=\"S4.T3.4.1.18.16.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.4.1.18.16.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.73</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section id=\"S4.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results for the Constrained Open-Book Setting.</h4>\n\n<div id=\"S4.SS2.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px2.p1.1\" class=\"ltx_p\">In Figure <a href=\"#S4.F5\" title=\"Figure 5 ‣ Results for the Constrained Open-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we present the results of the constrained open-book setting, with Mixtral-8x7B on TF questions and Llama-3-8B on MC questions across different RAG cutoff dates.<span id=\"footnote3\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Refer to Appendix <a href=\"#A2.SS4\" title=\"B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a> for results of other models in the constrained open-book setting.</span></span></span> For Mixtral-8x7B, as the RAG cutoff dates extend to closer to the resolution dates, we observe a clear improvement in performance, indicating the model benefits from increasingly updated information retrieval. However, there are noticeable performance drops immediately after each RAG cutoff date when compared to providing information up to the day before the resolution date (with the exception of the March 2024 cutoff). This highlights the importance of keeping up-to-date information for optimal RAG performance. Interestingly, RAG does not uniformly enhance performance. Llama-3-8B may perform worse than the closed-book setting when the RAG cutoff is prior to the knowledge cutoff dates, suggesting outdated information may negatively impact performance. Conversely, for more recent RAG cutoff dates that extend beyond the knowledge cutoff, significant performance improvements are observed (as illustrated by the curves with cutoffs in September 2023 and March 2024). Notably, across all different RAG cutoffs, the overall performance decline pattern persists, likely due to outdated internal representations and the model’s inherent knowledge limitations.</p>\n</div>\n<figure id=\"S4.F5\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"S4.F5.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"415\" height=\"176\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F5.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span id=\"S4.F5.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for the constrained open-book setting, evaluating Mixtral-8x7B on TF questions and Llama-3-8B on MC questions with different RAG cutoff dates.\n</span></figcaption>\n</figure>\n</section>\n<section id=\"S4.SS2.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Results for the Gold Article Setting.</h4>\n\n<div id=\"S4.SS2.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px3.p1.1\" class=\"ltx_p\">Figure <a href=\"#S4.F6\" title=\"Figure 6 ‣ Results for the Gold Article Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that when given access to the gold articles from which the questions are generated, LLM performance can be improved significantly to around 90%, demonstrating the answerability of Daily Oracle.<span id=\"footnote4\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Results for GPT-3.5 are provided and discussed in Appendix <a href=\"#A2.SS3\" title=\"B.3 Results for GPT-3.5 in the Gold Article Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>, as this older model performs relatively poorly and including it on the same scale would obscure the trends of other models.</span></span></span> However, most of the models still show declining trends. This is noteworthy because, ideally, LLMs are expected to achieve consistent accuracy regardless of the article’s publication date when answers are directly accessible. However, the outdated representations hinder their ability to consistently generate correct answers, even in a reading comprehension setting.</p>\n</div>\n<figure id=\"S4.F6\" class=\"ltx_figure\"><img src=\"./assets/x6.png\" id=\"S4.F6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"415\" height=\"168\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F6.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span id=\"S4.F6.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for the Gold Article Setting. Most of the models struggle with temporal generalization, even when provided with gold articles containing the answers.</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Discussion</h3>\n\n<section id=\"S4.SS3.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLMs’ Performance Evolution Across Time.</h4>\n\n<div id=\"S4.SS3.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.SSS0.Px1.p1.1\" class=\"ltx_p\">We observe several LLMs’ performance evolution patterns in Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: (1) <span id=\"S4.SS3.SSS0.Px1.p1.1.1\" class=\"ltx_text ltx_font_italic\">Gradual Decline in the Recent Past:</span> In the months before the knowledge cutoff date, which we call the <span id=\"S4.SS3.SSS0.Px1.p1.1.2\" class=\"ltx_text ltx_font_italic\">recent past</span>, we observe a gradual decline in model performance, as seen in Llama-3-8B, GPT-4, and Claude-3.5-Sonnet, likely due to a lack of representation of recent news in the training data. (2) <span id=\"S4.SS3.SSS0.Px1.p1.1.3\" class=\"ltx_text ltx_font_italic\">Rapid Decline in the Near Future:</span> In the <span id=\"S4.SS3.SSS0.Px1.p1.1.4\" class=\"ltx_text ltx_font_italic\">near future</span>, which we define as the months following a model’s knowledge cutoff date, sharp performance drops are observed in several models in MC questions. For instance, the decline in Claude-3.5-Sonnet and GPT-4 accelerates soon after their knowledge cutoffs. Most of the models, however, do not lose all the predictive power at once, as evidenced by the further decline into the farther future.</p>\n</div>\n<div id=\"S4.SS3.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S4.SS3.SSS0.Px1.p2.1\" class=\"ltx_p\">We explore this further by analyzing the slope of accuracy as a function of time. In Figure <a href=\"#S4.F7\" title=\"Figure 7 ‣ LLMs’ Performance Evolution Across Time. ‣ 4.3 Discussion ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we show how the slope changes as we fit a regression to an increasingly larger window of data, until we reach the full set of accuracies. Specifically, using the 5-month moving average of each model’s accuracy on MC questions (visualized in Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), we start by fitting a linear regression line on the first 10 months of data. We then add an additional month and compute a new regression on the larger window, repeating until we reach the final month, and applying an exponential decay weighting to past data to reduce the influence of distant observations. With this, we can analyze how the slope of our regression line changes as each month is added to the data. The slope in each case is negative after the cutoff data and for Claude-3.5-Sonnet, GPT-4, and Llama-3-8B, the slope eventually or immediately becomes more negative than it was at any point preceding the cutoff. Both Claude-3.5-Sonnet and Llama-3-8B have a crossover from positive to negative slope in late summer 2022, July and August, respectively, while GPT-4’s seems to occur slightly earlier, in March of 2022. For GPT-3.5, GPT-4, and Llama-3-8B, the slope becomes increasingly negative not long after the knowledge cutoff, giving evidence for a rapid decline in the <span id=\"S4.SS3.SSS0.Px1.p2.1.1\" class=\"ltx_text ltx_font_italic\">near future</span>. Likewise, the period preceding the cutoff shows a less negative slope and in some cases, most obviously in the case of GPT-3.5 and Llama-3-8B, a slightly negative but consistent slope, in other words, a gradual decline in the <span id=\"S4.SS3.SSS0.Px1.p2.1.2\" class=\"ltx_text ltx_font_italic\">recent past</span>.</p>\n</div>\n<figure id=\"S4.F7\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S4.F7.sf1\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x7.png\" id=\"S4.F7.sf1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"228\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F7.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S4.F7.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">(a) Claude-3.5-Sonnet</span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S4.F7.sf2\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x8.png\" id=\"S4.F7.sf2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"229\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F7.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S4.F7.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">(b) GPT-4</span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S4.F7.sf3\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x9.png\" id=\"S4.F7.sf3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"230\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F7.sf3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span><span id=\"S4.F7.sf3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">(c) Llama-3-8B</span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S4.F7.sf4\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x10.png\" id=\"S4.F7.sf4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"229\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F7.sf4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(d)</span> </span><span id=\"S4.F7.sf4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">(d) GPT-3.5</span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F7.10.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span id=\"S4.F7.8.4\" class=\"ltx_text\" style=\"font-size:90%;\">Coefficients for regressing accuracy on the MC questions against time, as the number of months grows. Using an initial window of 10 months, we progressively add data for additional months to our regression and plot the coefficient (slope) for the regression of accuracy against time. For our regression, we use the moving average of accuracy and apply exponentially decaying weights to older months (i.e. given a window of <math id=\"S4.F7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"S4.F7.5.1.m1.1b\"><mi id=\"S4.F7.5.1.m1.1.1\" xref=\"S4.F7.5.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.F7.5.1.m1.1c\"><ci id=\"S4.F7.5.1.m1.1.1.cmml\" xref=\"S4.F7.5.1.m1.1.1\">𝑘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F7.5.1.m1.1d\">k</annotation></semantics></math> months, we weight <math id=\"S4.F7.6.2.m2.1\" class=\"ltx_Math\" alttext=\"x_{t}\" display=\"inline\"><semantics id=\"S4.F7.6.2.m2.1b\"><msub id=\"S4.F7.6.2.m2.1.1\" xref=\"S4.F7.6.2.m2.1.1.cmml\"><mi id=\"S4.F7.6.2.m2.1.1.2\" xref=\"S4.F7.6.2.m2.1.1.2.cmml\">x</mi><mi id=\"S4.F7.6.2.m2.1.1.3\" xref=\"S4.F7.6.2.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F7.6.2.m2.1c\"><apply id=\"S4.F7.6.2.m2.1.1.cmml\" xref=\"S4.F7.6.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F7.6.2.m2.1.1.1.cmml\" xref=\"S4.F7.6.2.m2.1.1\">subscript</csymbol><ci id=\"S4.F7.6.2.m2.1.1.2.cmml\" xref=\"S4.F7.6.2.m2.1.1.2\">𝑥</ci><ci id=\"S4.F7.6.2.m2.1.1.3.cmml\" xref=\"S4.F7.6.2.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F7.6.2.m2.1d\">x_{t}</annotation></semantics></math> with <math id=\"S4.F7.7.3.m3.1\" class=\"ltx_Math\" alttext=\"\\lambda^{k-t}\" display=\"inline\"><semantics id=\"S4.F7.7.3.m3.1b\"><msup id=\"S4.F7.7.3.m3.1.1\" xref=\"S4.F7.7.3.m3.1.1.cmml\"><mi id=\"S4.F7.7.3.m3.1.1.2\" xref=\"S4.F7.7.3.m3.1.1.2.cmml\">λ</mi><mrow id=\"S4.F7.7.3.m3.1.1.3\" xref=\"S4.F7.7.3.m3.1.1.3.cmml\"><mi id=\"S4.F7.7.3.m3.1.1.3.2\" xref=\"S4.F7.7.3.m3.1.1.3.2.cmml\">k</mi><mo id=\"S4.F7.7.3.m3.1.1.3.1\" xref=\"S4.F7.7.3.m3.1.1.3.1.cmml\">−</mo><mi id=\"S4.F7.7.3.m3.1.1.3.3\" xref=\"S4.F7.7.3.m3.1.1.3.3.cmml\">t</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.F7.7.3.m3.1c\"><apply id=\"S4.F7.7.3.m3.1.1.cmml\" xref=\"S4.F7.7.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F7.7.3.m3.1.1.1.cmml\" xref=\"S4.F7.7.3.m3.1.1\">superscript</csymbol><ci id=\"S4.F7.7.3.m3.1.1.2.cmml\" xref=\"S4.F7.7.3.m3.1.1.2\">𝜆</ci><apply id=\"S4.F7.7.3.m3.1.1.3.cmml\" xref=\"S4.F7.7.3.m3.1.1.3\"><minus id=\"S4.F7.7.3.m3.1.1.3.1.cmml\" xref=\"S4.F7.7.3.m3.1.1.3.1\"></minus><ci id=\"S4.F7.7.3.m3.1.1.3.2.cmml\" xref=\"S4.F7.7.3.m3.1.1.3.2\">𝑘</ci><ci id=\"S4.F7.7.3.m3.1.1.3.3.cmml\" xref=\"S4.F7.7.3.m3.1.1.3.3\">𝑡</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F7.7.3.m3.1d\">\\lambda^{k-t}</annotation></semantics></math>; in this case <math id=\"S4.F7.8.4.m4.1\" class=\"ltx_Math\" alttext=\"\\lambda=0.995\" display=\"inline\"><semantics id=\"S4.F7.8.4.m4.1b\"><mrow id=\"S4.F7.8.4.m4.1.1\" xref=\"S4.F7.8.4.m4.1.1.cmml\"><mi id=\"S4.F7.8.4.m4.1.1.2\" xref=\"S4.F7.8.4.m4.1.1.2.cmml\">λ</mi><mo id=\"S4.F7.8.4.m4.1.1.1\" xref=\"S4.F7.8.4.m4.1.1.1.cmml\">=</mo><mn id=\"S4.F7.8.4.m4.1.1.3\" xref=\"S4.F7.8.4.m4.1.1.3.cmml\">0.995</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F7.8.4.m4.1c\"><apply id=\"S4.F7.8.4.m4.1.1.cmml\" xref=\"S4.F7.8.4.m4.1.1\"><eq id=\"S4.F7.8.4.m4.1.1.1.cmml\" xref=\"S4.F7.8.4.m4.1.1.1\"></eq><ci id=\"S4.F7.8.4.m4.1.1.2.cmml\" xref=\"S4.F7.8.4.m4.1.1.2\">𝜆</ci><cn type=\"float\" id=\"S4.F7.8.4.m4.1.1.3.cmml\" xref=\"S4.F7.8.4.m4.1.1.3\">0.995</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F7.8.4.m4.1d\">\\lambda=0.995</annotation></semantics></math>). In each case, the slope becomes more negative following the knowledge cutoff period compared to the months immediately preceding it.</span></figcaption>\n</figure>\n</section>\n<section id=\"S4.SS3.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Need for Continuous Pretraining.</h4>\n\n<div id=\"S4.SS3.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.SSS0.Px2.p1.1\" class=\"ltx_p\">The overall decline trend may come from two sources, the missing knowledge of future and a lack of up-to-date language representation. While the lack of knowledge can be partially recovered with information retrieval as shown in the constrained open-book setting, the gold article setting provides an “upper bound” of open-book retrieval. When provided the gold articles, the remaining decline in the model’s performance suggests continuous pre-training of LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et al.,, <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2022</a>; Jin et al.,, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2022</a>; <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">Ke et al., 2022a, </a>; <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">Ke et al., 2022b, </a>; Yıldız et al.,, <a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> is still needed in the context of news event forecasting to address outdated representations.</p>\n</div>\n</section>\n<section id=\"S4.SS3.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">TF &amp; MC Comparison.</h4>\n\n<div id=\"S4.SS3.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.SSS0.Px3.p1.1\" class=\"ltx_p\">All models except for Claude-3.5-Sonnet struggle with TF questions, where the degradation trends towards the random baseline accuracy of 50%, indicating that predicting if a future event will happen or not can be sometimes challenging for LLMs. In contrast, on MC questions, models tend to perform much better than the random baseline at 25%. There are two potential reasons that can explain the disparity. First, TF questions can be considered more open-ended than MC because the “No” answer contains other possible open-ended outcomes. Second, since the distractor choices are created by an LLM, they may not be as likely to happen as the true answer.</p>\n</div>\n</section>\n<section id=\"S4.SS3.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Consistent Performance Decline After September 2021.</h4>\n\n<div id=\"S4.SS3.SSS0.Px4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.SSS0.Px4.p1.1\" class=\"ltx_p\">Interestingly, Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reveals a higher rate of performance decline around September 2021, which is the knowledge cutoff date of GPT-3.5, across all models, particularly for MC questions. In contrast, performance remains relatively stable prior to this date. We hypothesize that this trend arises because the period up to September 2021 may be overrepresented in many pretraining corpora <cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et al.,, <a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">2020</a>; Gao et al.,, <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2020</a>; Kobayashi,, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2018</a>; Gokaslan and Cohen,, <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2019</a>; Zhu,, <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2015</a>; Rae et al.,, <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2019</a>; Tiedemann,, <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2016</a>; Saxton et al.,, <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>, compared to more recent periods. Another potential cause of this imbalance is an increasing number of websites restricting access to web crawlers <cite class=\"ltx_cite ltx_citemacro_citep\">(Longpre et al.,, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> after the rise of ChatGPT.</p>\n</div>\n</section>\n<section id=\"S4.SS3.SSS0.Px5\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations.</h4>\n\n<div id=\"S4.SS3.SSS0.Px5.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.SSS0.Px5.p1.1\" class=\"ltx_p\">On the data generation side, the generated questions as well as the distractor answers could contain biases from an outdated LLM, making the benchmark less reliable in the long run unless we upgrade the models. Additionally, generating questions from news articles can introduce bias by focusing only on events that have definitively occurred or not occurred. This approach overlooks speculative events, which may be of interest in forecasting questions but may not be covered in news articles, limiting the scope of the dataset. On the evaluation side, our paper proposes the continuous evaluation benchmark but at the time of the writing there isn’t a long enough time horizon on each model, especially after the cutoff dates, for a thorough analysis. Ideally, we would like to analyze the relation between the effect of knowledge and RAG cutoff dates but the trend seems to be weak within the time horizon available.</p>\n</div>\n</section>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion and Future Work</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para\">\n<p id=\"S5.p1.1\" class=\"ltx_p\">We introduce Daily Oracle, a continuously updated QA benchmark leveraging daily news to evaluate the temporal generalization and future prediction capabilities of LLMs. Our experiments reveal that while LLMs maintain a degree of predictive power over future events, their prediction accuracy exhibits a gradual decline over time across various models. Notably, while the stronger model Claude-3.5-Sonnet outperforms others significantly, it still exhibits around 12% performance drop in its post-knowledge cutoff period. Although RAG mitigates the effect of outdated knowledge, a noticeable decline in performance remains. Our findings underscore the necessity for ongoing model updates with more current information and emphasize the importance of disentangling missing knowledge from the lack of up-to-date representations.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para\">\n<p id=\"S5.p2.1\" class=\"ltx_p\">We hope this work will draw attention to the need for more practical applications of the continuous training of LLMs, driving advancements in adapting models to real-time data changes. In the future, alongside maintaining Daily Oracle, we plan to incorporate a broader range of models and explore how continuous pre-training and efficient adaptation can address the performance degradation challenges presented in our work.</p>\n</div>\n</section>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">We thank the Microsoft Accelerating Foundation Models Research program for providing Azure cloud compute credits for the GPT APIs. The compute was also supported by the NYU High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Agarwal and Nenkova,  (2022)</span>\n<span class=\"ltx_bibblock\">\nAgarwal, O. and Nenkova, A. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Temporal effects on pre-trained models for language processing tasks.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib1.1.1\" class=\"ltx_text ltx_font_italic\">Transactions of the Association for Computational Linguistics</span>.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Anderson and Schooler,  (1991)</span>\n<span class=\"ltx_bibblock\">\nAnderson, J. R. and Schooler, L. J. (1991).\n\n</span>\n<span class=\"ltx_bibblock\">Reflections of the environment in memory.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib2.1.1\" class=\"ltx_text ltx_font_italic\">Psychological Science</span>, 2(6):396–408.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Anthropic,  (2024)</span>\n<span class=\"ltx_bibblock\">\nAnthropic (2024).\n\n</span>\n<span class=\"ltx_bibblock\">The claude 3 model family: Opus, sonnet, haiku.\n\n</span>\n<span class=\"ltx_bibblock\"><a target=\"_blank\" href=\"https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</a>.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nChen, W., Wang, X., Wang, W. Y., and Wang, W. Y. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">A dataset for answering time-sensitive questions.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib4.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</span>.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dempsey et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nDempsey, W. H., Moreno, A., Scott, C. K., Dennis, M. L., Gustafson, D. H., Murphy, S. A., and Rehg, J. M. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">iSurvive: An interpretable, event-time prediction model for mHealth.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib5.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the 34th International Conference on Machine Learning</span>.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dubey et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">The llama 3 herd of models.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib6.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2407.21783</span>.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ester et al.,  (1996)</span>\n<span class=\"ltx_bibblock\">\nEster, M., Kriegel, H.-P., Sander, J., Xu, X., et al. (1996).\n\n</span>\n<span class=\"ltx_bibblock\">A density-based algorithm for discovering clusters in large spatial databases with noise.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib7.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</span>.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gao et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">The pile: An 800gb dataset of diverse text for language modeling.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib8.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2101.00027</span>.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gillingham et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nGillingham, K., Nordhaus, W., Anthoff, D., Blanford, G., Bosetti, V., Christensen, P., McJeon, H., and Reilly, J. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Modeling uncertainty in integrated assessment of climate change: A multimodel comparison.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib9.1.1\" class=\"ltx_text ltx_font_italic\">Journal of the Association of Environmental and Resource Economists</span>, 5(4):791–826.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gokaslan and Cohen,  (2019)</span>\n<span class=\"ltx_bibblock\">\nGokaslan, A. and Cohen, V. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Openwebtext corpus.\n\n</span>\n<span class=\"ltx_bibblock\"><a target=\"_blank\" href=\"http://Skylion007.github.io/OpenWebTextCorpus\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://Skylion007.github.io/OpenWebTextCorpus</a>.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Halawi et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nHalawi, D., Zhang, F., Yueh-Han, C., and Steinhardt, J. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Approaching human-level forecasting with language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib11.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2402.18563</span>.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hamborg et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nHamborg, F., Meuschke, N., Breitinger, C., and Gipp, B. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">news-please: A generic news crawler and extractor.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib12.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the 15th International Symposium of Information Science</span>.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jang et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nJang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S. J., and Seo, M. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Towards continual knowledge learning of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib13.1.1\" class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Mistral 7b.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib14.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2310.06825</span>.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Mixtral of experts.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib15.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2401.04088</span>.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jin et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nJin, W., Khanna, R., Kim, S., Lee, D.-H., Morstatter, F., Galstyan, A., and Ren, X. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">ForecastQA: A question answering challenge for event forecasting with temporal text data.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib16.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jin et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nJin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X., Arnold, A., and Ren, X. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Lifelong pretraining: Continually adapting language models to emerging corpora.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib17.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kasai et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nKasai, J., Sakaguchi, K., Le Bras, R., Asai, A., Yu, X., Radev, D., Smith, N. A., Choi, Y., Inui, K., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Realtime qa: what’s the answer right now?\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib18.1.1\" class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(19)</span>\n<span class=\"ltx_bibblock\">\nKe, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B. (2022a).\n\n</span>\n<span class=\"ltx_bibblock\">Continual pre-training of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib19.1.1\" class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(20)</span>\n<span class=\"ltx_bibblock\">\nKe, Z., Shao, Y., Lin, H., Xu, H., Shu, L., and Liu, B. (2022b).\n\n</span>\n<span class=\"ltx_bibblock\">Adapting a language model while preserving its general knowledge.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib20.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kobayashi,  (2018)</span>\n<span class=\"ltx_bibblock\">\nKobayashi, S. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Homemade bookcorpus.\n\n</span>\n<span class=\"ltx_bibblock\"><a target=\"_blank\" href=\"https://github.com/soskek/bookcorpus\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://github.com/soskek/bookcorpus</a>.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lazaridou et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nLazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T., Gimenez, M., de Masson d'Autume, C., Kocisky, T., Ruder, S., Yogatama, D., Cao, K., Young, S., and Blunsom, P. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Mind the gap: Assessing temporal generalization in neural language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib22.1.1\" class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lewis et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Retrieval-augmented generation for knowledge-intensive nlp tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib23.1.1\" class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li and Flanigan,  (2024)</span>\n<span class=\"ltx_bibblock\">\nLi, C. and Flanigan, J. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Task contamination: Language models may not be few-shot anymore.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib24.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the AAAI Conference on Artificial Intelligence</span>.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liska et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nLiska, A., Kocisky, T., Gribovskaya, E., Terzi, T., Sezener, E., Agrawal, D., Cyprien De Masson, D., Scholtes, T., Zaheer, M., Young, S., et al. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib25.1.1\" class=\"ltx_text ltx_font_italic\">International Conference on Machine Learning</span>.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Longpre et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nLongpre, S., Mahari, R., Lee, A., Lund, C., Oderinwale, H., Brannon, W., Saxena, N., Obeng-Marnu, N., South, T., Hunter, C., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Consent in crisis: The rapid decline of the ai data commons.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib26.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2407.14933</span>.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lopez-Lira and Tang,  (2023)</span>\n<span class=\"ltx_bibblock\">\nLopez-Lira, A. and Tang, Y. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Can chatgpt forecast stock price movements? return predictability and large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib27.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2304.07619</span>.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McIntosh et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nMcIntosh, T. R., Susnjak, T., Liu, T., Watters, P., and Halgamuge, M. N. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Inadequacies of large language model benchmarks in the era of generative artificial intelligence.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib28.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2402.09880</span>.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nagel,  (2016)</span>\n<span class=\"ltx_bibblock\">\nNagel, S. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Common crawl news dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><a target=\"_blank\" href=\"https://data.commoncrawl.org/crawl-data/CC-NEWS/index.html\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://data.commoncrawl.org/crawl-data/CC-NEWS/index.html</a>.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI,  (2023)</span>\n<span class=\"ltx_bibblock\">\nOpenAI (2023).\n\n</span>\n<span class=\"ltx_bibblock\">GPT-4 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib30.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2303.08774</span>.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI,  (2024)</span>\n<span class=\"ltx_bibblock\">\nOpenAI (2024).\n\n</span>\n<span class=\"ltx_bibblock\">New embedding models and api updates.\n\n</span>\n<span class=\"ltx_bibblock\"><a target=\"_blank\" href=\"https://openai.com/index/new-embedding-models-and-api-updates/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://openai.com/index/new-embedding-models-and-api-updates/</a>.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rae et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nRae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Compressive transformers for long-range sequence modelling.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib32.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1911.05507</span>.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Raffel et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Exploring the limits of transfer learning with a unified text-to-text transformer.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib33.1.1\" class=\"ltx_text ltx_font_italic\">Journal of Machine Learning Research</span>, 21:1–67.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Robertson et al.,  (1995)</span>\n<span class=\"ltx_bibblock\">\nRobertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., Gatford, M., et al. (1995).\n\n</span>\n<span class=\"ltx_bibblock\">Okapi at trec-3.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib34.1.1\" class=\"ltx_text ltx_font_italic\">Nist Special Publication Sp</span>, 109:109.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Röttger and Pierrehumbert,  (2021)</span>\n<span class=\"ltx_bibblock\">\nRöttger, P. and Pierrehumbert, J. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Temporal adaptation of BERT and performance on downstream document classification: Insights from social media.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib35.1.1\" class=\"ltx_text ltx_font_italic\">Findings of the Association for Computational Linguistics: EMNLP 2021</span>.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sainz et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nSainz, O., Campos, J., García-Ferrero, I., Etxaniz, J., de Lacalle, O. L., and Agirre, E. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib36.1.1\" class=\"ltx_text ltx_font_italic\">Findings of the Association for Computational Linguistics: EMNLP 2023</span>.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Saxton et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nSaxton, D., Grefenstette, E., Hill, F., and Kohli, P. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Analysing mathematical reasoning abilities of neural models.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib37.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1904.01557</span>.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Team et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nTeam, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Gemma 2: Improving open language models at a practical size.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib38.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2408.00118</span>.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tetlock and Gardner,  (2016)</span>\n<span class=\"ltx_bibblock\">\nTetlock, P. E. and Gardner, D. (2016).\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib39.1.1\" class=\"ltx_text ltx_font_italic\">Superforecasting: The art and science of prediction</span>.\n\n</span>\n<span class=\"ltx_bibblock\">Random House.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tiedemann,  (2016)</span>\n<span class=\"ltx_bibblock\">\nTiedemann, J. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Finding alternative translations in a large corpus of movie subtitle.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib40.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</span>.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vu et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le, Q., and Luong, T. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">FreshLLMs: Refreshing large language models with search engine augmentation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib41.1.1\" class=\"ltx_text ltx_font_italic\">Findings of the Association for Computational Linguistics ACL 2024</span>.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Chain-of-thought prompting elicits reasoning in large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib42.1.1\" class=\"ltx_text ltx_font_italic\">Advances in neural information processing systems</span>.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nXu, C., Guan, S., Greene, D., and Kechadi, M.-T. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Benchmark data contamination of large language models: A survey.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib43.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2406.04244</span>.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yan et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nYan, Q., Seraj, R., He, J., Meng, L., and Sylvain, T. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib44.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2310.01880</span>.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nYang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib45.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2407.10671</span>.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nYe, C., Hu, Z., Deng, Y., Huang, Z., Ma, M. D., Zhu, Y., and Wang, W. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Mirai: Evaluating llm agents for event forecasting.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib46.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2407.01231</span>.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yıldız et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nYıldız, Ç., Ravichandran, N. K., Punia, P., Bethge, M., and Ermis, B. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Investigating continual pretraining in large language models: Insights and implications.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib47.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2402.17400</span>.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang and Choi,  (2021)</span>\n<span class=\"ltx_bibblock\">\nZhang, M. and Choi, E. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">SituatedQA: Incorporating extra-linguistic contexts into QA.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib48.1.1\" class=\"ltx_text ltx_font_italic\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nZhang, Z., Cao, Y., Ye, C., Ma, Y., Liao, L., and Chua, T.-S. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Analyzing temporal complex events with large language models? a benchmark towards temporal, long context understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib49.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2406.02472</span>.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nZhu, C., Chen, N., Gao, Y., and Wang, B. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Is your llm outdated? evaluating llms at temporal generalization.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib50.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2405.08460</span>.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu,  (2015)</span>\n<span class=\"ltx_bibblock\">\nZhu, Y. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.\n\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib51.1.1\" class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1506.06724</span>.\n\n</span>\n</li>\n<li id=\"bib.bib52\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zou et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nZou, A., Xiao, T., Jia, R., Kwon, J., Mazeika, M., Li, R., Song, D., Steinhardt, J., Evans, O., and Hendrycks, D. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Forecasting future world events with neural networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span id=\"bib.bib52.1.1\" class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"Ax1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n\n</section>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Dataset Details</h2>\n\n<section id=\"A1.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Details for Article Selection</h3>\n\n<div id=\"A1.SS1.p1\" class=\"ltx_para\">\n<p id=\"A1.SS1.p1.1\" class=\"ltx_p\">We select daily articles that generate the QA pairs in two ways: (1) <span id=\"A1.SS1.p1.1.1\" class=\"ltx_text ltx_font_italic\">Random Selection:</span> We randomly sample three articles each day. (2) <span id=\"A1.SS1.p1.1.2\" class=\"ltx_text ltx_font_italic\">Hot Topic Selection:</span> To better capture daily events and reduce noise, we select three articles from the top three hot topics of the day. We identify these hot topics by applying the density-based clustering algorithm DBSCAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Ester et al.,, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">1996</a>)</cite> to the new articles based on TF-IDF (Term Frequency-Inverse Document Frequency) representations, forming clusters of news articles for each day. We filter out chaotic clusters by removing those with low average in-cluster cosine similarity scores, which typically correspond to clusters containing a large number of diverse articles. The top three clusters, determined by size, are assumed to represent the most discussed events, i.e. hot topics, since larger clusters indicate more articles covering the same event. One article is picked randomly from each of the top three clusters.</p>\n</div>\n</section>\n<section id=\"A1.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>QA Filtering Principles</h3>\n\n<div id=\"A1.SS2.p1\" class=\"ltx_para\">\n<p id=\"A1.SS2.p1.1\" class=\"ltx_p\">Seven principles of QA Filtering step in the data construction process: (1) <span id=\"A1.SS2.p1.1.1\" class=\"ltx_text ltx_font_italic\">Correctness of Answers:</span> The answer must be factually accurate and fully aligned with the information in the given article. (2) <span id=\"A1.SS2.p1.1.2\" class=\"ltx_text ltx_font_italic\">Non-answerability Before the Publication Date:</span> Since we treat the article’s publication date as the question’s resolution date, the question should not be definitively answerable based on information available before the article’s publication. (3) <span id=\"A1.SS2.p1.1.3\" class=\"ltx_text ltx_font_italic\">Absence of Information Leakage:</span> Questions must avoid revealing information that became known only after the article’s publication, maintaining fairness for pre-publication evaluation. (4) <span id=\"A1.SS2.p1.1.4\" class=\"ltx_text ltx_font_italic\">Objectivity:</span> Both questions and answers must rely on objective facts, avoiding subjective ideas from the authors. (5) <span id=\"A1.SS2.p1.1.5\" class=\"ltx_text ltx_font_italic\">Inclusion of a Clear Temporal Element: </span> Questions must contain a specific and clear reference to time, avoiding vague phrases like “in the future” or “soon.” (6) <span id=\"A1.SS2.p1.1.6\" class=\"ltx_text ltx_font_italic\">Public Interest:</span> The questions should address topics of broad public concern. (7) <span id=\"A1.SS2.p1.1.7\" class=\"ltx_text ltx_font_italic\">Non-obviousness of the Answer:</span> The answer should not be immediately predictable from the question and must provide new or non-trivial insights.</p>\n</div>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Experiment Details</h2>\n\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Baseline Models Information</h3>\n\n<div id=\"A2.SS1.p1\" class=\"ltx_para\">\n<p id=\"A2.SS1.p1.1\" class=\"ltx_p\">Table <a href=\"#A2.T4\" title=\"Table 4 ‣ B.1 Baseline Models Information ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> lists the LLM model versions used in our experiments.</p>\n</div>\n<figure id=\"A2.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"A2.T4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 4</span>: </span><span id=\"A2.T4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline Model Versions.</span></figcaption>\n<table id=\"A2.T4.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T4.4.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th id=\"A2.T4.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Model Version</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T4.4.2.1\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Claude-3.5-Sonnet</th>\n<td id=\"A2.T4.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">claude-3-5-sonnet-20240620</td>\n</tr>\n<tr id=\"A2.T4.4.3.2\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT-4</th>\n<td id=\"A2.T4.4.3.2.2\" class=\"ltx_td ltx_align_center\">gpt-4-1106-preview</td>\n</tr>\n<tr id=\"A2.T4.4.4.3\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT-3.5</th>\n<td id=\"A2.T4.4.4.3.2\" class=\"ltx_td ltx_align_center\">gpt-3.5-turbo-0125</td>\n</tr>\n<tr id=\"A2.T4.4.5.4\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mixtral-8x7B</th>\n<td id=\"A2.T4.4.5.4.2\" class=\"ltx_td ltx_align_center\">Mixtral-8x7B-Instruct-v0.1</td>\n</tr>\n<tr id=\"A2.T4.4.6.5\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral-7B</th>\n<td id=\"A2.T4.4.6.5.2\" class=\"ltx_td ltx_align_center\">Mistral-7B-Instruct-v0.3</td>\n</tr>\n<tr id=\"A2.T4.4.7.6\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Llama-3-8B</th>\n<td id=\"A2.T4.4.7.6.2\" class=\"ltx_td ltx_align_center\">Meta-Llama-3-8B-Instruct</td>\n</tr>\n<tr id=\"A2.T4.4.8.7\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Qwen-2-7B</th>\n<td id=\"A2.T4.4.8.7.2\" class=\"ltx_td ltx_align_center\">Qwen2-7B-Instruct</td>\n</tr>\n<tr id=\"A2.T4.4.9.8\" class=\"ltx_tr\">\n<th id=\"A2.T4.4.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Gemma-2-2B</th>\n<td id=\"A2.T4.4.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">gemma-2-2b-it</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section id=\"A2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Rejection Rates in the Closed-Book Setting</h3>\n\n<div id=\"A2.SS2.p1\" class=\"ltx_para\">\n<p id=\"A2.SS2.p1.1\" class=\"ltx_p\">Figure <a href=\"#A2.F8\" title=\"Figure 8 ‣ B.2 Rejection Rates in the Closed-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the rejection rates for the closed-book setting. We can see that the rejection rate increases throughout the time for Mistral-7B in TF questions and Mixtral-8x7B in both TF and MC questions. This high rejection rate in TF questions for these two models results in closed-book performance dropping below the random baseline of 50% in certain months, as shown in Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ Results for the Closed-Book Setting. ‣ 4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n</div>\n<figure id=\"A2.F8\" class=\"ltx_figure\"><img src=\"./assets/x11.png\" id=\"A2.F8.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"187\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F8.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span id=\"A2.F8.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Rejection rates for the closed-book setting. We plot the 5-month moving average rejection rates for True/False (TF) and Multiple Choice (MC) questions across different LLMs.</span></figcaption>\n</figure>\n</section>\n<section id=\"A2.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Results for GPT-3.5 in the Gold Article Setting</h3>\n\n<div id=\"A2.SS3.p1\" class=\"ltx_para\">\n<p id=\"A2.SS3.p1.1\" class=\"ltx_p\">To more effectively illustrate the trends of other models at a suitable scale, we display GPT-3.5’s performance in the gold article setting separately. As shown in Figure <a href=\"#A2.F9\" title=\"Figure 9 ‣ B.3 Results for GPT-3.5 in the Gold Article Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, this outdated model performs relatively poorly throughout. While its accuracy could improve with chain-of-thought prompting <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al.,, <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, we report its performance using the same prompt format as the other models for consistency in comparison. Nevertheless, a clear downward trend is observed in MC questions.</p>\n</div>\n<figure id=\"A2.F9\" class=\"ltx_figure\"><img src=\"./assets/x12.png\" id=\"A2.F9.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"177\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F9.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>: </span><span id=\"A2.F9.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for GPT-3.5 in the Gold Article Setting. Compared to other models achieving around 0.9 accuracy, GPT-3.5 performs worse in both MC questions and, more notably, in TF questions.</span></figcaption>\n</figure>\n</section>\n<section id=\"A2.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>More Results in the Constraint Open-Book Setting</h3>\n\n<div id=\"A2.SS4.p1\" class=\"ltx_para\">\n<p id=\"A2.SS4.p1.1\" class=\"ltx_p\">Figures <a href=\"#A2.F10\" title=\"Figure 10 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, <a href=\"#A2.F11\" title=\"Figure 11 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, <a href=\"#A2.F12\" title=\"Figure 12 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, <a href=\"#A2.F13\" title=\"Figure 13 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a href=\"#A2.F14\" title=\"Figure 14 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, <a href=\"#A2.F15\" title=\"Figure 15 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, and <a href=\"#A2.F16\" title=\"Figure 16 ‣ B.4 More Results in the Constraint Open-Book Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> show the constrained open-book evaluation results for more models. Similar patterns are observed as discussed in Section <a href=\"#S4.SS2\" title=\"4.2 Main Results ‣ 4 Experiments ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. Specifically, for Claude-3.5-Sonnet, the constrained open-book performance lags behind its closed-book performance, likely because it already has robust representations of world events, making RAG less effective. Additionally, GPT-3.5 is not included in the constrained open-book setting due to its unexpectedly poor performance in the gold article setting (Figure <a href=\"#A2.F9\" title=\"Figure 9 ‣ B.3 Results for GPT-3.5 in the Gold Article Setting ‣ Appendix B Experiment Details ‣ Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>) and budget limitations.</p>\n</div>\n<figure id=\"A2.F10\" class=\"ltx_figure\"><img src=\"./assets/x13.png\" id=\"A2.F10.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"155\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F10.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>: </span><span id=\"A2.F10.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for Claude-3.5-Sonnet in the constrained open-book setting.\n</span></figcaption>\n</figure>\n<figure id=\"A2.F11\" class=\"ltx_figure\"><img src=\"./assets/x14.png\" id=\"A2.F11.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"153\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F11.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>: </span><span id=\"A2.F11.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for GPT-4 in the constrained open-book setting.\n</span></figcaption>\n</figure>\n<figure id=\"A2.F12\" class=\"ltx_figure\"><img src=\"./assets/x15.png\" id=\"A2.F12.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"148\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F12.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 12</span>: </span><span id=\"A2.F12.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for Mixtral-8x7B in the constrained open-book setting.\n</span></figcaption>\n</figure>\n<figure id=\"A2.F13\" class=\"ltx_figure\"><img src=\"./assets/x16.png\" id=\"A2.F13.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"147\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F13.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 13</span>: </span><span id=\"A2.F13.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for Mistral-7B in the constrained open-book setting.\n</span></figcaption>\n</figure>\n<figure id=\"A2.F14\" class=\"ltx_figure\"><img src=\"./assets/x17.png\" id=\"A2.F14.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"153\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F14.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 14</span>: </span><span id=\"A2.F14.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for Llama-3-8B in the constrained open-book setting.\n</span></figcaption>\n</figure>\n<figure id=\"A2.F15\" class=\"ltx_figure\"><img src=\"./assets/x18.png\" id=\"A2.F15.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"146\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F15.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 15</span>: </span><span id=\"A2.F15.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for Qwen-2-7B in the constrained open-book setting.\n</span></figcaption>\n</figure>\n<figure id=\"A2.F16\" class=\"ltx_figure\"><img src=\"./assets/x19.png\" id=\"A2.F16.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"392\" height=\"153\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F16.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 16</span>: </span><span id=\"A2.F16.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for Gemma-2-2B in the constrained open-book setting.\n</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Prompts</h2>\n\n<div id=\"A3.p1\" class=\"ltx_para\">\n<p id=\"A3.p1.1\" class=\"ltx_p\">All the prompts we use are shown in this section. The QA generation prompts and evaluation prompts are adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, and the prompt to categorize our generated questions is taken from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al., (<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</p>\n</div>\n<figure id=\"A3.F17\" class=\"ltx_figure\">\n<span id=\"A3.F17.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F17.2.1\" class=\"ltx_p\"><span id=\"A3.F17.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">You are an expert in extracting summary and keypoint from articles.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Rules\n<br class=\"ltx_break\">1. Provide a comprehensive summary of the entire article in one paragraph, ensuring that all essential aspects are addressed. Your summary should include key statistics, notable dates, and any significant statements to fully convey the context and content of the news story.\n<br class=\"ltx_break\">2. Please provide one keypoint that summarizes the new event from the article with the following rules:\n<br class=\"ltx_break\">- Focus specifically on events that are newly occurring on the publication date of the article. If the article does not introduce a new event but instead discusses ongoing topics or is about non-news content like advertisements, state ‘No new event reported.’\n<br class=\"ltx_break\">- The point should be concise, accurate and complete, especially for numbers, names and dates.\n<br class=\"ltx_break\">- Basically NO “he, she, they, it, them, etc” are allowed. Please clearly write out the entity you are referencing in the point.\n<br class=\"ltx_break\">- You are not allowed to start with any of the phrases: the article discusses, the article shows, the article emphasizes, the article discusses, the speaker says, the speaker discusses, the author mentions, etc.\n<br class=\"ltx_break\"></span></span>\n<span id=\"A3.F17.2.2\" class=\"ltx_p\"><span id=\"A3.F17.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\"># Examples\n<br class=\"ltx_break\">Here are several examples of extracting keypoints from articles. Note that the articles in different examples are irrelevant.\n<br class=\"ltx_break\">## Example 1:\n<br class=\"ltx_break\">Article: Professional golfer Lexi Thompson has announced her retirement from professional golf at the end of the 2024 season at the age of 29. Thompson, an 11-time LPGA Tour champion, made the announcement ahead of her 18th consecutive US Women’s Open appearance. She turned professional in 2010 and won her first major at the 2014 Kraft Nabisco Championship. Despite enduring injuries that led to a drop in her world ranking, Thompson continued to compete at a high level. In her retirement announcement, Thompson expressed gratitude for the opportunities golf provided her and highlighted her excitement for the next chapter of her life.\n<br class=\"ltx_break\">Publishing Date: 2024-05-29, Wednesday\n<br class=\"ltx_break\">Keypoint: Lexi Thompson announced her retirement from professional golf at the end of the 2024 season at the age of 29.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## Example 2:\n<br class=\"ltx_break\">Article: India’s capital territory of Delhi sweltered to its highest-ever temperature of 49.9 degrees Celsius (121.8 degrees Fahrenheit) on Tuesday, as an oppressive heat wave forced authorities to impose water rationing.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">The Indian Meteorological Department (IMD) said the new record was measured in the suburb of Mungeshpur, surpassing Delhi’s previous high of 49.2 degrees Celsius (120.5 degrees Fahrenheit), observed in May 2022.\n<br class=\"ltx_break\">Publishing Date: 2024-05-29, Wednesday\n<br class=\"ltx_break\">Keypoint: Delhi experienced its highest-ever temperature of 49.9 degrees Celsius on 2024-05-28, Tuesday.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># New Article\n<br class=\"ltx_break\">Based on the provided rules and examples, please summarize the article and identify one key point that concludes the new event on the publishing date.\n<br class=\"ltx_break\">Article: {}\n<br class=\"ltx_break\">Publishing Date: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Output: Output should follow the format of\n<br class=\"ltx_break\">Summary:\n<br class=\"ltx_break\">Keypoint:\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F17.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 17</span>: </span><span id=\"A3.F17.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F17.5.2.1\" class=\"ltx_text ltx_font_italic\">Article Summary</span> step, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F18\" class=\"ltx_figure\">\n<span id=\"A3.F18.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F18.2.1\" class=\"ltx_p\"><span id=\"A3.F18.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">[Setup:] Today is {}. You will be testing people who are from the past, i.e. a week or a month ago. A recently published article will serve as the basis for your questioning. Your objective is to ask them questions that assess the accuracy and plausibility of their predictions about events.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">You will write question-answer pairs:\n<br class=\"ltx_break\">1. The question should challenge the person to predict an outcome or development related to the article’s content as if they were being asked one week or one month ago. Please provide the question that can be answered on {}, but only guessable not answerable before {}.\n<br class=\"ltx_break\">2. The answer MUST be based on factual information from the article. Ensure that the answers do not predict outcomes that have not been explicitly stated in the article. \n<br class=\"ltx_break\">\n<br class=\"ltx_break\">[Rules:] \n<br class=\"ltx_break\">Article: {}. \n<br class=\"ltx_break\">Publishing date: {} \n<br class=\"ltx_break\"></span></span>\n<span id=\"A3.F18.2.2\" class=\"ltx_p\"><span id=\"A3.F18.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Please generate four questions about the above article, along with answers. You should follow the instructions below: \n<br class=\"ltx_break\">1. Please turn the key point “{}” into the question, with focusing more on whether the event will happen.\n<br class=\"ltx_break\">2. The question should NOT be designed for reading comprehension. Please focus more on what happened rather than the implications after the event.\n<br class=\"ltx_break\">3. The question MUST be in future tense. \n<br class=\"ltx_break\">- Start the first question with “Will”, with the answer as “Yes”. \n<br class=\"ltx_break\">- Start the second question with “Will”, with the answer as “No”. \n<br class=\"ltx_break\">- Start the third and fourth questions with a phrase like “What will”, “Who will”, “Where will”, “Which xxx will”, “How much will”, or “How many will”. \n<br class=\"ltx_break\">4. There must be a time element in the question. It can be phrases like “In {} …”, “By {}, …”, “… in {}?”.\n<br class=\"ltx_break\">5. You MUST NOT use unclear implicit time element phrases like “in the future” or “in the upcoming weeks”.\n<br class=\"ltx_break\">6. You should avoid: questions that require numerical reasoning; questions that require substantial world knowledge.\n<br class=\"ltx_break\">7. The answer MUST be short and concise, avoiding using redundant words or repeating the information in the question.\n<br class=\"ltx_break\">8. The question must be grammatically correct and contain the information required to answer. NO “he, she, they, it, them, etc” allowed. Please clearly write out the entity you are referencing in the question.\n<br class=\"ltx_break\">9. The question MUST be able to be answered by the article.\n<br class=\"ltx_break\">10. The question MUST NOT include the information that came out just now. It should be understandable to people from the past. Avoid using “How will” or “Why will” questions, as they imply that the event has already occurred.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">[Suggested questions and questions to avoid are detailed below:]\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">- Keypoint: Delhi experienced its highest-ever temperature of 49.9 degrees Celsius on Tuesday, leading to water rationing due to the oppressive heat wave.\n<br class=\"ltx_break\">- Suggested Question: Will Delhi break the highest temperature record again by May 2024?\n<br class=\"ltx_break\">- Avoid This Question: Will extreme heat events continue to pose a threat to India’s development in the upcoming years?\n<br class=\"ltx_break\">- Reason to Avoid: The time constraint “in the upcoming years” is vague and the question can not be answered based on today’s knowledge.\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F18.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 18</span>: </span><span id=\"A3.F18.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F18.5.2.1\" class=\"ltx_text ltx_font_italic\">QA Generation</span> step (part 1), adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F19\" class=\"ltx_figure\">\n<span id=\"A3.F19.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F19.2.1\" class=\"ltx_p\"><span id=\"A3.F19.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">- Keypoint: Owners of nearly 84,000 older Nissan vehicles in the United States equipped with recalled, unrepaired Takata air bags, including models such as the 2002-2006 Nissan Sentra, are advised by NHTSA to immediately stop driving them due to safety concerns.\n<br class=\"ltx_break\">- Suggested Question: Will the older Nissan vehicles such as the 2002-2006 Nissan Sentra exhibit quality issues by May 2024?\n<br class=\"ltx_break\">- Avoid This Question: Will owners of the 2002-2006 Nissan Sentra, 2002-2004 Nissan Pathfinder, and 2002-2003 Infiniti QX4 heed the NHTSA’s advice to immediately stop driving their vehicles in late May 2024?\n<br class=\"ltx_break\">- Reason to Avoid: This question is overly specific. People from the past would not have known the “NHTSA’s advice”.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">- Keypoint: Children’s sketches of violent scenes, likely made by children aged 5-7 before the eruption of Mt. Vesuvius in 79 AD, have been uncovered at the archaeological park of Pompeii.\n<br class=\"ltx_break\">- Suggested Question: Will children’s sketches of violent scenes be discovered at the archaeological park of Pompeii by May 2024?\n<br class=\"ltx_break\">- Avoid This Question: Will the newly discovered children’s sketches at the archaeological park of Pompeii be available for public viewing by May 2024?\n<br class=\"ltx_break\">- Reason to Avoid: This question includes future events about newly discovered children’s sketches in Pompeii, which wouldn’t be known to a past audience.\n<br class=\"ltx_break\"></span></span>\n<span id=\"A3.F19.2.2\" class=\"ltx_p\"><span id=\"A3.F19.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">- Keypoint: North Korea has been sending “filth and garbage” across the border to South Korea using giant balloons as a new strategy, prompting South Korean authorities to warn of the objects landing in residential areas. The move, according to North Korean state media KCNA, was to retaliate against South Korean activists who often send materials to the North.\n<br class=\"ltx_break\">- Suggested Question: What will North Korea do to retaliate against South Korean activists who often send materials to the North by May 2024?\n<br class=\"ltx_break\">- Avoid This Question: Will North Korea continue using balloons to send items across the border to South Korea by May 2024?\n<br class=\"ltx_break\">- Reason to Avoid: The word “continue” should not be used here. The question MUST NOT include the information that came out just now.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">[Output:] Now please write four clear and concise question-answer pairs following the instructions and examples above. Once again the question should NOT be designed for reading comprehension but of forecasting interests. Also, vague and implicit time elements like “in the future”, “in the upcoming weeks” or “in the coming years” should NOT be used. The question should be able to answer on {}, but only guessable not answerable before {}. You should output the question along with its answer, in the format of \n<br class=\"ltx_break\">’”\n<br class=\"ltx_break\">Question 1: “Will xxx?”\n<br class=\"ltx_break\">Answer 1: Yes.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question 2: “Will xxx?”\n<br class=\"ltx_break\">Answer 2: No.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question 3: Either “What will xxx?”, “Who will xxx?”, “‘Where will xxx?”, “Which xxx will”, “How much will xxx?”, or “How many will xxx?”\n<br class=\"ltx_break\">Answer 3: xxx.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question 4: Either “What will xxx?”, “Who will xxx?”, “Where will xxx?”, “Which xxx will”, “How much will xxx?”, or “How many will xxx?”\n<br class=\"ltx_break\">Answer 4: xxx.\n<br class=\"ltx_break\">’”\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F19.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 19</span>: </span><span id=\"A3.F19.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F19.5.2.1\" class=\"ltx_text ltx_font_italic\">QA Generation</span> step (part 2), adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F20\" class=\"ltx_figure\">\n<span id=\"A3.F20.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F20.2.1\" class=\"ltx_p\"><span id=\"A3.F20.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"># Rules\n<br class=\"ltx_break\">Article: {}\n<br class=\"ltx_break\">Given the article, please generate three noising answers to the given questions, whose correct answers can be obtained from the article. Name the three noising answers as (b), (c) and (d) respectively. While (b), (c) and (d) should all be unambiguously incorrect, they should also make sense and be plausible.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Examples\n<br class=\"ltx_break\">Here are examples showing the output format. This example is NOT related to the noising answers you will generate.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: What will be the annual change in the UK’s Consumer Prices Index (CPI) for November 2021?\n<br class=\"ltx_break\">Correct Answer: ‘Less than 1.7%’\n<br class=\"ltx_break\">Noising Answers: \n<br class=\"ltx_break\">(b) ‘Between 1.7% and 2.2%, inclusive’\n<br class=\"ltx_break\">(c) ‘More than 2.2% but less than 2.9%’\n<br class=\"ltx_break\">(d) ‘2.9% or more’\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: Who will win the 2020 Georgia Democratic primary?\n<br class=\"ltx_break\">Correct Answer: ‘Joe Biden’\n<br class=\"ltx_break\">Noising Answers:\n<br class=\"ltx_break\">(b) ‘Michael Bloomberg’\n<br class=\"ltx_break\">(c) ‘Pete Buttigieg’\n<br class=\"ltx_break\">(d) ‘Someone else’\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: Before July 2020, will it be officially announced that the Tokyo 2020 Summer Olympics and/or Paralympics will be postponed, canceled, and/or relocated?\n<br class=\"ltx_break\">Correct Answer: Yes, the Olympic Games only\n<br class=\"ltx_break\">Noising Answers: \n<br class=\"ltx_break\">(b) ‘Yes, the Paralympic Games only’\n<br class=\"ltx_break\">(c) ‘Yes, both’\n<br class=\"ltx_break\">(d) ‘No’\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Input:\n<br class=\"ltx_break\">Question 1: {}\n<br class=\"ltx_break\">Correct Answer 1: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question 2: {}\n<br class=\"ltx_break\">Correct Answer 2: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Output: Now please generate three noising answers to the question, given the above article, instructions and examples. DO NOT output the backgrounds, the question or any other explanations.\n<br class=\"ltx_break\">Noising Answers 1:\n<br class=\"ltx_break\">(b) xxx.\n<br class=\"ltx_break\">(c) xxx.\n<br class=\"ltx_break\">(d) xxx.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Noising Answers 2:\n<br class=\"ltx_break\">(b) xxx.\n<br class=\"ltx_break\">(c) xxx.\n<br class=\"ltx_break\">(d) xxx.\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F20.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 20</span>: </span><span id=\"A3.F20.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F20.5.2.1\" class=\"ltx_text ltx_font_italic\">Misleading Choices Generation</span> step, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F21\" class=\"ltx_figure\">\n<span id=\"A3.F21.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F21.2.1\" class=\"ltx_p\"><span id=\"A3.F21.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"># Task\n<br class=\"ltx_break\">Please help evaluate the quality of question-answer pairs derived from the given news article. The questions will be presented to someone who has not seen the corresponding news article, in order to evaluate the accuracy and plausibility of the event prediction ability.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Inputs\n<br class=\"ltx_break\">Article: {}\n<br class=\"ltx_break\">Publishing Date: {}\n<br class=\"ltx_break\">Question 1: {}\n<br class=\"ltx_break\">Answer 1: {}\n<br class=\"ltx_break\">Question 2: {}\n<br class=\"ltx_break\">Answer 2: {}\n<br class=\"ltx_break\">Question 3: {}\n<br class=\"ltx_break\">Answer 3: {}\n<br class=\"ltx_break\">Question 4: {}\n<br class=\"ltx_break\">Answer 4: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Scoring Categories\n<br class=\"ltx_break\">## Correctness: Given the above article, please check if the answer is correct to the question with 100% certainty.\n<br class=\"ltx_break\">- 2 points: There is evidence in the article that the answer is correct with 100% certainty.\n<br class=\"ltx_break\">- 1 point: The answer generally aligns with the news facts but has minor inaccuracies or missing details.\n<br class=\"ltx_break\">- 0 point: Significantly misaligned with the news facts.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## Only Answerable on Publishing Date: Imagine traveling back in time to one week before the article’s publishing date ({}). At that time, you are asked the question without having seen this specific article, but you do have access to all earlier news articles. The question should ideally be only guessable—not definitively answerable—based on the information available at that time. That is, the answer should be able to be found in the given article, but it should not be obtainable from earlier articles. Note that past tense descriptions in the article DO NOT INFLUENCE this assessment.\n<br class=\"ltx_break\">- 2 points: The question is answerable on {}, but only guessable not answerable before {}.\n<br class=\"ltx_break\">- 1 point: Could be somewhat predicted before {}, but not with complete certainty.\n<br class=\"ltx_break\">- 0 point: A person (could be anyone, even an expert in the field) would be able to find an article (or many) published before {} that answers the question with 100% certainty.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### 0 point examples\n<br class=\"ltx_break\">Example 1:\n<br class=\"ltx_break\">Question: What will be one of Lexi Thompson’s career highlights in professional golf?\n<br class=\"ltx_break\">Answer: Winning 11 LPGA Tour titles.\n<br class=\"ltx_break\">Reasoning: This question is answerable with prior knowledge and does not test predictive ability related to the publishing date.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## No New Information: Ensure the question does not include new information that only became known on the publishing date, making it understandable for a past audience.\n<br class=\"ltx_break\">- 2 points: No new information from the publishing date are included.\n<br class=\"ltx_break\">- 1 point: Minor new information from the publishing date might be inferred but are not explicitly stated.\n<br class=\"ltx_break\">- 0 point: Includes clear new information from the publishing date, unsuitable for past understanding.\n\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### 0 point examples\n<br class=\"ltx_break\">Example 1:\n<br class=\"ltx_break\">Question: Will owners of the 2002-2006 Nissan Sentra, 2002-2004 Nissan Pathfinder, and 2002-2003 Infiniti QX4 heed the NHTSA’s advice to immediately stop driving their vehicles in late May 2024?\n<br class=\"ltx_break\">Reasoning: This question contains new information on the publishing date. People from the past would not have known the “NHTSA’s advice”.\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F21.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 21</span>: </span><span id=\"A3.F21.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F21.5.2.1\" class=\"ltx_text ltx_font_italic\">QA Filtering</span> step (part 1).</span></figcaption>\n</figure>\n<figure id=\"A3.F22\" class=\"ltx_figure\">\n<span id=\"A3.F22.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F22.2.1\" class=\"ltx_p\"><span id=\"A3.F22.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Example 2:\n<br class=\"ltx_break\">Question: “What will Lexi Thompson’s ranking be at the time of her retirement announcement in May 2024?”\n<br class=\"ltx_break\">Reasoning: This question contains the information that Lexi will annouce her retirement, which is not known to the people from the past.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Example 3:\n<br class=\"ltx_break\">Question: “Will the newly discovered children’s sketches at the archaeological park of Pompeii be available for public viewing by May 2024?”\n<br class=\"ltx_break\">Reasoning: This question includes future events about newly discovered children’s sketches in Pompeii, which wouldn’t be known to a past audience\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## Objectiveness: The answer should not rely more on the author’s personal views than on objective facts.\n<br class=\"ltx_break\">- 2 points: Completely objective, based strictly on reported facts.\n<br class=\"ltx_break\">- 1 point: Primarily objective, with minor subjective interpretations.\n<br class=\"ltx_break\">- 0 point: Largely subjective or opinion-based, lacking a factual basis.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## Clear Time Element: This category checks if the question has a clear element in it, without having vague phrases like “in the future” or “in the upcoming weeks”.\n<br class=\"ltx_break\">- 2 points: The question has clear time elements, like “by May 2024” or “in July 2023”.\n<br class=\"ltx_break\">- 1 point: The question includes a general timeframe, like “next month” or “this winter,” which allows for some estimation but lacks precise dates.\n<br class=\"ltx_break\">- 0 point: The question includes vague time phrases like “in the future” or “in the upcoming weeks,” which do not specify a clear or precise timeframe.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### 0 point examples\n<br class=\"ltx_break\">Example 1:\n<br class=\"ltx_break\">Question: Will extreme heat events continue to pose a threat to India’s development in the upcoming years?\n<br class=\"ltx_break\">Reasoning: The time constraint “in the upcoming years” is vague.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Example 2: \n<br class=\"ltx_break\">Question: “What will Illinois require from parents who monetize their children’s online activities starting in July?”\n<br class=\"ltx_break\">Reasoning: The mention of “July” specifies only the month and lacks the necessary detail of the year.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## Public Interest: Determine if the question addresses a topic of public concern.\n<br class=\"ltx_break\">- 2 points: The question covers a topic that widely affects or interests the public.\n<br class=\"ltx_break\">- 1 point: The question is of moderate interest, relevant to specific groups.\n<br class=\"ltx_break\">- 0 point: The topic is overly personal or localized, lacking relevance to the broader public.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### 0 point examples\n<br class=\"ltx_break\">Example 1:\n<br class=\"ltx_break\">Question: Will the exhibition ‘Fragile Beauty’ at London’s Victoria &amp; Albert Museum include both midcentury and contemporary works in May 2024?\n<br class=\"ltx_break\">Reasoning: The specific details of an personal art exhibition’s contents are generally of limited public interest.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">## Answer Not Too Obvious: This category evaluates whether the answer to a question is too predictable or straightforward based on the question itself.\n<br class=\"ltx_break\">- 2 points: The answer provides new or non-obvious insights, requiring additional context or understanding not explicit in the question.\n<br class=\"ltx_break\">- 1 point: The answer is somewhat predictable but includes minor additional information or a slight twist.\n<br class=\"ltx_break\">- 0 point: The answer directly restates or closely mirrors the question, offering no new details or insights.\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F22.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 22</span>: </span><span id=\"A3.F22.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F22.5.2.1\" class=\"ltx_text ltx_font_italic\">QA Filtering</span> step (part 2).</span></figcaption>\n</figure>\n<figure id=\"A3.F23\" class=\"ltx_figure\">\n<span id=\"A3.F23.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F23.2.1\" class=\"ltx_p\"><span id=\"A3.F23.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">### 0 point examples\n<br class=\"ltx_break\">Example 1:\n<br class=\"ltx_break\">Question: What will New York officials do to ensure safety for the ICC Men’s T20 Cricket World Cup following global threats from ISIS-K?\n<br class=\"ltx_break\">Answer: New York officials will implement increased safety precautions for the event.\nReasoning: The answer is straightforward and expected, as it directly restates the premise of the question without providing any new or specific details on how the safety precautions will be implemented or what they might entail.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Instructions\n<br class=\"ltx_break\">Evaluate each question-answer pair by assigning points in each of the categories based on the criteria provided. Please be strict on giving points. If the requirements of a category are not fulfilled, assign a point of 0.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"># Please strictly follow this output template:\n<br class=\"ltx_break\">*Question 1*\n<br class=\"ltx_break\">## Correctness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Only Answerable on Publishing Date\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## No New Information\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Objectiveness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Clear Time Element\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Public Interest\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Answer Not Too Obvious\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">*Question 2*\n<br class=\"ltx_break\">## Correctness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n## Only Answerable on Publishing Date\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## No New Information\n<br class=\"ltx_break\">- Reasoning:\n- Point:\n<br class=\"ltx_break\">## Objectiveness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Clear Time Element\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\"></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F23.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 23</span>: </span><span id=\"A3.F23.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F23.5.2.1\" class=\"ltx_text ltx_font_italic\">QA Filtering</span> step (part 3).</span></figcaption>\n</figure>\n<figure id=\"A3.F24\" class=\"ltx_figure\">\n<span id=\"A3.F24.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F24.2.1\" class=\"ltx_p\"><span id=\"A3.F24.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">## Public Interest\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Answer Not Too Obvious\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">*Question 3*\n<br class=\"ltx_break\">## Correctness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Only Answerable on Publishing Date\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## No New Information\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Objectiveness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Clear Time Element\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Public Interest\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Answer Not Too Obvious\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">*Question 4*\n<br class=\"ltx_break\">## Correctness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Only Answerable on Publishing Date\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## No New Information\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Objectiveness\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Clear Time Element\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Public Interest\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:\n<br class=\"ltx_break\">## Answer Not Too Obvious\n<br class=\"ltx_break\">- Reasoning:\n<br class=\"ltx_break\">- Point:</span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F24.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 24</span>: </span><span id=\"A3.F24.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt in the <span id=\"A3.F24.5.2.1\" class=\"ltx_text ltx_font_italic\">QA Filtering</span> step (part 4).</span></figcaption>\n</figure>\n<figure id=\"A3.F25\" class=\"ltx_figure\">\n<span id=\"A3.F25.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F25.2.1\" class=\"ltx_p\"><span id=\"A3.F25.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Prompt:<span id=\"A3.F25.2.1.1.1\" class=\"ltx_text ltx_font_medium\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"></span></span></span>\n<span id=\"A3.F25.2.2\" class=\"ltx_p\"><span id=\"A3.F25.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">User Prompt:<span id=\"A3.F25.2.2.1.1\" class=\"ltx_text ltx_font_medium\"> You should output your answer as either ‘Yes’ or ‘No’ WITHOUT anything else.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: {}\n<br class=\"ltx_break\">Choices: ‘Yes’ or ‘No’\n<br class=\"ltx_break\">[Output:] Your answer:</span></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F25.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 25</span>: </span><span id=\"A3.F25.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Closed-book evaluation prompt for TF questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F26\" class=\"ltx_figure\">\n<span id=\"A3.F26.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F26.2.1\" class=\"ltx_p\"><span id=\"A3.F26.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Prompt:<span id=\"A3.F26.2.1.1.1\" class=\"ltx_text ltx_font_medium\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"></span></span></span>\n<span id=\"A3.F26.2.2\" class=\"ltx_p\"><span id=\"A3.F26.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">User Prompt:<span id=\"A3.F26.2.2.1.1\" class=\"ltx_text ltx_font_medium\"> You should output your answer as either ‘(a)’, ‘(b)’, ‘(c)’ or ‘(d)’ WITHOUT anything else.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: {}\n<br class=\"ltx_break\">Choices: \n<br class=\"ltx_break\">(a) {}\n<br class=\"ltx_break\">(b) {}\n<br class=\"ltx_break\">(c) {}\n<br class=\"ltx_break\">(d) {}\n<br class=\"ltx_break\">[Output:] Your answer:</span></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F26.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 26</span>: </span><span id=\"A3.F26.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Closed-book evaluation prompt for MC questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F27\" class=\"ltx_figure\">\n<span id=\"A3.F27.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F27.2.1\" class=\"ltx_p\"><span id=\"A3.F27.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Prompt:<span id=\"A3.F27.2.1.1.1\" class=\"ltx_text ltx_font_medium\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"></span></span></span>\n<span id=\"A3.F27.2.2\" class=\"ltx_p\"><span id=\"A3.F27.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">User Prompt:<span id=\"A3.F27.2.2.1.1\" class=\"ltx_text ltx_font_medium\"> You should output your answer as either ‘Yes’ or ‘No’ WITHOUT anything else. Below are the top 5 relevant news article fragments retrieved for the question, which may or may not assist you in making a forecast.\n<br class=\"ltx_break\">Article 1: {} \n<br class=\"ltx_break\">Article 2: {} \n<br class=\"ltx_break\">Article 3: {}\n<br class=\"ltx_break\">Article 4: {}\n<br class=\"ltx_break\">Article 5: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: {}\n<br class=\"ltx_break\">Choices: ‘Yes’ or ‘No’\n<br class=\"ltx_break\">[Output:] Your answer:</span></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F27.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 27</span>: </span><span id=\"A3.F27.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Constrained open-book evaluation prompt for TF questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F28\" class=\"ltx_figure\">\n<span id=\"A3.F28.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F28.2.1\" class=\"ltx_p\"><span id=\"A3.F28.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Prompt:<span id=\"A3.F28.2.1.1.1\" class=\"ltx_text ltx_font_medium\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\"></span></span></span>\n<span id=\"A3.F28.2.2\" class=\"ltx_p\"><span id=\"A3.F28.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">User Prompt:<span id=\"A3.F28.2.2.1.1\" class=\"ltx_text ltx_font_medium\"> You should output your answer as either ‘(a)’, ‘(b)’, ‘(c)’ or ‘(d)’ WITHOUT anything else. Below are the top 5 relevant news article fragments retrieved for the question, which may or may not assist you in making a forecast.\n<br class=\"ltx_break\">Article 1: {}\n<br class=\"ltx_break\">Article 2: {}\n<br class=\"ltx_break\">Article 3: {}\n<br class=\"ltx_break\">Article 4: {}\n<br class=\"ltx_break\">Article 5: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: {}\n<br class=\"ltx_break\">Choices: \n<br class=\"ltx_break\">(a) {}\n<br class=\"ltx_break\">(b) {}\n<br class=\"ltx_break\">(c) {}\n<br class=\"ltx_break\">(d) {}\n<br class=\"ltx_break\">[Output:] Your answer:</span></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F28.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 28</span>: </span><span id=\"A3.F28.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Constrained open-book evaluation prompt for MC questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F29\" class=\"ltx_figure\">\n<span id=\"A3.F29.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F29.2.1\" class=\"ltx_p\"><span id=\"A3.F29.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Prompt:<span id=\"A3.F29.2.1.1.1\" class=\"ltx_text ltx_font_medium\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"></span>User Prompt:<span id=\"A3.F29.2.1.1.2\" class=\"ltx_text ltx_font_medium\"> You should output your answer as either ‘Yes’ or ‘No’ WITHOUT anything else. Below is the updated news article relevant to the question, which may help you in providing an answer.\n<br class=\"ltx_break\">Article: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: {}\n<br class=\"ltx_break\">Choices: ‘Yes’ or ‘No’\n<br class=\"ltx_break\">[Output:] Your answer:</span></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F29.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 29</span>: </span><span id=\"A3.F29.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Gold article evaluation prompt for TF questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F30\" class=\"ltx_figure\">\n<span id=\"A3.F30.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F30.2.1\" class=\"ltx_p\"><span id=\"A3.F30.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System Prompt:<span id=\"A3.F30.2.1.1.1\" class=\"ltx_text ltx_font_medium\"> You’re an expert in forecasting events. You will NEVER refuse to answer a forecasting question by saying “I cannot predict the future”, even if without 100% certainty.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\"></span>User Prompt:<span id=\"A3.F30.2.1.1.2\" class=\"ltx_text ltx_font_medium\"> You should output your answer as either ‘(a)’, ‘(b)’, ‘(c)’ or ‘(d)’ WITHOUT anything else. Below is the updated news article relevant to the question, which may help you in providing an answer.\n<br class=\"ltx_break\">Article: {}\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">Question: {}\n<br class=\"ltx_break\">Choices: \n<br class=\"ltx_break\">(a) {}\n<br class=\"ltx_break\">(b) {}\n<br class=\"ltx_break\">(c) {}\n<br class=\"ltx_break\">(d) {}\n<br class=\"ltx_break\">[Output:] Your answer:</span></span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F30.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 30</span>: </span><span id=\"A3.F30.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Gold article evaluation prompt for MC questions, adapted from <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al., (<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<figure id=\"A3.F31\" class=\"ltx_figure\">\n<span id=\"A3.F31.2\" class=\"ltx_inline-block ltx_align_center ltx_framed ltx_framed_rectangle\" style=\"border-color: #000000;\">\n<span id=\"A3.F31.2.1\" class=\"ltx_p\"><span id=\"A3.F31.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Question: {}\n<br class=\"ltx_break\">Options:\n<br class=\"ltx_break\">- Science &amp; Tech\n<br class=\"ltx_break\">- Healthcare &amp; Biology\n<br class=\"ltx_break\">- Economics &amp; Business\n<br class=\"ltx_break\">- Environment &amp; Energy\n<br class=\"ltx_break\">- Politics &amp; Governance\n<br class=\"ltx_break\">- Education &amp; Research\n<br class=\"ltx_break\">- Arts &amp; Recreation\n<br class=\"ltx_break\">- Security &amp; Defense\n<br class=\"ltx_break\">- Social Sciences\n<br class=\"ltx_break\">- Sports\n<br class=\"ltx_break\">- Other\n<br class=\"ltx_break\">Instruction: Assign a category for the given question.\n<br class=\"ltx_break\">Rules:\n<br class=\"ltx_break\">1. Make sure you only return one of the options from the option list.\n<br class=\"ltx_break\">2. Only output the category, and do not output any other words in your response.\n<br class=\"ltx_break\">3. You have to pick a string from the above categories.\n<br class=\"ltx_break\">Answer:</span></span>\n</span>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F31.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 31</span>: </span><span id=\"A3.F31.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Prompt to categorize the generated questions, taken from <cite class=\"ltx_cite ltx_citemacro_citet\">Halawi et al., (<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2411.08324",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:29.833Z"
}
