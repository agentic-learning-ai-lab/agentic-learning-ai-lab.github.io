{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing (NLP) tasks by leveraging knowledge acquired during large-scale pretraining <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al.,, <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2020</a>; Grattafiori et al.,, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2024</a>; Jiang et al.,, <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. These models can adapt to new tasks using only a few input and output examples provided in context, a process known as In-Context Learning (ICL) <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al.,, <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. However, ICL often struggles with complex reasoning or domain shifts, as it relies solely on a forward pass to interpret the examples. While methods like Test-Time Training (TTT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Akyürek et al.,, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> have shown that effective adaptation is possible with limited data, they can still be computationally expensive. This highlights the need for more efficient and effective approaches to task adaptation in LLMs.</p>\n</div>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Contrary to ICL’s reliance on a forward pass, prompt-based adaptation methods like Prompt Tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Lester et al.,, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> and Prefix Tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Li and Liang,, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> prepend a set of trainable vectors to each example input and optimize them via gradient descent. At a conceptual level, ICL harnesses the model’s ability to extract task-relevant information from the context of few-shot examples, while prompt-based adaptation methods optimize randomly initialized vectors to guide the model’s behavior in solving each example. Given these complementary approaches, it is natural to ask whether we can bridge them by directly optimizing the context of few-shot examples to steer the model more effectively.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">In this work, we introduce <span class=\"ltx_text ltx_font_italic\">Context Tuning</span>, a simple and effective method for few-shot learning that initializes trainable vectors from the few-shot examples of a novel task, then optimizes them to solve each example. We develop two variants: <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>, which applies Prompt Tuning to a soft prompt initialized from few-shot examples, and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, which applies Prefix Tuning to optimize the key-value (KV) cache derived from those same examples. While <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> achieves strong performance, it suffers from a quadratic training-time cost in the number of examples. Similarly, the recently proposed Test-Time Training (TTT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Akyürek et al.,, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> method, which fine-tunes model parameters with LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et al.,, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> on permutations of few-shot examples, also incurs quadratic cost. In contrast, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> achieves linear training time complexity while also outperforming <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and achieving competitive performance to TTT, thanks to the efficiency and per-layer conditioning of the KV cache. In addition, because <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> tunes the context and TTT tunes the model, the two methods are complementary: applying <span class=\"ltx_text ltx_font_italic\">CT-KV</span> to refine the model context after TTT’s weight updates leads to additional performance gains. A high-level comparison in Figure <a href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates <span class=\"ltx_text ltx_font_italic\">CT-KV</span>’s high efficiency and accuracy, whether used alone or in combination with TTT.</p>\n</div>\n<figure id=\"S1.F1\" class=\"ltx_figure ltx_align_floatright\"><img src=\"./assets/x1.png\" id=\"S1.F1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"299\" height=\"186\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>\nComparison of training-free, prompt-based adaptation, and <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> methods on solving 26 NLP-LR tasks from Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_text\" style=\"color:#0000FF;\">Circles</span> are baselines; <span class=\"ltx_text\" style=\"color:#FF0000;\">stars</span> are our methods; <span class=\"ltx_text ltx_font_bold\">bolded</span> methods attain the best performance-efficiency tradeoff.\n</figcaption>\n</figure>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">We situate these two approaches for few-shot learning with in-context examples – TTT that optimizes the model itself, and <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> that optimizes the model’s context – within a broader framework we term <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> (<span class=\"ltx_text ltx_font_italic\">ICO</span>). Under this framework, adaptation leverages the LLM’s ICL ability and either updates its parameters or its context representation. We evaluate ICL, prompt-based adaptation methods, and <span class=\"ltx_text ltx_font_italic\">ICO</span> techniques across a wide range of natural language and symbolic reasoning benchmarks, including CrossFit <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al.,, <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>, UnifiedQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Khashabi et al.,, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, BIG-Bench Hard (BBH) <cite class=\"ltx_cite ltx_citemacro_citep\">(Srivastava et al.,, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2023</a>; Suzgun et al.,, <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, MMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al.,, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>, and the Abstraction and Reasoning Corpus (ARC) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">Chollet, 2019b, </a>)</cite>. <span class=\"ltx_text ltx_font_italic\">CT-KV</span> significantly outperforms both ICL and prompt-based adaptation methods, while remaining competitive with the more computationally intensive TTT approach. Furthermore, we show that <span class=\"ltx_text ltx_font_italic\">CT-KV</span> can serve as a post-hoc refinement step following TTT, leading to improved few-shot adaptation performance compared to either method used in isolation.</p>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n\n<section id=\"S2.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Prompt-Based Adaptation.</h4>\n\n<div id=\"S2.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Prompt-Based Adaptation steers pretrained language models to solve downstream tasks by learning task-specific inputs while keeping the model weights frozen. AutoPrompt <cite class=\"ltx_cite ltx_citemacro_citep\">(Shin et al.,, <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> was an early method that constructed discrete prompts via gradient-based search. Prefix Tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Li and Liang,, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> introduced trainable continuous vectors that serve as a prefix to the model’s key-value cache at each layer, achieving strong performance on generation tasks with only a small number of trainable parameters. P-Tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">Liu et al., 2022b, </a>)</cite> appended soft prompts to the input and used an LSTM-based prompt encoder to model dependencies between prompt tokens. Prompt Tuning <cite class=\"ltx_cite ltx_citemacro_citep\">(Lester et al.,, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> simplified the approach by learning soft prompts solely at the input layer and demonstrated that performance improves with model scale. P-Tuning v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">Liu et al., 2022a, </a>)</cite> provided an optimized implementation of Prefix Tuning and extended it to natural language understanding tasks. While these works typically initialize their learnable prompts using high-level task instructions, random tokens, or unrelated words, <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> leverages the pretrained LLM’s ability to extract meaningful task-specific information directly from in-context demonstration pairs. Finally, <cite class=\"ltx_cite ltx_citemacro_citet\">Singhal et al., (<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> proposed Instruction Prompt Tuning, in which expert-curated few-shot demonstrations are prepended to a learned soft prompt. In contrast, <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> draws demonstration pairs directly from the dataset and uses them to initialize the prompt rather than prepending them as input.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">In-Context Learning.</h4>\n\n<div id=\"S2.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Introduced by <cite class=\"ltx_cite ltx_citemacro_citet\">Radford et al., (<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>, ICL has become a defining feature of large language models (LLMs), enabling them to perform novel tasks by conditioning on a few input-output demonstrations without any parameter updates. This behavior has been leveraged through various prompting strategies, such as Chain-of-Thought prompting to elicit reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al.,, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> and self-consistency decoding to reduce variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al.,, <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. Prior work has also explored selecting informative demonstrations <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al.,, <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2021</a>; Li and Qiu,, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, as well as meta-training over large sets of tasks to improve ICL generalization and inference-time efficiency <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">Min et al., 2022a, </a>; Chen et al.,, <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2022</a>; Muhtar et al.,, <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>. From a theoretical perspective, <cite class=\"ltx_cite ltx_citemacro_citet\">Dai et al., (<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Deutch et al., (<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> interpret ICL as performing implicit gradient descent; <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao, (<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> conceptualizes it as contextual retrieval within an associative memory framework; and <cite class=\"ltx_cite ltx_citemacro_citet\">Garg et al., (<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> demonstrates that transformers trained from scratch can learn complex function classes in-context. While these findings highlight ICL’s potential, recent studies <cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">Min et al., 2022b </a></cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Jang et al., (<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> show that LLMs often only rely on superficial patterns in the demonstrations rather than learning the underlying task. Our work further investigates these limitations by analyzing the intermediate key-value (KV) cache extracted from demonstration pairs in Section <a href=\"#S5.SS7\" title=\"5.7 Why does CT-KV Outperform ICL? ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.7</span></a>, showing that it fails to encode sufficient task information and addresses this shortcoming through gradient optimization.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Inference-Time Optimization.</h4>\n\n<div id=\"S2.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Our framework, <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span>, contributes to a broader class of methods that adapt models or their internal representations at inference time. Originally applied to object recognition <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et al.,, <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2020</a>; Gandelsman et al.,, <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, test-time training has since shown strong results in language modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Hardt and Sun,, <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, video generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Dalal et al.,, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, controllable language generation <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">Liu et al., 2024b, </a>)</cite>, and abstract reasoning <cite class=\"ltx_cite ltx_citemacro_citep\">(Bonnet and Macfarlane,, <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>. In diffusion models <cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et al.,, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2020</a>; Rombach et al.,, <a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, techniques such as classifier guidance and classifier-free guidance <cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal and Nichol,, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2021</a>; Ho,, <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> steer generation by optimizing intermediate outputs during sampling. These methods have enabled controllable text-to-image synthesis <cite class=\"ltx_cite ltx_citemacro_citep\">(Nichol et al.,, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, adjustable aesthetic attributes <cite class=\"ltx_cite ltx_citemacro_citep\">(Wallace et al.,, <a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, and improved sample diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al.,, <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>. More recently, <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> proposed test-time training of LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et al.,, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> parameters for ICL using a leave-one-out strategy, achieving state-of-the-art performance on the Abstraction and Reasoning Corpus (ARC) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">Chollet, 2019b, </a>; <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">Chollet, 2019a, </a>)</cite>. In contrast, <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> tunes a soft prompt or continuous prefix rather than updating model weights, and we evaluate it on a broader range of ICL tasks.</p>\n</div>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Background</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We introduce the mathematical formulation of ICL, Prefix Tuning, and Prompt Tuning. To set up the problem of single-task few-shot adaptation, we consider a language model <math id=\"S3.p1.m1\" class=\"ltx_Math\" alttext=\"p_{\\phi}\" display=\"inline\"><semantics><msub><mi>p</mi><mi>ϕ</mi></msub><annotation encoding=\"application/x-tex\">p_{\\phi}</annotation></semantics></math> with parameters <math id=\"S3.p1.m2\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics><mi>ϕ</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math>, <math id=\"S3.p1.m3\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> hidden dimensions, <math id=\"S3.p1.m4\" class=\"ltx_Math\" alttext=\"L\" display=\"inline\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> layers, a demonstration set</p>\n<table id=\"S3.Ex1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.Ex1.m1\" class=\"ltx_Math\" alttext=\"\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{k},\" display=\"block\"><semantics><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{k},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">and the goal of solving a new query <math id=\"S3.p1.m5\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math> from the same task. We denote the concatenated context of all demonstration pairs as <math id=\"S3.p1.m6\" class=\"ltx_Math\" alttext=\"\\mathcal{C}=[x_{1};y_{1};\\dots;x_{k};y_{k}]\" display=\"inline\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>;</mo><msub><mi>y</mi><mn>1</mn></msub><mo>;</mo><mi mathvariant=\"normal\">…</mi><mo>;</mo><msub><mi>x</mi><mi>k</mi></msub><mo>;</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}=[x_{1};y_{1};\\dots;x_{k};y_{k}]</annotation></semantics></math>.</p>\n</div>\n<section id=\"S3.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">In-Context Learning.</h4>\n\n<div id=\"S3.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">ICL concatenates all <math id=\"S3.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstration pairs followed by the query <math id=\"S3.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math>. The model then predicts <math id=\"S3.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"\\hat{y}_{q}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>q</mi></msub><annotation encoding=\"application/x-tex\">\\hat{y}_{q}</annotation></semantics></math> conditioned on this context:</p>\n<table id=\"S3.Ex2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.Ex2.m1\" class=\"ltx_math_unparsed\" alttext=\"\\hat{y}_{q}=\\arg\\max_{y}\\;p_{\\phi}\\bigl{(}y\\bigm{|}[\\,\\mathcal{C};x_{q}\\,]\\bigr{)}.\" display=\"block\"><semantics><mrow><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>q</mi></msub><mo>=</mo><mi>arg</mi><munder><mi>max</mi><mi>y</mi></munder><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mi>y</mi><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mo>;</mo><msub><mi>x</mi><mi>q</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y}_{q}=\\arg\\max_{y}\\;p_{\\phi}\\bigl{(}y\\bigm{|}[\\,\\mathcal{C};x_{q}\\,]\\bigr{)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">In ICL, there is no gradient-based optimization; instead, the model adapts by attending to the tokens of the demonstration pairs provided in context.</p>\n</div>\n</section>\n<section id=\"S3.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Prompt Tuning.</h4>\n\n<div id=\"S3.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In Prompt Tuning, the model parameters <math id=\"S3.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics><mi>ϕ</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math> remain fixed. Instead, <math id=\"S3.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> trainable soft prompt tokens <math id=\"S3.SS0.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"P\" display=\"inline\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> are prepended to each input and optimized via gradient descent:</p>\n<table id=\"S3.E1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E1.m1\" class=\"ltx_math_unparsed\" alttext=\"P^{*}=\\arg\\min_{P}\\sum_{i=1}^{k}-\\log p_{\\phi}\\bigl{(}y_{i}\\bigm{|}[\\,P;x_{i}\\,]\\bigr{)}.\" display=\"block\"><semantics><mrow><msup><mi>P</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><munder><mi>min</mi><mi>P</mi></munder><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo lspace=\"0em\">−</mo><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mi>P</mi><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">P^{*}=\\arg\\min_{P}\\sum_{i=1}^{k}-\\log p_{\\phi}\\bigl{(}y_{i}\\bigm{|}[\\,P;x_{i}\\,]\\bigr{)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">After optimizing on the demonstration pairs, the optimized soft prompt <math id=\"S3.SS0.SSS0.Px2.p1.m4\" class=\"ltx_Math\" alttext=\"P^{*}\" display=\"inline\"><semantics><msup><mi>P</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">P^{*}</annotation></semantics></math> can be used for inference:</p>\n<table id=\"S3.Ex3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.Ex3.m1\" class=\"ltx_math_unparsed\" alttext=\"\\hat{y}_{q}=\\arg\\max_{y}\\;p_{\\phi}\\bigl{(}y\\bigm{|}[\\,P^{*};x_{q}\\,]\\bigr{)}.\" display=\"block\"><semantics><mrow><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>q</mi></msub><mo>=</mo><mi>arg</mi><munder><mi>max</mi><mi>y</mi></munder><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mi>y</mi><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><msup><mi>P</mi><mo>∗</mo></msup><mo>;</mo><msub><mi>x</mi><mi>q</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y}_{q}=\\arg\\max_{y}\\;p_{\\phi}\\bigl{(}y\\bigm{|}[\\,P^{*};x_{q}\\,]\\bigr{)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"S3.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Prefix Tuning.</h4>\n\n<div id=\"S3.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Prefix Tuning also keeps <math id=\"S3.SS0.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics><mi>ϕ</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math> fixed but learns layer-wise prefixes of <math id=\"S3.SS0.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> trainable vectors for the keys and values in each transformer layer:</p>\n<table id=\"S3.Ex4\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.Ex4.m1\" class=\"ltx_Math\" alttext=\"\\Theta=\\{K_{j},V_{j}\\}_{j=1}^{L}.\" display=\"block\"><semantics><mrow><mrow><mi mathvariant=\"normal\">Θ</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>K</mi><mi>j</mi></msub><mo>,</mo><msub><mi>V</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\Theta=\\{K_{j},V_{j}\\}_{j=1}^{L}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">Each layer’s attention uses these prefixes by prepending <math id=\"S3.SS0.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"K_{j}\" display=\"inline\"><semantics><msub><mi>K</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">K_{j}</annotation></semantics></math> to its keys and <math id=\"S3.SS0.SSS0.Px3.p1.m4\" class=\"ltx_Math\" alttext=\"V_{j}\" display=\"inline\"><semantics><msub><mi>V</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">V_{j}</annotation></semantics></math> to its values. The prefixes are optimized to minimize</p>\n<table id=\"S3.E2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E2.m1\" class=\"ltx_math_unparsed\" alttext=\"\\Theta^{*}=\\arg\\min_{\\Theta}\\sum_{i=1}^{k}-\\log p_{\\phi}\\bigl{(}y_{i}\\bigm{|}[\\,\\Theta;x_{i}\\,]\\bigr{)}.\" display=\"block\"><semantics><mrow><msup><mi mathvariant=\"normal\">Θ</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><munder><mi>min</mi><mi mathvariant=\"normal\">Θ</mi></munder><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo lspace=\"0em\">−</mo><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mi mathvariant=\"normal\">Θ</mi><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\Theta^{*}=\\arg\\min_{\\Theta}\\sum_{i=1}^{k}-\\log p_{\\phi}\\bigl{(}y_{i}\\bigm{|}[\\,\\Theta;x_{i}\\,]\\bigr{)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">After obtaining <math id=\"S3.SS0.SSS0.Px3.p1.m5\" class=\"ltx_Math\" alttext=\"\\Theta^{*}\" display=\"inline\"><semantics><msup><mi mathvariant=\"normal\">Θ</mi><mo>∗</mo></msup><annotation encoding=\"application/x-tex\">\\Theta^{*}</annotation></semantics></math>, inference on the query <math id=\"S3.SS0.SSS0.Px3.p1.m6\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math> proceeds analogously to Prompt Tuning.</p>\n</div>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Context Tuning for In-Context Optimization</h2>\n\n<figure id=\"S4.F2\" class=\"ltx_figure\"><img src=\"./assets/x2.png\" id=\"S4.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"253\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>\n<span class=\"ltx_text ltx_font_italic\">CT-KV</span>, the variant of <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> that optimizes the key-value prefixes derived from in-context demonstration pairs. <span class=\"ltx_text ltx_font_italic\">CT-KV</span> (left) first initializes a prefix <math id=\"S4.F2.m7\" class=\"ltx_Math\" alttext=\"\\{K_{i},V_{i}\\}_{i=1}^{k}\" display=\"inline\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>K</mi><mi>i</mi></msub><mo>,</mo><msub><mi>V</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{i},V_{i}\\}_{i=1}^{k}</annotation></semantics></math> from demonstration pairs <math id=\"S4.F2.m8\" class=\"ltx_Math\" alttext=\"\\{(x_{i},y_{i})\\}_{i=1}^{k}\" display=\"inline\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i},y_{i})\\}_{i=1}^{k}</annotation></semantics></math>, then trains it to solve each pair. To prevent the model from simply retrieving the demonstration pair from the prefix, Leave-One-Out Masking prevents the model from attending to <math id=\"S4.F2.m9\" class=\"ltx_Math\" alttext=\"K_{i},V_{i}\" display=\"inline\"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub><mo>,</mo><msub><mi>V</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">K_{i},V_{i}</annotation></semantics></math> when solving pair <math id=\"S4.F2.m10\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. At generation time (right), the model conditions on all optimized prefixes <math id=\"S4.F2.m11\" class=\"ltx_Math\" alttext=\"\\{K_{i}^{*},V_{i}^{*}\\}_{i=1}^{k}\" display=\"inline\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>K</mi><mi>i</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>V</mi><mi>i</mi><mo>∗</mo></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><annotation encoding=\"application/x-tex\">\\{K_{i}^{*},V_{i}^{*}\\}_{i=1}^{k}</annotation></semantics></math> to solve query <math id=\"S4.F2.m12\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math>.\n</figcaption>\n</figure>\n<div id=\"S4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In this section, we introduce the mathematical formulation of <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> (<span class=\"ltx_text ltx_font_italic\">ICO</span>), a few-shot adaptation scheme that uses demonstrations in the context and performs gradient-based optimization on either the model parameters or a context representation. We then show that Test-Time Training (TTT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Akyürek et al.,, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> is an instance of <span class=\"ltx_text ltx_font_italic\">ICO</span>. Finally, we present Context Tuning, formalizing its <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span> variants along with the two additional design choices that drive their strong performance.</p>\n</div>\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>In-Context Optimization</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">To combine the strengths of supervised fine-tuning and LLMs’ inherent ability to learn from context, <span class=\"ltx_text ltx_font_italic\">ICO</span> unifies two prevalent techniques for few-shot learning: ICL and gradient-based optimization. Formally, the objective of <span class=\"ltx_text ltx_font_italic\">ICO</span> to minimize the loss</p>\n<table id=\"S4.E3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E3.m1\" class=\"ltx_math_unparsed\" alttext=\"\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\theta_{\\mathrm{context}}^{(i)}\\,;x_{i}\\,]\\right),\" display=\"block\"><semantics><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo lspace=\"0em\">−</mo><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><msubsup><mi>θ</mi><mi>context</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\theta_{\\mathrm{context}}^{(i)}\\,;x_{i}\\,]\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS1.p1.m1\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}^{(i)}\" display=\"inline\"><semantics><msubsup><mi>θ</mi><mi>context</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}^{(i)}</annotation></semantics></math> is a context representation derived from the set of demonstration pairs <math id=\"S4.SS1.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{k}\" display=\"inline\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{k}</annotation></semantics></math>. One may notice that this objective resembles Equations <a href=\"#S3.E1\" title=\"Equation 1 ‣ Prompt Tuning. ‣ 3 Background ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a href=\"#S3.E2\" title=\"Equation 2 ‣ Prefix Tuning. ‣ 3 Background ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> because traditional prompt-based adaptation methods also prepend additional contexts to inputs during optimization. Still, these contexts are randomly initialized instead of utilizing the demonstration pairs <math id=\"S4.SS1.p1.m3\" class=\"ltx_Math\" alttext=\"\\mathcal{D}\" display=\"inline\"><semantics><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>. Therefore, Prompt Tuning and Prefix Tuning are not instances of <span class=\"ltx_text ltx_font_italic\">ICO</span> by definition. Furthermore, since ICL does not perform gradient-based optimization at all, it also does not fall under <span class=\"ltx_text ltx_font_italic\">ICO</span>.</p>\n</div>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Test-Time Training as ICO</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">TTT <cite class=\"ltx_cite ltx_citemacro_citep\">(Akyürek et al.,, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> can be viewed as an instance of <span class=\"ltx_text ltx_font_italic\">ICO</span>. Specifically, TTT minimizes Equation <a href=\"#S4.E3\" title=\"Equation 3 ‣ 4.1 In-Context Optimization ‣ 4 Context Tuning for In-Context Optimization ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> by first initializing the model weights <math id=\"S4.SS2.p1.m1\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics><mi>ϕ</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math> from a pretrained model, then updating them with LoRA layers for parameter efficiency. At each optimization iteration, TTT dynamically sets</p>\n<table id=\"S4.Ex5\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.Ex5.m1\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}^{(i)}=\\mathcal{C}^{-i},\" display=\"block\"><semantics><mrow><mrow><msubsup><mi>θ</mi><mi>context</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><msup><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mrow><mo>−</mo><mi>i</mi></mrow></msup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}^{(i)}=\\mathcal{C}^{-i},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"S4.SS2.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathcal{C}^{-i}\" display=\"inline\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mrow><mo>−</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathcal{C}^{-i}</annotation></semantics></math> represents the concatenated tokens of a random permutation of demonstration pairs except for the <math id=\"S4.SS2.p1.m3\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th pair. Therefore, the optimization equation becomes:</p>\n<table id=\"S4.Ex6\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.Ex6.m1\" class=\"ltx_math_unparsed\" alttext=\"\\phi^{*}=\\arg\\min_{\\phi}\\;\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\mathcal{C}^{-i}\\,;x_{i}\\,]\\right).\" display=\"block\"><semantics><mrow><msup><mi>ϕ</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><munder><mi>min</mi><mi>ϕ</mi></munder><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo lspace=\"0em\">−</mo><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><msup><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mrow><mo>−</mo><mi>i</mi></mrow></msup><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\phi^{*}=\\arg\\min_{\\phi}\\;\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\mathcal{C}^{-i}\\,;x_{i}\\,]\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">To perform inference on the query input <math id=\"S4.SS2.p1.m4\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math>, TTT uses the optimized model weights and the concatenation of all demonstration pairs as context:</p>\n<table id=\"S4.Ex7\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.Ex7.m1\" class=\"ltx_math_unparsed\" alttext=\"\\hat{y}_{q}=\\arg\\max_{y}\\,p_{\\phi^{*}}\\left(y\\bigm{|}[\\,\\mathcal{C}\\,;x_{q}\\,]\\right).\" display=\"block\"><semantics><mrow><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>q</mi></msub><mo>=</mo><mi>arg</mi><munder><mi>max</mi><mi>y</mi></munder><msub><mi>p</mi><msup><mi>ϕ</mi><mo>∗</mo></msup></msub><mrow><mo>(</mo><mi>y</mi><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><mo lspace=\"0.170em\">;</mo><msub><mi>x</mi><mi>q</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{y}_{q}=\\arg\\max_{y}\\,p_{\\phi^{*}}\\left(y\\bigm{|}[\\,\\mathcal{C}\\,;x_{q}\\,]\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Context Tuning</h3>\n\n<div id=\"S4.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We design our <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> approach to be an instantiation of the <span class=\"ltx_text ltx_font_italic\">ICO</span> framework. In contrast to TTT, <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> freezes model parameters <math id=\"S4.SS3.p1.m1\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics><mi>ϕ</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math> and instead directly optimizes the lightweight context representation <math id=\"S4.SS3.p1.m2\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}\" display=\"inline\"><semantics><msub><mi>θ</mi><mi>context</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}</annotation></semantics></math> of the demonstration pairs.</p>\n<ul id=\"S4.I1\" class=\"ltx_itemize\">\n<li id=\"S4.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S4.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> initializes <math id=\"S4.I1.i1.p1.m1\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}=P_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi>θ</mi><mi>context</mi></msub><mo>=</mo><msub><mi>P</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}=P_{\\mathrm{CT}}</annotation></semantics></math> as the model’s prompt embeddings on <math id=\"S4.I1.i1.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathcal{C}\" display=\"inline\"><semantics><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math>, the concatenation of demonstration pairs.</p>\n</div>\n</li>\n<li id=\"S4.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S4.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-KV</span> initializes <math id=\"S4.I1.i2.p1.m1\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}=\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi>θ</mi><mi>context</mi></msub><mo>=</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}=\\Theta_{\\mathrm{CT}}</annotation></semantics></math> as a key-value prefix <math id=\"S4.I1.i2.p1.m2\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}=\\{K_{j},V_{j}\\}_{j=1}^{L}\" display=\"inline\"><semantics><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>K</mi><mi>j</mi></msub><mo>,</mo><msub><mi>V</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}=\\{K_{j},V_{j}\\}_{j=1}^{L}</annotation></semantics></math> obtained from the model’s layer-wise activations on <math id=\"S4.I1.i2.p1.m3\" class=\"ltx_Math\" alttext=\"\\mathcal{C}\" display=\"inline\"><semantics><mi class=\"ltx_font_mathcaligraphic\">𝒞</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math>.</p>\n</div>\n</li>\n</ul>\n<p class=\"ltx_p\">Furthermore, we introduce two design choices for both <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>. We study the performance impact of each in Section <a href=\"#S5.SS5\" title=\"5.5 Ablation Study ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>, demonstrating that both are crucial for achieving strong empirical gains.</p>\n</div>\n<section id=\"S4.SS3.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Leave-One-Out Masking.</h4>\n\n<div id=\"S4.SS3.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">To prevent the model from simply retrieving the answer <math id=\"S4.SS3.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"y_{i}\" display=\"inline\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> of the <math id=\"S4.SS3.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>th demonstration pair embedded in <math id=\"S4.SS3.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}\" display=\"inline\"><semantics><msub><mi>θ</mi><mi>context</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}</annotation></semantics></math> when predicting the output for <math id=\"S4.SS3.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"x_{i}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>, we construct</p>\n<table id=\"S4.Ex8\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.Ex8.m1\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}^{(i)}=\\begin{cases}P_{\\mathrm{CT}}^{-i}&amp;\\text{for {\\it CT-Prompt}{}},\\\\\n\\Theta_{\\mathrm{CT}}^{-i}&amp;\\text{for {\\it CT-KV}{}},\\end{cases}\" display=\"block\"><semantics><mrow><msubsup><mi>θ</mi><mi>context</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><msubsup><mi>P</mi><mi>CT</mi><mrow><mo>−</mo><mi>i</mi></mrow></msubsup></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mtext>for </mtext><mtext class=\"ltx_mathvariant_italic\">CT-Prompt</mtext></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><msubsup><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi><mrow><mo>−</mo><mi>i</mi></mrow></msubsup></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mtext>for </mtext><mtext class=\"ltx_mathvariant_italic\">CT-KV</mtext></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}^{(i)}=\\begin{cases}P_{\\mathrm{CT}}^{-i}&amp;\\text{for {\\it CT-Prompt}{}},\\\\\n\\Theta_{\\mathrm{CT}}^{-i}&amp;\\text{for {\\it CT-KV}{}},\\end{cases}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">and use it instead of <math id=\"S4.SS3.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}\" display=\"inline\"><semantics><msub><mi>θ</mi><mi>context</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}</annotation></semantics></math> in optimization. When conditioning on <math id=\"S4.SS3.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"P_{\\mathrm{CT}}^{-i}\" display=\"inline\"><semantics><msubsup><mi>P</mi><mi>CT</mi><mrow><mo>−</mo><mi>i</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">P_{\\mathrm{CT}}^{-i}</annotation></semantics></math> or <math id=\"S4.SS3.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}^{-i}\" display=\"inline\"><semantics><msubsup><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi><mrow><mo>−</mo><mi>i</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}^{-i}</annotation></semantics></math>, the trainable soft prompt tokens in <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> or prefix tokens in <span class=\"ltx_text ltx_font_italic\">CT-KV</span> corresponding to the in-context demonstration pair <math id=\"S4.SS3.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"(x_{i},y_{i})\" display=\"inline\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{i},y_{i})</annotation></semantics></math> are masked out from the attention view of the model. In contrast to TTT’s leave-one-out technique, which omits one demonstration pair in the context to update the model weights, our Leave-One-Out Masking operates on the derived context vectors with the model parameters frozen, ensuring that the optimization refines the context representation itself rather than relying on weight updates.</p>\n</div>\n</section>\n<section id=\"S4.SS3.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Token Dropout.</h4>\n\n<div id=\"S4.SS3.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Since <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> generally introduces a larger number of prompt or prefix tokens than traditional prompt-based adaptation techniques, we regularize training by randomly dropping tokens in <math id=\"S4.SS3.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}^{(i)}\" display=\"inline\"><semantics><msubsup><mi>θ</mi><mi>context</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}^{(i)}</annotation></semantics></math> with a fixed probability, denoted as <math id=\"S4.SS3.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"\\mathrm{TokenDrop}\" display=\"inline\"><semantics><mi>TokenDrop</mi><annotation encoding=\"application/x-tex\">\\mathrm{TokenDrop}</annotation></semantics></math>. During optimization, the loss is computed in the expectation over these stochastic dropout masks, encouraging the learned context to avoid overfitting to any single token.</p>\n</div>\n<div id=\"S4.SS3.SSS0.Px2.p2\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">Altogether, we arrive at the optimization equations for <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>:</p>\n<table id=\"A7.EGx1\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S4.Ex9\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\"><span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"S4.Ex9.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle P_{\\mathrm{CT}}^{*}=\\arg\\min_{P_{\\mathrm{CT}}}\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\mathrm{TokenDrop}\\left(P_{\\mathrm{CT}}^{-i}\\right)\\,;x_{i}]\\right),\" display=\"inline\"><semantics><mrow><msubsup><mi>P</mi><mi>CT</mi><mo>∗</mo></msubsup><mo>=</mo><mi>arg</mi><munder><mi>min</mi><msub><mi>P</mi><mi>CT</mi></msub></munder><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mo>−</mo><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mi>TokenDrop</mi><mrow><mo>(</mo><msubsup><mi>P</mi><mi>CT</mi><mrow><mo>−</mo><mi>i</mi></mrow></msubsup><mo rspace=\"0.170em\">)</mo></mrow><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle P_{\\mathrm{CT}}^{*}=\\arg\\min_{P_{\\mathrm{CT}}}\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\mathrm{TokenDrop}\\left(P_{\\mathrm{CT}}^{-i}\\right)\\,;x_{i}]\\right),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"S4.Ex10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\"><span class=\"ltx_text ltx_font_italic\">CT-KV</span>:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math id=\"S4.Ex10.m2\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\Theta_{\\mathrm{CT}}^{*}=\\arg\\min_{\\Theta_{\\mathrm{CT}}}\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\mathrm{TokenDrop}\\left(\\Theta_{\\mathrm{CT}}^{-i}\\right)\\,;x_{i}]\\right).\" display=\"inline\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi><mo>∗</mo></msubsup><mo>=</mo><mi>arg</mi><munder><mi>min</mi><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></munder><mstyle displaystyle=\"true\"><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover></mstyle><mo>−</mo><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><mi>TokenDrop</mi><mrow><mo>(</mo><msubsup><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi><mrow><mo>−</mo><mi>i</mi></mrow></msubsup><mo rspace=\"0.170em\">)</mo></mrow><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\Theta_{\\mathrm{CT}}^{*}=\\arg\\min_{\\Theta_{\\mathrm{CT}}}\\sum_{i=1}^{k}-\\log p_{\\phi}\\left(y_{i}\\bigm{|}[\\,\\mathrm{TokenDrop}\\left(\\Theta_{\\mathrm{CT}}^{-i}\\right)\\,;x_{i}]\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS3.SSS0.Px2.p3\" class=\"ltx_para ltx_noindent\">\n<p class=\"ltx_p\">To perform inference on the query <math id=\"S4.SS3.SSS0.Px2.p3.m1\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math>, <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span> use their respective optimized contexts:</p>\n<table id=\"A7.EGx2\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S4.Ex11\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S4.Ex11.m1\" class=\"ltx_math_unparsed\" alttext=\"\\displaystyle\\begin{aligned} \\text{{\\it CT-Prompt}{}:}\\quad&amp;\\hat{y}_{q}=\\arg\\max_{y}\\,p_{\\phi}\\left(y\\bigm{|}[\\,P_{\\mathrm{CT}}^{*}\\,;x_{q}\\,]\\right),\\\\\n\\text{{\\it CT-KV}{}:}\\quad&amp;\\hat{y}_{q}=\\arg\\max_{y}\\,p_{\\phi}\\left(y\\bigm{|}[\\,\\Theta_{\\mathrm{CT}}^{*}\\,;x_{q}\\,]\\right).\\end{aligned}\" display=\"inline\"><semantics><mtable columnspacing=\"0pt\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_right\" columnalign=\"right\"><mrow><mtext class=\"ltx_mathvariant_italic\">CT-Prompt</mtext><mtext>:</mtext></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>q</mi></msub><mo>=</mo><mi>arg</mi><munder><mi>max</mi><mi>y</mi></munder><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo>(</mo><mi>y</mi><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><msubsup><mi>P</mi><mi>CT</mi><mo>∗</mo></msubsup><mo>;</mo><msub><mi>x</mi><mi>q</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class=\"ltx_align_right\" columnalign=\"right\"><mrow><mtext class=\"ltx_mathvariant_italic\">CT-KV</mtext><mtext>:</mtext></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msub><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mi>q</mi></msub><mo>=</mo><mi>arg</mi><munder><mi>max</mi><mi>y</mi></munder><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo>(</mo><mi>y</mi><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">[</mo><msubsup><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi><mo>∗</mo></msubsup><mo>;</mo><msub><mi>x</mi><mi>q</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>)</mo></mrow><mo lspace=\"0em\">.</mo></mrow></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\displaystyle\\begin{aligned} \\text{{\\it CT-Prompt}{}:}\\quad&amp;\\hat{y}_{q}=\\arg\\max_{y}\\,p_{\\phi}\\left(y\\bigm{|}[\\,P_{\\mathrm{CT}}^{*}\\,;x_{q}\\,]\\right),\\\\\n\\text{{\\it CT-KV}{}:}\\quad&amp;\\hat{y}_{q}=\\arg\\max_{y}\\,p_{\\phi}\\left(y\\bigm{|}[\\,\\Theta_{\\mathrm{CT}}^{*}\\,;x_{q}\\,]\\right).\\end{aligned}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS3.SSS0.Px2.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">In practice, <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> requires recomputing layer-wise keys and values corresponding to <math id=\"S4.SS3.SSS0.Px2.p4.m1\" class=\"ltx_Math\" alttext=\"P_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mi>P</mi><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">P_{\\mathrm{CT}}</annotation></semantics></math>, while <span class=\"ltx_text ltx_font_italic\">CT-KV</span> does not for <math id=\"S4.SS3.SSS0.Px2.p4.m2\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}</annotation></semantics></math>. In the Appendix, we formally prove that for each optimization step, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> has lower time complexity than both TTT and <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> with respect to the number of demonstration pairs. Finally, we introduce TTT+<span class=\"ltx_text ltx_font_italic\">CT-KV</span>, which first performs TTT to update model weights <math id=\"S4.SS3.SSS0.Px2.p4.m3\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics><mi>ϕ</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math>, then applies <span class=\"ltx_text ltx_font_italic\">CT-KV</span> to refine the model’s demonstration context for improved performance.</p>\n</div>\n</section>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Experiments</h2>\n\n<section id=\"S5.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span>Datasets</h3>\n\n<div id=\"S5.SS1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We evaluate on a diverse set of challenging datasets for pretrained LLMs. We show a representative task example for each dataset in Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ 5.1 Datasets ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n<ul id=\"S5.I1\" class=\"ltx_itemize\">\n<li id=\"S5.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NLP-LR</span> is the low-resource dataset split introduced by <cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">Min et al., 2022a </a></cite>, encompassing over 26 NLP tasks from CrossFit <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al.,, <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> and UnifiedQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Khashabi et al.,, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, such as sentiment analysis and paraphrasing. Following <cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">Min et al., 2022a </a></cite>, we sample <math id=\"S5.I1.i1.p1.m1\" class=\"ltx_Math\" alttext=\"k=16\" display=\"inline\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math> demonstration pairs per task and evaluate task instances as multiple-choice problems.</p>\n</div>\n</li>\n<li id=\"S5.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Massive Multitask Language Understanding (MMLU)</span> is a diverse benchmark consisting of 57 subject-specific tasks, including mathematics, history, law, and various other domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al.,, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>. We sample <math id=\"S5.I1.i2.p1.m1\" class=\"ltx_Math\" alttext=\"k=16\" display=\"inline\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math> demonstration pairs per task and evaluate task instances as multiple-choice problems.</p>\n</div>\n</li>\n<li id=\"S5.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BIG-Bench Hard (BBH)</span> is a curated subset of BIG-Bench, consisting of 27 tasks across 23 task types that challenge pretrained LLMs with questions involving algorithmic puzzles, symbolic manipulation, and other complex reasoning domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Srivastava et al.,, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2023</a>; Suzgun et al.,, <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, we sample <math id=\"S5.I1.i3.p1.m1\" class=\"ltx_Math\" alttext=\"k=10\" display=\"inline\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">k=10</annotation></semantics></math> demonstration pairs per task and prepend trainable instructions to all of our methods. We evaluate these tasks as question-answering problems.</p>\n</div>\n</li>\n<li id=\"S5.I1.i4\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S5.I1.i4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Abstraction and Reasoning Corpus (ARC)</span> is a challenging symbolic reasoning benchmark with 400 evaluation tasks, each defined by a few grid transformation pairs and one or more query input grids <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">Chollet, 2019b, </a>)</cite>. Since the average number of available demonstration pairs is fewer than 4, we use all of them in context. Tasks are evaluated as question-answering problems.</p>\n</div>\n</li>\n</ul>\n</div>\n<div id=\"S5.SS1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Each dataset is formatted either as a multiple-choice task or a question-answering task. For multiple-choice problems, where the LLM must select an output from a predefined set of answers, we follow <cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">Min et al., 2022a </a></cite> and choose the option with the lowest loss. For question-answering tasks, the LLM has to generate an answer that matches the ground-truth output.</p>\n</div>\n<figure id=\"S5.F3\" class=\"ltx_figure\"><img src=\"./assets/x3.png\" id=\"S5.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"335\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>\nOne test pair from BBH, NLP-LR, and MMLU each, and 3 demonstration pairs followed by a test pair from ARC. BBH contains instructions that we prepend to model inputs. NLP-LR and MMLU contain multiple-choice options for the model to select. To avoid clutter, we show demonstration pairs from BBH, NLP-LR, and MMLU in the Appendix.\n</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2 </span>Models</h3>\n\n<div id=\"S5.SS2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">Min et al., 2022a </a></cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, we use GPT-2 and Llama3-8B for NLP-LR and BBH, respectively. To demonstrate that <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> performs well across different model sizes, we select Llama3.2-3B for MMLU. Due to computational constraints, we use Llama3.2-1B for ARC, which requires handling long input sequences. Since the pretrained Llama3.2-1B model cannot solve any of the 400 ARC evaluation tasks, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Franzen et al., (<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> by fine-tuning it on the ARC training split, which contains 400 tasks that do not overlap with the evaluation split. While the works of <cite class=\"ltx_cite ltx_citemacro_citet\">Franzen et al., (<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite> focus on achieving high scores in the ARC competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Chollet et al.,, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2025</a>)</cite>, our goal is to develop a method applicable across general few-shot problems. Therefore, we do not perform augmentation or voting for ARC. All pretrained model checkpoints were obtained from HuggingFace.</p>\n</div>\n</section>\n<section id=\"S5.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3 </span>Experiment Setup</h3>\n\n<div id=\"S5.SS3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">On top of zero-shot inference and ICL, we compare a variety of few-shot learning techniques to conduct a broad investigation of prompt-based adaptation strategies and methods under the <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> framework: Prompt Tuning, Prefix Tuning, TTT, <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>, <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, and TTT+<span class=\"ltx_text ltx_font_italic\">CT-KV</span>. We use greedy decoding for all question-answering tasks. All experiments in Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are run over 5 different sets of randomly selected demonstration pairs, except for ARC, which has a fixed set of demonstration pairs for each task.</p>\n</div>\n<div id=\"S5.SS3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">For Prompt Tuning and Prefix Tuning, we either set the number of trainable tokens <math id=\"S5.SS3.p2.m1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> to 32, or match it to the number of tokens that are in the demonstration pairs used by <span class=\"ltx_text ltx_font_italic\">Context Tuning</span>. Trainable soft prompts and prefixes are initialized using sampled token embeddings from the model, which we find yields the best baseline performance.</p>\n</div>\n<div id=\"S5.SS3.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">For ARC, we fine-tune our Llama3.2-1B checkpoint following the setup of <cite class=\"ltx_cite ltx_citemacro_citet\">Franzen et al., (<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, using 2 A100 GPUs for 24 epochs with a learning rate of <math id=\"S5.SS3.p3.m1\" class=\"ltx_Math\" alttext=\"2\\times 10^{-4}\" display=\"inline\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, cosine learning rate scheduler, 1 warmup epoch, and a global batch size of 32 (after gradient accumulation). All other experiments are conducted on a single A100 GPU, except NLP-LR, which is run on an RTX8000. For <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, we apply Leave-One-Out Masking from Section <a href=\"#S4\" title=\"4 Context Tuning for In-Context Optimization ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> across all datasets, except ARC, where performance improves without it. We elaborate on this decision in Section <a href=\"#S5.SS5\" title=\"5.5 Ablation Study ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n</div>\n<div id=\"S5.SS3.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">For completeness, we also compare alternative setups for both Prompt Tuning and Prefix Tuning. In the Appendix, we report results using uniformly initialized trainable parameters for both methods. We also include results for Prefix Tuning with an MLP parameterization, along with details of our hyperparameter search to support reproducibility. Overall, our evaluation spans a wide range of challenging tasks, model sizes from 1B to 8B parameters, varied numbers of demonstration pairs per task (<math id=\"S5.SS3.p4.m1\" class=\"ltx_Math\" alttext=\"k=2\" display=\"inline\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">k=2</annotation></semantics></math> to <math id=\"S5.SS3.p4.m2\" class=\"ltx_Math\" alttext=\"k=16\" display=\"inline\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>), and benchmarks with and without task instructions (e.g., BBH includes instructions, while the others do not).</p>\n</div>\n</section>\n<section id=\"S5.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.4 </span>Comparing Context Tuning to Baselines</h3>\n\n<figure id=\"S5.T1\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Acc. (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">T (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Acc. (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">T (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Acc. (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">T (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Acc. (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">T (s)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Baselines</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Shot</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m1\" class=\"ltx_Math\" alttext=\"34.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.62}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">34.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.62</mn></mrow><annotation encoding=\"application/x-tex\">34.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.62}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m2\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m3\" class=\"ltx_Math\" alttext=\"35.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.71}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">35.8</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.71</mn></mrow><annotation encoding=\"application/x-tex\">35.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.71}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m4\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m5\" class=\"ltx_Math\" alttext=\"40.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.43}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">40.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.43</mn></mrow><annotation encoding=\"application/x-tex\">40.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.43}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m6\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m7\" class=\"ltx_Math\" alttext=\"1.0\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m8\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ICL</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m9\" class=\"ltx_Math\" alttext=\"35.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.65}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">35.6</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.65</mn></mrow><annotation encoding=\"application/x-tex\">35.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.65}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m10\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m11\" class=\"ltx_Math\" alttext=\"41.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.57}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">41.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.57</mn></mrow><annotation encoding=\"application/x-tex\">41.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.57}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m12\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m13\" class=\"ltx_Math\" alttext=\"50.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.78}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">50.4</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.78</mn></mrow><annotation encoding=\"application/x-tex\">50.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.78}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m14\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m15\" class=\"ltx_Math\" alttext=\"13.3\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">13.3</mn><annotation encoding=\"application/x-tex\">13.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m16\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><mn mathsize=\"0.900em\">0</mn></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = 32)</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m17\" class=\"ltx_Math\" alttext=\"41.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.02}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">41.4</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.02</mn></mrow><annotation encoding=\"application/x-tex\">41.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.02}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m18\" class=\"ltx_Math\" alttext=\"147\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">147</mn><annotation encoding=\"application/x-tex\">147</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m19\" class=\"ltx_Math\" alttext=\"39.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.04}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">39.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.04</mn></mrow><annotation encoding=\"application/x-tex\">39.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.04}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m20\" class=\"ltx_Math\" alttext=\"15\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m21\" class=\"ltx_Math\" alttext=\"50.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.59}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">50.8</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.59</mn></mrow><annotation encoding=\"application/x-tex\">50.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.59}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m22\" class=\"ltx_Math\" alttext=\"7\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m23\" class=\"ltx_Math\" alttext=\"12.0\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">12.0</mn><annotation encoding=\"application/x-tex\">12.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m24\" class=\"ltx_Math\" alttext=\"13\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = # demo)</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m25\" class=\"ltx_Math\" alttext=\"38.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.23}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">38.8</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.23</mn></mrow><annotation encoding=\"application/x-tex\">38.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.23}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m26\" class=\"ltx_Math\" alttext=\"231\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">231</mn><annotation encoding=\"application/x-tex\">231</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m27\" class=\"ltx_Math\" alttext=\"37.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.23}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">37.3</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.23</mn></mrow><annotation encoding=\"application/x-tex\">37.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.23}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m28\" class=\"ltx_Math\" alttext=\"29\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">29</mn><annotation encoding=\"application/x-tex\">29</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m29\" class=\"ltx_Math\" alttext=\"47.5{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.84}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">47.5</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.84</mn></mrow><annotation encoding=\"application/x-tex\">47.5{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.84}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m30\" class=\"ltx_Math\" alttext=\"16\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m31\" class=\"ltx_Math\" alttext=\"14.5\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">14.5</mn><annotation encoding=\"application/x-tex\">14.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m32\" class=\"ltx_Math\" alttext=\"49\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">49</mn><annotation encoding=\"application/x-tex\">49</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = 32)</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m33\" class=\"ltx_Math\" alttext=\"42.0{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.85}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">42.0</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.85</mn></mrow><annotation encoding=\"application/x-tex\">42.0{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.85}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m34\" class=\"ltx_Math\" alttext=\"123\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">123</mn><annotation encoding=\"application/x-tex\">123</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m35\" class=\"ltx_Math\" alttext=\"39.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.94}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">39.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.94</mn></mrow><annotation encoding=\"application/x-tex\">39.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.94}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m36\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m37\" class=\"ltx_Math\" alttext=\"52.7{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.12}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">52.7</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.12</mn></mrow><annotation encoding=\"application/x-tex\">52.7{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.12}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m38\" class=\"ltx_Math\" alttext=\"7\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m39\" class=\"ltx_Math\" alttext=\"9.3\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m40\" class=\"ltx_Math\" alttext=\"14\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">14</mn><annotation encoding=\"application/x-tex\">14</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = # demo)</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m41\" class=\"ltx_Math\" alttext=\"41.1{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.89}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">41.1</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.89</mn></mrow><annotation encoding=\"application/x-tex\">41.1{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.89}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m42\" class=\"ltx_Math\" alttext=\"144\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">144</mn><annotation encoding=\"application/x-tex\">144</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m43\" class=\"ltx_Math\" alttext=\"38.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.81}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">38.8</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.81</mn></mrow><annotation encoding=\"application/x-tex\">38.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.81}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m44\" class=\"ltx_Math\" alttext=\"8\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m45\" class=\"ltx_Math\" alttext=\"52.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.15}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">52.8</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.15</mn></mrow><annotation encoding=\"application/x-tex\">52.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.15}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m46\" class=\"ltx_Math\" alttext=\"9\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m47\" class=\"ltx_Math\" alttext=\"20.5\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">20.5</mn><annotation encoding=\"application/x-tex\">20.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m48\" class=\"ltx_Math\" alttext=\"24\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTT</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m49\" class=\"ltx_Math\" alttext=\"44.1{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.65}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">44.1</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.65</mn></mrow><annotation encoding=\"application/x-tex\">44.1{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.65}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m50\" class=\"ltx_Math\" alttext=\"342\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">342</mn><annotation encoding=\"application/x-tex\">342</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m51\" class=\"ltx_Math\" alttext=\"43.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.55}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">43.6</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.55</mn></mrow><annotation encoding=\"application/x-tex\">43.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.55}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m52\" class=\"ltx_Math\" alttext=\"30\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">30</mn><annotation encoding=\"application/x-tex\">30</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m53\" class=\"ltx_Math\" alttext=\"57.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.13}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">57.8</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 1.13</mn></mrow><annotation encoding=\"application/x-tex\">57.8{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}1.13}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m54\" class=\"ltx_Math\" alttext=\"14\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">14</mn><annotation encoding=\"application/x-tex\">14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m55\" class=\"ltx_Math\" alttext=\"\\underline{23.8}\" display=\"inline\"><semantics><munder accentunder=\"true\"><mn mathsize=\"0.900em\">23.8</mn><mo stretchy=\"true\">¯</mo></munder><annotation encoding=\"application/x-tex\">\\underline{23.8}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m56\" class=\"ltx_Math\" alttext=\"56\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">56</mn><annotation encoding=\"application/x-tex\">56</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Our Methods</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-Prompt</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m57\" class=\"ltx_Math\" alttext=\"43.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.61}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">43.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.61</mn></mrow><annotation encoding=\"application/x-tex\">43.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.61}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m58\" class=\"ltx_Math\" alttext=\"228\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">228</mn><annotation encoding=\"application/x-tex\">228</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m59\" class=\"ltx_Math\" alttext=\"43.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.67}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">43.6</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.67</mn></mrow><annotation encoding=\"application/x-tex\">43.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.67}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m60\" class=\"ltx_Math\" alttext=\"33\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">33</mn><annotation encoding=\"application/x-tex\">33</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m61\" class=\"ltx_Math\" alttext=\"56.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.98}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">56.3</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.98</mn></mrow><annotation encoding=\"application/x-tex\">56.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.98}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m62\" class=\"ltx_Math\" alttext=\"14\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">14</mn><annotation encoding=\"application/x-tex\">14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m63\" class=\"ltx_Math\" alttext=\"22.5\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">22.5</mn><annotation encoding=\"application/x-tex\">22.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m64\" class=\"ltx_Math\" alttext=\"52\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">52</mn><annotation encoding=\"application/x-tex\">52</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m65\" class=\"ltx_Math\" alttext=\"\\underline{44.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.55}}}\" display=\"inline\"><semantics><munder accentunder=\"true\"><mrow><mn mathsize=\"0.900em\">44.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.55</mn></mrow><mo stretchy=\"true\">¯</mo></munder><annotation encoding=\"application/x-tex\">\\underline{44.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.55}}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m66\" class=\"ltx_Math\" alttext=\"145\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">145</mn><annotation encoding=\"application/x-tex\">145</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m67\" class=\"ltx_Math\" alttext=\"\\underline{43.7{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.54}}}\" display=\"inline\"><semantics><munder accentunder=\"true\"><mrow><mn mathsize=\"0.900em\">43.7</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.54</mn></mrow><mo stretchy=\"true\">¯</mo></munder><annotation encoding=\"application/x-tex\">\\underline{43.7{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.54}}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m68\" class=\"ltx_Math\" alttext=\"9\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m69\" class=\"ltx_Math\" alttext=\"\\underline{57.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.78}}}\" display=\"inline\"><semantics><munder accentunder=\"true\"><mrow><mn mathsize=\"0.900em\">57.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.78</mn></mrow><mo stretchy=\"true\">¯</mo></munder><annotation encoding=\"application/x-tex\">\\underline{57.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.78}}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m70\" class=\"ltx_Math\" alttext=\"7\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m71\" class=\"ltx_Math\" alttext=\"\\underline{23.8}\" display=\"inline\"><semantics><munder accentunder=\"true\"><mn mathsize=\"0.900em\">23.8</mn><mo stretchy=\"true\">¯</mo></munder><annotation encoding=\"application/x-tex\">\\underline{23.8}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m72\" class=\"ltx_Math\" alttext=\"26\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">26</mn><annotation encoding=\"application/x-tex\">26</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">TTT+</span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m73\" class=\"ltx_Math\" alttext=\"\\mathbf{47.6}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.53}}\" display=\"inline\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">47.6</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.53</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{47.6}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.53}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m74\" class=\"ltx_Math\" alttext=\"372\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">372</mn><annotation encoding=\"application/x-tex\">372</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m75\" class=\"ltx_Math\" alttext=\"\\mathbf{44.1}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.38}}\" display=\"inline\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">44.1</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.38</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{44.1}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.38}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m76\" class=\"ltx_Math\" alttext=\"34\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m77\" class=\"ltx_Math\" alttext=\"\\mathbf{58.2}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.73}}\" display=\"inline\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">58.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.73</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{58.2}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.73}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m78\" class=\"ltx_Math\" alttext=\"17\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m79\" class=\"ltx_Math\" alttext=\"\\mathbf{25.8}\" display=\"inline\"><semantics><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">25.8</mn><annotation encoding=\"application/x-tex\">\\mathbf{25.8}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"S5.T1.m80\" class=\"ltx_Math\" alttext=\"63\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">63</mn><annotation encoding=\"application/x-tex\">63</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>\nFew-shot learning performance on NLP-LR, MMLU, BBH, and ARC benchmarks. Each cell contains the accuracy (%) and training time per task (seconds), delimited by <span class=\"ltx_text ltx_font_typewriter\">/</span>. We show the means and <span class=\"ltx_text\" style=\"color:#808080;\">standard deviations</span> of accuracies over 5 seeds with different sets of demonstration pairs per task (except ARC because it has fixed demonstration pairs). The best accuracy is <span class=\"ltx_text ltx_font_bold\">bolded</span> and second best is <span class=\"ltx_text ltx_framed ltx_framed_underline\">underlined</span> for each benchmark.\n</figcaption>\n</figure>\n<div id=\"S5.SS4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the performance and training time per task for our baselines and methods across the four benchmarks. To fairly compare Prompt Tuning and Prefix Tuning with <span class=\"ltx_text ltx_font_italic\">Context Tuning</span>, “Prompt Tuning (m = # demo)” and “Prefix Tuning (m = # demo)” are configured to match the number of trainable parameters in <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, respectively, by setting <math id=\"S5.SS4.p1.m1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> to the number demonstration pair tokens. We report each method’s number of trainable parameters in the Appendix.</p>\n</div>\n<section id=\"S5.SS4.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Context Tuning outperforms Prompt Tuning and Prefix Tuning.</h4>\n\n<div id=\"S5.SS4.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> outperforms Prompt Tuning (m = 32), and <span class=\"ltx_text ltx_font_italic\">CT-KV</span> outperforms Prefix Tuning (m = 32), both by a wide margin across all benchmarks. Moreover, increasing <math id=\"S5.SS4.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> to match the number of demonstration tokens does not yield consistent improvements in Prompt Tuning or Prefix Tuning. Despite tuning the same number of parameters, these variants still underperform compared to <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>. This highlights the effectiveness of leveraging the model’s ICL capabilities by initializing the prompt or prefix with demonstration tokens.</p>\n</div>\n</section>\n<section id=\"S5.SS4.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-KV is more efficient than CT-Prompt.</h4>\n\n<div id=\"S5.SS4.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-KV</span> exhibits significantly lower training time per task compared to <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>. This observation aligns with the time complexity discussion in the Appendix: <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> incurs quadratic scaling in training time with the number of demonstration pairs, while <span class=\"ltx_text ltx_font_italic\">CT-KV</span> scales linearly. In addition to being faster, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> also outperforms <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> in accuracy by conditioning each transformer layer’s activations with layer-specific key and value vectors, rather than relying solely on input-level soft prompts.</p>\n</div>\n</section>\n<section id=\"S5.SS4.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-KV offers an efficient alternative to TTT, and the two are complementary.</h4>\n\n<div id=\"S5.SS4.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-KV</span> achieves performance comparable to TTT across NLP-LR, MMLU, and BBH, and solves the same number of ARC tasks. However, it requires at most half the training time per task compared to TTT on all benchmarks. This demonstrates that while the two methods converge to similar performance levels, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> is more efficient due to its linear time complexity to the number of demonstration pairs, in contrast to TTT’s quadratic time complexity. Furthermore, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> can be applied as a refinement step after TTT training of model weights, leading to higher performance on all benchmarks with minimal additional training time. This suggests that context and model-based adaptation methods within the <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> framework are complementary and can be effectively combined for few-shot learning.</p>\n</div>\n</section>\n<section id=\"S5.SS4.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Initialization from demonstration pairs lowers standard deviation in performance.</h4>\n\n<div id=\"S5.SS4.SSS0.Px4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Initializing the trainable prompt or prefix from demonstration pairs, rather than from random tokens, reduces sensitivity to random seeds in both <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>. This leads to more stable performance compared to Prompt Tuning and Prefix Tuning.</p>\n</div>\n</section>\n<section id=\"S5.SS4.SSS0.Px5\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-KV outperforms MetaICL on NLP-LR.</h4>\n\n<div id=\"S5.SS4.SSS0.Px5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-KV</span> achieves 44.2% accuracy on NLP-LR, surpassing the reported 43.3% accuracy of MetaICL <cite class=\"ltx_cite ltx_citemacro_citep\">(<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">Min et al., 2022a, </a>)</cite>. This demonstrates that inference-time, single-task optimization with <span class=\"ltx_text ltx_font_italic\">CT-KV</span> can rival the performance of approaches that fine-tune model weights across many tasks.</p>\n</div>\n</section>\n</section>\n<section id=\"S5.SS5\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.5 </span>Ablation Study</h3>\n\n<div id=\"S5.SS5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We perform ablations on our design choices for <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, namely Leave-One-Out Masking and Token Dropout. Table <a href=\"#S5.T2\" title=\"Table 2 ‣ 5.5 Ablation Study ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that across all benchmarks, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> without Token Dropout performs marginally worse than <span class=\"ltx_text ltx_font_italic\">CT-KV</span> with both components. This suggests that when tuning more parameters than traditional Prefix Tuning, applying dropout along the token dimension of <math id=\"S5.SS5.p1.m1\" class=\"ltx_Math\" alttext=\"\\Theta\" display=\"inline\"><semantics><mi mathvariant=\"normal\">Θ</mi><annotation encoding=\"application/x-tex\">\\Theta</annotation></semantics></math> serves as an effective regularization technique for improving generalization. For NLP-LR, BBH, and MMLU, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> performs significantly worse when Leave-One-Out Masking is not applied. This indicates that during training, it is crucial to mask out the portion of <math id=\"S5.SS5.p1.m2\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}\" display=\"inline\"><semantics><msub><mi>θ</mi><mi>context</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}</annotation></semantics></math> corresponding to the demonstration pair being solved, as it prevents the model from cheating by retrieving the target output directly from the prefix initialization. However, on ARC, the model performs better without Leave-One-Out Masking. We hypothesize this is because ARC evaluation tasks typically include very few demonstration pairs (fewer than 4), so masking out even one pair during training can meaningfully reduce the effectiveness of the prompt or prefix in ICL. We also observe that when neither Leave-One-Out Masking nor Token Dropout is applied, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> performs worse than ICL on MMLU and only marginally better on BBH, highlighting that these two design choices are essential to its overall performance.</p>\n</div>\n<figure id=\"S5.T2\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Neither</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m1\" class=\"ltx_Math\" alttext=\"41.0{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.75}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">41.0</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.75</mn></mrow><annotation encoding=\"application/x-tex\">41.0{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.75}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m2\" class=\"ltx_Math\" alttext=\"40.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.73}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">40.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.73</mn></mrow><annotation encoding=\"application/x-tex\">40.2{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.73}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m3\" class=\"ltx_Math\" alttext=\"51.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.76}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">51.4</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.76</mn></mrow><annotation encoding=\"application/x-tex\">51.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.76}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m4\" class=\"ltx_Math\" alttext=\"21.0\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">21.0</mn><annotation encoding=\"application/x-tex\">21.0</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">No Leave-One-Out Masking</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m5\" class=\"ltx_Math\" alttext=\"42.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.45}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">42.6</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.45</mn></mrow><annotation encoding=\"application/x-tex\">42.6{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.45}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m6\" class=\"ltx_Math\" alttext=\"41.5{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.65}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">41.5</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.65</mn></mrow><annotation encoding=\"application/x-tex\">41.5{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.65}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m7\" class=\"ltx_Math\" alttext=\"54.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.88}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">54.4</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.88</mn></mrow><annotation encoding=\"application/x-tex\">54.4{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.88}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m8\" class=\"ltx_Math\" alttext=\"\\mathbf{23.8}\" display=\"inline\"><semantics><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">23.8</mn><annotation encoding=\"application/x-tex\">\\mathbf{23.8}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">No Token Dropout</span></th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m9\" class=\"ltx_Math\" alttext=\"43.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.62}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">43.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.62</mn></mrow><annotation encoding=\"application/x-tex\">43.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.62}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m10\" class=\"ltx_Math\" alttext=\"42.7{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.62}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">42.7</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.62</mn></mrow><annotation encoding=\"application/x-tex\">42.7{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.62}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m11\" class=\"ltx_Math\" alttext=\"55.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.72}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">55.3</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.72</mn></mrow><annotation encoding=\"application/x-tex\">55.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.72}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m12\" class=\"ltx_Math\" alttext=\"21.0\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">21.0</mn><annotation encoding=\"application/x-tex\">21.0</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Both</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m13\" class=\"ltx_Math\" alttext=\"\\mathbf{44.2}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.55}}\" display=\"inline\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">44.2</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.55</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{44.2}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.55}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m14\" class=\"ltx_Math\" alttext=\"\\mathbf{43.7}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.54}}\" display=\"inline\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">43.7</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.54</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{43.7}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.54}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m15\" class=\"ltx_Math\" alttext=\"\\mathbf{57.9}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.78}}\" display=\"inline\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">57.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.78</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{57.9}{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.78}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T2.m16\" class=\"ltx_Math\" alttext=\"22.5\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">22.5</mn><annotation encoding=\"application/x-tex\">22.5</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>\nAblation study on the effects of Leave-One-Out Masking and Token Dropout in <span class=\"ltx_text ltx_font_italic\">CT-KV</span>. Means and standard deviations are computed over 5 seeds.\n</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS6\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.6 </span>Qualitative Analysis</h3>\n\n<div id=\"S5.SS6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We compare our <span class=\"ltx_text ltx_font_italic\">CT-KV</span> to ICL on the 400 ARC evaluation tasks. Table <a href=\"#S5.T3\" title=\"Table 3 ‣ 5.6 Qualitative Analysis ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the confusion matrix indicating the number of tasks solved or not solved by each method. <span class=\"ltx_text ltx_font_italic\">CT-KV</span> recovers 51 tasks that ICL fails to solve, demonstrating the benefit of tuning the key and value representations corresponding to the in-context demonstration pairs. However, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> fails to solve 9 tasks that ICL is able to, despite initializing its trainable prefix with the same demonstration pairs, suggesting that it can overfit to the few-shot examples.</p>\n</div>\n<figure id=\"S5.T3\" class=\"ltx_table ltx_align_floatright\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ICL correct</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ICL wrong</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text ltx_font_italic\">CT-KV</span><span class=\"ltx_text ltx_font_bold\"> correct</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text ltx_font_italic\">CT-KV</span><span class=\"ltx_text ltx_font_bold\"> wrong</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">296</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>\nConfusion matrix for the number of solved/unsolved ARC tasks by ICL and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>.\n</figcaption>\n</figure>\n<div id=\"S5.SS6.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Figure <a href=\"#S5.F4\" title=\"Figure 4 ‣ 5.6 Qualitative Analysis ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows one failure case for each method, where the other successfully solves the task. The task on the left illustrates that <span class=\"ltx_text ltx_font_italic\">CT-KV</span> can effectively adapt to the demonstration pairs to solve a geometric puzzle involving cropping the upper-left portion of objects in the query. On the right, we show a case where <span class=\"ltx_text ltx_font_italic\">CT-KV</span> makes an incorrect prediction. Since <span class=\"ltx_text ltx_font_italic\">CT-KV</span> performs optimization on the 3 demonstration pairs and two of them, illustrated on the right side of Figure <a href=\"#S5.F4\" title=\"Figure 4 ‣ 5.6 Qualitative Analysis ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, have answer grids that are 3-row by 4-column, we hypothesize that <span class=\"ltx_text ltx_font_italic\">CT-KV</span> became incorrectly biased toward predicting a grid of the same shape during optimization.</p>\n</div>\n<figure id=\"S5.F4\" class=\"ltx_figure\"><img src=\"./assets/x4.png\" id=\"S5.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"152\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>Left is an ARC task that <span class=\"ltx_text ltx_font_italic\">CT-KV</span> successfully solves, but ICL does not. Conversely, the task on the right is solved by ICL but not by <span class=\"ltx_text ltx_font_italic\">CT-KV</span>.</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS7\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.7 </span>Why does CT-KV Outperform ICL?</h3>\n\n<section id=\"S5.SS7.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Two-Stage Interpretation of ICL.</h4>\n\n<div id=\"S5.SS7.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that <span class=\"ltx_text ltx_font_italic\">CT-KV</span> significantly improves accuracy over ICL. To understand ICL’s limitations, we frame it as a two-stage process: first, the model encodes task-relevant information from the demonstration pairs into an intermediate key-value (KV) cache via a forward pass, denoted as <math id=\"S5.SS7.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}</annotation></semantics></math> and used by <span class=\"ltx_text ltx_font_italic\">CT-KV</span> to initialize <math id=\"S5.SS7.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"\\theta_{\\mathrm{context}}\" display=\"inline\"><semantics><msub><mi>θ</mi><mi>context</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{\\mathrm{context}}</annotation></semantics></math>); second, the model attends to this cache when generating an output for a new query input <math id=\"S5.SS7.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"x_{q}\" display=\"inline\"><semantics><msub><mi>x</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">x_{q}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"S5.SS7.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Demonstration Pair Retrieval Experiment.</h4>\n\n<div id=\"S5.SS7.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Since the second stage does not revisit the original demonstration tokens, the KV cache must contain all necessary information to solve input-output pairs from the task, including the demonstration pairs themselves. To assess how well this information is encoded, we conduct a simple diagnostic: we concatenate all <math id=\"S5.SS7.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstration pairs into the model’s context and then prompt it with the input from one of those same pairs. In this setup, the correct answer is already present in the context, so the model can either apply the task structure it has extracted from the other demonstration pairs or retrieve the correct output directly from the context.</p>\n</div>\n<div id=\"S5.SS7.SSS0.Px2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Table <a href=\"#S5.T4\" title=\"Table 4 ‣ Demonstration Pair Retrieval Experiment. ‣ 5.7 Why does CT-KV Outperform ICL? ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that even in this favorable setting, performance surprisingly remains far from perfect, suggesting that the KV cache often fails to encode the task adequately. <span class=\"ltx_text ltx_font_italic\">CT-KV</span> can be viewed as directly optimizing this KV cache <math id=\"S5.SS7.SSS0.Px2.p2.m1\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}</annotation></semantics></math> by applying gradient updates on the demonstration pairs to refine the task representation. To prevent overfitting through memorization, we additionally use a Leave-One-Out Masking technique: when optimizing for a given demonstration pair, we exclude it from the context the model can attend to, forcing the model to generalize from the remaining pairs.</p>\n</div>\n<figure id=\"S5.T4\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T4.m1\" class=\"ltx_Math\" alttext=\"81.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.32}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">81.9</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.32</mn></mrow><annotation encoding=\"application/x-tex\">81.9{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.32}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T4.m2\" class=\"ltx_Math\" alttext=\"84.1{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.45}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">84.1</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.45</mn></mrow><annotation encoding=\"application/x-tex\">84.1{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.45}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T4.m3\" class=\"ltx_Math\" alttext=\"89.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.43}}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">89.3</mn><mo lspace=\"0.392em\" mathsize=\"0.630em\">±</mo><mn mathcolor=\"#808080\" mathsize=\"0.630em\"> 0.43</mn></mrow><annotation encoding=\"application/x-tex\">89.3{\\scriptstyle\\,\\pm\\,{\\color[rgb]{.5,.5,.5}\\definecolor[named]{pgfstrokecolor}{rgb}{.5,.5,.5}\\pgfsys@color@gray@stroke{.5}\\pgfsys@color@gray@fill{.5}0.43}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S5.T4.m4\" class=\"ltx_Math\" alttext=\"22.6\" display=\"inline\"><semantics><mn mathsize=\"0.900em\">22.6</mn><annotation encoding=\"application/x-tex\">22.6</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>\nICL accuracy on demonstration pairs with the same experiment setup\nas Section <a href=\"#S5.SS3\" title=\"5.3 Experiment Setup ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>, but evaluating on demonstration pairs instead of query pairs. Means and standard deviations are computed over 5 seeds.</figcaption>\n</figure>\n<div id=\"S5.SS7.SSS0.Px2.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">This perspective highlights a key weakness of ICL: relying on a single forward pass to encode complex task behavior often results in an incomplete or lossy task representation. In contrast, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> uses gradient-based optimization to iteratively refine the cache by explicitly training the model to solve each demonstration pair, leading to a more effective and robust task encoding.</p>\n</div>\n</section>\n</section>\n</section>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Conclusion</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_italic\">Context Tuning</span>, a simple and effective method for improving few-shot learning in language models by directly optimizing a prompt or prefix initialized from demonstration tokens. Our method combines the strengths of ICL, which leverages pretrained knowledge by conditioning on task examples at inference time, and prompt-based adaptation, which efficiently adapts to new tasks by tuning a small number of parameters. By initializing the tunable prompt or prefix derived from demonstration tokens, <span class=\"ltx_text ltx_font_italic\">Context Tuning</span> enables the model to begin optimization from a task-aware starting point, leading to strong performance without updating model weights.</p>\n</div>\n<div id=\"S6.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We develop two versions of this approach: <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>, which tunes input-level soft prompts, and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, which tunes layer-specific key and value prefixes derived from the model’s activations on demonstration pairs. Across a broad set of benchmarks, both methods outperform ICL and traditional prompt-based tuning, with <span class=\"ltx_text ltx_font_italic\">CT-KV</span> offering a more favorable trade-off between performance and efficiency. Through ablation studies, we show that <span class=\"ltx_text ltx_font_italic\">CT-KV</span>’s performance depends critically on two design choices: Leave-One-Out Masking and Token Dropout. To better understand the performance gap between ICL and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, we interpret ICL as a two-stage process and find that it often encodes an incomplete representation of the task in its intermediate cache of demonstration pairs. <span class=\"ltx_text ltx_font_italic\">CT-KV</span> addresses this limitation by explicitly refining the cache through optimization, resulting in a more accurate representation for steering the model toward solving query inputs.</p>\n</div>\n<div id=\"S6.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">More broadly, we frame our method within the <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> framework, which encompasses approaches that leverage in-context demonstration pairs to adapt either the model weights or its context at inference time. This perspective connects <span class=\"ltx_text ltx_font_italic\">CT-KV</span> and Test-Time Training under a shared goal of improving task adaptation through inference-time optimization. Our findings highlight that optimizing the lightweight context, rather than the model, is a powerful and scalable direction for few-shot learning, achieving competitive performance to TTT with significantly less training time. Moreover, we show that <span class=\"ltx_text ltx_font_italic\">CT-KV</span> can be applied after TTT to further improve performance, suggesting that context and model adaptation can be effectively combined.</p>\n</div>\n<section id=\"S6.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations and Future Work.</h4>\n\n<div id=\"S6.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Section <a href=\"#S5.SS6\" title=\"5.6 Qualitative Analysis ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5.6</span></a> identifies a potential limitation of <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, where it may be prone to overfitting on certain tasks. Future directions to improve <span class=\"ltx_text ltx_font_italic\">CT-KV</span> include exploring stronger regularization techniques beyond Token Dropout, or applying KV cache compression techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Devoto et al.,, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2024</a>; Ge et al.,, <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2024</a>; <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">Liu et al., 2024a, </a>)</cite> to compress <span class=\"ltx_text ltx_font_italic\">CT-KV</span>’s initialization <math id=\"S6.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"\\Theta\" display=\"inline\"><semantics><mi mathvariant=\"normal\">Θ</mi><annotation encoding=\"application/x-tex\">\\Theta</annotation></semantics></math> before training, further improving overall efficiency.</p>\n</div>\n</section>\n</section>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgement</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We thank members of the NYU Agentic Learning AI Lab for their helpful discussions.\nThe work is supported in part by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) under grant RS-2024-00469482, funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research.\nJack Lu is supported by the NSERC PGS-D Scholarship. The compute is supported by the NYU High Performance Computing\nresources, services, and staff expertise.</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Akyürek et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nAkyürek, E., Damani, M., Zweiger, A., Qiu, L., Guo, H., Pari, J., Kim, Y., and Andreas, J. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">The surprising effectiveness of test-time training for few-shot learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2411.07279</span>.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bonnet and Macfarlane,  (2024)</span>\n<span class=\"ltx_bibblock\">\nBonnet, C. and Macfarlane, M. V. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Searching latent program spaces.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2411.08706</span>.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brown et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Language models are few-shot learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NeurIPS</span>.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nChen, Y., Zhong, R., Zha, S., Karypis, G., and He, H. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Meta-learning via language model in-context tuning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(5)</span>\n<span class=\"ltx_bibblock\">\nChollet, F. (2019a).\n\n</span>\n<span class=\"ltx_bibblock\">Abstraction and reasoning corpus for artificial general intelligence (arc-agi).\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(6)</span>\n<span class=\"ltx_bibblock\">\nChollet, F. (2019b).\n\n</span>\n<span class=\"ltx_bibblock\">On the measure of intelligence.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1911.01547</span>.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chollet et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nChollet, F., Knoop, M., Kamradt, G., and Landers, B. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">Arc prize 2024: Technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2412.04604</span>.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dai et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nDai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dalal et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nDalal, K., Koceja, D., Hussein, G., Xu, J., Zhao, Y., Song, Y., Han, S., Cheung, K. C., Kautz, J., Guestrin, C., Hashimoto, T., Koyejo, S., Choi, Y., Sun, Y., and Wang, X. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">One-minute video generation with test-time training.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">CVPR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Deutch et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nDeutch, G., Magar, N., Natan, T., and Dar, G. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">In-context learning and gradient descent revisited.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NAACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Devoto et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nDevoto, A., Zhao, Y., Scardapane, S., and Minervini, P. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">A simple and effective <math id=\"bib.bib11.m1\" class=\"ltx_Math\" alttext=\"l\\_2\" display=\"inline\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">l\\_2</annotation></semantics></math> norm-based strategy for kv cache compression.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP</span>.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dhariwal and Nichol,  (2021)</span>\n<span class=\"ltx_bibblock\">\nDhariwal, P. and Nichol, A. Q. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Diffusion models beat gans on image synthesis.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NeurIPS</span>.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Franzen et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nFranzen, D., Disselhoff, J., and Hartmann, D. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">The llm architect: Solving the arc challenge is a matter of perspective.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2505.07859</span>.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gandelsman et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nGandelsman, Y., Sun, Y., Chen, X., and Efros, A. A. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Test-time training with masked autoencoders.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NeurIPS</span>.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Garg et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nGarg, S., Tsipras, D., Liang, P., and Valiant, G. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">What can transformers learn in-context? a case study of simple function classes.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2208.01066</span>.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ge et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nGe, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Model tells you what to discard: Adaptive kv cache compression for llms.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML</span>.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grattafiori et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., and et al. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">The llama 3 herd of models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2407.21783</span>.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hardt and Sun,  (2024)</span>\n<span class=\"ltx_bibblock\">\nHardt, M. and Sun, Y. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Test-time training on nearest neighbors for large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hendrycks et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Measuring massive multitask language understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ho,  (2022)</span>\n<span class=\"ltx_bibblock\">\nHo, J. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Classifier-free diffusion guidance.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2207.12598</span>.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ho et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nHo, J., Jain, A., and Abbeel, P. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Denoising diffusion probabilistic models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arxiv:2006.11239</span>.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">LoRA: Low-rank adaptation of large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICLR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jang et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nJang, J., Jang, S., Kweon, W., Jeon, M., and Yu, H. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Rectifying demonstration shortcut in in-context learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Mistral 7b.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2310.06825</span>.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Khashabi et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">UNIFIEDQA: Crossing format boundaries with a single QA system.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP (Findings)</span>.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kirkpatrick et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Overcoming catastrophic forgetting in neural networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">PNAS</span>.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lester et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nLester, B., Al-Rfou, R., and Constant, N. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">The power of scale for parameter-efficient prompt tuning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP</span>.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li and Qiu,  (2023)</span>\n<span class=\"ltx_bibblock\">\nLi, X. and Qiu, X. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Finding supporting examples for in-context learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP (Findings)</span>.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li and Liang,  (2021)</span>\n<span class=\"ltx_bibblock\">\nLi, X. L. and Liang, P. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Prefix-tuning: Optimizing continuous prompts for generation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(30)</span>\n<span class=\"ltx_bibblock\">\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B. (2024a).\n\n</span>\n<span class=\"ltx_bibblock\">Minicache: KV cache compression in depth dimension for large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NeurIPS</span>.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nLiu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">What makes good in-context examples for gpt-<math id=\"bib.bib31.m1\" class=\"ltx_Math\" alttext=\"3\" display=\"inline\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(32)</span>\n<span class=\"ltx_bibblock\">\nLiu, S., Ye, H., Xing, L., and Zou, J. (2024b).\n\n</span>\n<span class=\"ltx_bibblock\">In-context vectors: making in context learning more effective and controllable through latent space steering.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML</span>.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(33)</span>\n<span class=\"ltx_bibblock\">\nLiu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J. (2022a).\n\n</span>\n<span class=\"ltx_bibblock\">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(34)</span>\n<span class=\"ltx_bibblock\">\nLiu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J. (2022b).\n\n</span>\n<span class=\"ltx_bibblock\">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lu et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nLu, J., Teehan, R., and Ren, M. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Procreate, don’t reproduce! propulsive energy diffusion for creative generation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ECCV</span>.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(36)</span>\n<span class=\"ltx_bibblock\">\nMin, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. (2022a).\n\n</span>\n<span class=\"ltx_bibblock\">MetaICL: Learning to learn in context.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NAACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(37)</span>\n<span class=\"ltx_bibblock\">\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. (2022b).\n\n</span>\n<span class=\"ltx_bibblock\">Rethinking the role of demonstrations: What makes in-context learning work?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP</span>.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Muhtar et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nMuhtar, D., Shen, Y., Yang, Y., Liu, X., Lu, Y., Liu, J., Zhan, Y., Sun, H., Deng, W., Sun, F., Zhang, X., Gao, J., Chen, W., and Zhang, Q. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Streamadapter: Efficient test time adaptation from contextual streams.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2411.09289</span>.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nichol et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nNichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">GLIDE: towards photorealistic image generation and editing with text-guided diffusion models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML</span>.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Radford et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Language models are unsupervised multitask learners.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">OpenAI</span>.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rombach et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">High-resolution image synthesis with latent diffusion models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">CVPR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shin et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nShin, T., Razeghi, Y., IV, R. L. L., Wallace, E., and Singh, S. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">AutoPrompt: Eliciting knowledge from language models with automatically generated prompts.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP</span>.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Singhal et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., Payne, P., Seneviratne, M., Gamble, P., Kelly, C., Scharli, N., Chowdhery, A., Mansfield, P., y Arcas, B. A., Webster, D., Corrado, G. S., Matias, Y., Chou, K., Gottweis, J., Tomasev, N., Liu, Y., Rajkomar, A., Barral, J., Semturs, C., Karthikesalingam, A., and Natarajan, V. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Large language models encode clinical knowledge.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Nature</span>.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Srivastava et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nSrivastava, A., Rastogi, A., Rao, A., Md-Shoeb, A.-A., Abid, A., Fisch, A., Brown, A., Santoro, A., Gupta, A., and et al. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">TMLR</span>.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sun et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nSun, Y., Wang, X., Zhuang, L., Miller, J., Hardt, M., and Efros, A. A. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Test-time training with self-supervision for generalization under distribution shifts.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML</span>.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Suzgun et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nSuzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Challenging big-bench tasks and whether chain-of-thought can solve them.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ACL</span>.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wallace et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nWallace, B., Gokul, A., Ermon, S., and Naik, N. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">End-to-end diffusion latent optimization improves classifier guidance.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICCV</span>.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nWang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Self-consistency improves chain of thought reasoning in language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML</span>.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Chain-of-thought prompting elicits reasoning in large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NeurIPS</span>.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nYe, Q., Lin, B. Y., and Ren, X. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">CrossFit: A few-shot learning challenge for cross-task generalization in NLP.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">EMNLP</span>.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao,  (2023)</span>\n<span class=\"ltx_bibblock\">\nZhao, J. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">In-context exemplars as clues to retrieving from large associative memory.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICML Neural Conversational AI</span>.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"Ax1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n\n</section>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Time Complexity</h2>\n\n<div id=\"A1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">At each training iteration, an LLM’s forward and backward passes are dominated by its self-attention operations. Consider a single attention head of dimension <math id=\"A1.p1.m1\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. Let <math id=\"A1.p1.m2\" class=\"ltx_Math\" alttext=\"L_{Q}\" display=\"inline\"><semantics><msub><mi>L</mi><mi>Q</mi></msub><annotation encoding=\"application/x-tex\">L_{Q}</annotation></semantics></math> denote the number of query tokens and <math id=\"A1.p1.m3\" class=\"ltx_Math\" alttext=\"L_{K}\" display=\"inline\"><semantics><msub><mi>L</mi><mi>K</mi></msub><annotation encoding=\"application/x-tex\">L_{K}</annotation></semantics></math> the number of key (and value) tokens. We form the query matrix <math id=\"A1.p1.m4\" class=\"ltx_Math\" alttext=\"\\mathbf{Q}\\in\\mathbb{R}^{L_{Q}\\times d}\" display=\"inline\"><semantics><mrow><mi>𝐐</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>L</mi><mi>Q</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}\\in\\mathbb{R}^{L_{Q}\\times d}</annotation></semantics></math> and the key and value vectors <math id=\"A1.p1.m5\" class=\"ltx_Math\" alttext=\"\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{L_{K}\\times d}\" display=\"inline\"><semantics><mrow><mrow><mi>𝐊</mi><mo>,</mo><mi>𝐕</mi></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>L</mi><mi>K</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{L_{K}\\times d}</annotation></semantics></math>, then compute</p>\n<table id=\"A1.Ex12\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"A1.Ex12.m1\" class=\"ltx_Math\" alttext=\"\\mathrm{Attention}\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\right)\\;=\\;\\mathrm{softmax}\\,\\left(\\tfrac{\\mathbf{Q}\\,\\mathbf{K}^{\\top}}{\\sqrt{d}}\\right)\\,\\mathbf{V},\" display=\"block\"><semantics><mrow><mrow><mrow><mi>Attention</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mi>𝐐</mi><mo>,</mo><mi>𝐊</mi><mo>,</mo><mi>𝐕</mi><mo rspace=\"0.280em\">)</mo></mrow></mrow><mo rspace=\"0.558em\">=</mo><mrow><mi>softmax</mi><mo lspace=\"0.170em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mstyle displaystyle=\"false\"><mfrac><mrow><mi>𝐐</mi><mo lspace=\"0.170em\" rspace=\"0em\">​</mo><msup><mi>𝐊</mi><mo>⊤</mo></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac></mstyle><mo>)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">​</mo><mi>𝐕</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Attention}\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\right)\\;=\\;\\mathrm{softmax}\\,\\left(\\tfrac{\\mathbf{Q}\\,\\mathbf{K}^{\\top}}{\\sqrt{d}}\\right)\\,\\mathbf{V},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">whose dominant cost is the matrix multiplication <math id=\"A1.p1.m6\" class=\"ltx_Math\" alttext=\"\\mathbf{Q}\\,\\mathbf{K}^{\\top}\" display=\"inline\"><semantics><mrow><mi>𝐐</mi><mo lspace=\"0.170em\" rspace=\"0em\">​</mo><msup><mi>𝐊</mi><mo>⊤</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}\\,\\mathbf{K}^{\\top}</annotation></semantics></math>, requiring <math id=\"A1.p1.m7\" class=\"ltx_Math\" alttext=\"O(L_{Q}\\,L_{K}\\,d)\" display=\"inline\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>L</mi><mi>Q</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><msub><mi>L</mi><mi>K</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>d</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(L_{Q}\\,L_{K}\\,d)</annotation></semantics></math> operations per head. Because <math id=\"A1.p1.m8\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is a constant for a given model, we omit it in our comparisons below.</p>\n</div>\n<div id=\"A1.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Next, let <math id=\"A1.p2.m1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> be the number of tokens in the task’s query and <math id=\"A1.p2.m2\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> the number of additional trainable prompt or prefix tokens per layer, we analyze how the training time of each method in the <span class=\"ltx_text ltx_font_italic\">In-Context Optimization</span> framework scales with <math id=\"A1.p2.m3\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math id=\"A1.p2.m4\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>.</p>\n</div>\n<section id=\"A1.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Test Time Training.</h4>\n\n<div id=\"A1.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">At each layer of each training iteration, TTT prepends <math id=\"A1.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> trainable tokens to the <math id=\"A1.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> query tokens and computes their keys and values, giving <math id=\"A1.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"L_{Q}=n+p\" display=\"inline\"><semantics><mrow><msub><mi>L</mi><mi>Q</mi></msub><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">L_{Q}=n+p</annotation></semantics></math> and <math id=\"A1.SS0.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"L_{K}=n+p\" display=\"inline\"><semantics><mrow><msub><mi>L</mi><mi>K</mi></msub><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">L_{K}=n+p</annotation></semantics></math> with a per-head cost of</p>\n<table id=\"A1.Ex13\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"A1.Ex13.m1\" class=\"ltx_Math\" alttext=\"O\\left((n+p)^{2}\\right).\" display=\"block\"><semantics><mrow><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>)</mo></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">O\\left((n+p)^{2}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"A1.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-Prompt.</h4>\n\n<div id=\"A1.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> prepends <math id=\"A1.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> trainable soft token embeddings to the query and computes their keys and values, also giving <math id=\"A1.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"L_{Q}=n+p\" display=\"inline\"><semantics><mrow><msub><mi>L</mi><mi>Q</mi></msub><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">L_{Q}=n+p</annotation></semantics></math> and <math id=\"A1.SS0.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"L_{K}=n+p\" display=\"inline\"><semantics><mrow><msub><mi>L</mi><mi>K</mi></msub><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">L_{K}=n+p</annotation></semantics></math> with a per-head cost of</p>\n<table id=\"A1.Ex14\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"A1.Ex14.m1\" class=\"ltx_Math\" alttext=\"O\\left((n+p)^{2}\\right).\" display=\"block\"><semantics><mrow><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>)</mo></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">O\\left((n+p)^{2}\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"A1.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-KV.</h4>\n\n<div id=\"A1.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Unlike from TTT and <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> prepends <math id=\"A1.SS0.SSS0.Px3.p1.m1\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> trainable tokens as past keys and values, so these tokens do not generate queries. This yields <math id=\"A1.SS0.SSS0.Px3.p1.m2\" class=\"ltx_Math\" alttext=\"L_{Q}=n\" display=\"inline\"><semantics><mrow><msub><mi>L</mi><mi>Q</mi></msub><mo>=</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">L_{Q}=n</annotation></semantics></math> and <math id=\"A1.SS0.SSS0.Px3.p1.m3\" class=\"ltx_Math\" alttext=\"L_{K}=n+p\" display=\"inline\"><semantics><mrow><msub><mi>L</mi><mi>K</mi></msub><mo>=</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">L_{K}=n+p</annotation></semantics></math> with a per-head cost of only</p>\n<table id=\"A1.Ex15\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"A1.Ex15.m1\" class=\"ltx_Math\" alttext=\"O\\left(n\\,(n+p)\\right).\" display=\"block\"><semantics><mrow><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mrow><mi>n</mi><mo lspace=\"0.170em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">O\\left(n\\,(n+p)\\right).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section id=\"A1.SS0.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Time Complexity for k Demonstrations</h4>\n\n<div id=\"A1.SS0.SSS0.Px4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Suppose we have <math id=\"A1.SS0.SSS0.Px4.p1.m1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstration pairs, each of length <math id=\"A1.SS0.SSS0.Px4.p1.m2\" class=\"ltx_Math\" alttext=\"\\ell\" display=\"inline\"><semantics><mi mathvariant=\"normal\">ℓ</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> (assuming equal length). In TTT, <math id=\"A1.SS0.SSS0.Px4.p1.m3\" class=\"ltx_Math\" alttext=\"n=\\ell\" display=\"inline\"><semantics><mrow><mi>n</mi><mo>=</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><annotation encoding=\"application/x-tex\">n=\\ell</annotation></semantics></math> is the length of a demonstration pair and <math id=\"A1.SS0.SSS0.Px4.p1.m4\" class=\"ltx_Math\" alttext=\"p=(k-1)\\ell\" display=\"inline\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow></mrow><annotation encoding=\"application/x-tex\">p=(k-1)\\ell</annotation></semantics></math> is the summed length of other demonstration pairs. For <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, <math id=\"A1.SS0.SSS0.Px4.p1.m5\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math id=\"A1.SS0.SSS0.Px4.p1.m6\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> have the same values as TTT because Leave One Out masks out one of the in-context demonstration pairs. Table <a href=\"#A1.T5\" title=\"Table 5 ‣ Time Complexity for k Demonstrations ‣ Appendix A Time Complexity ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes the per-head costs in <math id=\"A1.SS0.SSS0.Px4.p1.m7\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> and <math id=\"A1.SS0.SSS0.Px4.p1.m8\" class=\"ltx_Math\" alttext=\"\\ell\" display=\"inline\"><semantics><mi mathvariant=\"normal\">ℓ</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, showing that both <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and TTT incur quadratic cost in <math id=\"A1.SS0.SSS0.Px4.p1.m9\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, while <span class=\"ltx_text ltx_font_italic\">CT-KV</span> grows only linearly in <math id=\"A1.SS0.SSS0.Px4.p1.m10\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>. This <math id=\"A1.SS0.SSS0.Px4.p1.m11\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-fold reduction in self-attention complexity explains <span class=\"ltx_text ltx_font_italic\">CT-KV</span>’s faster empirical training speed in Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<figure id=\"A1.T5\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A1.T5.m1\" class=\"ltx_Math\" alttext=\"L_{Q}\" display=\"inline\"><semantics><msub><mi>L</mi><mi>Q</mi></msub><annotation encoding=\"application/x-tex\">L_{Q}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A1.T5.m2\" class=\"ltx_Math\" alttext=\"L_{K}\" display=\"inline\"><semantics><msub><mi>L</mi><mi>K</mi></msub><annotation encoding=\"application/x-tex\">L_{K}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Per-Head Cost</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">TTT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"A1.T5.m3\" class=\"ltx_Math\" alttext=\"k\\ell\" display=\"inline\"><semantics><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><annotation encoding=\"application/x-tex\">k\\ell</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"A1.T5.m4\" class=\"ltx_Math\" alttext=\"k\\ell\" display=\"inline\"><semantics><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><annotation encoding=\"application/x-tex\">k\\ell</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"A1.T5.m5\" class=\"ltx_Math\" alttext=\"O\\left((k\\ell)^{2}\\right)\" display=\"inline\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O\\left((k\\ell)^{2}\\right)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CT-Prompt</th>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A1.T5.m6\" class=\"ltx_Math\" alttext=\"k\\ell\" display=\"inline\"><semantics><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><annotation encoding=\"application/x-tex\">k\\ell</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A1.T5.m7\" class=\"ltx_Math\" alttext=\"k\\ell\" display=\"inline\"><semantics><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><annotation encoding=\"application/x-tex\">k\\ell</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A1.T5.m8\" class=\"ltx_Math\" alttext=\"O\\left((k\\ell)^{2}\\right)\" display=\"inline\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><msup><mrow><mo stretchy=\"false\">(</mo><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mn>2</mn></msup><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O\\left((k\\ell)^{2}\\right)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">CT-KV</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"A1.T5.m9\" class=\"ltx_Math\" alttext=\"\\ell\" display=\"inline\"><semantics><mi mathvariant=\"normal\">ℓ</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"A1.T5.m10\" class=\"ltx_Math\" alttext=\"k\\ell\" display=\"inline\"><semantics><mrow><mi>k</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathvariant=\"normal\">ℓ</mi></mrow><annotation encoding=\"application/x-tex\">k\\ell</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"A1.T5.m11\" class=\"ltx_Math\" alttext=\"O\\left(k\\,\\ell^{2}\\right)\" display=\"inline\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo>(</mo><mrow><mi>k</mi><mo lspace=\"0.170em\" rspace=\"0em\">​</mo><msup><mi mathvariant=\"normal\">ℓ</mi><mn>2</mn></msup></mrow><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O\\left(k\\,\\ell^{2}\\right)</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>\nPer-head self-attention time complexity for methods with <math id=\"A1.T5.m14\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstration pairs of length <math id=\"A1.T5.m15\" class=\"ltx_Math\" alttext=\"\\ell\" display=\"inline\"><semantics><mi mathvariant=\"normal\">ℓ</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>.\n</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Prompt Tuning and Prefix Tuning with Other Initialization Schemes</h2>\n\n<figure id=\"A2.T6\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (</span><math id=\"A2.T6.m1\" class=\"ltx_Math\" alttext=\"m=32\" display=\"inline\"><semantics><mrow><mi mathsize=\"0.900em\">m</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">32</mn></mrow><annotation encoding=\"application/x-tex\">m=32</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, uniform)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (</span><math id=\"A2.T6.m2\" class=\"ltx_Math\" alttext=\"m=32\" display=\"inline\"><semantics><mrow><mi mathsize=\"0.900em\">m</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">32</mn></mrow><annotation encoding=\"application/x-tex\">m=32</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, token)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (</span><math id=\"A2.T6.m3\" class=\"ltx_Math\" alttext=\"m=32\" display=\"inline\"><semantics><mrow><mi mathsize=\"0.900em\">m</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">32</mn></mrow><annotation encoding=\"application/x-tex\">m=32</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, uniform)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.22</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (</span><math id=\"A2.T6.m4\" class=\"ltx_Math\" alttext=\"m=32\" display=\"inline\"><semantics><mrow><mi mathsize=\"0.900em\">m</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">32</mn></mrow><annotation encoding=\"application/x-tex\">m=32</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, MLP)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.03</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (</span><math id=\"A2.T6.m5\" class=\"ltx_Math\" alttext=\"m=32\" display=\"inline\"><semantics><mrow><mi mathsize=\"0.900em\">m</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">32</mn></mrow><annotation encoding=\"application/x-tex\">m=32</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, token)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.3</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>\nAblation of initialization schemes for Prompt Tuning and Prefix Tuning. We show the means of accuracies over 5 seeds with different sets of demonstration pairs per task (except for ARC because it has fixed demonstration pairs).</figcaption>\n</figure>\n<div id=\"A2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we reported Prompt Tuning and Prefix Tuning results using only random-token initialization for their trainable prompts and prefixes. Here, we also follow <cite class=\"ltx_cite ltx_citemacro_citet\">Lester et al., (<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> in initializing prompts from a uniform distribution, and <cite class=\"ltx_cite ltx_citemacro_citet\">Li and Liang, (<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>)</cite> in initializing prefixes either from a uniform distribution or from a seed prefix passed through a two-layer MLP (hidden size 512). Table <a href=\"#A2.T6\" title=\"Table 6 ‣ Appendix B Prompt Tuning and Prefix Tuning with Other Initialization Schemes ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that both Prompt Tuning and Prefix Tuning perform best with random-token initialization, confirming the findings of those works. Therefore, even when compared against these alternative initialization schemes, <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> and <span class=\"ltx_text ltx_font_italic\">CT-KV</span> continue to deliver superior performance.</p>\n</div>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>More Details on Experiment Setup</h2>\n\n<div id=\"A3.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We detail below our hyperparameter settings for the results reported in Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table <a href=\"#A2.T6\" title=\"Table 6 ‣ Appendix B Prompt Tuning and Prefix Tuning with Other Initialization Schemes ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. For TTT, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>: using a LoRA learning rate of 1e-4, sampling a random permutation of the <math id=\"A3.p1.m1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstration pairs at each training step, and setting the LoRA rank to <math id=\"A3.p1.m2\" class=\"ltx_Math\" alttext=\"128\" display=\"inline\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> for ARC and <math id=\"A3.p1.m3\" class=\"ltx_Math\" alttext=\"64\" display=\"inline\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math> for all other tasks. In our TTT+<span class=\"ltx_text ltx_font_italic\">CT-KV</span> experiments, we find that using a small number of <span class=\"ltx_text ltx_font_italic\">CT-KV</span> training iterations and lower learning rates further boosts performance on top of a TTT-adapted model.</p>\n</div>\n<div id=\"A3.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">For all other experiments, we search over learning rates 3e-4, 1e-3, 3e-3 and Token Dropout rates <math id=\"A3.p2.m1\" class=\"ltx_Math\" alttext=\"0,0.05,0.1\" display=\"inline\"><semantics><mrow><mn>0</mn><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">0,0.05,0.1</annotation></semantics></math>. We search training iterations <math id=\"A3.p2.m2\" class=\"ltx_Math\" alttext=\"150,200,250,300\" display=\"inline\"><semantics><mrow><mn>150</mn><mo>,</mo><mn>200</mn><mo>,</mo><mn>250</mn><mo>,</mo><mn>300</mn></mrow><annotation encoding=\"application/x-tex\">150,200,250,300</annotation></semantics></math> for NLP-LR and ARC experiments, <math id=\"A3.p2.m3\" class=\"ltx_Math\" alttext=\"15,20,25,30\" display=\"inline\"><semantics><mrow><mn>15</mn><mo>,</mo><mn>20</mn><mo>,</mo><mn>25</mn><mo>,</mo><mn>30</mn></mrow><annotation encoding=\"application/x-tex\">15,20,25,30</annotation></semantics></math> for MMLU experiments, and <math id=\"A3.p2.m4\" class=\"ltx_Math\" alttext=\"12,16,20,24\" display=\"inline\"><semantics><mrow><mn>12</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>20</mn><mo>,</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">12,16,20,24</annotation></semantics></math> for BBH experiments. Table <a href=\"#A3.T7\" title=\"Table 7 ‣ Appendix C More Details on Experiment Setup ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows our hyperparameter choices. For fair comparison, hyperparameter sweeps are performed for all methods. For <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span>, <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, and TTT+<span class=\"ltx_text ltx_font_italic\">CT-KV</span>, we use Token Dropout rates of <math id=\"A3.p2.m5\" class=\"ltx_Math\" alttext=\"0.05\" display=\"inline\"><semantics><mn>0.05</mn><annotation encoding=\"application/x-tex\">0.05</annotation></semantics></math> for NLP-LR and <math id=\"A3.p2.m6\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math> for MMLU, BBH, and ARC.</p>\n</div>\n<div id=\"A3.p3\" class=\"ltx_para\">\n<p class=\"ltx_p\">Experiments for NLP-LR are performed on a single RTX8000, while all other experiments are conducted on a single A100. All experiments use the Adam optimizer, a cosine learning rate scheduler with no warm-up, bfloat16 precision, and up to 32GB of CPU RAM.</p>\n</div>\n<div id=\"A3.p4\" class=\"ltx_para\">\n<p class=\"ltx_p\">To fairly compare efficiency, we train each method with the largest batch size possible for the GPU used in its experiment. Since TTT, Prompt Tuning (m = # demo), and <span class=\"ltx_text ltx_font_italic\">CT-Prompt</span> use more memory than other methods due to computing larger <math id=\"A3.p4.m1\" class=\"ltx_Math\" alttext=\"QK^{T}\" display=\"inline\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">QK^{T}</annotation></semantics></math> matrices (as shown in our derivation in Section <a href=\"#A1\" title=\"Appendix A Time Complexity ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>), we limit their batch sizes to <math id=\"A3.p4.m2\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> for NLP-LR, MMLU, and ARC, and <math id=\"A3.p4.m3\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> for BBH. MMLU and BBH models use gradient checkpointing. For all other methods, we use batch size <math id=\"A3.p4.m4\" class=\"ltx_Math\" alttext=\"16\" display=\"inline\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math> for NLP-LR, <math id=\"A3.p4.m5\" class=\"ltx_Math\" alttext=\"8\" display=\"inline\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> for MMLU with gradient accumulation of <math id=\"A3.p4.m6\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, <math id=\"A3.p4.m7\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> for BBH with gradient accumulation of <math id=\"A3.p4.m8\" class=\"ltx_Math\" alttext=\"5\" display=\"inline\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>, and full batch for ARC (depending on each task’s number of demonstration pairs). All models, unless noted, do not require gradient checkpointing.</p>\n</div>\n<figure id=\"A3.T7\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\"># iters</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\"># iters</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\"># iters</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LR</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\"># iters</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = 32, uniform)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = 32, token)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = # demo, token)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = 32, uniform)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = 32, MLP)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = 32, token)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = # demo, token)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-Prompt</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTT</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">250</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">TTT+</span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1e-3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>\nLearning rates (LR) and number of training iterations (# iters) used for each method and benchmark.\n</figcaption>\n</figure>\n</section>\n<section id=\"A4\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Parameter-Efficient Variants of CT-KV</h2>\n\n<div id=\"A4.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In this section, we explore two variants of <span class=\"ltx_text ltx_font_italic\">CT-KV</span> that reduce the number of trainable prefix parameters. We use the notations from Section <a href=\"#S4\" title=\"4 Context Tuning for In-Context Optimization ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n</div>\n<section id=\"A4.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-V</h4>\n\n<div id=\"A4.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We partition the trainable prefix <math id=\"A4.SS0.SSS0.Px1.p1.m1\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}</annotation></semantics></math> into its key and value components, <math id=\"A4.SS0.SSS0.Px1.p1.m2\" class=\"ltx_Math\" alttext=\"\\Theta_{K}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>K</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{K}</annotation></semantics></math> and <math id=\"A4.SS0.SSS0.Px1.p1.m3\" class=\"ltx_Math\" alttext=\"\\Theta_{V}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{V}</annotation></semantics></math>. Inspired by <cite class=\"ltx_cite ltx_citemacro_citet\">Kirkpatrick et al., (<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>, we estimate the importance of each trainable parameter <math id=\"A4.SS0.SSS0.Px1.p1.m4\" class=\"ltx_Math\" alttext=\"\\Theta_{j}\\in\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>j</mi></msub><mo>∈</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Theta_{j}\\in\\Theta_{\\mathrm{CT}}</annotation></semantics></math> by computing its diagonal Fisher term over the <math id=\"A4.SS0.SSS0.Px1.p1.m5\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstrations in <math id=\"A4.SS0.SSS0.Px1.p1.m6\" class=\"ltx_Math\" alttext=\"\\mathcal{D}\" display=\"inline\"><semantics><mi class=\"ltx_font_mathcaligraphic\">𝒟</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>:</p>\n<table id=\"A4.Ex16\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"A4.Ex16.m1\" class=\"ltx_math_unparsed\" alttext=\"\\hat{F}_{j}=\\frac{1}{k}\\sum_{i=1}^{k}\\left(\\nabla_{\\Theta_{j}}\\log p_{\\phi}(y_{i}\\bigm{|}\\Theta_{\\mathrm{CT}},x_{i})\\right)^{2}.\" display=\"block\"><semantics><mrow><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo movablelimits=\"false\" rspace=\"0em\">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msup><mrow><mo>(</mo><msub><mo rspace=\"0.167em\">∇</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>j</mi></msub></msub><mi>log</mi><msub><mi>p</mi><mi>ϕ</mi></msub><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo lspace=\"0em\" mathsize=\"1.200em\" rspace=\"0em\">|</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow><mn>2</mn></msup><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{F}_{j}=\\frac{1}{k}\\sum_{i=1}^{k}\\left(\\nabla_{\\Theta_{j}}\\log p_{\\phi}(y_{i}\\bigm{|}\\Theta_{\\mathrm{CT}},x_{i})\\right)^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\"><math id=\"A4.SS0.SSS0.Px1.p1.m7\" class=\"ltx_Math\" alttext=\"\\hat{F}_{j}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\hat{F}_{j}</annotation></semantics></math> provides a relative estimate of how much a change in each parameter <math id=\"A4.SS0.SSS0.Px1.p1.m8\" class=\"ltx_Math\" alttext=\"\\Theta_{j}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{j}</annotation></semantics></math> affects the model’s ability to solve each demonstration pair, representing its importance during training. By averaging <math id=\"A4.SS0.SSS0.Px1.p1.m9\" class=\"ltx_Math\" alttext=\"\\hat{F}_{j}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\hat{F}_{j}</annotation></semantics></math> across parameters in <math id=\"A4.SS0.SSS0.Px1.p1.m10\" class=\"ltx_Math\" alttext=\"\\Theta_{K}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>K</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{K}</annotation></semantics></math> and <math id=\"A4.SS0.SSS0.Px1.p1.m11\" class=\"ltx_Math\" alttext=\"\\Theta_{V}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{V}</annotation></semantics></math>, we obtain two scalar estimates, <math id=\"A4.SS0.SSS0.Px1.p1.m12\" class=\"ltx_Math\" alttext=\"\\hat{F}_{K}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>K</mi></msub><annotation encoding=\"application/x-tex\">\\hat{F}_{K}</annotation></semantics></math> and <math id=\"A4.SS0.SSS0.Px1.p1.m13\" class=\"ltx_Math\" alttext=\"\\hat{F}_{V}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>V</mi></msub><annotation encoding=\"application/x-tex\">\\hat{F}_{V}</annotation></semantics></math>, indicating the relative importance of the trainable keys and values, respectively. Based on our findings in Table <a href=\"#A4.T8\" title=\"Table 8 ‣ CT-V ‣ Appendix D Parameter-Efficient Variants of CT-KV ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we conclude that <math id=\"A4.SS0.SSS0.Px1.p1.m14\" class=\"ltx_Math\" alttext=\"\\hat{F}_{V}\\gg\\hat{F}_{K}\" display=\"inline\"><semantics><mrow><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>V</mi></msub><mo>≫</mo><msub><mover accent=\"true\"><mi>F</mi><mo>^</mo></mover><mi>K</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{F}_{V}\\gg\\hat{F}_{K}</annotation></semantics></math> for most tasks, suggesting that values play a more significant role. By freezing <math id=\"A4.SS0.SSS0.Px1.p1.m15\" class=\"ltx_Math\" alttext=\"\\Theta_{K}\\subset\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>K</mi></msub><mo>⊂</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Theta_{K}\\subset\\Theta_{\\mathrm{CT}}</annotation></semantics></math> and training only <math id=\"A4.SS0.SSS0.Px1.p1.m16\" class=\"ltx_Math\" alttext=\"\\Theta_{V}\\subset\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>V</mi></msub><mo>⊂</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Theta_{V}\\subset\\Theta_{\\mathrm{CT}}</annotation></semantics></math>, we arrive at <span class=\"ltx_text ltx_font_italic\">CT-V</span>, which reduces the number of trainable parameters in <span class=\"ltx_text ltx_font_italic\">CT-KV</span> by exactly half.</p>\n</div>\n<figure id=\"A4.T8\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A4.T8.m1\" class=\"ltx_Math\" alttext=\"\\hat{F}_{K}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi mathsize=\"0.900em\">F</mi><mo mathsize=\"0.900em\">^</mo></mover><mi mathsize=\"0.900em\">K</mi></msub><annotation encoding=\"application/x-tex\">\\hat{F}_{K}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A4.T8.m2\" class=\"ltx_Math\" alttext=\"\\hat{F}_{V}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi mathsize=\"0.900em\">F</mi><mo mathsize=\"0.900em\">^</mo></mover><mi mathsize=\"0.900em\">V</mi></msub><annotation encoding=\"application/x-tex\">\\hat{F}_{V}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ARC</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"A4.T8.m3\" class=\"ltx_Math\" alttext=\"2.43\\times 10^{-9}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">2.43</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">9</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.43\\times 10^{-9}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"A4.T8.m4\" class=\"ltx_Math\" alttext=\"1.02\\times 10^{-7}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">1.02</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">7</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.02\\times 10^{-7}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">BBH</span></th>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A4.T8.m5\" class=\"ltx_Math\" alttext=\"1.89\\times 10^{-6}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">1.89</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.89\\times 10^{-6}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A4.T8.m6\" class=\"ltx_Math\" alttext=\"3.99\\times 10^{-4}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">3.99</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.99\\times 10^{-4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">NLP-LR</span></th>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A4.T8.m7\" class=\"ltx_Math\" alttext=\"1.44\\times 10^{-8}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">1.44</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">8</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.44\\times 10^{-8}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math id=\"A4.T8.m8\" class=\"ltx_Math\" alttext=\"8.32\\times 10^{-8}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">8.32</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">8</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">8.32\\times 10^{-8}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">MMLU</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"A4.T8.m9\" class=\"ltx_Math\" alttext=\"2.81\\times 10^{-8}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">2.81</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">8</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.81\\times 10^{-8}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"A4.T8.m10\" class=\"ltx_Math\" alttext=\"1.42\\times 10^{-6}\" display=\"inline\"><semantics><mrow><mn mathsize=\"0.900em\">1.42</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.42\\times 10^{-6}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>\nAverage Fisher information for the trainable key parameters <math id=\"A4.T8.m14\" class=\"ltx_Math\" alttext=\"\\Theta_{K}\\subset\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>K</mi></msub><mo>⊂</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Theta_{K}\\subset\\Theta_{\\mathrm{CT}}</annotation></semantics></math> and value parameters <math id=\"A4.T8.m15\" class=\"ltx_Math\" alttext=\"\\Theta_{V}\\subset\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>V</mi></msub><mo>⊂</mo><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Theta_{V}\\subset\\Theta_{\\mathrm{CT}}</annotation></semantics></math> across 5 random selections of <math id=\"A4.T8.m16\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> demonstration pairs over each dataset.\n</figcaption>\n</figure>\n</section>\n<section id=\"A4.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CT-Prefix</h4>\n\n<div id=\"A4.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We freeze <math id=\"A4.SS0.SSS0.Px2.p1.m1\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{CT}}</annotation></semantics></math>, average the parameters across tokens to obtain an average prefix <math id=\"A4.SS0.SSS0.Px2.p1.m2\" class=\"ltx_Math\" alttext=\"\\bar{\\Theta}_{\\mathrm{CT}}\" display=\"inline\"><semantics><msub><mover accent=\"true\"><mi mathvariant=\"normal\">Θ</mi><mo>¯</mo></mover><mi>CT</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\Theta}_{\\mathrm{CT}}</annotation></semantics></math>, and then form a new trainable <math id=\"A4.SS0.SSS0.Px2.p1.m3\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>-token prefix <math id=\"A4.SS0.SSS0.Px2.p1.m4\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{prefix}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>prefix</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{prefix}}</annotation></semantics></math> by adding small Gaussian perturbations:</p>\n<table id=\"A4.Ex17\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"A4.Ex17.m1\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{prefix}}=\\{\\bar{\\Theta}_{\\mathrm{CT}}+\\epsilon_{i}\\}_{i=1}^{m},\" display=\"block\"><semantics><mrow><mrow><msub><mi mathvariant=\"normal\">Θ</mi><mi>prefix</mi></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mover accent=\"true\"><mi mathvariant=\"normal\">Θ</mi><mo>¯</mo></mover><mi>CT</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{prefix}}=\\{\\bar{\\Theta}_{\\mathrm{CT}}+\\epsilon_{i}\\}_{i=1}^{m},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where <math id=\"A4.SS0.SSS0.Px2.p1.m5\" class=\"ltx_Math\" alttext=\"\\epsilon_{i}\\in\\mathcal{N}(0,0.02)\" display=\"inline\"><semantics><mrow><msub><mi>ϵ</mi><mi>i</mi></msub><mo>∈</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">𝒩</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>0.02</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\epsilon_{i}\\in\\mathcal{N}(0,0.02)</annotation></semantics></math>. The model additionally conditions on <math id=\"A4.SS0.SSS0.Px2.p1.m6\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{prefix}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>prefix</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{prefix}}</annotation></semantics></math>, analogous to Prefix Tuning. Since we only train <math id=\"A4.SS0.SSS0.Px2.p1.m7\" class=\"ltx_Math\" alttext=\"\\Theta_{\\mathrm{prefix}}\" display=\"inline\"><semantics><msub><mi mathvariant=\"normal\">Θ</mi><mi>prefix</mi></msub><annotation encoding=\"application/x-tex\">\\Theta_{\\mathrm{prefix}}</annotation></semantics></math>, this variant has the same number of trainable parameters as Prefix Tuning with <math id=\"A4.SS0.SSS0.Px2.p1.m8\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> tokens.</p>\n</div>\n<div id=\"A4.SS0.SSS0.Px2.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">We evaluate <span class=\"ltx_text ltx_font_italic\">CT-V</span> and <span class=\"ltx_text ltx_font_italic\">CT-Prefix</span> across all benchmarks and compare them to <span class=\"ltx_text ltx_font_italic\">CT-KV</span> in Table <a href=\"#A4.T9\" title=\"Table 9 ‣ CT-Prefix ‣ Appendix D Parameter-Efficient Variants of CT-KV ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, showing that both parameter-efficient variants retain most of the performance gain of <span class=\"ltx_text ltx_font_italic\">CT-KV</span> and outperform Prefix Tuning. For <span class=\"ltx_text ltx_font_italic\">CT-V</span>, we use the same hyperparameters as <span class=\"ltx_text ltx_font_italic\">CT-KV</span> from Section <a href=\"#A3\" title=\"Appendix C More Details on Experiment Setup ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For <span class=\"ltx_text ltx_font_italic\">CT-Prefix</span>, we find that higher learning rates, 1e-1 for NLP-LR and 5e-2 for other datasets, are needed for better performance.</p>\n</div>\n<figure id=\"A4.T9\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = 32)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-Prefix</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-V</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.8</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>\nAccuracies (%) of <span class=\"ltx_text ltx_font_italic\">CT-KV</span>, its parameter-efficient variants, and Prefix Tuning across benchmarks, averaged over 5 seeds (except for ARC because it has fixed demonstration pairs).\n</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A5\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Number of Trainable Parameters</h2>\n\n<div id=\"A5.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">Corresponding to the performance shown in Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we report the average number of trainable parameters for each method across tasks in Table <a href=\"#A5.T10\" title=\"Table 10 ‣ Appendix E Number of Trainable Parameters ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Note that although <span class=\"ltx_text ltx_font_italic\">CT-KV</span>’s number of trainable parameters scales with the number of demonstration tokens, it still trains significantly fewer parameters on average per task than the number of LoRA parameters used by TTT. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Akyürek et al., (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>, we use task instructions for BBH and set the instruction prompt or prefix to be trainable as well. We omit Zero-Shot and ICL from this comparison because they do not involve any trainable parameters.</p>\n</div>\n<figure id=\"A5.T10\" class=\"ltx_table\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLP-LR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BBH</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = 32)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">229</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt Tuning (m = # demo)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">578</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2160</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3656</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2743</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = 32)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2949</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1835</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3668</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">524</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prefix Tuning (m = # demo)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41634</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40327</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58501</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">21944</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTT</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">47186</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">89915</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">157286</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">84935</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-Prompt</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">578</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2160</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3656</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2743</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-Prefix</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2949</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1835</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3668</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">524</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Context Tuning</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20817</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20163</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29250</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10972</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41634</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40327</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58501</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">21944</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">TTT+</span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CT-KV</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">88820</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">130242</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">215787</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">106878</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>\nNumber of trainable parameters (in thousands) for each method across benchmarks, corresponding to entries in Table <a href=\"#S5.T1\" title=\"Table 1 ‣ 5.4 Comparing Context Tuning to Baselines ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\n</figcaption>\n</figure>\n</section>\n<section id=\"A6\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix F </span>Qualitative Samples vs. Training Iteration</h2>\n\n<figure id=\"A6.F5\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"A6.F5.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"538\" height=\"680\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>\nFor each of the two ARC tasks at the top and bottom, we display 4 demonstration query-answer pairs, the test query, and LLM predictions at <span class=\"ltx_text ltx_font_italic\">CT-KV</span> training iterations 0, 50, 100, 150, 200. Note that iteration 0 is equivalent to ICL. We color-code the iterations of correct predictions in green and incorrect predictions in red.\n</figcaption>\n</figure>\n<div id=\"A6.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">In this section, we select sample tasks from question-answering datasets to illustrate how autoregressively generated answers gradually improve with <span class=\"ltx_text ltx_font_italic\">CT-KV</span> training. We present two ARC tasks in Figure <a href=\"#A6.F5\" title=\"Figure 5 ‣ Appendix F Qualitative Samples vs. Training Iteration ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. In the top task, the model’s prediction at iteration 0 (equivalent to ICL) shows a strong bias toward filling orange squares with yellow. As <span class=\"ltx_text ltx_font_italic\">CT-KV</span> training progresses, the model gradually learns to fill each orange square with the correct color. Similarly, in the bottom task, the model first learns that only grey grid cells can turn red, and then correctly completes the cross shapes.</p>\n</div>\n<figure id=\"A6.F6\" class=\"ltx_figure\"><img src=\"./assets/x6.png\" id=\"A6.F6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"336\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>\nWe display LLM predictions at <span class=\"ltx_text ltx_font_italic\">CT-KV</span> training iterations 0, 12, 16, 20 for two queries from the task “word sorting” in BBH. We omit showing the <math id=\"A6.F6.m2\" class=\"ltx_Math\" alttext=\"16\" display=\"inline\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math> demonstration pairs of each task for brevity. We color-code the iterations of correct predictions in green and incorrect predictions in red.\n</figcaption>\n</figure>\n<div id=\"A6.p2\" class=\"ltx_para\">\n<p class=\"ltx_p\">Similarly, for BBH, in Figure <a href=\"#A6.F6\" title=\"Figure 6 ‣ Appendix F Qualitative Samples vs. Training Iteration ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>’s top query, the model initially predicts “padre, panicking” and “schoolmate, suburbia” in reversed order at iteration 0. During <span class=\"ltx_text ltx_font_italic\">CT-KV</span> training, the model learns to use the second letter of each word for sorting and eventually answers the query correctly. Likewise, for the bottom query, <span class=\"ltx_text ltx_font_italic\">CT-KV</span> helps the model avoid omitting the word “scrumptious” from its outputs and sort the words “sidereal, siena” into the correct order based on their second letters.</p>\n</div>\n</section>\n<section id=\"A7\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix G </span>Demonstration Pairs for Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ 5.1 Datasets ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>\n</h2>\n\n<div id=\"A7.p1\" class=\"ltx_para\">\n<p class=\"ltx_p\">We present three demonstration pairs of datasets: BBH, NLP-LR, and MMLU in Figure <a href=\"#A7.F7\" title=\"Figure 7 ‣ Appendix G Demonstration Pairs for Figure 3 ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Figure <a href=\"#A7.F8\" title=\"Figure 8 ‣ Appendix G Demonstration Pairs for Figure 3 ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, and Figure <a href=\"#A7.F9\" title=\"Figure 9 ‣ Appendix G Demonstration Pairs for Figure 3 ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, respectively.</p>\n</div>\n<figure id=\"A7.F7\" class=\"ltx_figure\"><img src=\"./assets/x7.png\" id=\"A7.F7.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"598\" height=\"550\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>3 demonstration pairs for the BBH task from Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ 5.1 Datasets ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</figcaption>\n</figure>\n<figure id=\"A7.F8\" class=\"ltx_figure\"><img src=\"./assets/x8.png\" id=\"A7.F8.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"218\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>3 demonstration pairs for the NLP-LR task from Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ 5.1 Datasets ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</figcaption>\n</figure>\n<figure id=\"A7.F9\" class=\"ltx_figure\"><img src=\"./assets/x9.png\" id=\"A7.F9.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"221\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>3 demonstration pairs for the MMLU task from Figure <a href=\"#S5.F3\" title=\"Figure 3 ‣ 5.1 Datasets ‣ 5 Experiments ‣ Context Tuning for In-Context Optimization\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2507.04221",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:26.386Z"
}
