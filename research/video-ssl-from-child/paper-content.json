{
  "html": "<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Introduction</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">Children develop sophisticated visual models of the world early in their development. Whether this feat requires strong innate inductive biases or whether it can be achieved simply by applying highly generic but scalable learning algorithms to a child’s visual experience is arguably one of the most significant open questions in developmental psychology <span id=\"Sx1.p1.1.1\" class=\"ltx_text\">(</span>?, ?, ?).</p>\n</div>\n<div id=\"Sx1.p2\" class=\"ltx_para\">\n<p id=\"Sx1.p2.1\" class=\"ltx_p\">Recent advances in our ability to collect large-scale, longitudinal video datasets recorded from the perspective of young children over the course of their early development <span id=\"Sx1.p2.1.1\" class=\"ltx_text\">(</span>?, ?) and the development of highly effective generic self-supervised learning (SSL) algorithms in machine learning <span id=\"Sx1.p2.1.2\" class=\"ltx_text\">(</span>?, ?, ?) are allowing us to finally begin to tackle this modern version of the age-old <span id=\"Sx1.p2.1.3\" class=\"ltx_text ltx_font_italic\">nature vs. nurture</span> question.</p>\n</div>\n<div id=\"Sx1.p3\" class=\"ltx_para\">\n<p id=\"Sx1.p3.1\" class=\"ltx_p\">Several recent works have already taken advantage of these advances to try to understand what kinds of visual capabilities can be learned from large-scale, developmentally realistic video data using highly generic, state-of-the-art SSL algorithms and without assuming strong inductive biases <span id=\"Sx1.p3.1.1\" class=\"ltx_text\">(</span>?, ?, ?, ?, ?, ?). These works typically use image-based SSL algorithms and, as a result, only consider visual capabilities that can be learned from static images, such as object recognition. However, the visual world is intrinsically temporal and important aspects of it can only be learned if this temporal dimension is taken into account. For example, the acquisition of <span id=\"Sx1.p3.1.2\" class=\"ltx_text ltx_font_italic\">action concepts</span> or the ability to predict the changes unfolding in a visual scene both require temporal information.</p>\n</div>\n<div id=\"Sx1.p4\" class=\"ltx_para\">\n<p id=\"Sx1.p4.1\" class=\"ltx_p\">Here, we address this shortcoming by training self-supervised <span id=\"Sx1.p4.1.1\" class=\"ltx_text ltx_font_italic\">video models</span> on a large-scale, longitudinal, developmentally realistic video dataset, namely SAYCam <span id=\"Sx1.p4.1.2\" class=\"ltx_text\">(</span>?, ?). We evaluate the capabilities of the trained models on several downstream tasks, compare them against a number of reference models, and provide both qualitative and quantitative insights into the learned video representations. Code and models are available from the following repository: <a target=\"_blank\" href=\"https://github.com/eminorhan/video-models\" title=\"\" class=\"ltx_ref ltx_href\">https://github.com/eminorhan/video-models</a>.</p>\n</div>\n</section>\n<section id=\"Sx2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Methods</h2>\n\n<section id=\"Sx2.SSx1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Training data</h3>\n\n<section id=\"Sx2.SSx1.SSSx1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">SAYCam.</h4>\n\n<div id=\"Sx2.SSx1.SSSx1.p1\" class=\"ltx_para\">\n<p id=\"Sx2.SSx1.SSSx1.p1.1\" class=\"ltx_p\">We use the SAYCam dataset as a realistic proxy of the visual experiences of a developing child <span id=\"Sx2.SSx1.SSSx1.p1.1.1\" class=\"ltx_text\">(</span>?, ?). SAYCam is a large, longitudinal audiovisual dataset of headcam recordings collected from three young children (S, A, and Y) during the course of their early development (6-31 months). It contains 194, 141, and 137 hours of video from S, A, and Y, respectively, for a total of 472 hours of video. The data from each child typically consists of 1-2 hours of continuous, natural, uninstructed recordings per week. We train models on the combined data from all three children (denoted SAY below), as well as on data from child S only.</p>\n</div>\n</section>\n<section id=\"Sx2.SSx1.SSSx2\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">Kinetics-700.</h4>\n\n<div id=\"Sx2.SSx1.SSSx2.p1\" class=\"ltx_para\">\n<p id=\"Sx2.SSx1.SSSx2.p1.1\" class=\"ltx_p\">To investigate the effect of training data on the model behavior and performance, we also train models on the full Kinetics-700 dataset <span id=\"Sx2.SSx1.SSSx2.p1.1.1\" class=\"ltx_text\">(</span>?, ?) and a randomly selected 200-hour subset of it (denoted Kinetics-200h below). The latter contains roughly the same amount of video data as child S in SAYCam and is intended as a length-matched control for child S to isolate the effect of data type alone. Kinetics-700 is a large and diverse dataset of short YouTube clips representing 700 fine-grained action categories such as <span id=\"Sx2.SSx1.SSSx2.p1.1.2\" class=\"ltx_text ltx_font_italic\">playing poker</span>, <span id=\"Sx2.SSx1.SSSx2.p1.1.3\" class=\"ltx_text ltx_font_italic\">polishing furniture</span>, <span id=\"Sx2.SSx1.SSSx2.p1.1.4\" class=\"ltx_text ltx_font_italic\">cutting cake</span>, <span id=\"Sx2.SSx1.SSSx2.p1.1.5\" class=\"ltx_text ltx_font_italic\">ironing hair</span>, <span id=\"Sx2.SSx1.SSSx2.p1.1.6\" class=\"ltx_text ltx_font_italic\">etc.</span> The Kinetics-700 training set contains 536K video clips, each clip typically lasting shorter than 10 seconds, for a total of 1330 hours of video. Thus, compared to SAYCam, the videos in Kinetics-700 are much more diverse in content and have a much shorter time scale.</p>\n</div>\n<figure id=\"Sx2.F1\" class=\"ltx_figure\"><img src=\"./assets/x1.png\" id=\"Sx2.F1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"268\" height=\"101\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Illustration of the spatiotemporal MAE objective. The top row shows the original sequence of frames (from child S in SAYCam). The middle row shows the masked sequence, where 90% of the spatiotemporal “patches” are randomly masked out. The bottom row shows the predictions from a model trained on child S. The model is trained to predict the masked patches from the visible patches at the pixel level.</figcaption>\n</figure>\n<figure id=\"Sx2.F2\" class=\"ltx_figure\"><img src=\"./assets/x2.png\" id=\"Sx2.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"268\" height=\"198\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Top-5 validation accuracy in the SSV2 (a) and Kinetics-700 (b) action recognition tasks. Results are shown for both 10-shot (left) and 50-shot conditions (right). Dashed horizontal lines show the chance baseline. <span id=\"Sx2.F2.5.1\" class=\"ltx_text\" style=\"color:#FF8000;\">Orange</span> represents the models pretrained with the child headcam data. <span id=\"Sx2.F2.6.2\" class=\"ltx_text\" style=\"color:#00FFFF;\">Cyan</span> represents the models pretrained with Kinetics-700 data. <span id=\"Sx2.F2.7.3\" class=\"ltx_text\" style=\"color:#FF00FF;\">Magenta</span> represents a purely image-based reference model. <span id=\"Sx2.F2.8.4\" class=\"ltx_text\" style=\"color:#00FF00;\">Green</span> represents a reference model trained from scratch on the downstream task only (no pretraining).</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"Sx2.SSx2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Model architecture</h3>\n\n<div id=\"Sx2.SSx2.p1\" class=\"ltx_para\">\n<p id=\"Sx2.SSx2.p1.4\" class=\"ltx_p\">We use vision transformers (ViTs) as our model architecture <span id=\"Sx2.SSx2.p1.4.1\" class=\"ltx_text\">(</span>?, ?). This choice is effectively dictated by our choice of SSL algorithm, as described below. We use a large 633M parameter model (ViT-H/14) in all our experiments. We temporally subsample the videos at a rate of 3.75 frames/second and feed the model input clips consisting of 16 consecutive frames with a spatial resolution of 224<math id=\"Sx2.SSx2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"Sx2.SSx2.p1.1.m1.1a\"><mo id=\"Sx2.SSx2.p1.1.m1.1.1\" xref=\"Sx2.SSx2.p1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"Sx2.SSx2.p1.1.m1.1b\"><times id=\"Sx2.SSx2.p1.1.m1.1.1.cmml\" xref=\"Sx2.SSx2.p1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx2.SSx2.p1.1.m1.1c\">\\times</annotation></semantics></math>224 pixels. Each modeled clip is thus roughly 4.3 seconds long. These clips are divided into 2<math id=\"Sx2.SSx2.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"Sx2.SSx2.p1.2.m2.1a\"><mo id=\"Sx2.SSx2.p1.2.m2.1.1\" xref=\"Sx2.SSx2.p1.2.m2.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"Sx2.SSx2.p1.2.m2.1b\"><times id=\"Sx2.SSx2.p1.2.m2.1.1.cmml\" xref=\"Sx2.SSx2.p1.2.m2.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx2.SSx2.p1.2.m2.1c\">\\times</annotation></semantics></math>14<math id=\"Sx2.SSx2.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"Sx2.SSx2.p1.3.m3.1a\"><mo id=\"Sx2.SSx2.p1.3.m3.1.1\" xref=\"Sx2.SSx2.p1.3.m3.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"Sx2.SSx2.p1.3.m3.1b\"><times id=\"Sx2.SSx2.p1.3.m3.1.1.cmml\" xref=\"Sx2.SSx2.p1.3.m3.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx2.SSx2.p1.3.m3.1c\">\\times</annotation></semantics></math>14 three-dimensional boxes or “patches” (<span id=\"Sx2.SSx2.p1.4.2\" class=\"ltx_text ltx_font_italic\">i.e.</span> 2 frames in the temporal dimension, 14<math id=\"Sx2.SSx2.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"Sx2.SSx2.p1.4.m4.1a\"><mo id=\"Sx2.SSx2.p1.4.m4.1.1\" xref=\"Sx2.SSx2.p1.4.m4.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"Sx2.SSx2.p1.4.m4.1b\"><times id=\"Sx2.SSx2.p1.4.m4.1.1.cmml\" xref=\"Sx2.SSx2.p1.4.m4.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx2.SSx2.p1.4.m4.1c\">\\times</annotation></semantics></math>14 pixels in the spatial dimensions). The patches are then linearly projected onto a common patch embedding space and separable (and learnable) spatial and temporal position embeddings are added to the patch embedding of each patch, helping to identify its spatial and temporal position. The rest of the architecture is a standard transformer model operating on the flattened patch representations. Since this is a standard architecture, we refer the reader to <span id=\"Sx2.SSx2.p1.4.3\" class=\"ltx_text\"></span>? (?) for further details.</p>\n</div>\n</section>\n<section id=\"Sx2.SSx3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">SSL algorithm</h3>\n\n<div id=\"Sx2.SSx3.p1\" class=\"ltx_para\">\n<p id=\"Sx2.SSx3.p1.1\" class=\"ltx_p\">We use spatiotemporal masked autoencoders (MAEs) as our SSL algorithm of choice <span id=\"Sx2.SSx3.p1.1.1\" class=\"ltx_text\">(</span>?, ?), which is a straightforward extension of the image-based MAEs <span id=\"Sx2.SSx3.p1.1.2\" class=\"ltx_text\">(</span>?, ?) to video data. The basic idea in spatiotemporal MAEs is to randomly mask out a large proportion (90%) of the three-dimensional “patches” described above and to predict these masked patches from high-level representations of the visible patches (Figure <a href=\"#Sx2.F1\" title=\"Figure 1 ‣ Kinetics-700. ‣ Training data ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). An MAE consists of an encoder and a decoder, both vanilla transformer models. The encoder only processes the visible patches and its output is passed through the decoder, which is typically much smaller than the encoder, to predict the values of the masked patches in the pixel space. In addition to being a highly efficient, generic, state-of-the-art SSL algorithm for video representation learning, MAEs also have the advantage of requiring very minimal data augmentation (we only use random resized crops and horizontal flips in the spatial domain). This is relevant for our purposes, because SSL algorithms that require heavy data augmentation strategies make the input less “human-like”.</p>\n</div>\n<figure id=\"Sx2.F3\" class=\"ltx_figure\"><img src=\"./assets/x3.png\" id=\"Sx2.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"547\" height=\"221\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>t-SNE embeddings of video clips from SSV2. Each point corresponds to a clip from the validation set. Clips belonging to 10 “developmentally realistic” action categories (shown in the legend) are highlighted with different colors. (Left) Results from a 0-shot model pretrained on child S, but not finetuned with any data from SSV2. (Middle) Results from a model pretrained on child S and finetuned on 10-shot SSV2. (Right) Results from a model pretrained on child S and finetuned on 50-shot SSV2. Numbers in parentheses represent the top-5 validation accuracy for the corresponding categories in the 50-shot condition.</figcaption>\n</figure>\n</section>\n<section id=\"Sx2.SSx4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Evaluation</h3>\n\n<div id=\"Sx2.SSx4.p1\" class=\"ltx_para\">\n<p id=\"Sx2.SSx4.p1.1\" class=\"ltx_p\">Once the MAE models are pretrained with the child headcam data (or with other reference data), we evaluate the quality of the learned video representations through a number of downstream tasks. As is standard with MAEs, we use supervised finetuning in order to evaluate the models in downstream tasks. However, to ensure that the representations are learned mostly through SSL (and not through supervised finetuning) and also in the interest of psychological plausibility, we adopt a few-shot finetuning setting, where we only use a small number of labeled examples to finetune the models.</p>\n</div>\n<div id=\"Sx2.SSx4.p2\" class=\"ltx_para\">\n<p id=\"Sx2.SSx4.p2.1\" class=\"ltx_p\">For video-based evaluation, we consider two fine-grained action recognition tasks: <span id=\"Sx2.SSx4.p2.1.1\" class=\"ltx_text ltx_font_bold\">Kinetics-700</span> and <span id=\"Sx2.SSx4.p2.1.2\" class=\"ltx_text ltx_font_bold\">Something-Something-V2</span> (SSV2) <span id=\"Sx2.SSx4.p2.1.3\" class=\"ltx_text\">(</span>?, ?). Kinetics-700 consists of short YouTube clips of 700 fine-grained action categories, whereas SSV2 contains short clips of people performing 174 different classes of fine-grained actions. The main difference between Kinetics-700 and SSV2 is that in SSV2, people perform <span id=\"Sx2.SSx4.p2.1.4\" class=\"ltx_text ltx_font_italic\">instructed</span> actions, where the objects involved in the action are specified and carefully varied across clips to abstract the <span id=\"Sx2.SSx4.p2.1.5\" class=\"ltx_text ltx_font_italic\">action concept</span> itself as a template. The videos in Kinetics-700, on the other hand, are “in the wild” clips downloaded from YouTube, so they are much less controlled with respect to potential shortcuts or biases in the data.</p>\n</div>\n<div id=\"Sx2.SSx4.p3\" class=\"ltx_para\">\n<p id=\"Sx2.SSx4.p3.1\" class=\"ltx_p\">For both tasks, we consider 10-shot and 50-shot supervised finetuning scenarios, using 10 and 50 training examples per class, respectively, to finetune the entire model (but with layer-wise learning rate decay as a regularizer so that top layers are modified more than bottom layers). In Kinetics-700, the 10-shot setting uses 17 hours of video and the 50-shot setting uses 87 hours of video in total for finetuning. In SSV2, the 10-shot setting uses roughly 2 hours of video and the 50-shot setting uses roughly 9 hours of video in total for finetuning. Thus, in every case, the amount of supervised video data used for finetuning is significantly less than the amount of video data used for SSL during pretraining (<span id=\"Sx2.SSx4.p3.1.1\" class=\"ltx_text ltx_font_italic\">cf.</span> 194 hours of video from child S in SAYCam).</p>\n</div>\n</section>\n</section>\n<section id=\"Sx3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Results</h2>\n\n<section id=\"Sx3.SSx1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Result 1: <span id=\"Sx3.SSx1.1.1\" class=\"ltx_text ltx_font_italic\">Video models trained with the child headcam data are competitive with models trained on a large and diverse set of YouTube clips.</span>\n</h3>\n\n<div id=\"Sx3.SSx1.p1\" class=\"ltx_para\">\n<p id=\"Sx3.SSx1.p1.1\" class=\"ltx_p\">Figure <a href=\"#Sx2.F2\" title=\"Figure 2 ‣ Kinetics-700. ‣ Training data ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>a-b show the results for the SSV2 and Kinetics action recognition tasks, respectively, in both 10-shot and 50-shot supervised finetuning conditions. We observe a substantial pretraining benefit in all cases: <span id=\"Sx3.SSx1.p1.1.1\" class=\"ltx_text ltx_font_italic\">e.g.</span> compare the models pretrained on <span id=\"Sx3.SSx1.p1.1.2\" class=\"ltx_text\" style=\"color:#FF8000;\">SAYCam</span> or <span id=\"Sx3.SSx1.p1.1.3\" class=\"ltx_text\" style=\"color:#00FFFF;\">Kinetics-700</span> with reference models trained from <span id=\"Sx3.SSx1.p1.1.4\" class=\"ltx_text\" style=\"color:#00FF00;\">scratch</span> on the downstream task without any pretraining (Figure <a href=\"#Sx2.F2\" title=\"Figure 2 ‣ Kinetics-700. ‣ Training data ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Video pretraining with SAYCam seems to be remarkably effective for downstream action recognition tasks. SAYCam-pretrained models are competitive with models pretrained on the much larger and more diverse Kinetics-700 dataset. Comparing the model pretrained on child S with the model pretrained on the length-matched Kinetics-200h dataset, in particular, we find that they perform very similarly in all cases.</p>\n</div>\n<div id=\"Sx3.SSx1.p2\" class=\"ltx_para\">\n<p id=\"Sx3.SSx1.p2.1\" class=\"ltx_p\">To investigate the extent to which these action recognition tasks genuinely require temporal information, as opposed to being susceptible to simpler image-based strategies such as recognizing the typical objects or scenes in individual frames that tend to co-occur with the given action categories, we next created a purely image-based reference model that did not use any temporal information (S-Img). This model was pretrained on the headcam data from child S using an image-based MAE <span id=\"Sx3.SSx1.p2.1.1\" class=\"ltx_text\">(</span>?, ?) with 80% masking ratio and then finetuned on the downstream task, again in a purely image-based fashion (the model was evaluated by presenting the evaluation clips to the model frame by frame and averaging the predictions). This image-based reference model performed substantially worse than its video-based counterpart in SSV2 (<span id=\"Sx3.SSx1.p2.1.2\" class=\"ltx_text ltx_font_italic\">cf.</span> <span id=\"Sx3.SSx1.p2.1.3\" class=\"ltx_text\" style=\"color:#FF8000;\">S</span> <span id=\"Sx3.SSx1.p2.1.4\" class=\"ltx_text ltx_font_italic\">vs.</span> <span id=\"Sx3.SSx1.p2.1.5\" class=\"ltx_text\" style=\"color:#FF00FF;\">S-Img</span> in Figure <a href=\"#Sx2.F2\" title=\"Figure 2 ‣ Kinetics-700. ‣ Training data ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>a), suggesting that this task genuinely requires learning the temporal regularities corresponding to different actions. Surprisingly, however, S-Img outperformed its video-based counterpart in the Kinetics-700 task (<span id=\"Sx3.SSx1.p2.1.6\" class=\"ltx_text ltx_font_italic\">cf.</span> <span id=\"Sx3.SSx1.p2.1.7\" class=\"ltx_text\" style=\"color:#FF8000;\">S</span> <span id=\"Sx3.SSx1.p2.1.8\" class=\"ltx_text ltx_font_italic\">vs.</span> <span id=\"Sx3.SSx1.p2.1.9\" class=\"ltx_text\" style=\"color:#FF00FF;\">S-Img</span> in Figure <a href=\"#Sx2.F2\" title=\"Figure 2 ‣ Kinetics-700. ‣ Training data ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>b), suggesting that it may be possible to achieve high accuracies in this task by exploiting frame-based regularities and without using any temporal information at all. This result also illustrates the difficulty of creating datasets with “in the wild” internet data that are robust to shortcut learning strategies <span id=\"Sx3.SSx1.p2.1.10\" class=\"ltx_text\">(</span>?, ?).</p>\n</div>\n</section>\n<section id=\"Sx3.SSx2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Result 2: <span id=\"Sx3.SSx2.1.1\" class=\"ltx_text ltx_font_italic\">Video models trained with the child headcam data can learn to recognize developmentally realistic action categories from a small amount of labeled examples.</span>\n</h3>\n\n<div id=\"Sx3.SSx2.p1\" class=\"ltx_para\">\n<p id=\"Sx3.SSx2.p1.1\" class=\"ltx_p\">We next isolated 10 action categories in SSV2 that could be considered “developmentally realistic” in the sense that a 30-month old child would be expected to understand them (the cutoff age for SAYCam is 31 months). We manually selected these categories from SSV2 as follows: we only considered simple, general action categories that involve at most two objects, thus excluding more detailed action categories describing a specific mode of action, for instance, or more complex categories that involve more than two objects. For each of the remaining categories, we then checked the “item trajectory” of the basal verb describing the action on Wordbank <span id=\"Sx3.SSx2.p1.1.1\" class=\"ltx_text\">(</span>?, ?) and included only those that are produced by most children by 30 months of age (<span id=\"Sx3.SSx2.p1.1.2\" class=\"ltx_text ltx_font_italic\">i.e.</span> “proportion of children producing” <math id=\"Sx3.SSx2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;\" display=\"inline\"><semantics id=\"Sx3.SSx2.p1.1.m1.1a\"><mo id=\"Sx3.SSx2.p1.1.m1.1.1\" xref=\"Sx3.SSx2.p1.1.m1.1.1.cmml\">&gt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"Sx3.SSx2.p1.1.m1.1b\"><gt id=\"Sx3.SSx2.p1.1.m1.1.1.cmml\" xref=\"Sx3.SSx2.p1.1.m1.1.1\"></gt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx3.SSx2.p1.1.m1.1c\">&gt;</annotation></semantics></math> 0.5). The final list of these developmentally realistic action categories can be found in the legend of Figure <a href=\"#Sx2.F3\" title=\"Figure 3 ‣ SSL algorithm ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n</div>\n<div id=\"Sx3.SSx2.p2\" class=\"ltx_para\">\n<p id=\"Sx3.SSx2.p2.1\" class=\"ltx_p\">Figure <a href=\"#Sx2.F3\" title=\"Figure 3 ‣ SSL algorithm ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows t-SNE embeddings of these categories obtained from three different models: (i) a model pretrained on child S, but not finetuned on SSV2 at all (0-shot); (ii) a model pretrained on child S and finetuned on 10-shot SSV2; and (iii) a model pretrained on child S and finetuned on 50-shot SSV2. The 0-shot embeddings do not contain much category structure, attesting to the difficulty of this action recognition task. However, category-related semantic structure immediately begins to emerge as the model is finetuned with even a small amount of labeled examples (10-shot), and improves further with more labeled examples (50-shot).</p>\n</div>\n<div id=\"Sx3.SSx2.p3\" class=\"ltx_para\">\n<p id=\"Sx3.SSx2.p3.1\" class=\"ltx_p\">Interestingly, we find that the model performs worse on this developmentally realistic subset of categories compared to the remaining categories: 36% <span id=\"Sx3.SSx2.p3.1.1\" class=\"ltx_text ltx_font_italic\">vs.</span> 43% top-5 validation accuracy in the 50-shot condition (chance: 3%). This could be because these are broader, more general categories compared to the rest. Indeed, the top 7 categories with the highest accuracy in the 50-shot condition were: <span id=\"Sx3.SSx2.p3.1.2\" class=\"ltx_text ltx_font_italic\">burying something in something</span> (87%), <span id=\"Sx3.SSx2.p3.1.3\" class=\"ltx_text ltx_font_italic\">trying to pour something into something but missing so it spills next to it</span> (85%), <span id=\"Sx3.SSx2.p3.1.4\" class=\"ltx_text ltx_font_italic\">pouring something into something until it overflows</span> (76%), <span id=\"Sx3.SSx2.p3.1.5\" class=\"ltx_text ltx_font_italic\">pretending to sprinkle air onto something</span> (71%), <span id=\"Sx3.SSx2.p3.1.6\" class=\"ltx_text ltx_font_italic\">turning the camera left while filming something</span> (69%), <span id=\"Sx3.SSx2.p3.1.7\" class=\"ltx_text ltx_font_italic\">lifting a surface with something on it until it starts sliding down</span> (68%), and <span id=\"Sx3.SSx2.p3.1.8\" class=\"ltx_text ltx_font_italic\">pulling two ends of something so that it separates into two pieces</span> (67%). These are typically more specific, more detailed action categories than the categories in our developmentally realistic subset.</p>\n</div>\n</section>\n<section id=\"Sx3.SSx3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Result 3: <span id=\"Sx3.SSx3.1.1\" class=\"ltx_text ltx_font_italic\">Downstream performance scales favorably with the amount of child headcam data used for SSL.</span>\n</h3>\n\n<figure id=\"Sx3.F4\" class=\"ltx_figure\"><img src=\"./assets/x4.png\" id=\"Sx3.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"268\" height=\"115\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>A data size scaling experiment for child S. Spatiotemporal MAEs are trained on all data from child S and on subsets of it over a four orders of magnitude range in data size. The performance of the resulting models are evaluated in the SSV2 (a) and Kinetics-700 (b) action recognition tasks (individual dots). A log-linear model is fit to the data to extrapolate performance beyond the 194 hours of data we currently have from S. Shaded areas represent 95% confidence intervals around the linear fits. A developmentally relevant time scale of 2.5 years is indicated by the vertical dashed line.</figcaption>\n</figure>\n<div id=\"Sx3.SSx3.p1\" class=\"ltx_para\">\n<p id=\"Sx3.SSx3.p1.1\" class=\"ltx_p\">Although SAYCam is currently the largest developmentally realistic, longitudinal video dataset of its kind, the amount of data available from each child in SAYCam is quite limited compared to the amount of visual experience a child actually receives while growing up. For example, we have 194 hours of video available from child S, which corresponds to roughly 8 days (or equivalent to 16 days of visual experience if we factor in 12 hours/day of sleep). This is two orders of magnitude smaller than the amount of visual experience a child receives by the age of 4-5. This invites the natural question: what would happen if we had developmentally more realistic amounts of data, <span id=\"Sx3.SSx3.p1.1.1\" class=\"ltx_text ltx_font_italic\">e.g.</span> two orders of magnitude more data?</p>\n</div>\n<div id=\"Sx3.SSx3.p2\" class=\"ltx_para\">\n<p id=\"Sx3.SSx3.p2.1\" class=\"ltx_p\">To address this question, we performed a data size scaling experiment. We systematically varied the amount of video data from child S used for SSL pretraining: specifically, we trained spatiotemporal MAE models on all 194 hours of data (100%), on 19.4 hours of data (10%), on 1.94 hours of data (1%), and finally on 0.194 hours of data (0.1%) from S, thus covering a four orders of magnitude range in data size. We then evaluated these models on the downstream action recognition tasks and, using a simple log-linear scaling function, extrapolated their performance a few orders of magnitude beyond the amount of data we currently have from child S.</p>\n</div>\n<div id=\"Sx3.SSx3.p3\" class=\"ltx_para\">\n<p id=\"Sx3.SSx3.p3.2\" class=\"ltx_p\">The results of this scaling experiment are shown in Figure <a href=\"#Sx3.F4\" title=\"Figure 4 ‣ Result 3: Downstream performance scales favorably with the amount of child headcam data used for SSL. ‣ Results ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We estimate substantial improvements in action recognition (on average, <math id=\"Sx3.SSx3.p3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim 20\\%\" display=\"inline\"><semantics id=\"Sx3.SSx3.p3.1.m1.1a\"><mrow id=\"Sx3.SSx3.p3.1.m1.1.1\" xref=\"Sx3.SSx3.p3.1.m1.1.1.cmml\"><mi id=\"Sx3.SSx3.p3.1.m1.1.1.2\" xref=\"Sx3.SSx3.p3.1.m1.1.1.2.cmml\"></mi><mo id=\"Sx3.SSx3.p3.1.m1.1.1.1\" xref=\"Sx3.SSx3.p3.1.m1.1.1.1.cmml\">∼</mo><mrow id=\"Sx3.SSx3.p3.1.m1.1.1.3\" xref=\"Sx3.SSx3.p3.1.m1.1.1.3.cmml\"><mn id=\"Sx3.SSx3.p3.1.m1.1.1.3.2\" xref=\"Sx3.SSx3.p3.1.m1.1.1.3.2.cmml\">20</mn><mo id=\"Sx3.SSx3.p3.1.m1.1.1.3.1\" xref=\"Sx3.SSx3.p3.1.m1.1.1.3.1.cmml\">%</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"Sx3.SSx3.p3.1.m1.1b\"><apply id=\"Sx3.SSx3.p3.1.m1.1.1.cmml\" xref=\"Sx3.SSx3.p3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"Sx3.SSx3.p3.1.m1.1.1.1.cmml\" xref=\"Sx3.SSx3.p3.1.m1.1.1.1\">similar-to</csymbol><csymbol cd=\"latexml\" id=\"Sx3.SSx3.p3.1.m1.1.1.2.cmml\" xref=\"Sx3.SSx3.p3.1.m1.1.1.2\">absent</csymbol><apply id=\"Sx3.SSx3.p3.1.m1.1.1.3.cmml\" xref=\"Sx3.SSx3.p3.1.m1.1.1.3\"><csymbol cd=\"latexml\" id=\"Sx3.SSx3.p3.1.m1.1.1.3.1.cmml\" xref=\"Sx3.SSx3.p3.1.m1.1.1.3.1\">percent</csymbol><cn type=\"integer\" id=\"Sx3.SSx3.p3.1.m1.1.1.3.2.cmml\" xref=\"Sx3.SSx3.p3.1.m1.1.1.3.2\">20</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx3.SSx3.p3.1.m1.1c\">\\sim 20\\%</annotation></semantics></math> absolute improvement in accuracy in the SSV2 50-shot task and <math id=\"Sx3.SSx3.p3.2.m2.1\" class=\"ltx_Math\" alttext=\"\\sim 15\\%\" display=\"inline\"><semantics id=\"Sx3.SSx3.p3.2.m2.1a\"><mrow id=\"Sx3.SSx3.p3.2.m2.1.1\" xref=\"Sx3.SSx3.p3.2.m2.1.1.cmml\"><mi id=\"Sx3.SSx3.p3.2.m2.1.1.2\" xref=\"Sx3.SSx3.p3.2.m2.1.1.2.cmml\"></mi><mo id=\"Sx3.SSx3.p3.2.m2.1.1.1\" xref=\"Sx3.SSx3.p3.2.m2.1.1.1.cmml\">∼</mo><mrow id=\"Sx3.SSx3.p3.2.m2.1.1.3\" xref=\"Sx3.SSx3.p3.2.m2.1.1.3.cmml\"><mn id=\"Sx3.SSx3.p3.2.m2.1.1.3.2\" xref=\"Sx3.SSx3.p3.2.m2.1.1.3.2.cmml\">15</mn><mo id=\"Sx3.SSx3.p3.2.m2.1.1.3.1\" xref=\"Sx3.SSx3.p3.2.m2.1.1.3.1.cmml\">%</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"Sx3.SSx3.p3.2.m2.1b\"><apply id=\"Sx3.SSx3.p3.2.m2.1.1.cmml\" xref=\"Sx3.SSx3.p3.2.m2.1.1\"><csymbol cd=\"latexml\" id=\"Sx3.SSx3.p3.2.m2.1.1.1.cmml\" xref=\"Sx3.SSx3.p3.2.m2.1.1.1\">similar-to</csymbol><csymbol cd=\"latexml\" id=\"Sx3.SSx3.p3.2.m2.1.1.2.cmml\" xref=\"Sx3.SSx3.p3.2.m2.1.1.2\">absent</csymbol><apply id=\"Sx3.SSx3.p3.2.m2.1.1.3.cmml\" xref=\"Sx3.SSx3.p3.2.m2.1.1.3\"><csymbol cd=\"latexml\" id=\"Sx3.SSx3.p3.2.m2.1.1.3.1.cmml\" xref=\"Sx3.SSx3.p3.2.m2.1.1.3.1\">percent</csymbol><cn type=\"integer\" id=\"Sx3.SSx3.p3.2.m2.1.1.3.2.cmml\" xref=\"Sx3.SSx3.p3.2.m2.1.1.3.2\">15</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Sx3.SSx3.p3.2.m2.1c\">\\sim 15\\%</annotation></semantics></math> improvement in the Kinetics-700 50-shot task) with a two orders of magnitude increase in data size alone without any other changes. Further improvements may be possible by scaling up the input and model sizes as well <span id=\"Sx3.SSx3.p3.2.1\" class=\"ltx_text\">(</span>?, ?).</p>\n</div>\n<figure id=\"Sx3.F5\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"Sx3.F5.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"538\" height=\"207\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>Two examples illustrating the emergent video interpolation capabilities in a spatiotemporal MAE model trained on child S. In both cases, the top row shows the original sequence of frames; the middle row shows the masked sequence where the four central frames are masked out; and the bottom row shows the model completions of the masked frames. The original sequences are from another child in SAYCam, namely A. Note that the model completions in both cases are not simple copies of the nearby visible frames; they are rather novel frames that indicate some understanding of the consequences of the camera movements on the image. Further examples can be found at <a target=\"_blank\" href=\"https://doi.org/10.5281/zenodo.10573165\" title=\"\" class=\"ltx_ref ltx_href\">this link</a>.</figcaption>\n</figure>\n</section>\n<section id=\"Sx3.SSx4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Result 4: <span id=\"Sx3.SSx4.1.1\" class=\"ltx_text ltx_font_italic\">Emergent video interpolation capabilities in video models trained with the child headcam data.</span>\n</h3>\n\n<div id=\"Sx3.SSx4.p1\" class=\"ltx_para\">\n<p id=\"Sx3.SSx4.p1.1\" class=\"ltx_p\">We next devised a simple qualitative test to probe the models’ understanding of the spatiotemporal dynamics over short video clips. In this test, we mask out the 4 central frames in the middle of a clip consisting of 16 consecutive frames. We then ask the model to “fill in” this central part, given the rest of the clip. To make sure that the behavior we observe is due to the out-of-distribution (<span id=\"Sx3.SSx4.p1.1.1\" class=\"ltx_text ltx_font_italic\">ood</span>) generalization capacity of the model and not simply due to memorization or <span id=\"Sx3.SSx4.p1.1.2\" class=\"ltx_text ltx_font_italic\">iid</span> generalization, we use the model trained on child S and evaluate it with clips from another child in SAYCam, namely A. Note that this task is doubly challenging for the model: (i) the model was not trained with this particular masking strategy during pretraining (it was instead trained with random spatiotemporal masking as in Figure <a href=\"#Sx2.F1\" title=\"Figure 1 ‣ Kinetics-700. ‣ Training data ‣ Methods ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) and (ii) the model did not see any of these clips during training.</p>\n</div>\n<div id=\"Sx3.SSx4.p2\" class=\"ltx_para\">\n<p id=\"Sx3.SSx4.p2.1\" class=\"ltx_p\">Despite these challenges, the model was often able to provide plausible completions of the clips. Visual inspection of the model’s completions suggests that in many cases the model does not just utilize simplistic strategies such as copying the nearest visible frames, but rather generates plausible, novel completions that indicate some understanding of the movement of objects and other visual features across the image over the course of a clip. Figure <a href=\"#Sx3.F5\" title=\"Figure 5 ‣ Result 3: Downstream performance scales favorably with the amount of child headcam data used for SSL. ‣ Results ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents two qualitative examples illustrating this point.</p>\n</div>\n</section>\n<section id=\"Sx3.SSx5\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">Result 5: <span id=\"Sx3.SSx5.1.1\" class=\"ltx_text ltx_font_italic\">Video models learn more robust object representations than image-based models trained with the same data.</span>\n</h3>\n\n<div id=\"Sx3.SSx5.p1\" class=\"ltx_para\">\n<p id=\"Sx3.SSx5.p1.1\" class=\"ltx_p\">Finally, we sought to compare the <span id=\"Sx3.SSx5.p1.1.1\" class=\"ltx_text ltx_font_italic\">object representations</span> that emerge in video models trained with the spatiotemporal MAE algorithm with those learned by image-based MAEs trained on the same data. We conjectured that since spatiotemporal MAEs learn to track visual representations over time, this could lead to more accurate or more robust object representations. To test this hypothesis, we evaluated both spatiotemporal MAEs and image-based MAEs trained with the headcam data from child S on two downstream visual object recognition tasks: ImageNet <span id=\"Sx3.SSx5.p1.1.2\" class=\"ltx_text\">(</span>?, ?) and out-of-distribution (OOD) ImageNet <span id=\"Sx3.SSx5.p1.1.3\" class=\"ltx_text\">(</span>?, ?). ImageNet is a standard benchmark for real-world visual object recognition and consists of a large collection of high-quality, photographic images belonging to 1000 different object categories. OOD ImageNet is an out-of-distribution version of ImageNet, which consists of 17 different evaluation tasks each generated by applying a different type of perturbation or transformation to images from the ImageNet validation set (<span id=\"Sx3.SSx5.p1.1.4\" class=\"ltx_text ltx_font_italic\">e.g.</span> changing the colors of the image, taking the silhouettes of the objects, stylizing the image, adding different types of noise to the image). Together, these two benchmarks evaluate the <span id=\"Sx3.SSx5.p1.1.5\" class=\"ltx_text ltx_font_italic\">accuracy</span> and <span id=\"Sx3.SSx5.p1.1.6\" class=\"ltx_text ltx_font_italic\">robustness</span> of real-word object recognition.</p>\n</div>\n<div id=\"Sx3.SSx5.p2\" class=\"ltx_para\">\n<p id=\"Sx3.SSx5.p2.1\" class=\"ltx_p\">We again adopt a few-shot supervised finetuning setup for our evaluations. Specifically, we finetune both video-based and image-based MAEs with 2% of the training data from ImageNet, which corresponds to 25-26 images per class. To finetune the video model on image data, we simply repeat each image 16 times before feeding it to the model, thus effectively creating a static video clip with 16 frames. This allows us to use the pretrained video model without any alterations in the architecture.</p>\n</div>\n<figure id=\"Sx3.F6\" class=\"ltx_figure\"><img src=\"./assets/x6.png\" id=\"Sx3.F6.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"242\" height=\"202\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>Recognition accuracy on ImageNet (a) and on OOD ImageNet (b). <span id=\"Sx3.F6.7.1\" class=\"ltx_text\" style=\"color:#FF8000;\">S-Vid</span> corresponds to a model pretrained with video-based SSL on child S; <span id=\"Sx3.F6.8.2\" class=\"ltx_text\" style=\"color:#FF00FF;\">S-Img</span> is a model pretrained with image-based SSL on child S; <span id=\"Sx3.F6.9.3\" class=\"ltx_text\" style=\"color:#00FF00;\">Scratch-Img</span> is a model trained from scratch with 2% of ImageNet training data without any pretraining. The pretrained models were also finetuned with 2% of ImageNet training data. Dashed horizontal lines show the chance baseline. (c) A simple sequence of frames from child S and the corresponding last-layer attention maps from <span id=\"Sx3.F6.10.4\" class=\"ltx_text\" style=\"color:#FF8000;\">S-Vid</span> and <span id=\"Sx3.F6.11.5\" class=\"ltx_text\" style=\"color:#FF00FF;\">S-Img</span>. Attention maps were obtained with respect to the <span id=\"Sx3.F6.12.6\" class=\"ltx_text ltx_font_typewriter\">cls</span> token. Further examples can be found at <a target=\"_blank\" href=\"https://doi.org/10.5281/zenodo.10573317\" title=\"\" class=\"ltx_ref ltx_href\">this link</a>.</figcaption>\n</figure>\n<div id=\"Sx3.SSx5.p3\" class=\"ltx_para\">\n<p id=\"Sx3.SSx5.p3.1\" class=\"ltx_p\">The results are shown in Figure <a href=\"#Sx3.F6\" title=\"Figure 6 ‣ Result 5: Video models learn more robust object representations than image-based models trained with the same data. ‣ Results ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>a-b. On ImageNet, the model pretrained with image-based SSL on the headcam data from child S outperforms the model pretrained with video-based SSL on the same data (<span id=\"Sx3.SSx5.p3.1.1\" class=\"ltx_text\" style=\"color:#FF00FF;\">S-Img</span>: 57% <span id=\"Sx3.SSx5.p3.1.2\" class=\"ltx_text ltx_font_italic\">vs.</span> <span id=\"Sx3.SSx5.p3.1.3\" class=\"ltx_text\" style=\"color:#FF8000;\">S-Vid</span>: 47%). However, on OOD ImageNet, the pattern is reversed (<span id=\"Sx3.SSx5.p3.1.4\" class=\"ltx_text\" style=\"color:#FF00FF;\">S-Img</span>: 30% <span id=\"Sx3.SSx5.p3.1.5\" class=\"ltx_text ltx_font_italic\">vs.</span> <span id=\"Sx3.SSx5.p3.1.6\" class=\"ltx_text\" style=\"color:#FF8000;\">S-Vid</span>: 34%), suggesting that although less accurate for fine-grained recognition, the object representations learned by the video model may be more robust to perturbations and transformations than those learned by the image-based model. The performance difference between the video model and the image-based model was particularly salient on the <span id=\"Sx3.SSx5.p3.1.7\" class=\"ltx_text ltx_font_italic\">silhoutte</span> (+12% in favor of the video model), <span id=\"Sx3.SSx5.p3.1.8\" class=\"ltx_text ltx_font_italic\">sketch</span> (+9%), and <span id=\"Sx3.SSx5.p3.1.9\" class=\"ltx_text ltx_font_italic\">stylized</span> (+8%) subtasks in OOD ImageNet, suggesting a qualitatively different type of representation that may be more shape-sensitive and less texture-sensitive than in the image-based model. The respective attention maps from the two models seem to confirm this suggestion (Figure <a href=\"#Sx3.F6\" title=\"Figure 6 ‣ Result 5: Video models learn more robust object representations than image-based models trained with the same data. ‣ Results ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>c).</p>\n</div>\n<div id=\"Sx3.SSx5.p4\" class=\"ltx_para\">\n<p id=\"Sx3.SSx5.p4.1\" class=\"ltx_p\">The image-based model being more texture sensitive could also explain its superior performance on ImageNet (Figure <a href=\"#Sx3.F6\" title=\"Figure 6 ‣ Result 5: Video models learn more robust object representations than image-based models trained with the same data. ‣ Results ‣ Self-supervised learning of video representations from a child’s perspective\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>a): the model could be relying on less robust, texture-based features in order to perform well on this task. We leave a more complete investigation of the possible reasons behind the gap between image-based SSL and video-based SSL on ImageNet and the reversal of this gap on OOD ImageNet to future work (<span id=\"Sx3.SSx5.p4.1.1\" class=\"ltx_text ltx_font_italic\">e.g.</span> the difference in masking ratio per frame between image-based <span id=\"Sx3.SSx5.p4.1.2\" class=\"ltx_text ltx_font_italic\">vs.</span> video-based SSL: 80% <span id=\"Sx3.SSx5.p4.1.3\" class=\"ltx_text ltx_font_italic\">vs.</span> 90%).</p>\n</div>\n<div id=\"Sx3.SSx5.p5\" class=\"ltx_para\">\n<p id=\"Sx3.SSx5.p5.1\" class=\"ltx_p\">A recent work also reported more robust and more human-aligned object representations with video-based pretraining compared to image-based pretraining <span id=\"Sx3.SSx5.p5.1.1\" class=\"ltx_text\">(</span>?, ?). However, that work did not use the same datasets for video-based and image-based pretraining. In contrast, by using the exact same data (headcam videos from child S) and the same model architecture (ViT-H/14) for both video-based and image-based pretraining, we can isolate the effect of video-based <span id=\"Sx3.SSx5.p5.1.2\" class=\"ltx_text ltx_font_italic\">vs.</span> image-based pretraining objectives on the robustness of the learned object representations.</p>\n</div>\n</section>\n</section>\n<section id=\"Sx4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Discussion</h2>\n\n<div id=\"Sx4.p1\" class=\"ltx_para\">\n<p id=\"Sx4.p1.1\" class=\"ltx_p\">We trained video models on a large-scale, longitudinal dataset of headcam recordings collected from the perspective of a young child during their early development, using a highly generic SSL algorithm (spatiotemporal MAEs) and without assuming any strong inductive biases. These models learn a non-trivial amount of visual knowledge about temporal aspects of the world through self-supervised learning from a few hundred hours of headcam videos, enabling them to recognize challenging <span id=\"Sx4.p1.1.1\" class=\"ltx_text ltx_font_italic\">action concepts</span> from a small amount of labeled examples (Results 1-2) and to interpolate unseen videos with plausible and novel completions (Result 4). They also display favorable data size scaling in downstream tasks (Result 3), suggesting that we can expect to see substantial improvements in model capabilities if we can train our models with developmentally more realistic amounts of headcam data (<span id=\"Sx4.p1.1.2\" class=\"ltx_text ltx_font_italic\">i.e.</span> roughly two orders of magnitude more data than we currently have). Finally, video models learn more robust object representations that appear to be less texture-sensitive than image-based models trained on the same data (Result 5).</p>\n</div>\n<div id=\"Sx4.p2\" class=\"ltx_para\">\n<p id=\"Sx4.p2.1\" class=\"ltx_p\">Our work has a number of limitations that should be kept in mind when interpreting our results. First, we only considered a limited number of tasks to evaluate the capabilities of the models. Future work should consider a larger, more diverse range of tasks that can probe the capabilities of the models, <span id=\"Sx4.p2.1.1\" class=\"ltx_text ltx_font_italic\">i.e.</span> what they can and cannot do, in much more detail. In particular, intuitive physics is an important aspect of a child’s internal model of the world that we have not rigorously evaluated in our models <span id=\"Sx4.p2.1.2\" class=\"ltx_text\">(</span>?, ?, ?).</p>\n</div>\n<div id=\"Sx4.p3\" class=\"ltx_para\">\n<p id=\"Sx4.p3.1\" class=\"ltx_p\">Second, although we were able to generate plausible completions with spatiotemporal MAEs (Result 4), MAEs, in general, are not designed to be generative models: they do not have a well-defined likelihood function and, because of their mean squared error objective, they tend to generate low-quality, blurry predictions. This suggests the need to train true generative video models like autoregressive models or diffusion models on SAYCam, in addition to MAEs. Such models would also enhance our understanding of model capabilities.</p>\n</div>\n<div id=\"Sx4.p4\" class=\"ltx_para\">\n<p id=\"Sx4.p4.1\" class=\"ltx_p\">Third, throughout this work, we have adopted a few-shot supervised finetuning paradigm for evaluating our trained models (typically using only tens of labeled examples per class). However, it is unclear if this paradigm is psychologically realistic enough. Future work should aim to train models directly with developmentally realistic multimodal data sources instead, <span id=\"Sx4.p4.1.1\" class=\"ltx_text ltx_font_italic\">e.g.</span> paired linguistic and visual inputs. This is crucial to more rigorously address the <span id=\"Sx4.p4.1.2\" class=\"ltx_text ltx_font_italic\">nature vs. nurture</span> question that fundamentally motivates our work.</p>\n</div>\n<div id=\"Sx4.p5\" class=\"ltx_para\">\n<p id=\"Sx4.p5.1\" class=\"ltx_p\">By demonstrating a sample of the non-trivial visual capabilities that can be learned generically from a few hundred hours of longitudinal headcam videos recorded from the perspective of a developing child, our work contributes to the burgeoning interaction between developmental psychology and modern machine learning <span id=\"Sx4.p5.1.1\" class=\"ltx_text\">(</span>?, ?).</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bibx1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bambach, Crandall, Smith,  YuBambach et al.<span id=\"bib.bibx1.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nBambach, S., Crandall, D., Smith, L.,  Yu, C. \n</span>\n<span class=\"ltx_bibblock\">(2018).\n\n</span>\n<span class=\"ltx_bibblock\">Toddler-inspired visual object learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx1.3.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx1.4.2\" class=\"ltx_emph ltx_font_italic\">31</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al.<span id=\"bib.bibx2.3.3.1\" class=\"ltx_text\"></span>Caron et al.<span id=\"bib.bibx2.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nCaron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P.,  Joulin, A. \n</span>\n<span class=\"ltx_bibblock\">(2021).\n\n</span>\n<span class=\"ltx_bibblock\">Emerging properties in self-supervised vision transformers.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx2.5.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2104.14294</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dosovitskiy et al.<span id=\"bib.bibx3.3.3.1\" class=\"ltx_text\"></span>Dosovitskiy et al.<span id=\"bib.bibx3.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … others \n</span>\n<span class=\"ltx_bibblock\">(2020).\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 16x16 words: transformers for image recognition at scale.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bibx3.5.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations.</em>\n\n\n</span>\n</li>\n<li id=\"bib.bibx4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Elman, Bates,  JohnsonElman et al.<span id=\"bib.bibx4.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nElman, J. L., Bates, E. A.,  Johnson, M. H. \n</span>\n<span class=\"ltx_bibblock\">(1996).\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx4.3.1\" class=\"ltx_emph ltx_font_italic\">Rethinking innateness: A connectionist perspective on development</em> (Vol.<span id=\"bib.bibx4.4.2\" class=\"ltx_text\"></span> 10).\n\n</span>\n<span class=\"ltx_bibblock\">MIT Press.\n\n\n</span>\n</li>\n<li id=\"bib.bibx5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Feichtenhofer, Li, He, et al.<span id=\"bib.bibx5.3.3.1\" class=\"ltx_text\"></span>Feichtenhofer et al.<span id=\"bib.bibx5.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nFeichtenhofer, C., Li, Y., He, K., et al. \n</span>\n<span class=\"ltx_bibblock\">(2022).\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders as spatiotemporal learners.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx5.5.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx5.6.2\" class=\"ltx_emph ltx_font_italic\">35</em>, 35946–35958.\n\n\n</span>\n</li>\n<li id=\"bib.bibx6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Frank, Braginsky, Yurovsky,  MarchmanFrank et al.<span id=\"bib.bibx6.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nFrank, M. C., Braginsky, M., Yurovsky, D.,  Marchman, V. A. \n</span>\n<span class=\"ltx_bibblock\">(2017).\n\n</span>\n<span class=\"ltx_bibblock\">Wordbank: An open repository for developmental vocabulary data.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx6.3.1\" class=\"ltx_emph ltx_font_italic\">Journal of Child Language</em>, <em id=\"bib.bibx6.4.2\" class=\"ltx_emph ltx_font_italic\">44</em>(3), 677–694.\n\n\n</span>\n</li>\n<li id=\"bib.bibx7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Geirhos et al.<span id=\"bib.bibx7.3.3.1\" class=\"ltx_text\"></span>Geirhos et al.<span id=\"bib.bibx7.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M.,  Wichmann, F. A. \n</span>\n<span class=\"ltx_bibblock\">(2020).\n\n</span>\n<span class=\"ltx_bibblock\">Shortcut learning in deep neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx7.5.1\" class=\"ltx_emph ltx_font_italic\">Nature Machine Intelligence</em>, <em id=\"bib.bibx7.6.2\" class=\"ltx_emph ltx_font_italic\">2</em>(11), 665–673.\n\n\n</span>\n</li>\n<li id=\"bib.bibx8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Geirhos et al.<span id=\"bib.bibx8.3.3.1\" class=\"ltx_text\"></span>Geirhos et al.<span id=\"bib.bibx8.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nGeirhos, R., Narayanappa, K., Mitzkus, B., Thieringer, T., Bethge, M., Wichmann, F. A.,  Brendel, W. \n</span>\n<span class=\"ltx_bibblock\">(2021).\n\n</span>\n<span class=\"ltx_bibblock\">Partial success in closing the gap between human and machine vision.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx8.5.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx8.6.2\" class=\"ltx_emph ltx_font_italic\">34</em>, 23885–23899.\n\n\n</span>\n</li>\n<li id=\"bib.bibx9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Goyal et al.<span id=\"bib.bibx9.3.3.1\" class=\"ltx_text\"></span>Goyal et al.<span id=\"bib.bibx9.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nGoyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., … others \n</span>\n<span class=\"ltx_bibblock\">(2017).\n\n</span>\n<span class=\"ltx_bibblock\">The ”something something” video database for learning and evaluating visual common sense.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bibx9.5.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the ieee international conference on computer vision</em> (pp.<span id=\"bib.bibx9.6.2\" class=\"ltx_text\"></span> 5842–5850).\n\n\n</span>\n</li>\n<li id=\"bib.bibx10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.<span id=\"bib.bibx10.3.3.1\" class=\"ltx_text\"></span>He et al.<span id=\"bib.bibx10.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nHe, K., Chen, X., Xie, S., Li, Y., Dollár, P.,  Girshick, R. \n</span>\n<span class=\"ltx_bibblock\">(2022).\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders are scalable vision learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bibx10.5.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp.<span id=\"bib.bibx10.6.2\" class=\"ltx_text\"></span> 16000–16009).\n\n\n</span>\n</li>\n<li id=\"bib.bibx11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OrhanOrhan</span>\n<span class=\"ltx_bibblock\">\nOrhan, A. E. \n</span>\n<span class=\"ltx_bibblock\">(2023).\n\n</span>\n<span class=\"ltx_bibblock\">Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx11.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2308.03712</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Orhan, Gupta,  LakeOrhan et al.<span id=\"bib.bibx12.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nOrhan, A. E., Gupta, V.,  Lake, B. M. \n</span>\n<span class=\"ltx_bibblock\">(2020).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning through the eyes of a child.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx12.3.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx12.4.2\" class=\"ltx_emph ltx_font_italic\">33</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Orhan  LakeOrhan  Lake</span>\n<span class=\"ltx_bibblock\">\nOrhan, A. E.,  Lake, B. M. \n</span>\n<span class=\"ltx_bibblock\">(2023).\n\n</span>\n<span class=\"ltx_bibblock\">Learning high-level visual representations from a child’s perspective without strong inductive biases.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx13.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2305.15372</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Parthasarathy, Eslami, Carreira,  HenaffParthasarathy et al.<span id=\"bib.bibx14.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nParthasarathy, N., Eslami, S. A., Carreira, J.,  Henaff, O. J. \n</span>\n<span class=\"ltx_bibblock\">(2023).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised video pretraining yields robust and more human-aligned visual representations.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx14.3.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx14.4.2\" class=\"ltx_emph ltx_font_italic\">37</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Piloto, Weinstein, Battaglia,  BotvinickPiloto et al.<span id=\"bib.bibx15.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nPiloto, L. S., Weinstein, A., Battaglia, P.,  Botvinick, M. \n</span>\n<span class=\"ltx_bibblock\">(2022).\n\n</span>\n<span class=\"ltx_bibblock\">Intuitive physics learning in a deep-learning model inspired by developmental psychology.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx15.3.1\" class=\"ltx_emph ltx_font_italic\">Nature Human Behaviour</em>, <em id=\"bib.bibx15.4.2\" class=\"ltx_emph ltx_font_italic\">6</em>(9), 1257–1267.\n\n\n</span>\n</li>\n<li id=\"bib.bibx16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Russakovsky et al.<span id=\"bib.bibx16.3.3.1\" class=\"ltx_text\"></span>Russakovsky et al.<span id=\"bib.bibx16.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., … others \n</span>\n<span class=\"ltx_bibblock\">(2015).\n\n</span>\n<span class=\"ltx_bibblock\">ImageNet large scale visual recognition challenge.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx16.5.1\" class=\"ltx_emph ltx_font_italic\">International Journal of Computer Vision</em>, <em id=\"bib.bibx16.6.2\" class=\"ltx_emph ltx_font_italic\">115</em>, 211–252.\n\n\n</span>\n</li>\n<li id=\"bib.bibx17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Smaira et al.<span id=\"bib.bibx17.3.3.1\" class=\"ltx_text\"></span>Smaira et al.<span id=\"bib.bibx17.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nSmaira, L., Carreira, J., Noland, E., Clancy, E., Wu, A.,  Zisserman, A. \n</span>\n<span class=\"ltx_bibblock\">(2020).\n\n</span>\n<span class=\"ltx_bibblock\">A short note on the Kinetics-700-2020 human action dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx17.5.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2010.10864</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Smith et al.<span id=\"bib.bibx18.3.3.1\" class=\"ltx_text\"></span>Smith et al.<span id=\"bib.bibx18.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nSmith, K., Mei, L., Yao, S., Wu, J., Spelke, E., Tenenbaum, J.,  Ullman, T. \n</span>\n<span class=\"ltx_bibblock\">(2019).\n\n</span>\n<span class=\"ltx_bibblock\">Modeling expectation violation in intuitive physics with coarse probabilistic object representations.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx18.5.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx18.6.2\" class=\"ltx_emph ltx_font_italic\">32</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">SpelkeSpelke</span>\n<span class=\"ltx_bibblock\">\nSpelke, E. \n</span>\n<span class=\"ltx_bibblock\">(1994).\n\n</span>\n<span class=\"ltx_bibblock\">Initial knowledge: Six suggestions.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx19.1.1\" class=\"ltx_emph ltx_font_italic\">Cognition</em>, <em id=\"bib.bibx19.2.2\" class=\"ltx_emph ltx_font_italic\">50</em>(1-3), 431–445.\n\n\n</span>\n</li>\n<li id=\"bib.bibx20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sullivan, Mei, Perfors, Wojcik,  FrankSullivan et al.<span id=\"bib.bibx20.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nSullivan, J., Mei, M., Perfors, A., Wojcik, E.,  Frank, M. C. \n</span>\n<span class=\"ltx_bibblock\">(2021).\n\n</span>\n<span class=\"ltx_bibblock\">SAYCam: A large, longitudinal audiovisual dataset recorded from the infant’s perspective.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx20.3.1\" class=\"ltx_emph ltx_font_italic\">Open Mind</em>, <em id=\"bib.bibx20.4.2\" class=\"ltx_emph ltx_font_italic\">5</em>, 20–29.\n\n\n</span>\n</li>\n<li id=\"bib.bibx21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zaadnoordijk, Besold,  CusackZaadnoordijk et al.<span id=\"bib.bibx21.2.2.1\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nZaadnoordijk, L., Besold, T. R.,  Cusack, R. \n</span>\n<span class=\"ltx_bibblock\">(2022).\n\n</span>\n<span class=\"ltx_bibblock\">Lessons from infant learning for unsupervised machine learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx21.3.1\" class=\"ltx_emph ltx_font_italic\">Nature Machine Intelligence</em>, <em id=\"bib.bibx21.4.2\" class=\"ltx_emph ltx_font_italic\">4</em>(6), 510–520.\n\n\n</span>\n</li>\n<li id=\"bib.bibx22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhuang et al.<span id=\"bib.bibx22.3.3.1\" class=\"ltx_text\"></span>Zhuang et al.<span id=\"bib.bibx22.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nZhuang, C., Xiang, V., Bai, Y., Jia, X., Turk-Browne, N., Norman, K., … Yamins, D. L. \n</span>\n<span class=\"ltx_bibblock\">(2022).\n\n</span>\n<span class=\"ltx_bibblock\">How well do unsupervised learning algorithms model human real-time and life-long learning?\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx22.5.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, <em id=\"bib.bibx22.6.2\" class=\"ltx_emph ltx_font_italic\">36</em>.\n\n\n</span>\n</li>\n<li id=\"bib.bibx23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhuang et al.<span id=\"bib.bibx23.3.3.1\" class=\"ltx_text\"></span>Zhuang et al.<span id=\"bib.bibx23.4.4.2\" class=\"ltx_text\"></span></span>\n<span class=\"ltx_bibblock\">\nZhuang, C., Yan, S., Nayebi, A., Schrimpf, M., Frank, M. C., DiCarlo, J. J.,  Yamins, D. L. \n</span>\n<span class=\"ltx_bibblock\">(2021).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised neural network models of the ventral visual stream.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bibx23.5.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the National Academy of Sciences</em>, <em id=\"bib.bibx23.6.2\" class=\"ltx_emph ltx_font_italic\">118</em>(3), e2014196118.\n\n\n</span>\n</li>\n</ul>\n</section>",
  "css": "",
  "arxiv_id": "2402.00300",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:38.111Z"
}
