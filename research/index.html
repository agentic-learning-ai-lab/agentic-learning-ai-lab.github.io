<!doctype html>

<html lang="en">

<head>
    <!-- Open Graph / Facebook -->
    <meta property="og:title" content="Research | Agentic Learning AI Lab" />
    <meta property="og:description" content="Read our rcent research articles" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://agenticlearning.ai/research" />
    <!--Replace with the current website url-->
    <meta property="og:image" content="https://agenticlearning.ai/assets/images/background/bg1.jpg" />
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>agentic learning ai lab</title>
    <meta name="description" content="" />
    <!-- <link
        rel="shortcut icon"
        href="./assets/logo.png"
        type="image/x-icon"
    /> -->
    
    <link rel="stylesheet" href="/css/tailwind-runtime.css" />
    <link rel="stylesheet" href="/css/tailwind-build.css" />
    <link rel="stylesheet" href="/css/index.css" />
    
    <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/icon?family=Material+Icons"
    />
    <link
        rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&display=swap"
    />
    <link
        rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-icons/1.11.3/font/bootstrap-icons.min.css"
        integrity="sha512-dPXYcDub/aeb08c63jRq/k6GaKccl256JQy/AnOq7CAnEZ9FzSL9wSbcZkMp4R26vBsMLFYH4kQ67/bbV8XaCQ=="
        crossorigin="anonymous"
        referrerpolicy="no-referrer"
    />
    <script src="https://www.google.com/recaptcha/api.js" async defer></script>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-44MVTGBV0D"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-44MVTGBV0D');
    </script>
    
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@emailjs/browser@4/dist/email.min.js"></script>
    <script type="text/javascript">
        (function() {
            // https://dashboard.emailjs.com/admin/account
            emailjs.init({
              publicKey: "y6ebNBhEpEzmyqS8F",
            });
        })();
    </script>
    
    <script
        src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.0/gsap.min.js"
        integrity="sha512-B1lby8cGcAUU3GR+Fd809/ZxgHbfwJMp0jLTVfHiArTuUt++VqSlJpaJvhNtRf3NERaxDNmmxkdx2o+aHd4bvw=="
        crossorigin="anonymous"
        referrerpolicy="no-referrer"
    ></script>
    <script
        src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.0/ScrollTrigger.min.js"
        integrity="sha512-AY2+JxnBETJ0wcXnLPCcZJIJx0eimyhz3OJ55k2Jx4RtYC+XdIi2VtJQ+tP3BaTst4otlGG1TtPJ9fKrAUnRdQ=="
        crossorigin="anonymous"
        referrerpolicy="no-referrer"
    ></script>
    
    <script src="/search.js"></script></head>

<body class="tw-flex tw-min-h-[100vh] tw-flex-col tw-bg-[#fff] tw-font-mono">
    <header
        class="tw-absolute tw-top-0 tw-z-20 tw-flex tw-h-[120px] tw-w-full tw-bg-opacity-0 tw-px-[5%] max-lg:tw-mr-auto max-lg:tw-px-4 lg:tw-justify-between">
        <a class="tw-h-[120px] tw-p-[4px] tw-w-[120px] tw-text-2xl tw-font-medium" href="/">
            agentic learning <br /> ai lab
        </a>
        <div class="collapsible-header animated-collapse max-lg:tw-shadow-md" id="collapsed-header-items">
            <div
                class="tw-flex tw-h-full tw-w-max tw-gap-5 tw-text-base tw-text-black max-lg:tw-mt-[10px] max-lg:tw-flex-col max-lg:tw-place-items-end max-lg:tw-gap-2 lg:tw-place-items-center">
                <button onclick="openSearch()"
                    class="header-links tw-cursor-pointer tw-bg-transparent tw-border-none tw-flex tw-items-center tw-gap-2 max-lg:tw-w-full max-lg:tw-justify-end lg:tw-hidden"
                    aria-label="Search">
                    <span>search</span>
                    <i class="bi bi-search"></i>
                </button>
                <a class="header-links" href="/"> home </a>
                <a class="header-links" href="/research/"> research </a>
                <a class="header-links" href="/people/"> people </a>
                <a class="header-links" href="/contact/"> contact </a>
                <a class="header-links"
                    href="https://www.givecampus.com/campaigns/25654/donations/new?designation=supportofprofmengyerensagenticlearningailabresearch&">
                    donate </a>
            </div>
        </div>
        <button class="bi bi-list tw-absolute tw-right-3 tw-top-3 tw-z-50 tw-text-3xl tw-text-black lg:tw-hidden"
            onclick="toggleHeader()" aria-label="menu" id="collapse-btn"></button>
    </header>
    <section class="tw-flex tw-w-full tw-flex-col tw-p-[5%] tw-mt-[150px] max-lg:tw-p-4">
        <h3 class="text-left tw-text-4xl tw-font-medium max-md:tw-text-2xl">
            Research
        </h3>
        <div class="tw-my-4 max-md:tw-h-[3px] max-md:tw-w-[40px] tw-h-[5px] tw-w-[60px] tw-bg-gray-300 ">
        </div>

        <div id="flexContainer"
            class="tw-mt-4 tw-flex tw-flex-wrap tw-place-content-center tw-gap-y-4 tw-gap-x-4 tw-justify-around">
            <a href="/research/arq">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/arq.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions</h3>
                    <p class="tw-mt-4">
                        Authors: Frank (Zequan) Wu and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        The Forward-Forward (FF) Algorithm is a recently proposed learning procedure for neural networks that employs two forward passes instead of the traditional forward and backward passes used in backpropagation. However, FF remains largely confined to supervised settings, leaving a gap at domains where learning signals can be yielded more naturally such as RL. In this work, inspired by FF's goodness function using layer activity statistics, we introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value estimation method that applies a goodness function and action conditioning for local RL using temporal difference learning. Despite its simplicity and biological grounding, our approach achieves superior performance compared to state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind Control Suite benchmarks, while also outperforming algorithms trained with backpropagation on most tasks.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-10-08
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2510.06649 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/midway-network">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/midway_network.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics</h3>
                    <p class="tw-mt-4">
                        Authors: Chris Hoang and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Object recognition and motion understanding are key components of perception that complement each other. While self-supervised learning methods have shown promise in their ability to learn from unlabeled data, they have primarily focused on obtaining rich representations for either recognition or motion rather than both in tandem. On the other hand, latent dynamics modeling has been used in decision making to learn latent representations of observations and their transformations over time for control and planning tasks. In this work, we present Midway Network, a new self-supervised learning architecture that is the first to learn strong visual representations for both object recognition and motion understanding solely from natural videos, by extending latent dynamics modeling to this domain. Midway Network leverages a midway top-down path to infer motion latents between video frames, as well as a dense forward prediction objective and hierarchical structure to tackle the complex, multi-object scenes of natural videos. We demonstrate that after pretraining on two large-scale natural video datasets, Midway Network achieves strong performance on both semantic segmentation and optical flow tasks relative to prior self-supervised learning methods. We also show that Midway Network's learned dynamics can capture high-level correspondence via a novel analysis method based on forward feature perturbation.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-10-07
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2510.05558 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/stream-mem">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/stream_mem.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</h3>
                    <p class="tw-mt-4">
                        Authors: Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-08-21
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2508.15717 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/context-tuning">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/context_tuning.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Context Tuning for In-Context Optimization</h3>
                    <p class="tw-mt-4">
                        Authors: Jack Lu, Ryan Teehan, Zhenbang Yang, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of LLMs without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model's inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-07-06
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2507.04221 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/discrete-jepa">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/discrete_jepa.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Discrete JEPA: Learning Discrete Token Representations without Reconstruction</h3>
                    <p class="tw-mt-4">
                        Authors: Junyeob Baek, Hosung Lee, Chris Hoang, Mengye Ren, and Sungjin Ahn
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-06-22
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2506.14373 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/replay-can-provably-increase-forgetting">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/replay_can_provably_increase_forgetting.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Replay Can Provably Increase Forgetting</h3>
                    <p class="tw-mt-4">
                        Authors: Yasaman Mahdaviyeh, James Lucas, Mengye Ren, Andreas S. Tolias, Richard Zemel, and Toniann Pitassi
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Continual learning seeks to enable machine learning systems to solve an increasing corpus of tasks sequentially. A critical challenge for continual learning is forgetting, where the performance on previously learned tasks decreases as new tasks are introduced. One of the commonly used techniques to mitigate forgetting, sample replay, has been shown empirically to reduce forgetting by retaining some examples from old tasks and including them in new training episodes. In this work, we provide a theoretical analysis of sample replay in an over-parameterized continual linear regression setting, where each task is given by a linear subspace and with enough replay samples, one would be able to eliminate forgetting. Our analysis focuses on sample replay and highlights the role of the replayed samples and the relationship between task subspaces. Surprisingly, we find that, even in a noiseless setting, forgetting can be non-monotonic with respect to the number of replay samples. We present tasks where replay can be harmful with respect to worst-case settings, and also in distributional settings where replay of randomly selected samples increases forgetting in expectation. We also give empirical evidence that harmful replay is not limited to training with linear models by showing similar behavior for a neural networks equipped with SGD. Through experiments on a commonly used benchmark, we provide additional evidence that, even in seemingly benign scenarios, performance of the replay heavily depends on the choice of replay samples and the relationship between tasks.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-06-04
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2506.04377 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/memory-storyboard">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/memory_storyboard.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos</h3>
                    <p class="tw-mt-4">
                        Authors: Yanlai Yang and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Self-supervised learning holds the promise to learn good representations from real-world continuous uncurated data streams. However, most existing works in visual self-supervised learning focus on static images or artificial data streams. Towards exploring a more realistic learning substrate, we investigate streaming self-supervised learning from long-form real-world egocentric video streams. Inspired by the event segmentation mechanism in human perception and memory, we propose "Memory Storyboard" that groups recent past frames into temporal segments for more effective summarization of the past visual streams for memory replay. To accommodate efficient temporal segmentation, we propose a two-tier memory hierarchy: the recent past is stored in a short-term memory, and the storyboard temporal segments are then transferred to a long-term memory. Experiments on real-world egocentric video datasets including SAYCam and KrishnaCam show that contrastive learning objectives on top of storyboard frames result in semantically meaningful representations which outperform those produced by state-of-the-art unsupervised continual learning methods.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2025-01-21
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2501.12254 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/are-llms-prescient">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/are_llms_prescient.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Are LLMs Prescient? A Continuous Evaluation using Daily News as Oracle</h3>
                    <p class="tw-mt-4">
                        Authors: Amelia (Hui) Dai, Ryan Teehan, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to model updates and an evolving information landscape. Moreover, they often lack the ability to assess how model performance evolves over time, as they consist of static questions without a temporal dimension. To address these, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict "future" events based on pre-training data. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) can enhance prediction accuracy, the degradation persists, highlighting the need for ongoing model updates.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-11-13
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2411.08324 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/poodle">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/poodle.webp
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">PooDLe: Pooled and Dense Self-Supervised Learning from Naturalistic Videos</h3>
                    <p class="tw-mt-4">
                        Authors: Alex N. Wang, Chris Hoang, Yuwen Xiong, Yann LeCun, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos. We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-08-20
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2408.11208 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/procreate">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/procreate.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation</h3>
                    <p class="tw-mt-4">
                        Authors: Jack Lu, Ryan Teehan, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-08-05
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2408.02226 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/osiris">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/osiris.webp
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Integrating Present and Past in Unsupervised Continual Learning</h3>
                    <p class="tw-mt-4">
                        Authors: Yipeng Zhang, Laurent Charlin, Richard S. Zemel, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        We formulate a unifying framework for unsupervised continual learning (UCL), which disentangles learning objectives that are specific to the present and the past data, encompassing stability, plasticity, and cross-task consolidation. The framework reveals that many existing UCL approaches overlook cross-task consolidation and try to balance plasticity and stability in a shared embedding space. This results in worse performance due to a lack of within-task data diversity and reduced effectiveness in learning the current task. Our method, Osiris, which explicitly optimizes all three objectives on separate embedding spaces, achieves state-of-the-art performance on all benchmarks, including two novel benchmarks proposed in this paper featuring semantically structured task sequences. Compared to standard benchmarks, these two structured benchmarks more closely resemble visual signals received by humans and animals when navigating real-world environments. Finally, we show some preliminary evidence that continual models can benefit from such realistic learning scenarios.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-04-29
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2404.19132 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/college">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/college.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">CoLLEGe: Concept Embedding Generation for Large Language Models</h3>
                    <p class="tw-mt-4">
                        Authors: Ryan Teehan, Brenden M. Lake, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting without task-specific training.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-03-22
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2403.15362 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/anticipatory-recovery">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/reawakening.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Reawakening Knowledge: Anticipatory Recovery from Catastrophic Interference via Structured Training</h3>
                    <p class="tw-mt-4">
                        Authors: Yanlai Yang, Matt Jones, Michael C. Mozer, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-03-14
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2403.09613 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/video-ssl-from-child">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/ssl_childs_perspective.webp
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Self-Supervised Learning of Video Representations from a Child's Perspective</h3>
                    <p class="tw-mt-4">
                        Authors: A. Emin Orhan, Wentao Wang, Alex N. Wang, Mengye Ren, and Brenden M. Lake
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more accurate and more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2024-02-01
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2402.00300 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/learning-forgetting-llms">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/learning_and_forgetting_llm.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Learning and Forgetting Unsafe Examples in Large Language Models</h3>
                    <p class="tw-mt-4">
                        Authors: Jiachen Zhao, Zhun Deng, David Madras, James Zou, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the "ForgetFilter" algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2023-12-20
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2312.12736 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
            <a href="/research/lifelong-memory">
            <div class="page-item">
            <div class="safari-padding-fix">
                <!-- tw-rounded-lg  -->
                <!-- hover:tw-shadow-lg tw-transition-shadow tw-duration-300 -->
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full
            tw-border-transparent tw-border-2 hover:tw-border-solid  hover:tw-border-gray-600
            tw-overflow-hidden tw-transition-all tw-duration-200">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <!-- tw-rounded-lg -->
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden ">
                        <img src=/assets/images/papers/lifelong_memory.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos</h3>
                    <p class="tw-mt-4">
                        Authors: Ying Wang, Yanlai Yang, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D.
                    </p>
                    <p class="tw-mt-4">
                    Published: 2023-12-07
                    </p>
                    <div  class="tw-mt-4">
                    <!-- <a href=https://arxiv.org/abs/2312.05269 class="tw-mt-4"> -->
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    <!-- </a> -->
                    </div>
                </div>
            </div>
            </div>
            </div>
            </a>
        </div>

        <div class="pagination-controls tw-mt-12 tw-flex tw-justify-center tw-items-center tw-gap-2">
            <button id="prevButton" class="tw-px-6 tw-py-1 tw-text-black disabled:tw-text-gray-300"
                onclick="prevPage()">Prev</button>
            <span id="pageNumbers" class="tw-flex tw-gap-2"></span>
            <button id="nextButton" class="tw-px-6 tw-py-1 tw-text-black disabled:tw-text-gray-300"
                onclick="nextPage()">Next</button>
        </div>

    </section>

    <hr class="tw-mt-4" />
    <footer class="tw-mt-auto tw-flex tw-px-[5%] max-md:tw-px-4 max-lg:tw-px-4  tw-min-h-[100px] tw-w-full tw-place-items-center tw-gap-3 tw-text-black
    ">
        <div class="tw-flex tw-gap-6 tw-text-sm">
            60 Fifth Ave
            <br />
            New York
            <br />
            
        </div>
        <div class="tw-flex tw-gap-6 tw-text-2xl tw-justify-end tw-ml-auto tw-px-2">
            <a href="https://github.com/agentic-learning-ai-lab" aria-label="Github">
                <i class="bi bi-github"></i>
            </a>
            
            <a href="https://x.com/agentic_ai_lab" aria-label="X">
                <i class="bi bi-twitter-x"></i>
            </a>
    
            <a href="https://bsky.app/profile/agentic-ai-lab.bsky.social" aria-label="Bluesky">
                <i class="bi bi-bluesky"></i>
            </a>
        </div>
    </footer></body>
<script src="/index.js"></script>
<script src="/research.js"></script>

</html>