{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<figure class=\"ltx_figure ltx_align_floatright\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"109\" id=\"S1.F1.g1\" src=\"./assets/x1.png\" width=\"269\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>LifelongMemory employs natural language descriptions to create an episodic memory. It uses an LLM to sift through past events, retrieving specific moments in response to queries.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">Long-form egocentric video understanding has the potential to make a tremendous impact in real-life applications such as personalized AI assistants.\nImagine awkward moments when you find yourself asking “where did I put my glasses” or “what is the person’s name I just talked to.” A personalized AI assistant with a video memory can help us search for answers to questions like these. It takes in a question in the form of a natural language, and outputs either an answer or a video playback of the exact moment when the event of interest took place.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">However, despite the progress made on video and natural language understanding in deep learning, long-form egocentric video question answering remains challenging for two reasons. First, unlike short-form videos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">14</a>]</cite> that usually only contain one single scene and action, long-form egocentric videos can involve multiple scenes where the camera wearers perform numerous tasks and interact with different people and objects. The abundance of details and long-range temporal dependencies make successful information retrieval difficult. Previous methods develop better video features to capture low-level action and object information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">31</a>]</cite>, yet fall short of long-form video understanding <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">13</a>]</cite>.\nSecond, question answering may require sophisticated reasoning of events and oftentimes end-to-end models do not have enough supervision data to generalize and correctly understand different types of questions <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"166\" id=\"S1.F2.g1\" src=\"./assets/x2.png\" width=\"538\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>\nOur LifelongMemory Framework for Long-form Video Understanding. The video inputs are first converted into captions using a pretrained MLLM and then condensed via <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F2.2.1\">Caption Digest</em>. Next, captions and queries are processed by an LLM to predict coarse temporal windows (for NLQ) or answers (for QA) with explanations and confidence levels. For the NLQ task, the predicted windows are further refined by a pre-trained NLQ model. For the video QA task, we ensemble the predictions of multiple runs and select the answers with the highest confidence. </figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">To address these two challenges simultaneously, we propose a unified framework, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p3.1.1\">LifelongMemory</em>, for long-form video question answering using large language models (LLMs). We compress long video inputs into concise text descriptions with our proposed <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p3.1.2\">Caption Digest</em> component. The text format can be then augmented to the context of an LLM for answering the questions and locating the most relevant time window. The LLM is capable of general question answering, and unlike end-to-end models, it has zero-shot generalization. Moreover, we also prompt the LLM to produce a confidence level with a textual explanation, both of which help refine the predictions and enhance the interpretability of the model outputs.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">Our proposed framework achieves superior performance on two benchmarks for long-form egocentric video understanding, including multi-choice Video Question Answering (Video QA) and natural language query (NLQ). For zero-shot evaluation on the EgoSchema video QA benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">27</a>]</cite>, our method achieves the state of the art which doubles the accuracy of pretrained video QA models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">42</a>]</cite> and significantly outperforms other LLM-based methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">35</a>]</cite>. In the Ego4D NLQ challenge, our method is able to increase the precision of pretrained NLQ models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">15</a>]</cite> by providing coarse-grained candidate temporal windows in zero shot. In summary, our contributions are as follows:</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\">We propose a novel framework, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.I1.i1.p1.1.1\">LifelongMemory</em>, that integrates pre-trained MLLMs to answer questions in long-form egocentric videos. It leverages the remarkable reasoning capabilities of LLMs to tackle the challenge of long-range temporal understanding.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\">Our method significantly outperforms prior models and concurrent LLM-based solutions on EgoSchema, and remain highly competitive on Ego4D NLQ.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i3.p1.1\">Our framework enhances the interpretability and reliability of the results by providing a confidence level and textual explanation of its prediction, revealing the reasoning process of LLMs.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n<div class=\"ltx_para\" id=\"S2.p1\">\n<p class=\"ltx_p\" id=\"S2.p1.1\">Multimodal Large Language Models (MLLMs) have recently demonstrated their impressive capabilities in various downstream vision-language tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">26</a>]</cite>. In this paper, we discuss how to utilize frozen MLLMs for long-form video understanding by experimenting with two specific tasks—Video Question Answering (Video QA) and Natural Language Queries (NLQ)—both of which require comprehensive understanding and reasoning of texts and videos. In the following paragraphs, we survey prior works on MLLMs, Video QA, and NLQ.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Multimodal Large Language Models.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px1.p1.1\">Large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">51</a>, <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">7</a>]</cite> have demonstrated their excellent ability to understand and reason with natural language inputs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">52</a>]</cite>. To extend this understanding and reasoning ability beyond text, many prior works have explored incorporating other modalities, especially visual perception, into LLMs. This leads to the rise of multimodal large language models (MLLMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">46</a>]</cite>. LLaVA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">23</a>]</cite> connects the CLIP visual encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">29</a>]</cite> with the language decoder of an LLM and finetune them end-to-end on multimodal instruction-following data, achieving competitive performance in general-purpose visual and language understanding. LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">55</a>]</cite> adds visual conditioning on the input to pre-trained LLMs, and finetunes them on Ego4D narrations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">13</a>]</cite> to create automatic video narrators. These MLLMs serve as critical components in a broad range of downstream applications, showcasing their strength in reasoning on multimodal data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"#bib.bib54\" title=\"\">54</a>, <a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">43</a>]</cite>. Our work shares the successes in utilizing MLLMs for understanding and reasoning on text and image/video data. Although most current open-sourced MLLMs (such as LLaVA and LaViLa) only take one image or a very short video as inputs, our proposed framework can integrate those pretrained MLLMs and leverage them for the challenging task of long-form video understanding.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Video Question Answering with Multimodal LLMs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px2.p1.1\">The success of LLMs in text QA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">1</a>]</cite> leads to an increasing trend of applying MLLMs in video QA tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">35</a>, <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">53</a>]</cite>. Due to the computational burden of large-scale pertaining, many prior works have explored leveraging pretrained (M)LLMs for zero-shot or few-shot QA. R2A <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">28</a>]</cite> retrieves textual descriptions from an external text corpus based on the similarity of video frames and text features encoded by CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">29</a>]</cite>, then uses a pretrained LLM to generate answers given the question and the retrieved descriptions. However, retrieval from a pre-defined text corpus hinders scaling to unseen videos and results in vague or inaccurate video descriptions that can decrease the accuracy of the subsequent QA step. Instead of obtaining captions by retrieval, some prior works utilize a pretrained captioning model to generate high-quality descriptions of videos. VidIL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">37</a>]</cite> obtains frame-level captions from BLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">20</a>]</cite> and retrieves labels of objects, attributes, and events from pre-defined vocabularies using CLIP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">29</a>]</cite>, then feed all these information into an LLM with a few labeled examples. Despite its good performance on short videos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">39</a>]</cite>, this approach is not suitable for long videos in the wild because (i) pre-defined vocabularies limit the application of the approach and (ii) a large number of noisy and redundant low-level details can distract the LLM from the main task. Our proposed framework is more efficient and flexible: It is able to perform zero-shot video QA that only utilizes a concise list of distilled captions without requiring a fixed keyword vocabulary. Most relevant to our work, Socratic models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">47</a>]</cite> show qualitative examples of LLM’s zero-shot performance on some toy examples, where the key moments of the input video are converted into a textual record by a captioning model. Concurrent work Vamos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">35</a>]</cite> and LLoVi <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a>]</cite> employ a similar approach of using a captioning model to bridge videos and LLMs for zero-shot video QA. Empirical evaluations show that our proposed framework significantly outperforms these two on EgoSchema <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">27</a>]</cite>, a zero-shot video QA benchmark designed for long-range temporal understanding.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Natural Language Queries in Egocentric Videos.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px3.p1.1\">The Natural Language Queries (NLQ) task involves localizing the temporal window corresponding to the answer to a question in a long video clip. This task is challenging for end-to-end supervised video localization models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">50</a>]</cite> due to the sparsity of annotations and the length of videos in the dataset. Prior works have focused on constructing a hierarchical structure, augmenting the NLQ dataset and developing better video features through large-scale pretraining. ReLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">25</a>]</cite> proposes a novel multi-scale cross-modal transformer architecture, a video frame-level contrastive loss, and two data augmentation strategies. InternVideo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>]</cite> improves the quality of video features by carefully pre-training and fine-tuning a VideoMAE-L Model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">32</a>]</cite>, and ensemble the features and predictions. More recently, NaQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>]</cite> introduces a data augmentation strategy to transform video narrations into training data for the NLQ task, alleviating the problem of sparse annotation. NaQ++ ReLER, obtained by training the ReLER model with NaQ data, was the previous state-of-the-art method for Ego4D NLQ. GroundNLQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">15</a>]</cite> is the current state-of-the-art for this benchmark. It adopts a two-stage pre-training strategy to respectively train a video feature extractor and a grounding model on video narrations, and finally finetune the grounding model on annotated data. Our work is complementary to these prior works in that they can be used in the last stage of our proposed framework to produce more fine-grained predictions based on the predictions of the frozen LLM.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>LifelongMemory</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\" id=\"S3.p1.1\">In this section, we introduce our proposed LifelongMemory framework. To tackle the challenge of long-form videos, we first transform egocentric videos into a comprehensive yet concise textual log and then further condense the information via <em class=\"ltx_emph ltx_font_italic\" id=\"S3.p1.1.1\">Caption Digest</em>. Then, we use an LLM to predict answers (for Video QA) or coarse temporal windows (for NLQ), along with confidence and explanation for interpretability. Finally, the predictions are further refined depending on the task. <a class=\"ltx_ref\" href=\"#S1.F2\" title=\"In 1 Introduction ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a> outlines the workflow and we describe different stages in detail below.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Egocentric Video Captioning</h3>\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.p1.1\">We begin by summarizing the raw footage into a list of captions using pre-trained MLLMs (<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.p1.1.1\">e.g</em>.<span class=\"ltx_text\" id=\"S3.SS1.p1.1.2\"></span> LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">55</a>]</cite>).\nWe sample image frames or short video clips at a fixed interval, and produce a line of caption per clip.\nThe text descriptions as a form of episodic memory enable the transformation of complex egocentric video footage into a coherent log of daily activities, capturing life’s narrative in a more accessible and compressed format.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Digest.</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS0.Px1.p1.1\">Raw captions produced by MLLMs, however, can be rather verbose and repetitive, and consequently hinder the downstream reasoning process, especially for long-form videos. We propose to create a caption digest to condense the information. Moreover, we aim to increase the relevance of the captions in relation to the target queries. <a class=\"ltx_ref\" href=\"#S3.F3\" title=\"In Caption Digest. ‣ 3.1 Egocentric Video Captioning ‣ 3 LifelongMemory ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an example of the caption digest process.\nFirst, we remove uninformative captions (<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS0.Px1.p1.1.1\">e.g</em>.<span class=\"ltx_text\" id=\"S3.SS1.SSS0.Px1.p1.1.2\"></span> “looks around …”). Second, we remove captions that are not relevant to the query by comparing the embedding similarity. Third, we gather adjacent captions that share a high similarity score and use an LLM to produce a single concise caption. The condensed list of captions then augments the context of the LLM for further reasoning and processing.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"254\" id=\"S3.F3.g1\" src=\"./assets/x3.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>\nExample Caption Processing and LLM Reasoning for NLQ. 1) We use a multimodal LLM (MLLM) to produce captions from a list of short video clips.\n2) Content and query similarity filters are then applied to remove redundant and irrelevant captions. Similar consecutive captions are merged by an LLM. 3) An LLM is instructed to take inputs from the list of condensed captions and retrieve the most relevant interval candidates. The same procedure is performed on the QA task.\n</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>LLM Reasoning</h3>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">With the list of condensed captions with their corresponding time interval from the previous stage, we leverage an LLM here for its impressive zero-shot context understanding and reasoning capability.\nWe combine captions and queries into an instructive and contextualized prompt. A snippet of the instruction template is shown in <a class=\"ltx_ref\" href=\"#S3.F3\" title=\"In Caption Digest. ‣ 3.1 Egocentric Video Captioning ‣ 3 LifelongMemory ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>. The full prompt and a discussion of the prompt designs are in <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Prompting ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS2.p2.1\">We particularly instruct the LLM to aggregate information and imagine the visual scene underlying the given captions. The LLM is instructed to take into consideration the full context in the template and utilize different pieces of information to produce the most probable answer.\nFor example, when asking “<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p2.1.1\">Who did I interact with when I was shopping?</em>”, the LLM is able to filter all captions and produce a list of intervals involving “<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p2.1.2\">person x talking to C</em>” where <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p2.1.3\">C</em> is the subject in the video and <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p2.1.4\">X</em> refers to the other person. The LLM is also instructed to consider the loss of information when converting videos into concise captions. For example, one query asks “<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p2.1.5\">What size of washer did I pick ?</em>” but there are no captions explicitly mentioning the washers. In this example, the LLM displays its capability to capture implicit information and infer based on context. The LLM answers “<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p2.1.6\">choosing the time points where I picked items from the table or the floor, as these instances may provide more context about the objects and their locations</em>.” By grasping nuanced relationships and dependencies within the given context, LLM is able to filter out the most relevant information from the extensive video captions.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p3\">\n<p class=\"ltx_p\" id=\"S3.SS2.p3.1\">In addition to the predicted answers, we also ask the LLM to explain its predictions for more interpretability. Specifically, we ask the LLM to output a sentence of explanation to encourage introspective thinking and rate its confidence in the output out of three confidence levels. The verbalized confidence strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">38</a>]</cite> can help us control the precision of the output in later stages.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>Vote by Confidence (Video QA)</h3>\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.p1.1\">To increase the reliability of LLM predictions for video QA, we ensemble\nthe LLM’s predictions using voting by confidence. We repeatly perform the LLM reasoning step where the LLM is prompted to generate predictions based on the same input in each run. From the pool of predictions, the answer with the highest confidence score is selected. In cases where multiple answers have the same highest confidence, a random selection is performed. By focusing on the most confident predictions, this ensemble step can further improve the accuracy and robustness of the results.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>Fine-grained Interval Refinement (NLQ)</h3>\n<div class=\"ltx_para\" id=\"S3.SS4.p1\">\n<p class=\"ltx_p\" id=\"S3.SS4.p1.8\">Since the time intervals are subsampled, to obtain a fine-grained interval prediction for the NLQ task, we revisit the video inputs and enhance our LLM interval predictions in the last stage.\nFor this goal, we employ a pretrained NLQ model and feed in candidate intervals predicted by our previous stage. The intervals are padded with a small window of size <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.1.m1.1\"><semantics id=\"S3.SS4.p1.1.m1.1a\"><mi id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">α</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\"><ci id=\"S3.SS4.p1.1.m1.1.1.cmml\" xref=\"S3.SS4.p1.1.m1.1.1\">𝛼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">\\alpha</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">italic_α</annotation></semantics></math>.\nSpecifically, for each <math alttext=\"(s_{i},e_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.2.m2.2\"><semantics id=\"S3.SS4.p1.2.m2.2a\"><mrow id=\"S3.SS4.p1.2.m2.2.2.2\" xref=\"S3.SS4.p1.2.m2.2.2.3.cmml\"><mo id=\"S3.SS4.p1.2.m2.2.2.2.3\" stretchy=\"false\" xref=\"S3.SS4.p1.2.m2.2.2.3.cmml\">(</mo><msub id=\"S3.SS4.p1.2.m2.1.1.1.1\" xref=\"S3.SS4.p1.2.m2.1.1.1.1.cmml\"><mi id=\"S3.SS4.p1.2.m2.1.1.1.1.2\" xref=\"S3.SS4.p1.2.m2.1.1.1.1.2.cmml\">s</mi><mi id=\"S3.SS4.p1.2.m2.1.1.1.1.3\" xref=\"S3.SS4.p1.2.m2.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S3.SS4.p1.2.m2.2.2.2.4\" xref=\"S3.SS4.p1.2.m2.2.2.3.cmml\">,</mo><msub id=\"S3.SS4.p1.2.m2.2.2.2.2\" xref=\"S3.SS4.p1.2.m2.2.2.2.2.cmml\"><mi id=\"S3.SS4.p1.2.m2.2.2.2.2.2\" xref=\"S3.SS4.p1.2.m2.2.2.2.2.2.cmml\">e</mi><mi id=\"S3.SS4.p1.2.m2.2.2.2.2.3\" xref=\"S3.SS4.p1.2.m2.2.2.2.2.3.cmml\">i</mi></msub><mo id=\"S3.SS4.p1.2.m2.2.2.2.5\" stretchy=\"false\" xref=\"S3.SS4.p1.2.m2.2.2.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.2.m2.2b\"><interval closure=\"open\" id=\"S3.SS4.p1.2.m2.2.2.3.cmml\" xref=\"S3.SS4.p1.2.m2.2.2.2\"><apply id=\"S3.SS4.p1.2.m2.1.1.1.1.cmml\" xref=\"S3.SS4.p1.2.m2.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.2.m2.1.1.1.1.1.cmml\" xref=\"S3.SS4.p1.2.m2.1.1.1.1\">subscript</csymbol><ci id=\"S3.SS4.p1.2.m2.1.1.1.1.2.cmml\" xref=\"S3.SS4.p1.2.m2.1.1.1.1.2\">𝑠</ci><ci id=\"S3.SS4.p1.2.m2.1.1.1.1.3.cmml\" xref=\"S3.SS4.p1.2.m2.1.1.1.1.3\">𝑖</ci></apply><apply id=\"S3.SS4.p1.2.m2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.2.m2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.2.m2.2.2.2.2.1.cmml\" xref=\"S3.SS4.p1.2.m2.2.2.2.2\">subscript</csymbol><ci id=\"S3.SS4.p1.2.m2.2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.2.m2.2.2.2.2.2\">𝑒</ci><ci id=\"S3.SS4.p1.2.m2.2.2.2.2.3.cmml\" xref=\"S3.SS4.p1.2.m2.2.2.2.2.3\">𝑖</ci></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.2.m2.2c\">(s_{i},e_{i})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.2.m2.2d\">( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>, the new start time is <math alttext=\"s^{\\prime}_{i}=\\max(s_{i}-\\alpha,s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.3.m3.3\"><semantics id=\"S3.SS4.p1.3.m3.3a\"><mrow id=\"S3.SS4.p1.3.m3.3.3\" xref=\"S3.SS4.p1.3.m3.3.3.cmml\"><msubsup id=\"S3.SS4.p1.3.m3.3.3.3\" xref=\"S3.SS4.p1.3.m3.3.3.3.cmml\"><mi id=\"S3.SS4.p1.3.m3.3.3.3.2.2\" xref=\"S3.SS4.p1.3.m3.3.3.3.2.2.cmml\">s</mi><mi id=\"S3.SS4.p1.3.m3.3.3.3.3\" xref=\"S3.SS4.p1.3.m3.3.3.3.3.cmml\">i</mi><mo id=\"S3.SS4.p1.3.m3.3.3.3.2.3\" xref=\"S3.SS4.p1.3.m3.3.3.3.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.3.m3.3.3.2\" xref=\"S3.SS4.p1.3.m3.3.3.2.cmml\">=</mo><mrow id=\"S3.SS4.p1.3.m3.3.3.1.1\" xref=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\"><mi id=\"S3.SS4.p1.3.m3.1.1\" xref=\"S3.SS4.p1.3.m3.1.1.cmml\">max</mi><mo id=\"S3.SS4.p1.3.m3.3.3.1.1a\" xref=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\">⁡</mo><mrow id=\"S3.SS4.p1.3.m3.3.3.1.1.1\" xref=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\"><mo id=\"S3.SS4.p1.3.m3.3.3.1.1.1.2\" stretchy=\"false\" xref=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\">(</mo><mrow id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.cmml\"><msub id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.cmml\"><mi id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.2\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.2.cmml\">s</mi><mi id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.3\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.3.cmml\">i</mi></msub><mo id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.1\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.1.cmml\">−</mo><mi id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.3\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.3.cmml\">α</mi></mrow><mo id=\"S3.SS4.p1.3.m3.3.3.1.1.1.3\" xref=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\">,</mo><mi id=\"S3.SS4.p1.3.m3.2.2\" xref=\"S3.SS4.p1.3.m3.2.2.cmml\">s</mi><mo id=\"S3.SS4.p1.3.m3.3.3.1.1.1.4\" stretchy=\"false\" xref=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.3.m3.3b\"><apply id=\"S3.SS4.p1.3.m3.3.3.cmml\" xref=\"S3.SS4.p1.3.m3.3.3\"><eq id=\"S3.SS4.p1.3.m3.3.3.2.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.2\"></eq><apply id=\"S3.SS4.p1.3.m3.3.3.3.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.3.m3.3.3.3.1.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3\">subscript</csymbol><apply id=\"S3.SS4.p1.3.m3.3.3.3.2.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.3.m3.3.3.3.2.1.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3\">superscript</csymbol><ci id=\"S3.SS4.p1.3.m3.3.3.3.2.2.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3.2.2\">𝑠</ci><ci id=\"S3.SS4.p1.3.m3.3.3.3.2.3.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3.2.3\">′</ci></apply><ci id=\"S3.SS4.p1.3.m3.3.3.3.3.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.3.3\">𝑖</ci></apply><apply id=\"S3.SS4.p1.3.m3.3.3.1.2.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1\"><max id=\"S3.SS4.p1.3.m3.1.1.cmml\" xref=\"S3.SS4.p1.3.m3.1.1\"></max><apply id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1\"><minus id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.1.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.1\"></minus><apply id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.1.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2\">subscript</csymbol><ci id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.2.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.2\">𝑠</ci><ci id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.3.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.2.3\">𝑖</ci></apply><ci id=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.3.cmml\" xref=\"S3.SS4.p1.3.m3.3.3.1.1.1.1.3\">𝛼</ci></apply><ci id=\"S3.SS4.p1.3.m3.2.2.cmml\" xref=\"S3.SS4.p1.3.m3.2.2\">𝑠</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.3.m3.3c\">s^{\\prime}_{i}=\\max(s_{i}-\\alpha,s)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.3.m3.3d\">italic_s start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_max ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_α , italic_s )</annotation></semantics></math> and the new end time is <math alttext=\"e^{\\prime}_{i}=\\min(e_{i}+\\alpha,e)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.4.m4.3\"><semantics id=\"S3.SS4.p1.4.m4.3a\"><mrow id=\"S3.SS4.p1.4.m4.3.3\" xref=\"S3.SS4.p1.4.m4.3.3.cmml\"><msubsup id=\"S3.SS4.p1.4.m4.3.3.3\" xref=\"S3.SS4.p1.4.m4.3.3.3.cmml\"><mi id=\"S3.SS4.p1.4.m4.3.3.3.2.2\" xref=\"S3.SS4.p1.4.m4.3.3.3.2.2.cmml\">e</mi><mi id=\"S3.SS4.p1.4.m4.3.3.3.3\" xref=\"S3.SS4.p1.4.m4.3.3.3.3.cmml\">i</mi><mo id=\"S3.SS4.p1.4.m4.3.3.3.2.3\" xref=\"S3.SS4.p1.4.m4.3.3.3.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.4.m4.3.3.2\" xref=\"S3.SS4.p1.4.m4.3.3.2.cmml\">=</mo><mrow id=\"S3.SS4.p1.4.m4.3.3.1.1\" xref=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\"><mi id=\"S3.SS4.p1.4.m4.1.1\" xref=\"S3.SS4.p1.4.m4.1.1.cmml\">min</mi><mo id=\"S3.SS4.p1.4.m4.3.3.1.1a\" xref=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\">⁡</mo><mrow id=\"S3.SS4.p1.4.m4.3.3.1.1.1\" xref=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\"><mo id=\"S3.SS4.p1.4.m4.3.3.1.1.1.2\" stretchy=\"false\" xref=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\">(</mo><mrow id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.cmml\"><msub id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.cmml\"><mi id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.2\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.2.cmml\">e</mi><mi id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.3\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.3.cmml\">i</mi></msub><mo id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.1\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.1.cmml\">+</mo><mi id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.3\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.3.cmml\">α</mi></mrow><mo id=\"S3.SS4.p1.4.m4.3.3.1.1.1.3\" xref=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\">,</mo><mi id=\"S3.SS4.p1.4.m4.2.2\" xref=\"S3.SS4.p1.4.m4.2.2.cmml\">e</mi><mo id=\"S3.SS4.p1.4.m4.3.3.1.1.1.4\" stretchy=\"false\" xref=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.4.m4.3b\"><apply id=\"S3.SS4.p1.4.m4.3.3.cmml\" xref=\"S3.SS4.p1.4.m4.3.3\"><eq id=\"S3.SS4.p1.4.m4.3.3.2.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.2\"></eq><apply id=\"S3.SS4.p1.4.m4.3.3.3.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.4.m4.3.3.3.1.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3\">subscript</csymbol><apply id=\"S3.SS4.p1.4.m4.3.3.3.2.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.4.m4.3.3.3.2.1.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3\">superscript</csymbol><ci id=\"S3.SS4.p1.4.m4.3.3.3.2.2.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3.2.2\">𝑒</ci><ci id=\"S3.SS4.p1.4.m4.3.3.3.2.3.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3.2.3\">′</ci></apply><ci id=\"S3.SS4.p1.4.m4.3.3.3.3.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.3.3\">𝑖</ci></apply><apply id=\"S3.SS4.p1.4.m4.3.3.1.2.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1\"><min id=\"S3.SS4.p1.4.m4.1.1.cmml\" xref=\"S3.SS4.p1.4.m4.1.1\"></min><apply id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1\"><plus id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.1.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.1\"></plus><apply id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.1.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2\">subscript</csymbol><ci id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.2.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.2\">𝑒</ci><ci id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.3.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.2.3\">𝑖</ci></apply><ci id=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.3.cmml\" xref=\"S3.SS4.p1.4.m4.3.3.1.1.1.1.3\">𝛼</ci></apply><ci id=\"S3.SS4.p1.4.m4.2.2.cmml\" xref=\"S3.SS4.p1.4.m4.2.2\">𝑒</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.4.m4.3c\">e^{\\prime}_{i}=\\min(e_{i}+\\alpha,e)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.4.m4.3d\">italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_min ( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_α , italic_e )</annotation></semantics></math> where <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.5.m5.1\"><semantics id=\"S3.SS4.p1.5.m5.1a\"><mi id=\"S3.SS4.p1.5.m5.1.1\" xref=\"S3.SS4.p1.5.m5.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.5.m5.1b\"><ci id=\"S3.SS4.p1.5.m5.1.1.cmml\" xref=\"S3.SS4.p1.5.m5.1.1\">𝑠</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.5.m5.1c\">s</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.5.m5.1d\">italic_s</annotation></semantics></math> and <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.6.m6.1\"><semantics id=\"S3.SS4.p1.6.m6.1a\"><mi id=\"S3.SS4.p1.6.m6.1.1\" xref=\"S3.SS4.p1.6.m6.1.1.cmml\">e</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.6.m6.1b\"><ci id=\"S3.SS4.p1.6.m6.1.1.cmml\" xref=\"S3.SS4.p1.6.m6.1.1\">𝑒</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.6.m6.1c\">e</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.6.m6.1d\">italic_e</annotation></semantics></math> are the start and end time of the original clip. Then we extract video clips <math alttext=\"[v_{1},v_{2}\\dots v_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.7.m7.2\"><semantics id=\"S3.SS4.p1.7.m7.2a\"><mrow id=\"S3.SS4.p1.7.m7.2.2.2\" xref=\"S3.SS4.p1.7.m7.2.2.3.cmml\"><mo id=\"S3.SS4.p1.7.m7.2.2.2.3\" stretchy=\"false\" xref=\"S3.SS4.p1.7.m7.2.2.3.cmml\">[</mo><msub id=\"S3.SS4.p1.7.m7.1.1.1.1\" xref=\"S3.SS4.p1.7.m7.1.1.1.1.cmml\"><mi id=\"S3.SS4.p1.7.m7.1.1.1.1.2\" xref=\"S3.SS4.p1.7.m7.1.1.1.1.2.cmml\">v</mi><mn id=\"S3.SS4.p1.7.m7.1.1.1.1.3\" xref=\"S3.SS4.p1.7.m7.1.1.1.1.3.cmml\">1</mn></msub><mo id=\"S3.SS4.p1.7.m7.2.2.2.4\" xref=\"S3.SS4.p1.7.m7.2.2.3.cmml\">,</mo><mrow id=\"S3.SS4.p1.7.m7.2.2.2.2\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.cmml\"><msub id=\"S3.SS4.p1.7.m7.2.2.2.2.2\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2.cmml\"><mi id=\"S3.SS4.p1.7.m7.2.2.2.2.2.2\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2.2.cmml\">v</mi><mn id=\"S3.SS4.p1.7.m7.2.2.2.2.2.3\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2.3.cmml\">2</mn></msub><mo id=\"S3.SS4.p1.7.m7.2.2.2.2.1\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S3.SS4.p1.7.m7.2.2.2.2.3\" mathvariant=\"normal\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.3.cmml\">…</mi><mo id=\"S3.SS4.p1.7.m7.2.2.2.2.1a\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.1.cmml\">⁢</mo><msub id=\"S3.SS4.p1.7.m7.2.2.2.2.4\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4.cmml\"><mi id=\"S3.SS4.p1.7.m7.2.2.2.2.4.2\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4.2.cmml\">v</mi><mi id=\"S3.SS4.p1.7.m7.2.2.2.2.4.3\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4.3.cmml\">n</mi></msub></mrow><mo id=\"S3.SS4.p1.7.m7.2.2.2.5\" stretchy=\"false\" xref=\"S3.SS4.p1.7.m7.2.2.3.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.7.m7.2b\"><interval closure=\"closed\" id=\"S3.SS4.p1.7.m7.2.2.3.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2\"><apply id=\"S3.SS4.p1.7.m7.1.1.1.1.cmml\" xref=\"S3.SS4.p1.7.m7.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.7.m7.1.1.1.1.1.cmml\" xref=\"S3.SS4.p1.7.m7.1.1.1.1\">subscript</csymbol><ci id=\"S3.SS4.p1.7.m7.1.1.1.1.2.cmml\" xref=\"S3.SS4.p1.7.m7.1.1.1.1.2\">𝑣</ci><cn id=\"S3.SS4.p1.7.m7.1.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS4.p1.7.m7.1.1.1.1.3\">1</cn></apply><apply id=\"S3.SS4.p1.7.m7.2.2.2.2.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2\"><times id=\"S3.SS4.p1.7.m7.2.2.2.2.1.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.1\"></times><apply id=\"S3.SS4.p1.7.m7.2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.7.m7.2.2.2.2.2.1.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2\">subscript</csymbol><ci id=\"S3.SS4.p1.7.m7.2.2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2.2\">𝑣</ci><cn id=\"S3.SS4.p1.7.m7.2.2.2.2.2.3.cmml\" type=\"integer\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.2.3\">2</cn></apply><ci id=\"S3.SS4.p1.7.m7.2.2.2.2.3.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.3\">…</ci><apply id=\"S3.SS4.p1.7.m7.2.2.2.2.4.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.7.m7.2.2.2.2.4.1.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4\">subscript</csymbol><ci id=\"S3.SS4.p1.7.m7.2.2.2.2.4.2.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4.2\">𝑣</ci><ci id=\"S3.SS4.p1.7.m7.2.2.2.2.4.3.cmml\" xref=\"S3.SS4.p1.7.m7.2.2.2.2.4.3\">𝑛</ci></apply></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.7.m7.2c\">[v_{1},v_{2}\\dots v_{n}]</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.7.m7.2d\">[ italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT … italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> according to the predicted intervals <math alttext=\"[(s^{\\prime}_{1},e^{\\prime}_{1}),(s^{\\prime}_{2},e^{\\prime}_{2})\\dots(s^{%\n\\prime}_{n},e^{\\prime}_{n})]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.8.m8.2\"><semantics id=\"S3.SS4.p1.8.m8.2a\"><mrow id=\"S3.SS4.p1.8.m8.2.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.3.cmml\"><mo id=\"S3.SS4.p1.8.m8.2.2.2.3\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.2.2.3.cmml\">[</mo><mrow id=\"S3.SS4.p1.8.m8.1.1.1.1.2\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.3.cmml\"><mo id=\"S3.SS4.p1.8.m8.1.1.1.1.2.3\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.3.cmml\">(</mo><msubsup id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.cmml\"><mi id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.2\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.2.cmml\">s</mi><mn id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.3\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.3.cmml\">1</mn><mo id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.3\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.8.m8.1.1.1.1.2.4\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.3.cmml\">,</mo><msubsup id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.cmml\"><mi id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.2\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.2.cmml\">e</mi><mn id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.3\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.3.cmml\">1</mn><mo id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.3\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.8.m8.1.1.1.1.2.5\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.3.cmml\">)</mo></mrow><mo id=\"S3.SS4.p1.8.m8.2.2.2.4\" xref=\"S3.SS4.p1.8.m8.2.2.3.cmml\">,</mo><mrow id=\"S3.SS4.p1.8.m8.2.2.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.cmml\"><mrow id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.3.cmml\"><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.3\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.3.cmml\">(</mo><msubsup id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.cmml\"><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.2.cmml\">s</mi><mn id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.3.cmml\">2</mn><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.4\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.3.cmml\">,</mo><msubsup id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.cmml\"><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.2.cmml\">e</mi><mn id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.3.cmml\">2</mn><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.5\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.3.cmml\">)</mo></mrow><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.5\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.5.cmml\">⁢</mo><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.6\" mathvariant=\"normal\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.6.cmml\">…</mi><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.5a\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.5.cmml\">⁢</mo><mrow id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.3.cmml\"><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.3\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.3.cmml\">(</mo><msubsup id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.cmml\"><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.2.cmml\">s</mi><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.3.cmml\">n</mi><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.4\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.3.cmml\">,</mo><msubsup id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.cmml\"><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.2\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.2.cmml\">e</mi><mi id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.3.cmml\">n</mi><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.3\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.5\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.3.cmml\">)</mo></mrow></mrow><mo id=\"S3.SS4.p1.8.m8.2.2.2.5\" stretchy=\"false\" xref=\"S3.SS4.p1.8.m8.2.2.3.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.8.m8.2b\"><interval closure=\"closed\" id=\"S3.SS4.p1.8.m8.2.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2\"><interval closure=\"open\" id=\"S3.SS4.p1.8.m8.1.1.1.1.3.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2\"><apply id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.1.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1\">subscript</csymbol><apply id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1\">superscript</csymbol><ci id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.2\">𝑠</ci><ci id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.2.3\">′</ci></apply><cn id=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.1.1.3\">1</cn></apply><apply id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2\">subscript</csymbol><apply id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2\">superscript</csymbol><ci id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.2\">𝑒</ci><ci id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.2.3\">′</ci></apply><cn id=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.3.cmml\" type=\"integer\" xref=\"S3.SS4.p1.8.m8.1.1.1.1.2.2.3\">1</cn></apply></interval><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2\"><times id=\"S3.SS4.p1.8.m8.2.2.2.2.5.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.5\"></times><interval closure=\"open\" id=\"S3.SS4.p1.8.m8.2.2.2.2.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2\"><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1\">subscript</csymbol><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1\">superscript</csymbol><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.2\">𝑠</ci><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.2.3\">′</ci></apply><cn id=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.1.1.1.3\">2</cn></apply><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2\">subscript</csymbol><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2\">superscript</csymbol><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.2\">𝑒</ci><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.2.3\">′</ci></apply><cn id=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.3.cmml\" type=\"integer\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.2.2.2.3\">2</cn></apply></interval><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.6.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.6\">…</ci><interval closure=\"open\" id=\"S3.SS4.p1.8.m8.2.2.2.2.4.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2\"><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1\">subscript</csymbol><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1\">superscript</csymbol><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.2\">𝑠</ci><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.2.3\">′</ci></apply><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.3.1.1.3\">𝑛</ci></apply><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2\">subscript</csymbol><apply id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.1.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2\">superscript</csymbol><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.2.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.2\">𝑒</ci><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.2.3\">′</ci></apply><ci id=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.3.cmml\" xref=\"S3.SS4.p1.8.m8.2.2.2.2.4.2.2.3\">𝑛</ci></apply></interval></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.8.m8.2c\">[(s^{\\prime}_{1},e^{\\prime}_{1}),(s^{\\prime}_{2},e^{\\prime}_{2})\\dots(s^{%\n\\prime}_{n},e^{\\prime}_{n})]</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.8.m8.2d\">[ ( italic_s start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_s start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) … ( italic_s start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ]</annotation></semantics></math>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS4.p2\">\n<p class=\"ltx_p\" id=\"S3.SS4.p2.1\">When the prediction for a certain query contains multiple candidate intervals, we feed them along with the target query into a classifier that is trained on NLQ data to select the optimal candidate <math alttext=\"v^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.1.m1.1\"><semantics id=\"S3.SS4.p2.1.m1.1a\"><msup id=\"S3.SS4.p2.1.m1.1.1\" xref=\"S3.SS4.p2.1.m1.1.1.cmml\"><mi id=\"S3.SS4.p2.1.m1.1.1.2\" xref=\"S3.SS4.p2.1.m1.1.1.2.cmml\">v</mi><mo id=\"S3.SS4.p2.1.m1.1.1.3\" xref=\"S3.SS4.p2.1.m1.1.1.3.cmml\">∗</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p2.1.m1.1b\"><apply id=\"S3.SS4.p2.1.m1.1.1.cmml\" xref=\"S3.SS4.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p2.1.m1.1.1.1.cmml\" xref=\"S3.SS4.p2.1.m1.1.1\">superscript</csymbol><ci id=\"S3.SS4.p2.1.m1.1.1.2.cmml\" xref=\"S3.SS4.p2.1.m1.1.1.2\">𝑣</ci><times id=\"S3.SS4.p2.1.m1.1.1.3.cmml\" xref=\"S3.SS4.p2.1.m1.1.1.3\"></times></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p2.1.m1.1c\">v^{*}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p2.1.m1.1d\">italic_v start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>. For queries without predictions (<em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS4.p2.1.1\">i.e</em>.<span class=\"ltx_text\" id=\"S3.SS4.p2.1.2\"></span> “NA”), we simply use the original full video. Localization within a coarse temporal window makes the NLQ task easier compared to doing it on the original full-length video.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\" id=\"S4.p1.1\">In this section, we evaluate our LifelongMemory framework in real-world egocentric video query tasks.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Experiment Setup</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">EgoSchema.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px1.p1.1\">The EgoSchema dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">27</a>]</cite> consists of over 5,000 question-answer pairs for 250 hours of Ego4D videos covering a wide range of human daily activities. For each question, the correct answer needs to be selected from 5 choices based on a three-minute egocentric video. The dataset is curated by human annotators to ensure all questions require long-term temporal understanding. We use the subset provided by EgoSchema, which contains 500 question-answer pairs, for ablation studies on prompt designs, captioning choices, and voting by confidence, then use the best setup for evaluation on the full benchmark.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Ego4D NLQ.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px2.p1.1\">The Ego4D dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">13</a>]</cite> is an egocentric video dataset including a wide variety of daily life activities recorded by individuals wearing cameras. The NLQ task, as one of the episodic memory tasks of Ego4D, requires localizing a temporal window of the video to answer a natural language query. The NLQ annotations are from 227 hours of videos, with a total of 19,200 queries spanning 13 query templates. The train/val/test split (60%, 20%, 20%) is composed of disjoint sets of video clips. The average video length is approximately 8.7 minutes, while the average duration of a response window is only 9.3 seconds, representing on average only 2% of the full video.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation Metrics.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px3.p1.1\">For the EgoSchema dataset, we use accuracy to evaluate our framework since it is a multi-choice QA task. For the NLQ dataset, we adopt different metrics for different stages as below. In the LLM Reasoning stage where we only have coarse-grained predictions, we evaluate on the validation set with metrics including <span class=\"ltx_text ltx_font_bold\" id=\"S4.SS1.SSS0.Px3.p1.1.1\">(i)</span> the ratio of predictions that overlap with the ground truth (denoted as <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS0.Px3.p1.1.2\">Overlap</em>), <span class=\"ltx_text ltx_font_bold\" id=\"S4.SS1.SSS0.Px3.p1.1.3\">(ii)</span> and the proportion of predictions where at least one candidate achieves an Intersection over Union (IoU) greater than 0.3 with the ground truth (denoted as <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS0.Px3.p1.1.4\">IoU*@0.3</em>). During the refinement stage for NLQ, we obtain fine-grained predictions so we can evaluate the test dataset using the standard NLQ metrics – <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS0.Px3.p1.1.5\">R@1 IoU@0.3</em> and <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS0.Px3.p1.1.6\">R@1 IoU@0.5</em>, which is the recall of top one prediction having IoU with the ground truth larger than the threshold {0.3, 0.5}.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Sources.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px4.p1.1\">We experiment with machine-generated captions and human-annotated captions and test the effect of text-conditioned captioning.</p>\n<ul class=\"ltx_itemize\" id=\"S4.I1\">\n<li class=\"ltx_item\" id=\"S4.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i1.p1.1.1\">LLaVA</span>: LLaVa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">24</a>]</cite> is a multimodal LLM pre-trained on a diverse set of 1.2M publicly available data, including various multimodal question-answering and reasoning tasks. To encourage LLaVA to generate captions that are relevant to the query while not introducing false positives, we follow the template proposed by LLaVA-1.5<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">23</a>]</cite> and adopt the prompt <em class=\"ltx_emph ltx_font_italic\" id=\"S4.I1.i1.p1.1.2\">“If there are factual errors in the questions, provide a precise description of the image; if not, proceed answering the question. [queries].”</em></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i2.p1.1.1\">LaViLa</span>: LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">55</a>]</cite> is a multimodal LLM pre-trained on the video-narration pairs from Ego4D and is thus capable of generating captions that mimic the ground-truth descriptions of the video. Each caption is generated using 4 frames uniformly taken from a two-second video clip.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S4.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i3.p1.1.1\">Ego4D Narrations</span>: Ego4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">13</a>]</cite> narrations include written sentence narrations in English from human annotators, describing a diverse set of activities in the dataset. The annotated narrations contain on average 13.2 sentences per minute of video, which is not as dense as the LLaVA and LaViLa captions that we sample every 2 seconds.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Digest Details.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px5.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px5.p1.1\">The generated captions are distilled through filtering and merging in this step. We first remove ambiguous captions containing keywords that are associated with blurry and noisy frames. Then, we filter out irrelevant captions based on the similarity scores between the embedding of queries and captions encoded by LaViLa. Lastly, we identify groups of similar consecutive captions by calculating the similarity scores of the embeddings of neighboring captions and merge captions in the same group by querying GPT-3.5 with prompt <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS0.Px5.p1.1.1\">“In this task, you will merge a list of captions into a single, concise caption. Focus on clarity and brevity while ensuring no critical details are lost in the merging process.”</em></p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px6\">\n<h4 class=\"ltx_title ltx_title_paragraph\">NLQ Refinement Details.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px6.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS0.Px6.p1.1\">For the refinement stage, we train a classifier on the NLQ train set to select the optimal candidate from multiple LLM predictions. To construct a video dataset similar to the real LLM predictions, we randomly shift and scale the ground-truth temporal windows.\nWe then mark those intervals that have IoU with the ground truth larger than 0.5 as positives and randomly pick the same amount of negative samples from intervals with IoU less than 0.1. We utilize video features encoded by InternVideo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>]</cite> and EgoVLP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">22</a>]</cite> and adapt VSLNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">49</a>]</cite>, a span-based localization network, to this video classification task, where we replace the localization head with a classification head.\nAfter obtaining the optimal candidate temporal windows, we extend them by a window size of <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px6.p1.1.m1.1\"><semantics id=\"S4.SS1.SSS0.Px6.p1.1.m1.1a\"><mi id=\"S4.SS1.SSS0.Px6.p1.1.m1.1.1\" xref=\"S4.SS1.SSS0.Px6.p1.1.m1.1.1.cmml\">α</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px6.p1.1.m1.1b\"><ci id=\"S4.SS1.SSS0.Px6.p1.1.m1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px6.p1.1.m1.1.1\">𝛼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px6.p1.1.m1.1c\">\\alpha</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.SSS0.Px6.p1.1.m1.1d\">italic_α</annotation></semantics></math> to provide more context to the NLQ model and then feed them into the state-of-the-art NLQ model, NaQ++ReLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">25</a>]</cite> and GroundNLQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">15</a>]</cite>, which have been finetuned on the Ego4D NLQ dataset. This gives us fine-grained predicted temporal windows that can reflect the answers to the target queries.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"609\" id=\"S4.F4.g1\" src=\"./assets/x4.png\" width=\"538\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>EgoSchema QA and Ego4D NLQ examples using LaViLa and GPT-4. The groud-truth answers are in <span class=\"ltx_text\" id=\"S4.F4.3.1\" style=\"color:#FF0000;\">red</span> and the LLM predictions are in <span class=\"ltx_text\" id=\"S4.F4.4.2\" style=\"color:#4F71BE;\">blue</span>. The sampled frames are manually picked from the raw video input to show key events related to the query.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Qualitative Results</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.p1.1\">We visualize <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS2.p1.1.1\">LifelongMemory</em> results in Figure <a class=\"ltx_ref\" href=\"#S4.F4\" title=\"Figure 4 ‣ NLQ Refinement Details. ‣ 4.1 Experiment Setup ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. For the EgoSchema QA examples, we show that the captions capture objects and actions in the scene very well, and the LLM is then able to answer the question correctly, using its reasoning capability. For the NLQ examples, we show that many LLM predictions have high-quality overlaps with the ground-truth windows (without interval refinement). We note that many successful retrievals rely on high-quality captions and we expect there can be a large room for future improvement with a stronger captioning model. In both datasets, the LLM is able to explain its predictions with a confidence level, enhancing the interpretability of the results. We provide more qualitative examples in <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Qualitative Results ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Quantitative Results</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">EgoSchema Benchmark Results.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px1.p1.1\">Our method achieves state-of-the-art performance on EgoSchema, as shown in <a class=\"ltx_ref\" href=\"#S4.T1\" title=\"In EgoSchema Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>. Due to the challenge of long-form videos, prior state-of-the-art video QA models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">42</a>]</cite> struggle at this task with an accuracy not much better than random (20%). When compared with concurrent works – LLoVi <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a>]</cite> and Vamos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">35</a>]</cite> – that also leverage GPT-4, our approach outperforms them with a significant margin of over 10%. These empirical results confirm LifelongMemory is a simple yet effective framework that can reason and answer questions of very long egocentric videos.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Zero-shot QA on EgoSchema with Different Models. * represents ensembled results using vote by confidence. Subset is 500 question-answer pairs provided by EgoSchema for validation. </figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T1.3\" style=\"width:303.5pt;height:177.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-48.1pt,28.2pt) scale(0.759257492204681,0.759257492204681) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T1.3.1.1.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.1.1\" style=\"font-size:80%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.3.1.1.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.2.1\" style=\"font-size:80%;\">  LLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.3.1.1.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.3.1\" style=\"font-size:80%;\">  Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.3.1.1.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.4.1\" style=\"font-size:80%;\">  Subset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.3.1.1.1.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.1.1.5.1\" style=\"font-size:80%;\">  Full</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.3.1.2.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.1.1\" style=\"font-size:80%;\">FrozenBiLM  </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">42</a><span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.1.3.2\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.2.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.2.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.2.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.3.1\" style=\"font-size:80%;\">90 frames</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.2.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.4.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.1.2.1.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.2.1.5.1\" style=\"font-size:80%;\">26.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.3.2.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.1.1\" style=\"font-size:80%;\">InternVideo </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.1.3.2\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.3.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.2.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.3.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.3.1\" style=\"font-size:80%;\">90 frames</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.3.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.4.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.3.2.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.3.2.5.1\" style=\"font-size:80%;\">32.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.4.3.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.1.1\" style=\"font-size:80%;\">Vamos </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">35</a><span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.1.3.2\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.3.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.2.1\" style=\"font-size:80%;\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.3.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.3.1\" style=\"font-size:80%;\">mixed</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.3.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.4.1\" style=\"font-size:80%;\">51.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.4.3.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.4.3.5.1\" style=\"font-size:80%;\">48.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.5.4.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.1.1\" style=\"font-size:80%;\">LLoVi </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a><span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.1.3.2\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.4.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.2.1\" style=\"font-size:80%;\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.4.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.3.1\" style=\"font-size:80%;\">180 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.4.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.4.1\" style=\"font-size:80%;\">57.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.5.4.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.5.4.5.1\" style=\"font-size:80%;\">50.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.6.5.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.1.1\" style=\"font-size:80%;\">LLoVi </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.1.2.1\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a><span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.1.3.2\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.5.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.2.1\" style=\"font-size:80%;\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.5.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.3.1\" style=\"font-size:80%;\">180 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.5.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.4.1\" style=\"font-size:80%;\">58.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.6.5.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.6.5.5.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.7.6.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.7.6.1.1\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.7.6.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.7.6.2.1\" style=\"font-size:80%;\">Llama3-8B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.7.6.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.7.6.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.7.6.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.7.6.4.1\" style=\"font-size:80%;\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.7.6.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.7.6.5.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.8.7.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.8.7.1.1\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.8.7.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.8.7.2.1\" style=\"font-size:80%;\">GPT-3.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.8.7.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.8.7.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.8.7.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.8.7.4.1\" style=\"font-size:80%;\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.8.7.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.8.7.5.1\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.9.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.9.8.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.9.8.1.1\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.9.8.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.9.8.2.1\" style=\"font-size:80%;\">Claude-3-Haiku</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.9.8.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.9.8.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.9.8.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.9.8.4.1\" style=\"font-size:80%;\">64.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.9.8.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.9.8.5.1\" style=\"font-size:80%;\">55.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.10.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.10.9.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.10.9.1.1\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.10.9.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.10.9.2.1\" style=\"font-size:80%;\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.10.9.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.10.9.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.10.9.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.10.9.4.1\" style=\"font-size:80%;\">68.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.10.9.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.10.9.5.1\" style=\"font-size:80%;\">62.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.11.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.11.10.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.11.10.1.1\" style=\"font-size:80%;\">Ours*</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.11.10.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.11.10.2.1\" style=\"font-size:80%;\">GPT-4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.11.10.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.11.10.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.11.10.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.11.10.4.1\" style=\"font-size:80%;\">69.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.11.10.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.11.10.5.1\" style=\"font-size:80%;\">62.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.12.11\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.3.1.12.11.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.12.11.1.1\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.12.11.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.12.11.2.1\" style=\"font-size:80%;\">GPT-4o</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.12.11.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.12.11.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.12.11.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.12.11.4.1\" style=\"font-size:80%;\">70.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.1.12.11.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.12.11.5.1\" style=\"font-size:80%;\">64.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.1.13.12\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T1.3.1.13.12.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.13.12.1.1\" style=\"font-size:80%;\">Ours*</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.13.12.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.13.12.2.1\" style=\"font-size:80%;\">GPT-4o</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.13.12.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T1.3.1.13.12.3.1\" style=\"font-size:80%;\">90 captions</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.13.12.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.13.12.4.1\" style=\"font-size:80%;\">72.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.3.1.13.12.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.1.13.12.5.1\" style=\"font-size:80%;\">64.7</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Ego4D NLQ Benchmark Results.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px2.p1.1\">We compare the performance of our method and two other competitive methods on the Ego4D NLQ benchmark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Our results of GroundNLQ are slightly lower than their reported numbers. Since GroundNLQ has not released all checkpoints, we are unable to reproduce the results.</span></span></span> in <a class=\"ltx_ref\" href=\"#S4.T2\" title=\"In Ego4D NLQ Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our method with Ego4D ground-truth narrations achieves the best performance in the validation set and our method with GroundNLQ as the refinement model achieves the best performance in the test set. LifelongMemory is a flexible framework that can be plugged into any pretrained captioning model and video localization model, suggesting the potential of our method for future improvement using better pretrained MLLMs.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Ego4D NLQ benchmark results, using GPT-4. Our approach filters out noisy content in the video for the pretrained NLQ models, increasing the precision of the predictions of the NLQ models. Reported metrics all use predictions that rank the first.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:260.2pt;height:104pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-50.1pt,20.0pt) scale(0.72201010148188,0.72201010148188) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.1.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1.1\">Method</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T2.1.1.1.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.2.1\">Set</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T2.1.1.1.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.3.1\">Mean</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T2.1.1.1.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.4.1\">IoU=0.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T2.1.1.1.1.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.5.1\">IoU=0.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.2.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">NaQ++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">val</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20.20</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">25.00</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.2.2.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">15.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.3.3.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Ours (LaViLa, NaQ++)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.3.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">val</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.3.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">19.00</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.3.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">23.40</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.3.3.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">14.61</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.4.4.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Ours (Ego4D, NaQ++)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.4.4.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">val</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.4.4.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.3.1\">21.09</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.4.4.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.4.1\">26.12</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.4.4.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.4.5.1\">16.06</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.5.5.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">NaQ++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.5.5.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.5.5.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">17.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.5.5.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">21.70</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T2.1.1.5.5.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">13.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.6.6.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Ours (LaViLa, NaQ++)</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.6.6.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.6.6.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">18.06</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.6.6.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">22.28</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.6.6.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">13.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.7.7.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">GroundNLQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">15</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.7.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.7.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20.08</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.7.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">23.43</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T2.1.1.7.7.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">16.71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T2.1.1.8.8.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Ours (LaViLa, GroundNLQ)</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T2.1.1.8.8.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">test</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T2.1.1.8.8.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.8.3.1\">20.27</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T2.1.1.8.8.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.8.4.1\">23.68</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T2.1.1.8.8.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.8.5.1\">16.86</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>NLQ performance using different caption and LLM components. The <span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.10.1\">bold</span> number denotes the highest and the <span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T3.11.2\">underlined</span> the second highest. <math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.2.m1.1\"><semantics id=\"S4.T3.2.m1.1b\"><mo id=\"S4.T3.2.m1.1.1\" xref=\"S4.T3.2.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.m1.1c\"><ci id=\"S4.T3.2.m1.1.1.cmml\" xref=\"S4.T3.2.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.m1.1d\">\\dagger</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.2.m1.1e\">†</annotation></semantics></math> represents predictions with a confidence level of 3. <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.12.3\">NA</em> represents the ratio of null predictions, and all other metrics do not include null predictions. <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.13.4\">Count</em> represents the number of captions, and <em class=\"ltx_emph ltx_font_italic\" id=\"S4.T3.14.5\">Length</em> represents the average word count of captions.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T3.4\" style=\"width:390.3pt;height:102.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-11.3pt,3.0pt) scale(0.945277464635219,0.945277464635219) ;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.4.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T3.4.2.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.2.2.3.1\">Captions</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.4.2.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.2.2.4.1\">Count</em></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.4.2.2.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.2.2.5.1\">Length</em></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.4.2.2.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.2.2.6.1\">LLM</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.4.2.2.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.2.2.7.1\">Overlap</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.3.1.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.3.1.1.1.1\">Overlap</em> <math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.3.1.1.1.m1.1\"><semantics id=\"S4.T3.3.1.1.1.m1.1a\"><mo id=\"S4.T3.3.1.1.1.m1.1.1\" xref=\"S4.T3.3.1.1.1.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.1.1.1.m1.1b\"><ci id=\"S4.T3.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.3.1.1.1.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.1.1.1.m1.1c\">\\dagger</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.3.1.1.1.m1.1d\">†</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.4.2.2.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.2.2.8.1\">IoU*@0.3</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.4.2.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.2.2.2.1\">IoU*@0.3</em> <math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.4.2.2.2.m1.1\"><semantics id=\"S4.T3.4.2.2.2.m1.1a\"><mo id=\"S4.T3.4.2.2.2.m1.1.1\" xref=\"S4.T3.4.2.2.2.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.4.2.2.2.m1.1b\"><ci id=\"S4.T3.4.2.2.2.m1.1.1.cmml\" xref=\"S4.T3.4.2.2.2.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.4.2.2.2.m1.1c\">\\dagger</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.4.2.2.2.m1.1d\">†</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S4.T3.4.2.2.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T3.4.2.2.9.1\">NA</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.3.1.1\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.3.1.1.1\">Ego4D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.3.1.2\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.3.1.2.1\">109</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.3.1.3\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.3.1.3.1\">7.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.3.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">GPT-4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.3.1.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.2.3.1.5.1\">51.73</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.3.1.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.2.3.1.6.1\">53.98</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.3.1.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.2.3.1.7.1\">15.99</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.3.1.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.2.3.1.8.1\">27.38</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.3.1.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">40.29</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.4.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.2.4.2.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.4.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">31.27</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.4.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">33.15</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.4.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">0.91</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.4.2.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">4.34</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.4.2.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">94.13</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.5.3.1\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.5.3.1.1\">LaViLa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.5.3.2\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.5.3.2.1\">186</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.5.3.3\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.5.3.3.1\">6.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.5.3.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">GPT-4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T3.4.2.5.3.5.1\">36.61</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T3.4.2.5.3.6.1\">38.35</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T3.4.2.5.3.7.1\">9.74</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T3.4.2.5.3.8.1\">19.22</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T3.4.2.5.3.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">47.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.6.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.2.6.4.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">GPT-3.5</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20.47</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">22.33</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.29</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">4.78</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T3.4.2.6.4.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">89.75</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" id=\"S4.T3.4.2.7.5.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.7.5.1.1\">LLaVA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.7.5.2.1\">250</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" id=\"S4.T3.4.2.7.5.3.1\">52.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">GPT-4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">6.42</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">8.79</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.50</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">2.71</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T3.4.2.7.5.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">60.92</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<figure class=\"ltx_table\" id=\"S4.T4\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Ego4D NLQ and EgoSchema QA performance using LaViLa + GPT-4, with different frame sampling intervals and digest strategy. </figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T4.2\" style=\"width:390.3pt;height:84.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-13.7pt,3.0pt) scale(0.934432072844841,0.934432072844841) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2.3.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T4.2.2.3.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T4.2.2.3.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\" id=\"S4.T4.2.2.3.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.3.1.3.1\">Ego4D NLQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt\" colspan=\"2\" id=\"S4.T4.2.2.3.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.3.1.4.1\">EgoSchema</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.2.2.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.3.1\">Freq.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S4.T4.2.2.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.4.1\">Digest</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.2.2.2.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.5.1\"># Captions</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.2.2.2.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T4.2.2.2.6.1\">Overlap</em></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.1.1.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T4.1.1.1.1.1\">Overlap</em><math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.1.1.1.1.m1.1\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mo id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">\\dagger</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.1.1.1.1.m1.1d\">†</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.2.2.2.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T4.2.2.2.7.1\">IoU*@0.3</em></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S4.T4.2.2.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T4.2.2.2.2.1\">IoU*@0.3</em><math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.2.2.2.2.m1.1\"><semantics id=\"S4.T4.2.2.2.2.m1.1a\"><mo id=\"S4.T4.2.2.2.2.m1.1.1\" xref=\"S4.T4.2.2.2.2.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.2.m1.1b\"><ci id=\"S4.T4.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.2.m1.1c\">\\dagger</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.2.2.2.2.m1.1d\">†</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.2.2.2.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.8.1\"># Captions</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T4.2.2.2.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T4.2.2.2.9.1\">Acc</em></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2.4.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">4s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.2.2.4.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">33.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">36.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">6.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.2.2.4.1.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">14.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.2.2.4.1.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">26.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2.5.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">2s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.2.2.5.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Yes</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">186</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.5.2.4.1\">36.61</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.5.2.5.1\">38.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.5.2.6.1\">9.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.2.2.5.2.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.5.2.7.1\">19.22</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.5.2.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.5.2.9.1\">68.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2.6.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">2s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.2.2.6.3.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">250</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">23.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">23.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.6\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">4.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T4.2.2.6.3.7\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">11.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.8\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.2.2.6.3.9\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.6.3.9.1\">68.0</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Captioning Model Choices.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px3.p1.1\">We compare the effect of different caption sources in <a class=\"ltx_ref\" href=\"#S4.T3\" title=\"In Ego4D NLQ Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>. Although LLaVA generates longer captions conditioned on the queries, the performance of LaViLa is significantly better than LLaVA. This indicates the necessity of adopting an egocentric captioning model that focuses on the core activity of the individual. Despite the effectiveness of LaViLa in this task, we identify that LaViLa tends to generate false positive captions as it is finetuned on Ego4D data. We thus evaluate the ground-truth captions provided by the Ego4D Narrations data and observe that it achieves the best performance with significantly fewer captions. This confirms our assumption that an accurate well-crafted set of captions can effectively summarize the information of the camera wearer’s activity in egocentric videos.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Choices.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px4.p1.1\">We compare the effect of different LLMs for EgoSchema and NLQ respectively in <a class=\"ltx_ref\" href=\"#S4.T1\" title=\"In EgoSchema Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"#S4.T3\" title=\"In Ego4D NLQ Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>. We observe that GPT-4 and GPT-4o significantly outperform GPT-3.5 and open-source models like Llama <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2</a>]</cite> and Vicuna <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">7</a>]</cite> for both datasets. Note that the performance drop caused by weaker LLMs is much larger for the NLQ task because this task requires more precise instruction following capabilities: weaker models often misunderstand the prompt and output an answer instead of a list of temporal intervals, leading to a high NA ratio. As our framework is agnostic to LLMs—it’s very easy to plug in a future version of LLMs to further boost the performance.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Digest.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px5.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px5.p1.1\">We evaluate the effect of caption digest in <a class=\"ltx_ref\" href=\"#S4.T4\" title=\"In Ego4D NLQ Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>. With <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS3.SSS0.Px5.p1.1.1\">Caption Digest</em>, we discard ambiguous and irrelevant captions and use LLM to merge similar ones as described in <a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Experiment Setup ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For NLQ, this technique significantly improves both metrics by around 10%, suggesting that a concise context leads to a much better retrieval performance. However, similar effects are not observed in EgoSchema as the original undigested context length is already relatively small (<em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS3.SSS0.Px5.p1.1.2\">i.e</em>.<span class=\"ltx_text\" id=\"S4.SS3.SSS0.Px5.p1.1.3\"></span> less than 100 captions). Since reduced context lengths save the computation costs, we adopt caption digest for both datasets.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T5\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Effect of explanation and confidence levels.</figcaption>\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.3\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.3.4.1\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T5.3.4.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T5.3.4.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.4.1.2.1\">Ego4D NLQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_tt\" id=\"S4.T5.3.4.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.4.1.3.1\">EgoSchema</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.5.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.3.5.2.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.5.2.1.1\">Conf. Level</span></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.3.5.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T5.3.5.2.2.1\">Overlap</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T5.3.5.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T5.3.5.2.3.1\">IoU*@0.3</em></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.3.5.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.5.2.4.1\">Acc.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.1.1.1.m1.1\"><semantics id=\"S4.T5.1.1.1.m1.1a\"><mo id=\"S4.T5.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.m1.1b\"><geq id=\"S4.T5.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.m1.1c\">\\geq</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T5.1.1.1.m1.1d\">≥</annotation></semantics></math> 1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.1.1.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">36.46</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">9.63</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.1.1.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">68.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.2.2.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.2.2.1.m1.1\"><semantics id=\"S4.T5.2.2.1.m1.1a\"><mo id=\"S4.T5.2.2.1.m1.1.1\" xref=\"S4.T5.2.2.1.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.2.2.1.m1.1b\"><geq id=\"S4.T5.2.2.1.m1.1.1.cmml\" xref=\"S4.T5.2.2.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.2.2.1.m1.1c\">\\geq</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T5.2.2.1.m1.1d\">≥</annotation></semantics></math> 2</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.2.2.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">36.49</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T5.2.2.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">17.52</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.2.2.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">69.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.3.3.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\n<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.3.3.1.m1.1\"><semantics id=\"S4.T5.3.3.1.m1.1a\"><mo id=\"S4.T5.3.3.1.m1.1.1\" xref=\"S4.T5.3.3.1.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.3.3.1.m1.1b\"><geq id=\"S4.T5.3.3.1.m1.1.1.cmml\" xref=\"S4.T5.3.3.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.3.3.1.m1.1c\">\\geq</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T5.3.3.1.m1.1d\">≥</annotation></semantics></math> 3</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.3.3.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.2.1\">38.20</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" id=\"S4.T5.3.3.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.3.1\">19.06</span></td>\n<td class=\"ltx_td ltx_align_right\" id=\"S4.T5.3.3.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.3.4.1\">74.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.6.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T5.3.6.3.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.6.3.1.1\">Explanation</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.3.6.3.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T5.3.6.3.2.1\">Overlap</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T5.3.6.3.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S4.T5.3.6.3.3.1\">IoU*@0.3</em></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.3.6.3.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.6.3.4.1\">Acc.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.7.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T5.3.7.4.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">No</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.3.7.4.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">32.73</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" id=\"S4.T5.3.7.4.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">8.65</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.3.7.4.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">64.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.3.8.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S4.T5.3.8.5.1\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Yes</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T5.3.8.5.2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.8.5.2.1\">36.61</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" id=\"S4.T5.3.8.5.3\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.8.5.3.1\">9.74</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" id=\"S4.T5.3.8.5.4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.3.8.5.4.1\">68.0</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px6\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Sampling Interval.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px6.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px6.p1.1\">Given the same captioning models and preprocessing process, smaller caption intervals lead to higher performance as they provide richer contexts for the LLM. Since each Ego4D video contains a large amount of activities, coarse-grained captioning is very likely to miss key moments and results in a loss of information. Decreasing the sampling frequency of captioning leads to a large drop in the accuracy of predictions of both NLQ and EgoSchema, as shown in <a class=\"ltx_ref\" href=\"#S4.T4\" title=\"In Ego4D NLQ Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>. It is worth noting that using very limited captions leads to a very low EgoSchema accuracy that is not much better than random guess (20%) due to the significant information loss.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px7\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Effect of Explanation.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px7.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px7.p1.1\">We also experiment with different prompts in <a class=\"ltx_ref\" href=\"#S4.T5\" title=\"In Caption Digest. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>. To encourage LLM reasoning step by step, we provide detailed instructions on how to retrieve the temporal windows and answer the queries while explicitly asking it to explain its prediction. The request for explanation encourages the LLM to reason step by step and improves the performance in both datasets. Moreover, providing textual explanations also increases the interpretability and reliability of the model outputs.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS3.SSS0.Px8\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Effect of Confidence Levels.</h4>\n<div class=\"ltx_para\" id=\"S4.SS3.SSS0.Px8.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.SSS0.Px8.p1.1\">To encourage the LLM to make more reliable predictions, we also explicitly ask the LLM to predict a confidence level for each of its own outputs. We report the relationship between scores and their confidence levels in <a class=\"ltx_ref\" href=\"#S4.T5\" title=\"In Caption Digest. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>. The increase in confidence scores leads to an increase in accuracy in both datasets, suggesting the verbalized confidence scores are calibrated. For EgoSchema, we also use confidence level to vote during model ensembling, leading to a 0.1-0.3% increase in test accuracy as shown in <a class=\"ltx_ref\" href=\"#S4.T1\" title=\"In EgoSchema Benchmark Results. ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Error Analysis</h3>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS4.p1.1\">The majority of errors stem from the captioning step, where inevitable information loss occurs during the transformation from long video inputs into text, as shown in <a class=\"ltx_ref\" href=\"#S4.F5\" title=\"Figure 5 ‣ 4.4 Error Analysis ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. For NLQ with insufficient information, we encourage the LLM to make null predictions and rely on the refinement stage to make the final prediction based on the full input video. On the contrary, we encourage the LLM to select the most plausible answer for EgoSchema when uncertain because we don’t rely on a pretrained QA model in the refinement stage. Our prompts are included in <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Prompting ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p2\">\n<p class=\"ltx_p\" id=\"S4.SS4.p2.1\">We also observe that sometimes the LLM proposes multiple temporal windows for NLQ that seem to be reasonable, but only one ground-truth answer is available, as shown in <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Ambiguity in NLQ Annotations ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">C</span></a>. This suggests some NLQ queries are ambiguous and require more careful annotations.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"310\" id=\"S4.F5.g1\" src=\"./assets/x5.png\" width=\"568\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>Error caused by insufficient captioning. The upper figure is an NLQ example and the lower figure is an EgoSchema example. LLM predictions are in <span class=\"ltx_text\" id=\"S4.F5.3.1\" style=\"color:#4F71BE;\">blue</span> boxes and the ground truth is in <span class=\"ltx_text\" id=\"S4.F5.4.2\" style=\"color:#FF0000;\">red</span>.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\" id=\"S5.p1.1\">In this paper, we propose LifelongMemory, a novel framework that leverages pre-trained MLLMs for answering natural language queries in long-form egocentric videos. To address the challenges of long-range temporal dynamics, we condense the input videos into a concise textual log and utilize an LLM to comprehend the context and answer the given queries. Our method achieves state-of-the-art performance on EgoSchema and remains highly competitive on Ego4D NLQ, with enhanced interpretability provided by verbalized confidence and explanation. LifelongMemory showcases the potential of leveraging LLMs in video understanding and opens up opportunities for personalized AIs that can answer daily queries for individuals requiring assistance.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgment</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\" id=\"Sx1.p1.1\">We would like to thank the Microsoft Accelerating Foundation Models Research program for providing cloud compute credits for running some parts of our LLM experiments. This work was also supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Achiam et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nAchiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., et al.\n\n</span>\n<span class=\"ltx_bibblock\">GPT-4 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib1.1.1\">arXiv preprint arXiv:2303.08774</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">AI@Meta [2024]</span>\n<span class=\"ltx_bibblock\">\nAI@Meta.\n\n</span>\n<span class=\"ltx_bibblock\">Llama 3 model card.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib2.1.1\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md\" title=\"\">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a></em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brown et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Language models are few-shot learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib3.1.1\">NeurIPS</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bubeck et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Sparks of artificial general intelligence: Early experiments with gpt-4.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib4.1.1\">arXiv preprint arXiv:2303.12712</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nChen, G., Xing, S., Chen, Z., Wang, Y., Li, K., Li, Y., Liu, Y., Wang, J., Zheng, Y.-D., Huang, B., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Internvideo-ego4d: A pack of champion solutions to ego4d challenges.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib5.1.1\">arXiv preprint arXiv:2211.09529</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nChen, L., Li, B., Shen, S., Yang, J., Li, C., Keutzer, K., Darrell, T., and Liu, Z.\n\n</span>\n<span class=\"ltx_bibblock\">Large language models are visual reasoning coordinators.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib6.1.1\">arXiv preprint arXiv:2310.15166</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chiang et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P.\n\n</span>\n<span class=\"ltx_bibblock\">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib7.1.1\">https://lmsys.org/blog/2023-03-30-vicuna/</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chowdhery et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al.\n\n</span>\n<span class=\"ltx_bibblock\">PaLM: Scaling language modeling with pathways.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib8.1.1\">arXiv preprint arXiv:2204.02311</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Driess et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.\n\n</span>\n<span class=\"ltx_bibblock\">PaLM-E: An embodied multimodal language model.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib9.1.1\">arXiv preprint arXiv:2303.03378</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Du et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nDu, Y., Yang, M., Florence, P., Xia, F., Wahid, A., Ichter, B., Sermanet, P., Yu, T., Abbeel, P., Tenenbaum, J. B., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Video language planning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib10.1.1\">arXiv preprint arXiv:2310.10625</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Feichtenhofer et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nFeichtenhofer, C., Fan, H., Malik, J., and He, K.\n\n</span>\n<span class=\"ltx_bibblock\">Slowfast networks for video recognition.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib11.1.1\">ICCV</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fu et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nFu, T.-J., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., and Liu, Z.\n\n</span>\n<span class=\"ltx_bibblock\">An empirical study of end-to-end video-language transformers with masked visual modeling.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib12.1.1\">CVPR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grauman et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nGrauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Ego4d: Around the world in 3,000 hours of egocentric video.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib13.1.1\">CVPR</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hendricks et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nHendricks, L. A., Wang, O., Shechtman, E., Sivic, J., Darrell, T., and Russell, B.\n\n</span>\n<span class=\"ltx_bibblock\">Localizing moments in video with natural language.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib14.1.1\">ICCV</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hou et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nHou, Z., Ji, L., Gao, D., Zhong, W., Yan, K., Li, C., Chan, W. K., Ngo, C.-W., Duan, N., and Shou, M. Z.\n\n</span>\n<span class=\"ltx_bibblock\">Groundnlq @ ego4d natural language queries challenge 2023.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib15.1.1\">arXiv preprint arXiv:2306.15255</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Huang &amp; Chang [2023]</span>\n<span class=\"ltx_bibblock\">\nHuang, J. and Chang, K. C.-C.\n\n</span>\n<span class=\"ltx_bibblock\">Towards reasoning in large language models: A survey.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib16.1.1\">Findings of ACL</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kay et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., and Zisserman, A.\n\n</span>\n<span class=\"ltx_bibblock\">Kinetics400 dataset: The kinetics human action video dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib17.1.1\">arXiv preprint arXiv:1705.06950</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2023a]</span>\n<span class=\"ltx_bibblock\">\nLi, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., and Shan, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Seed-bench-2: Benchmarking multimodal large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib18.1.1\">arXiv preprint arXiv:2311.17092</em>, 2023a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2023b]</span>\n<span class=\"ltx_bibblock\">\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Seed-bench: Benchmarking multimodal llms with generative comprehension.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib19.1.1\">arXiv preprint arXiv:2307.16125</em>, 2023b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nLi, J., Li, D., Xiong, C., and Hoi, S.\n\n</span>\n<span class=\"ltx_bibblock\">BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib20.1.1\">ICML</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lin et al. [2022a]</span>\n<span class=\"ltx_bibblock\">\nLin, K. Q., Wang, A. J., Soldan, M., Wray, M., Yan, R., Xu, E. Z., Gao, D., Tu, R., Zhao, W., Kong, W., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Egocentric video-language pretraining@ ego4d challenge 2022.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib21.1.1\">arXiv preprint arXiv:2207.01622</em>, 2022a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lin et al. [2022b]</span>\n<span class=\"ltx_bibblock\">\nLin, K. Q., Wang, J., Soldan, M., Wray, M., Yan, R., Xu, Z., Gao, D., Tu, R.-C., Zhao, W., Kong, W., Cai, C., HongFa, W., Damen, D., Ghanem, B., Liu, W., and Shou, M. Z.\n\n</span>\n<span class=\"ltx_bibblock\">Egocentric video-language pretraining.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib22.1.1\">NeurIPS</em>, 2022b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2023a]</span>\n<span class=\"ltx_bibblock\">\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\n\n</span>\n<span class=\"ltx_bibblock\">Improved baselines with visual instruction tuning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib23.1.1\">arXiv preprint arXiv:2310.03744</em>, 2023a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2023b]</span>\n<span class=\"ltx_bibblock\">\nLiu, H., Li, C., Wu, Q., and Lee, Y. J.\n\n</span>\n<span class=\"ltx_bibblock\">Visual instruction tuning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib24.1.1\">NeurIPS</em>, 2023b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nLiu, N., Wang, X., Li, X., Yang, Y., and Zhuang, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Reler@ zju-alibaba submission to the ego4d natural language queries challenge 2022.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib25.1.1\">arXiv preprint arXiv:2207.00383</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2023c]</span>\n<span class=\"ltx_bibblock\">\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D.\n\n</span>\n<span class=\"ltx_bibblock\">MMBench: Is your multi-modal model an all-around player?\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib26.1.1\">arXiv preprint arXiv:2307.06281</em>, 2023c.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mangalam et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nMangalam, K., Akshulakov, R., and Malik, J.\n\n</span>\n<span class=\"ltx_bibblock\">Egoschema: A diagnostic benchmark for very long-form video language understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib27.1.1\">arXiv preprint arXiv:2312.17235</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Pan et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nPan, J., Lin, Z., Ge, Y., Zhu, X., Zhang, R., Wang, Y., Qiao, Y., and Li, H.\n\n</span>\n<span class=\"ltx_bibblock\">Retrieving-to-answer: Zero-shot video question answering with frozen large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib28.1.1\">arXiv preprint arXiv:2306.11732</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Radford et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Learning transferable visual models from natural language supervision.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib29.1.1\">ICML</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ramakrishnan et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nRamakrishnan, S. K., Al-Halah, Z., and Grauman, K.\n\n</span>\n<span class=\"ltx_bibblock\">NaQ: Leveraging narrations as queries to supervise episodic memory.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib30.1.1\">CVPR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shao et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nShao, J., Wang, X., Quan, R., and Yang, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Action sensitivity learning for the ego4d episodic memory challenge 2023.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib31.1.1\">arXiv preprint arXiv:2306.09172</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tong et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nTong, Z., Song, Y., Wang, J., and Wang, L.\n\n</span>\n<span class=\"ltx_bibblock\">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib32.1.1\">arXiv preprint arXiv:2203.12602</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Touvron et al. [2023a]</span>\n<span class=\"ltx_bibblock\">\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Llama: Open and efficient foundation language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib33.1.1\">arXiv preprint arXiv:2302.13971</em>, 2023a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Touvron et al. [2023b]</span>\n<span class=\"ltx_bibblock\">\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Llama 2: Open foundation and fine-tuned chat models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib34.1.1\">arXiv preprint arXiv:2307.09288</em>, 2023b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nWang, S., Zhao, Q., Do, M. Q., Agarwal, N., Lee, K., and Sun, C.\n\n</span>\n<span class=\"ltx_bibblock\">Vamos: Versatile action models for video understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib35.1.1\">arXiv preprint arXiv:2311.13627</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nWang, Y., Chen, W., Han, X., Lin, X., Zhao, H., Liu, Y., Zhai, B., Yuan, J., You, Q., and Yang, H.\n\n</span>\n<span class=\"ltx_bibblock\">Exploring the reasoning abilities of multimodal large language models (mllms): A comprehensive survey on emerging trends in multimodal reasoning.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib36.1.1\">arXiv preprint arXiv:2401.06805</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nWang, Z., Li, M., Xu, R., Zhou, L., Lei, J., Lin, X., Wang, S., Yang, Z., Zhu, C., Hoiem, D., et al.\n\n</span>\n<span class=\"ltx_bibblock\">Language models with image descriptors are strong few-shot video-language learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib37.1.1\">NeurIPS</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xiong et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nXiong, M., Hu, Z., Lu, X., Li, Y., Fu, J., He, J., and Hooi, B.\n\n</span>\n<span class=\"ltx_bibblock\">Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in llms.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib38.1.1\">arXiv preprint arXiv:2306.13063</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nXu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X., and Zhuang, Y.\n\n</span>\n<span class=\"ltx_bibblock\">Video question answering via gradually refined attention over appearance and motion.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib39.1.1\">ACM MM</em>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu et al. [2016]</span>\n<span class=\"ltx_bibblock\">\nXu, J., Mei, T., Yao, T., and Rui, Y.\n\n</span>\n<span class=\"ltx_bibblock\">MSR-VTT: A large video description dataset for bridging video and language.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib40.1.1\">CVPR</em>, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nYang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.\n\n</span>\n<span class=\"ltx_bibblock\">Just ask: Learning to answer questions from millions of narrated videos.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib41.1.1\">CVPR</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nYang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.\n\n</span>\n<span class=\"ltx_bibblock\">Zero-shot video question answering via frozen bidirectional language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib42.1.1\">NeurIPS</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nYang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C., Zeng, M., and Wang, L.\n\n</span>\n<span class=\"ltx_bibblock\">Mm-react: Prompting chatgpt for multimodal reasoning and action.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib43.1.1\">arXiv preprint arXiv:2303.11381</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., Li, C., Xu, Y., Chen, H., Tian, J., Qi, Q., Zhang, J., and Huang, F.\n\n</span>\n<span class=\"ltx_bibblock\">mplug-owl: Modularization empowers large language models with multimodality.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib44.1.1\">arXiv preprint arXiv:2304.14178</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yin et al. [2024a]</span>\n<span class=\"ltx_bibblock\">\nYin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E.\n\n</span>\n<span class=\"ltx_bibblock\">A survey on multimodal large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib45.1.1\">arXiv preprint arXiv:2306.13549</em>, 2024a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yin et al. [2024b]</span>\n<span class=\"ltx_bibblock\">\nYin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E.\n\n</span>\n<span class=\"ltx_bibblock\">Mm-llms: Recent advances in multimodal large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib46.1.1\">arXiv preprint arXiv:2401.13601</em>, 2024b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zeng et al. [2023]</span>\n<span class=\"ltx_bibblock\">\nZeng, A., Attarian, M., brian ichter, Choromanski, K. M., Wong, A., Welker, S., Tombari, F., Purohit, A., Ryoo, M. S., Sindhwani, V., Lee, J., Vanhoucke, V., and Florence, P.\n\n</span>\n<span class=\"ltx_bibblock\">Socratic models: Composing zero-shot multimodal reasoning with language.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib47.1.1\">ICLR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2023a]</span>\n<span class=\"ltx_bibblock\">\nZhang, C., Lu, T., Islam, M. M., Wang, Z., Yu, S., Bansal, M., and Bertasius, G.\n\n</span>\n<span class=\"ltx_bibblock\">A simple llm framework for long-range video question-answering.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib48.1.1\">arXiv preprint arXiv:2312.17235</em>, 2023a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2020a]</span>\n<span class=\"ltx_bibblock\">\nZhang, H., Sun, A., Jing, W., and Zhou, J. T.\n\n</span>\n<span class=\"ltx_bibblock\">Span-based localizing network for natural language video localization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib49.1.1\">ACL</em>, 2020a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2020b]</span>\n<span class=\"ltx_bibblock\">\nZhang, S., Peng, H., Fu, J., and Luo, J.\n\n</span>\n<span class=\"ltx_bibblock\">Learning 2d temporal adjacent networks formoment localization with natural language.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib50.1.1\">AAAI</em>, 2020b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al.\n\n</span>\n<span class=\"ltx_bibblock\">OPT: Open pre-trained transformer language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib51.1.1\">arXiv preprint arXiv:2205.01068</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2024]</span>\n<span class=\"ltx_bibblock\">\nZhang, Y., Mao, S., Ge, T., Wang, X., de Wynter, A., Xia, Y., Wu, W., Song, T., Lan, M., and Wei, F.\n\n</span>\n<span class=\"ltx_bibblock\">Llm as a mastermind: A survey of strategic reasoning with large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib52.1.1\">arXiv preprint arXiv:2404.01230</em>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2023b]</span>\n<span class=\"ltx_bibblock\">\nZhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A.\n\n</span>\n<span class=\"ltx_bibblock\">Multimodal chain-of-thought reasoning in language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib53.1.1\">arXiv preprint arXiv:2302.00923</em>, 2023b.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao et al. [2023a]</span>\n<span class=\"ltx_bibblock\">\nZhao, X., Li, M., Weber, C., Hafez, M. B., and Wermter, S.\n\n</span>\n<span class=\"ltx_bibblock\">Chat with the environment: Interactive multimodal perception using large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib54.1.1\">IROS</em>, 2023a.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao et al. [2023b]</span>\n<span class=\"ltx_bibblock\">\nZhao, Y., Misra, I., Krähenbühl, P., and Girdhar, R.\n\n</span>\n<span class=\"ltx_bibblock\">Learning video representations from large language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib55.1.1\">CVPR</em>, 2023b.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Prompting</h2>\n<div class=\"ltx_para\" id=\"A1.p1\">\n<p class=\"ltx_p\" id=\"A1.p1.1\">We provide the complete prompt used for EgoSchema (<a class=\"ltx_ref\" href=\"#A1.F6\" title=\"In Appendix A Prompting ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">6</span></a>) and Ego4D NLQ (<a class=\"ltx_ref\" href=\"#A1.F7\" title=\"In Appendix A Prompting ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>). As the NLQ dataset contains multiple queries for one video clip, we avoid passing the same caption list multiple times by including all queries of the same clip in the prompt to reduce the cost of API calls. We provide an instructive prompt with detailed steps and ask the LLM to produce responses in the structured format to expedite post-processing. Note that we encourage the LLM to refuse to answer NLQ questions if the context is not informative so we can feed the full-length video into the refinement stage later. In contrast, we encourage the LLM to pick the most possible answer for EgoSchema because we must provide an answer to each question and there is no refinement stage for the QA task.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A1.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"553\" id=\"A1.F6.g1\" src=\"./assets/x6.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>System prompt and user prompt for video QA (EgoSchema). The text in <span class=\"ltx_text\" id=\"A1.F6.2.1\" style=\"color:#0000FF;\">blue</span> should be replaced by the captions and the corresponding question-answer pair.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A1.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"786\" id=\"A1.F7.g1\" src=\"./assets/x7.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>System prompt and user prompt for NLQ. The text in <span class=\"ltx_text\" id=\"A1.F7.2.1\" style=\"color:#0000FF;\">blue</span> should be replaced by the queries and captions in the video clip.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Qualitative Results</h2>\n<div class=\"ltx_para\" id=\"A2.p1\">\n<p class=\"ltx_p\" id=\"A2.p1.1\">We visualize the outputs of the LLM in <a class=\"ltx_ref\" href=\"#A2.F8\" title=\"In Appendix B Additional Qualitative Results ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">8</span></a> and <a class=\"ltx_ref\" href=\"#A2.F9\" title=\"In Appendix B Additional Qualitative Results ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">9</span></a>. As shown by abundant qualitative examples, the LLM can produce high-quality answers in zero shot. It is worth noting that the machine-generated captions may contain objects that are not present in the video or miss critical information that can potentially answer the target query. Based on the imperfect captions, the LLM is still able to capture the key event and produce high-quality responses, suggesting a more powerful captioning model will further boost the performance.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_missing ltx_missing_image\" id=\"A2.F8.g1\" src=\"\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>NLQ Examples. Each figure represents a two-second 30fps video clip (which is 60 frames). LLM interval predictions are denoted as <span class=\"ltx_text\" id=\"A2.F8.3.1\" style=\"color:#4F71BE;\">blue</span> boxes and the ground truth is in <span class=\"ltx_text\" id=\"A2.F8.4.2\" style=\"color:#FF0000;\">red</span>. LLM engine here is GPT-4 and the captioning model is LaViLa. To illustrate the reasoning skills of LLMs, we show the raw LLM predictions without any refinement.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A2.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"760\" id=\"A2.F9.g1\" src=\"./assets/x9.png\" width=\"598\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>EgoSchema Examples. Each figure represents a two-second 30fps video clip (which is 60 frames). LLM predictions are denoted as <span class=\"ltx_text\" id=\"A2.F9.3.1\" style=\"color:#4F71BE;\">blue</span> boxes and the ground truth is in <span class=\"ltx_text\" id=\"A2.F9.4.2\" style=\"color:#FF0000;\">red</span>. LLM engine here is GPT-4 and the captioning model is LaViLa.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Ambiguity in NLQ Annotations</h2>\n<div class=\"ltx_para\" id=\"A3.p1\">\n<p class=\"ltx_p\" id=\"A3.p1.1\">LLMs generate more than one interval when there are multiple temporal windows that can potentially answer the given query. We observe that some temporal windows proposed by the LLM seem reasonable despite only one ground-truth answer available in Ego4D NLQ annotations. These queries need to be filtered or modified to reduce ambiguity.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"659\" id=\"A3.F10.g1\" src=\"./assets/x10.png\" width=\"598\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>Examples of Ambiguous Queries. LLM predictions are in <span class=\"ltx_text\" id=\"A3.F10.3.1\" style=\"color:#4F71BE;\">blue</span> and the ground truth is in <span class=\"ltx_text\" id=\"A3.F10.4.2\" style=\"color:#FF0000;\">red</span>.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2312.05269",
  "source": "arxiv-experimental",
  "generated": "2025-10-18T01:15:44.369Z"
}
