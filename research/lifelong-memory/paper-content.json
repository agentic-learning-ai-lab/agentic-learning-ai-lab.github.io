{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p id=\"S1.p1.1\" class=\"ltx_p\">Given an egocentric video clip and a query expressed in natural language, the natural language query task involves localizing a temporal window in the given video where the answer to a query can be deduced. These queries, such as “where did I put the keys” and “how many drawers did I open,” are closely related to the camera wearer’s daily life experience and resemble the situations where people recall their past actions or the location of misplaced items. This task has wide applications in building personalized AI assistants for people’s daily lives, especially for individuals suffering memory conditions.</p>\n</div>\n<figure id=\"S1.F1\" class=\"ltx_figure\"><img src=\"./assets/x1.png\" id=\"S1.F1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"230\" height=\"102\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S1.F1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span id=\"S1.F1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">LifelongMemory employs natural language descriptions to create an episodic memory. It uses an LLM to sift through past events, retrieving specific moments in response to queries.</span></figcaption>\n</figure>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p id=\"S1.p2.1\" class=\"ltx_p\">The Natural Language Queries (NLQ) challenge in the Ego4D benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> is a recent large-scale benchmark for the egocentric video query localization task. A myriad of methods have been explored for this challenge, focusing primarily on improvements in network architecture and leveraging pre-training for better image and video features <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>, <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">29</span></a>]</cite>. All of these methods still rely on training directly on the Ego4D dataset, and many suffer from capturing long-range temporal dependencies due to the richness of information in egocentric videos.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p id=\"S1.p3.1\" class=\"ltx_p\">Recent advances in large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>, <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>]</cite> and large-scale vision-language pre-training <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>, <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>, <a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">33</span></a>]</cite> have demonstrated the remarkable capabilities of multimodal LLMs (MLLMs) in their comprehension of both visual and natural language inputs, including competitive zero-shot performance across various vision and natural language learning benchmarks. Motivated by the success of these off-the-shelf models, we propose LifelongMemory, a novel framework that combines multiple pre-trained MLLMs to solve the NLQ task.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p id=\"S1.p4.1\" class=\"ltx_p\">To address the unique challenge of long video inputs in Ego4D, we propose to condense the information in each video with captioning models. We use pre-trained MLLMs to generate a descriptive narrative that includes sufficient details about the environment, the subject’s actions, and other relevant objects. These captions are further used to prompt an LLM to predict the corresponding temporal windows given a natural language query.\nWe only require a coarse-grained prediction containing multiple intervals that potentially contain the answer at this stage. In the end, similar to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite>, we use a pre-trained NLQ model to further refine the response windows.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para\">\n<p id=\"S1.p5.1\" class=\"ltx_p\">In this paper, we show that our proposed method based on off-the-shelf pre-trained LLMs can achieve competitive performance compared to prior supervised learning methods, while having better interpretability with textual explanations of the predictions.\nWhen using the official narrations of the Ego4D dataset as captions, our method surpasses the baseline model on the mean recall (r@1) metric of the Ego4D validation set, suggesting the vast potential of our method for future improvement.\nThis demonstrates the effectiveness of combining multiple pre-trained multimodal LLMs in a proper pipeline when applying to complex vision-language tasks. In our empirical experiments, we examined the important design decisions and hyperparameter choices for our proposed pipeline which are critical for good performance. These choices include task-aware captioning, summarization, different ways of prompting, and other implementation details.</p>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n\n<figure id=\"S2.F2\" class=\"ltx_figure\"><img src=\"./assets/x2.png\" id=\"S2.F2.1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"380\" height=\"63\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S2.F2.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span id=\"S2.F2.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">\nOur LifelongMemory framework for the video natural language query task. Stage 1 involves converting input video into captions using a captioning model. The raw captions are then condensed and digested using similarity scores. In Stage 2, captions and queries are processed by an LLM to predict temporal windows with explanations and confidence levels. Stage 3 refines these predictions with a pre-trained NLQ model, identifying fine-grained temporal windows from the filtered video.</span></figcaption>\n</figure>\n<section id=\"S2.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Large Language Models for Multi-modal Learning.</h4>\n\n<div id=\"S2.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px1.p1.1\" class=\"ltx_p\">Large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>, <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">42</span></a>, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>, <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>, <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>]</cite> have demonstrated an excellent ability to understand natural language inputs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>]</cite>. Many prior works have explored the usage of LLMs in multimodal learning. LlamaIndex <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite> provides a data framework to integrate many different data types into a shared encoding for LLMs to ingest. PaLM-E <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> adds sensor data from a robot to PaLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>]</cite> to obtain general capabilities in visual, language, and robotic tasks in a single model. LLaVa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> connects the CLIP visual encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> with the language decoder of an LLM and finetune them end-to-end on multimodal instruction-following data, and the resulting model achieves competitive performance in general-purpose visual and language understanding. LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite> adds a conditioning on visual input to pre-trained LLMs, and finetunes them to create automatic video narrators. The resulting model achieves competitive performance across a number of action recognition and video captioning tasks. The newly released GPT-4V <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite> enables GPT-4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> to analyze image inputs, greatly expanding the capabilities of the model.</p>\n</div>\n<div id=\"S2.SS0.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px1.p2.1\" class=\"ltx_p\">Other applications include open vocabulary semantic segmentation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite>, learning from interactive perception <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></a>]</cite>, long-horizon planning with visual input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>, multimodal reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">39</span></a>]</cite>, and coordinating other language models for visual reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>. Most relevant to our work, the Socratic models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>]</cite> quantitatively evaluate the zero-shot reasoning capability of LLM on image captioning and video-to-text retrieval and demonstrate that the performance is on par with current standards, while illustrative examples are shown to highlight the potential for broader multimodal applications such as egocentric video question answering and robotic perception and planning. Our work draws inspiration from those examples and proposes a concrete framework with quantitative evaluations. To the best of our knowledge, our work is the first to use a general purpose chat-based LLM in the NLQ task. Similar to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></a>]</cite>, we employ a text-based reasoning pipeline; in addition, we propose a novel pipeline for generating temporal localization outputs.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Video Understanding with Pre-trained Large Language Models.</h4>\n\n<div id=\"S2.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px2.p1.1\" class=\"ltx_p\">Specific to the video domain, many recent works have explored applying frozen LLMs to video understanding tasks, including video question answering (VideoQA) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">37</span></a>, <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">38</span></a>, <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">43</span></a>]</cite>, video sequence modeling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>. Most relevant to our work, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">35</span></a>]</cite> proposes using a captioning model to summarize the video content on both the frame level and the visual token level, and use text summarization as part of the prompt for few-shot in-context learning of different video-language tasks such as video captioning and VideoQA. Our work differs in that 1) we focus on the zero-shot setting and do not provide any ground-truth examples to the LLM in the prompt; 2) our experiments focus on egocentric videos; and 3) we use only the frame-level captions and do not impose any special structure on the caption texts (such as ”First, …, Then, …”). Also similar to our work, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> proposes a framework for VideoQA using a frozen LLM to retrieve text descriptions of a video clip from an external text corpus, and using another frozen LLM to generate the answer from the question and the retrieved descriptions. Our method differs mainly in that we use a pre-trained captioning model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite> to generate narrations for each video clip instead of retrieving descriptions of a fixed external text corpus, and is therefore more flexible when dealing with complex scenes. Concurrent work, Vamos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">34</span></a>]</cite>, also adopts a similar idea of using a captioning model to connect video inputs and LLMs, but with a focus on exploring the effect of different video representations (vision, action, and caption) in fine-tuning the LLM for action anticipation and VideoQA tasks.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Natural Language Queries in Egocentric Videos.</h4>\n\n<div id=\"S2.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px3.p1.1\" class=\"ltx_p\">The Natural Language Queries (NLQ) task, which involves localizing the temporal window corresponding to the answer to a question in a long video clip, is a challenging task in the Ego4D benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> due to the sparsity of annotations and the length of videos in the dataset. Recent works have made much progress on top of the VSLNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> baseline. ReLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> proposes a novel multi-scale cross-modal transformer architecture, a video frame-level contrastive loss, and two data augmentation strategies. InternVideo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> improves the quality of video features by carefully pre-training and fine-tuning a VideoMAE-L Model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>, and ensemble the features and predictions. More recently, NaQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> introduces a data augmentation strategy to transform video narrations into training data for the NLQ task, alleviating the problem of sparse annotation. NaQ++, the main baseline we compare to in this paper on the Ego4D NLQ benchmark, is obtained by training the ReLER model with NaQ data. More recently, GroundNLQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> wins the CVPR 2023 Ego4D NLQ Challenge, and is the current state-of-the-art for this benchmark. It adopts a two-stage pre-training strategy to respectively train a video feature extractor and a grounding model on video narrations, and finally finetune the grounding model on annotated data.\nOur work is complementary to these prior works in that they can be used in the last stage of our proposed framework to produce more fine-grained predictions based on the predictions of the frozen LLM.</p>\n</div>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>LifelongMemory</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p id=\"S3.p1.1\" class=\"ltx_p\">In this section, we describe our proposed LifelongMemory framework.\nLifelongMemory converts egocentric videos into a series of captions and performs natural language queries using LLMs. The whole process can be decomposed into three stages, shown in Figure <a href=\"#S2.F2\" title=\"Figure 2 ‣ 2 Related Work ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>: 1) the first stage summarizes the video context into a comprehensive text description, possibly achieved by an image or a video captioning model; 2) the second stage involves using an LLM to predict coarse temporal windows based on the text description and the query; and 3) the last stage utilizes a pre-trained NLQ model to refine the intervals produced by the LLM. Next, we describe the three stages in detail.</p>\n</div>\n<section id=\"S3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Egocentric Video Captioning</h3>\n\n<div id=\"S3.SS1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.p1.1\" class=\"ltx_p\">We begin by transcribing the raw footage into a list of captions using pre-trained MLLMs (<em id=\"S3.SS1.p1.1.1\" class=\"ltx_emph ltx_font_italic\">e.g</em>.<span id=\"S3.SS1.p1.1.2\" class=\"ltx_text\"></span> LaViLa).\nWe sample image frames or short video clips at fixed intervals, and produced a line of caption per clip.\nThe text descriptions as a form of episodic memory enables the transformation of complex egocentric video footage into a coherent log of daily activities, capturing life’s narrative in a more accessible and compressed format.</p>\n</div>\n<section id=\"S3.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Digest.</h4>\n\n<div id=\"S3.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">Raw captions, however, can be rather verbose and repetitive, and consequently hinder the downstream retrieval process. We propose to create a caption digest to condense the information. Moreover, we aim to increase the relevance of the captions in relation to the target queries. Figure <a href=\"#S3.F3\" title=\"Figure 3 ‣ Caption Digest. ‣ 3.1 Egocentric Video Captioning ‣ 3 LifelongMemory ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an example of the caption digest process.\nFirst, we remove uninformative captions (<em id=\"S3.SS1.SSS0.Px1.p1.1.1\" class=\"ltx_emph ltx_font_italic\">e.g</em>.<span id=\"S3.SS1.SSS0.Px1.p1.1.2\" class=\"ltx_text\"></span> “looks around the house”). Second, we remove captions that are not relevant to the query by comparing the embedding similarity. Third, we gather adjacent captions that share a high similarity score and use an LLM to produce a single concise caption. The condensed list of captions is then sent to the next for further processing.</p>\n</div>\n<figure id=\"S3.F3\" class=\"ltx_figure\"><img src=\"./assets/x3.png\" id=\"S3.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"141\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span id=\"S3.F3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">\nExample caption processing and LLM reasoning. 1) We use a multimodal LLM (MLLM) to produce captions from a list of short video clips.\n2) Content and query similarity filters are then applied to remove redundant and irrelevant captions. Similar consecutive captions are merged by an LLM. 3) An LLM is instructed to take inputs from the list of condensed captions and retrieve the most relevant interval candidates.\n</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>LLM Reasoning</h3>\n\n<div id=\"S3.SS2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.p1.1\" class=\"ltx_p\">With the list of condensed captions with their corresponding time interval from the previous stage, we leverage an LLM here for its outstanding contextual understanding and reasoning skills.\nWe then combine captions and queries into a template to form an instructive and contextualized prompt. A snippet of the instruction template is shown in Figure <a href=\"#S3.F3\" title=\"Figure 3 ‣ Caption Digest. ‣ 3.1 Egocentric Video Captioning ‣ 3 LifelongMemory ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Due to the space constraint, we include the full prompt in <a href=\"#A1\" title=\"Appendix A Prompting ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nWhen processing a question, LLMs can take into consideration the full context in the template and utilize different pieces of information to produce the most probable answer.\nFor example, when asking “<em id=\"S3.SS2.p1.1.1\" class=\"ltx_emph ltx_font_italic\">Who did I interact with when I was shopping?</em>”, the LLM is able to filter all captions and produce a list of intervals involving “<em id=\"S3.SS2.p1.1.2\" class=\"ltx_emph ltx_font_italic\">person x talking to C</em>” where <em id=\"S3.SS2.p1.1.3\" class=\"ltx_emph ltx_font_italic\">C</em> is the subject in the video and <em id=\"S3.SS2.p1.1.4\" class=\"ltx_emph ltx_font_italic\">X</em> refers to the other person.</p>\n</div>\n<div id=\"S3.SS2.p2\" class=\"ltx_para\">\n<p id=\"S3.SS2.p2.1\" class=\"ltx_p\">Since it is inevitable to lose information when converting videos into texts, we particularly instruct LLMs to imagine the visual scene underlying the given captions. For example, one query asks “<em id=\"S3.SS2.p2.1.1\" class=\"ltx_emph ltx_font_italic\">What size of washer did I pick ?</em>” but there are no captions explicitly mentioning the washers. In this example, the LLM displays its capability to capture implicit information and infer based on context. The LLM answers “<em id=\"S3.SS2.p2.1.2\" class=\"ltx_emph ltx_font_italic\">choosing the time points where I picked items from the table or the floor, as these instances may provide more context about the objects and their locations</em>.” By grasping nuanced relationships and dependencies within the given context, LLM is able to filter out the most relevant information within the extensive video captions provided.</p>\n</div>\n<section id=\"S3.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Output Format.</h4>\n\n<div id=\"S3.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.SSS0.Px1.p1.1\" class=\"ltx_p\">We instruct the LLM to output three types of information in the output:</p>\n<ul id=\"S3.I1\" class=\"ltx_itemize\">\n<li id=\"S3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S3.I1.i1.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i1.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i1.p1.1.1\" class=\"ltx_text ltx_font_bold\">Candidate intervals:</span> The LLM can output a list of candidate intervals in the form of <math id=\"S3.I1.i1.p1.1.m1.2\" class=\"ltx_Math\" alttext=\"[(s_{1},e_{1}),(s_{2},e_{2})\\dots(s_{n},e_{n})]\" display=\"inline\"><semantics id=\"S3.I1.i1.p1.1.m1.2a\"><mrow id=\"S3.I1.i1.p1.1.m1.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.2.2.2.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.3.cmml\">[</mo><mrow id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml\"><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.3\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml\">(</mo><msub id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.cmml\"><mi id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.2\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.2.cmml\">s</mi><mn id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.3\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.3.cmml\">1</mn></msub><mo id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.4\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml\">,</mo><msub id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.cmml\"><mi id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.2.cmml\">e</mi><mn id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.3\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.3.cmml\">1</mn></msub><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.5\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml\">)</mo></mrow><mo id=\"S3.I1.i1.p1.1.m1.2.2.2.4\" xref=\"S3.I1.i1.p1.1.m1.2.2.3.cmml\">,</mo><mrow id=\"S3.I1.i1.p1.1.m1.2.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.cmml\"><mrow id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml\">(</mo><msub id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.cmml\"><mi id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.2.cmml\">s</mi><mn id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.3.cmml\">2</mn></msub><mo id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.4\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml\">,</mo><msub id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.cmml\"><mi id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.2.cmml\">e</mi><mn id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.3.cmml\">2</mn></msub><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.5\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.5\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.5.cmml\">​</mo><mi mathvariant=\"normal\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.6\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.6.cmml\">…</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.5a\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.5.cmml\">​</mo><mrow id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.3.cmml\"><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.3.cmml\">(</mo><msub id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.cmml\"><mi id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.2.cmml\">s</mi><mi id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.3.cmml\">n</mi></msub><mo id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.4\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.3.cmml\">,</mo><msub id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.cmml\"><mi id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.2\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.2.cmml\">e</mi><mi id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.3\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.3.cmml\">n</mi></msub><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.5\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.3.cmml\">)</mo></mrow></mrow><mo stretchy=\"false\" id=\"S3.I1.i1.p1.1.m1.2.2.2.5\" xref=\"S3.I1.i1.p1.1.m1.2.2.3.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.I1.i1.p1.1.m1.2b\"><interval closure=\"closed\" id=\"S3.I1.i1.p1.1.m1.2.2.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2\"><interval closure=\"open\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2\"><apply id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.2\">𝑠</ci><cn type=\"integer\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.1.1.3\">1</cn></apply><apply id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2\">subscript</csymbol><ci id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.2\">𝑒</ci><cn type=\"integer\" id=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.1.1.1.1.2.2.3\">1</cn></apply></interval><apply id=\"S3.I1.i1.p1.1.m1.2.2.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2\"><times id=\"S3.I1.i1.p1.1.m1.2.2.2.2.5.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.5\"></times><interval closure=\"open\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2\"><apply id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1\">subscript</csymbol><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.2\">𝑠</ci><cn type=\"integer\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.1.1.1.3\">2</cn></apply><apply id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2\">subscript</csymbol><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.2\">𝑒</ci><cn type=\"integer\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.2.2.2.3\">2</cn></apply></interval><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.6.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.6\">…</ci><interval closure=\"open\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2\"><apply id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1\">subscript</csymbol><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.2\">𝑠</ci><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.3.1.1.3\">𝑛</ci></apply><apply id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.1.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2\">subscript</csymbol><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.2.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.2\">𝑒</ci><ci id=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.3.cmml\" xref=\"S3.I1.i1.p1.1.m1.2.2.2.2.4.2.2.3\">𝑛</ci></apply></interval></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.I1.i1.p1.1.m1.2c\">[(s_{1},e_{1}),(s_{2},e_{2})\\dots(s_{n},e_{n})]</annotation></semantics></math>. Overlapping intervals are detected and merged.</p>\n</div>\n</li>\n<li id=\"S3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S3.I1.i2.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i2.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i2.p1.1.1\" class=\"ltx_text ltx_font_bold\">Explanation:</span> We also ask the LLM to output a sentence of explanation for more interpretability and introspective thinking.</p>\n</div>\n</li>\n<li id=\"S3.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S3.I1.i3.p1\" class=\"ltx_para\">\n<p id=\"S3.I1.i3.p1.1\" class=\"ltx_p\"><span id=\"S3.I1.i3.p1.1.1\" class=\"ltx_text ltx_font_bold\">Confidence level:</span> Lastly, we ask the LLM to rate its confidence on the output, out of three confidence levels. The verbalized confidence strategy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">36</span></a>]</cite> can help us control the precision of the output in later stages.</p>\n</div>\n</li>\n</ul>\n<p id=\"S3.SS2.SSS0.Px1.p1.2\" class=\"ltx_p\">If no relevant information is found, the LLM will output “NA” to avoid false positive.</p>\n</div>\n</section>\n</section>\n<section id=\"S3.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>Fine-grained Interval Refinement</h3>\n\n<div id=\"S3.SS3.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.p1.8\" class=\"ltx_p\">Since the time intervals are subsampled, to obtain a fine-grained interval prediction, we revisit the video inputs and enhance our LLM interval predictions in the last stage.\nFor this goal, we employ a pretrained NLQ model and feed in candidate intervals predicted by our previous stage. The intervals are padded with a small window of size <math id=\"S3.SS3.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S3.SS3.p1.1.m1.1a\"><mi id=\"S3.SS3.p1.1.m1.1.1\" xref=\"S3.SS3.p1.1.m1.1.1.cmml\">α</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.1.m1.1b\"><ci id=\"S3.SS3.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.p1.1.m1.1.1\">𝛼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.1.m1.1c\">\\alpha</annotation></semantics></math>.\nSpecifically, for each <math id=\"S3.SS3.p1.2.m2.2\" class=\"ltx_Math\" alttext=\"(s_{i},e_{i})\" display=\"inline\"><semantics id=\"S3.SS3.p1.2.m2.2a\"><mrow id=\"S3.SS3.p1.2.m2.2.2.2\" xref=\"S3.SS3.p1.2.m2.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.2.m2.2.2.2.3\" xref=\"S3.SS3.p1.2.m2.2.2.3.cmml\">(</mo><msub id=\"S3.SS3.p1.2.m2.1.1.1.1\" xref=\"S3.SS3.p1.2.m2.1.1.1.1.cmml\"><mi id=\"S3.SS3.p1.2.m2.1.1.1.1.2\" xref=\"S3.SS3.p1.2.m2.1.1.1.1.2.cmml\">s</mi><mi id=\"S3.SS3.p1.2.m2.1.1.1.1.3\" xref=\"S3.SS3.p1.2.m2.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S3.SS3.p1.2.m2.2.2.2.4\" xref=\"S3.SS3.p1.2.m2.2.2.3.cmml\">,</mo><msub id=\"S3.SS3.p1.2.m2.2.2.2.2\" xref=\"S3.SS3.p1.2.m2.2.2.2.2.cmml\"><mi id=\"S3.SS3.p1.2.m2.2.2.2.2.2\" xref=\"S3.SS3.p1.2.m2.2.2.2.2.2.cmml\">e</mi><mi id=\"S3.SS3.p1.2.m2.2.2.2.2.3\" xref=\"S3.SS3.p1.2.m2.2.2.2.2.3.cmml\">i</mi></msub><mo stretchy=\"false\" id=\"S3.SS3.p1.2.m2.2.2.2.5\" xref=\"S3.SS3.p1.2.m2.2.2.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.2.m2.2b\"><interval closure=\"open\" id=\"S3.SS3.p1.2.m2.2.2.3.cmml\" xref=\"S3.SS3.p1.2.m2.2.2.2\"><apply id=\"S3.SS3.p1.2.m2.1.1.1.1.cmml\" xref=\"S3.SS3.p1.2.m2.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.2.m2.1.1.1.1.1.cmml\" xref=\"S3.SS3.p1.2.m2.1.1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p1.2.m2.1.1.1.1.2.cmml\" xref=\"S3.SS3.p1.2.m2.1.1.1.1.2\">𝑠</ci><ci id=\"S3.SS3.p1.2.m2.1.1.1.1.3.cmml\" xref=\"S3.SS3.p1.2.m2.1.1.1.1.3\">𝑖</ci></apply><apply id=\"S3.SS3.p1.2.m2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.2.m2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.2.m2.2.2.2.2.1.cmml\" xref=\"S3.SS3.p1.2.m2.2.2.2.2\">subscript</csymbol><ci id=\"S3.SS3.p1.2.m2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.2.m2.2.2.2.2.2\">𝑒</ci><ci id=\"S3.SS3.p1.2.m2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p1.2.m2.2.2.2.2.3\">𝑖</ci></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.2.m2.2c\">(s_{i},e_{i})</annotation></semantics></math>, the new start time is <math id=\"S3.SS3.p1.3.m3.3\" class=\"ltx_Math\" alttext=\"s^{\\prime}_{i}=\\min(s_{i}-\\alpha,s)\" display=\"inline\"><semantics id=\"S3.SS3.p1.3.m3.3a\"><mrow id=\"S3.SS3.p1.3.m3.3.3\" xref=\"S3.SS3.p1.3.m3.3.3.cmml\"><msubsup id=\"S3.SS3.p1.3.m3.3.3.3\" xref=\"S3.SS3.p1.3.m3.3.3.3.cmml\"><mi id=\"S3.SS3.p1.3.m3.3.3.3.2.2\" xref=\"S3.SS3.p1.3.m3.3.3.3.2.2.cmml\">s</mi><mi id=\"S3.SS3.p1.3.m3.3.3.3.3\" xref=\"S3.SS3.p1.3.m3.3.3.3.3.cmml\">i</mi><mo id=\"S3.SS3.p1.3.m3.3.3.3.2.3\" xref=\"S3.SS3.p1.3.m3.3.3.3.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS3.p1.3.m3.3.3.2\" xref=\"S3.SS3.p1.3.m3.3.3.2.cmml\">=</mo><mrow id=\"S3.SS3.p1.3.m3.3.3.1.1\" xref=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\"><mi id=\"S3.SS3.p1.3.m3.1.1\" xref=\"S3.SS3.p1.3.m3.1.1.cmml\">min</mi><mo id=\"S3.SS3.p1.3.m3.3.3.1.1a\" xref=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\">⁡</mo><mrow id=\"S3.SS3.p1.3.m3.3.3.1.1.1\" xref=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.3.m3.3.3.1.1.1.2\" xref=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\">(</mo><mrow id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.cmml\"><msub id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.cmml\"><mi id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.2\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.2.cmml\">s</mi><mi id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.3\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.3.cmml\">i</mi></msub><mo id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.1\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.1.cmml\">−</mo><mi id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.3\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.3.cmml\">α</mi></mrow><mo id=\"S3.SS3.p1.3.m3.3.3.1.1.1.3\" xref=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\">,</mo><mi id=\"S3.SS3.p1.3.m3.2.2\" xref=\"S3.SS3.p1.3.m3.2.2.cmml\">s</mi><mo stretchy=\"false\" id=\"S3.SS3.p1.3.m3.3.3.1.1.1.4\" xref=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.3.m3.3b\"><apply id=\"S3.SS3.p1.3.m3.3.3.cmml\" xref=\"S3.SS3.p1.3.m3.3.3\"><eq id=\"S3.SS3.p1.3.m3.3.3.2.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.2\"></eq><apply id=\"S3.SS3.p1.3.m3.3.3.3.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.3.m3.3.3.3.1.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3\">subscript</csymbol><apply id=\"S3.SS3.p1.3.m3.3.3.3.2.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.3.m3.3.3.3.2.1.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3\">superscript</csymbol><ci id=\"S3.SS3.p1.3.m3.3.3.3.2.2.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3.2.2\">𝑠</ci><ci id=\"S3.SS3.p1.3.m3.3.3.3.2.3.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3.2.3\">′</ci></apply><ci id=\"S3.SS3.p1.3.m3.3.3.3.3.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.3.3\">𝑖</ci></apply><apply id=\"S3.SS3.p1.3.m3.3.3.1.2.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1\"><min id=\"S3.SS3.p1.3.m3.1.1.cmml\" xref=\"S3.SS3.p1.3.m3.1.1\"></min><apply id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1\"><minus id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.1.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.1\"></minus><apply id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.1.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2\">subscript</csymbol><ci id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.2.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.2\">𝑠</ci><ci id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.3.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.2.3\">𝑖</ci></apply><ci id=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.3.cmml\" xref=\"S3.SS3.p1.3.m3.3.3.1.1.1.1.3\">𝛼</ci></apply><ci id=\"S3.SS3.p1.3.m3.2.2.cmml\" xref=\"S3.SS3.p1.3.m3.2.2\">𝑠</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.3.m3.3c\">s^{\\prime}_{i}=\\min(s_{i}-\\alpha,s)</annotation></semantics></math> and the new end time is <math id=\"S3.SS3.p1.4.m4.3\" class=\"ltx_Math\" alttext=\"e^{\\prime}_{i}=\\max(e_{i}+\\alpha,e)\" display=\"inline\"><semantics id=\"S3.SS3.p1.4.m4.3a\"><mrow id=\"S3.SS3.p1.4.m4.3.3\" xref=\"S3.SS3.p1.4.m4.3.3.cmml\"><msubsup id=\"S3.SS3.p1.4.m4.3.3.3\" xref=\"S3.SS3.p1.4.m4.3.3.3.cmml\"><mi id=\"S3.SS3.p1.4.m4.3.3.3.2.2\" xref=\"S3.SS3.p1.4.m4.3.3.3.2.2.cmml\">e</mi><mi id=\"S3.SS3.p1.4.m4.3.3.3.3\" xref=\"S3.SS3.p1.4.m4.3.3.3.3.cmml\">i</mi><mo id=\"S3.SS3.p1.4.m4.3.3.3.2.3\" xref=\"S3.SS3.p1.4.m4.3.3.3.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS3.p1.4.m4.3.3.2\" xref=\"S3.SS3.p1.4.m4.3.3.2.cmml\">=</mo><mrow id=\"S3.SS3.p1.4.m4.3.3.1.1\" xref=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\"><mi id=\"S3.SS3.p1.4.m4.1.1\" xref=\"S3.SS3.p1.4.m4.1.1.cmml\">max</mi><mo id=\"S3.SS3.p1.4.m4.3.3.1.1a\" xref=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\">⁡</mo><mrow id=\"S3.SS3.p1.4.m4.3.3.1.1.1\" xref=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.4.m4.3.3.1.1.1.2\" xref=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\">(</mo><mrow id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.cmml\"><msub id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.cmml\"><mi id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.2\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.2.cmml\">e</mi><mi id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.3\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.3.cmml\">i</mi></msub><mo id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.1\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.1.cmml\">+</mo><mi id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.3\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.3.cmml\">α</mi></mrow><mo id=\"S3.SS3.p1.4.m4.3.3.1.1.1.3\" xref=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\">,</mo><mi id=\"S3.SS3.p1.4.m4.2.2\" xref=\"S3.SS3.p1.4.m4.2.2.cmml\">e</mi><mo stretchy=\"false\" id=\"S3.SS3.p1.4.m4.3.3.1.1.1.4\" xref=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.4.m4.3b\"><apply id=\"S3.SS3.p1.4.m4.3.3.cmml\" xref=\"S3.SS3.p1.4.m4.3.3\"><eq id=\"S3.SS3.p1.4.m4.3.3.2.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.2\"></eq><apply id=\"S3.SS3.p1.4.m4.3.3.3.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.4.m4.3.3.3.1.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3\">subscript</csymbol><apply id=\"S3.SS3.p1.4.m4.3.3.3.2.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.4.m4.3.3.3.2.1.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3\">superscript</csymbol><ci id=\"S3.SS3.p1.4.m4.3.3.3.2.2.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3.2.2\">𝑒</ci><ci id=\"S3.SS3.p1.4.m4.3.3.3.2.3.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3.2.3\">′</ci></apply><ci id=\"S3.SS3.p1.4.m4.3.3.3.3.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.3.3\">𝑖</ci></apply><apply id=\"S3.SS3.p1.4.m4.3.3.1.2.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1\"><max id=\"S3.SS3.p1.4.m4.1.1.cmml\" xref=\"S3.SS3.p1.4.m4.1.1\"></max><apply id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1\"><plus id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.1.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.1\"></plus><apply id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.1.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2\">subscript</csymbol><ci id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.2.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.2\">𝑒</ci><ci id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.3.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.2.3\">𝑖</ci></apply><ci id=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.3.cmml\" xref=\"S3.SS3.p1.4.m4.3.3.1.1.1.1.3\">𝛼</ci></apply><ci id=\"S3.SS3.p1.4.m4.2.2.cmml\" xref=\"S3.SS3.p1.4.m4.2.2\">𝑒</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.4.m4.3c\">e^{\\prime}_{i}=\\max(e_{i}+\\alpha,e)</annotation></semantics></math> where <math id=\"S3.SS3.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"s\" display=\"inline\"><semantics id=\"S3.SS3.p1.5.m5.1a\"><mi id=\"S3.SS3.p1.5.m5.1.1\" xref=\"S3.SS3.p1.5.m5.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.5.m5.1b\"><ci id=\"S3.SS3.p1.5.m5.1.1.cmml\" xref=\"S3.SS3.p1.5.m5.1.1\">𝑠</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.5.m5.1c\">s</annotation></semantics></math> and <math id=\"S3.SS3.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"e\" display=\"inline\"><semantics id=\"S3.SS3.p1.6.m6.1a\"><mi id=\"S3.SS3.p1.6.m6.1.1\" xref=\"S3.SS3.p1.6.m6.1.1.cmml\">e</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.6.m6.1b\"><ci id=\"S3.SS3.p1.6.m6.1.1.cmml\" xref=\"S3.SS3.p1.6.m6.1.1\">𝑒</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.6.m6.1c\">e</annotation></semantics></math> are the start and end time of the original clip. Then we extract video clips <math id=\"S3.SS3.p1.7.m7.2\" class=\"ltx_Math\" alttext=\"[v_{1},v_{2}\\dots v_{n}]\" display=\"inline\"><semantics id=\"S3.SS3.p1.7.m7.2a\"><mrow id=\"S3.SS3.p1.7.m7.2.2.2\" xref=\"S3.SS3.p1.7.m7.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.7.m7.2.2.2.3\" xref=\"S3.SS3.p1.7.m7.2.2.3.cmml\">[</mo><msub id=\"S3.SS3.p1.7.m7.1.1.1.1\" xref=\"S3.SS3.p1.7.m7.1.1.1.1.cmml\"><mi id=\"S3.SS3.p1.7.m7.1.1.1.1.2\" xref=\"S3.SS3.p1.7.m7.1.1.1.1.2.cmml\">v</mi><mn id=\"S3.SS3.p1.7.m7.1.1.1.1.3\" xref=\"S3.SS3.p1.7.m7.1.1.1.1.3.cmml\">1</mn></msub><mo id=\"S3.SS3.p1.7.m7.2.2.2.4\" xref=\"S3.SS3.p1.7.m7.2.2.3.cmml\">,</mo><mrow id=\"S3.SS3.p1.7.m7.2.2.2.2\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.cmml\"><msub id=\"S3.SS3.p1.7.m7.2.2.2.2.2\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2.cmml\"><mi id=\"S3.SS3.p1.7.m7.2.2.2.2.2.2\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2.2.cmml\">v</mi><mn id=\"S3.SS3.p1.7.m7.2.2.2.2.2.3\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2.3.cmml\">2</mn></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p1.7.m7.2.2.2.2.1\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.1.cmml\">​</mo><mi mathvariant=\"normal\" id=\"S3.SS3.p1.7.m7.2.2.2.2.3\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.3.cmml\">…</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p1.7.m7.2.2.2.2.1a\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.1.cmml\">​</mo><msub id=\"S3.SS3.p1.7.m7.2.2.2.2.4\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4.cmml\"><mi id=\"S3.SS3.p1.7.m7.2.2.2.2.4.2\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4.2.cmml\">v</mi><mi id=\"S3.SS3.p1.7.m7.2.2.2.2.4.3\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4.3.cmml\">n</mi></msub></mrow><mo stretchy=\"false\" id=\"S3.SS3.p1.7.m7.2.2.2.5\" xref=\"S3.SS3.p1.7.m7.2.2.3.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.7.m7.2b\"><interval closure=\"closed\" id=\"S3.SS3.p1.7.m7.2.2.3.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2\"><apply id=\"S3.SS3.p1.7.m7.1.1.1.1.cmml\" xref=\"S3.SS3.p1.7.m7.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.7.m7.1.1.1.1.1.cmml\" xref=\"S3.SS3.p1.7.m7.1.1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p1.7.m7.1.1.1.1.2.cmml\" xref=\"S3.SS3.p1.7.m7.1.1.1.1.2\">𝑣</ci><cn type=\"integer\" id=\"S3.SS3.p1.7.m7.1.1.1.1.3.cmml\" xref=\"S3.SS3.p1.7.m7.1.1.1.1.3\">1</cn></apply><apply id=\"S3.SS3.p1.7.m7.2.2.2.2.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2\"><times id=\"S3.SS3.p1.7.m7.2.2.2.2.1.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.1\"></times><apply id=\"S3.SS3.p1.7.m7.2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.7.m7.2.2.2.2.2.1.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2\">subscript</csymbol><ci id=\"S3.SS3.p1.7.m7.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2.2\">𝑣</ci><cn type=\"integer\" id=\"S3.SS3.p1.7.m7.2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.2.3\">2</cn></apply><ci id=\"S3.SS3.p1.7.m7.2.2.2.2.3.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.3\">…</ci><apply id=\"S3.SS3.p1.7.m7.2.2.2.2.4.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.7.m7.2.2.2.2.4.1.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4\">subscript</csymbol><ci id=\"S3.SS3.p1.7.m7.2.2.2.2.4.2.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4.2\">𝑣</ci><ci id=\"S3.SS3.p1.7.m7.2.2.2.2.4.3.cmml\" xref=\"S3.SS3.p1.7.m7.2.2.2.2.4.3\">𝑛</ci></apply></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.7.m7.2c\">[v_{1},v_{2}\\dots v_{n}]</annotation></semantics></math> according to the predicted intervals <math id=\"S3.SS3.p1.8.m8.2\" class=\"ltx_Math\" alttext=\"[(s^{\\prime}_{1},e^{\\prime}_{1}),(s^{\\prime}_{2},e^{\\prime}_{2})\\dots(s^{\\prime}_{n},e^{\\prime}_{n})]\" display=\"inline\"><semantics id=\"S3.SS3.p1.8.m8.2a\"><mrow id=\"S3.SS3.p1.8.m8.2.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.2.2.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.3.cmml\">[</mo><mrow id=\"S3.SS3.p1.8.m8.1.1.1.1.2\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.3.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.1.1.1.1.2.3\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.3.cmml\">(</mo><msubsup id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.cmml\"><mi id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2.cmml\">s</mi><mn id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.3\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.3.cmml\">1</mn><mo id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS3.p1.8.m8.1.1.1.1.2.4\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.3.cmml\">,</mo><msubsup id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.cmml\"><mi id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.2\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.2.cmml\">e</mi><mn id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.3\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.3.cmml\">1</mn><mo id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.3\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.3.cmml\">′</mo></msubsup><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.1.1.1.1.2.5\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.3.cmml\">)</mo></mrow><mo id=\"S3.SS3.p1.8.m8.2.2.2.4\" xref=\"S3.SS3.p1.8.m8.2.2.3.cmml\">,</mo><mrow id=\"S3.SS3.p1.8.m8.2.2.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.cmml\"><mrow id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.3.cmml\">(</mo><msubsup id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.cmml\"><mi id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.2.cmml\">s</mi><mn id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.3.cmml\">2</mn><mo id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.4\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.3.cmml\">,</mo><msubsup id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.cmml\"><mi id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.2.cmml\">e</mi><mn id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.3.cmml\">2</mn><mo id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.3.cmml\">′</mo></msubsup><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.5\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.3.cmml\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p1.8.m8.2.2.2.2.5\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.5.cmml\">​</mo><mi mathvariant=\"normal\" id=\"S3.SS3.p1.8.m8.2.2.2.2.6\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.6.cmml\">…</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p1.8.m8.2.2.2.2.5a\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.5.cmml\">​</mo><mrow id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.3.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.3.cmml\">(</mo><msubsup id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.cmml\"><mi id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.2.cmml\">s</mi><mi id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.3.cmml\">n</mi><mo id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.4\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.3.cmml\">,</mo><msubsup id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.cmml\"><mi id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.2\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.2.cmml\">e</mi><mi id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.3.cmml\">n</mi><mo id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.3\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.3.cmml\">′</mo></msubsup><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.5\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.3.cmml\">)</mo></mrow></mrow><mo stretchy=\"false\" id=\"S3.SS3.p1.8.m8.2.2.2.5\" xref=\"S3.SS3.p1.8.m8.2.2.3.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p1.8.m8.2b\"><interval closure=\"closed\" id=\"S3.SS3.p1.8.m8.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2\"><interval closure=\"open\" id=\"S3.SS3.p1.8.m8.1.1.1.1.3.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2\"><apply id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.1.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1\">subscript</csymbol><apply id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1\">superscript</csymbol><ci id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2\">𝑠</ci><ci id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3\">′</ci></apply><cn type=\"integer\" id=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.3.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.1.1.3\">1</cn></apply><apply id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2\">subscript</csymbol><apply id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2\">superscript</csymbol><ci id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.2\">𝑒</ci><ci id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.2.3\">′</ci></apply><cn type=\"integer\" id=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.1.1.1.1.2.2.3\">1</cn></apply></interval><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2\"><times id=\"S3.SS3.p1.8.m8.2.2.2.2.5.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.5\"></times><interval closure=\"open\" id=\"S3.SS3.p1.8.m8.2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2\"><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1\">subscript</csymbol><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1\">superscript</csymbol><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.2\">𝑠</ci><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.2.3\">′</ci></apply><cn type=\"integer\" id=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.1.1.1.3\">2</cn></apply><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2\">subscript</csymbol><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2\">superscript</csymbol><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.2\">𝑒</ci><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.2.3\">′</ci></apply><cn type=\"integer\" id=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.2.2.2.3\">2</cn></apply></interval><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.6.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.6\">…</ci><interval closure=\"open\" id=\"S3.SS3.p1.8.m8.2.2.2.2.4.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2\"><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1\">subscript</csymbol><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1\">superscript</csymbol><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.2\">𝑠</ci><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.2.3\">′</ci></apply><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.3.1.1.3\">𝑛</ci></apply><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2\">subscript</csymbol><apply id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.1.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2\">superscript</csymbol><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.2.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.2\">𝑒</ci><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.2.3\">′</ci></apply><ci id=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.3.cmml\" xref=\"S3.SS3.p1.8.m8.2.2.2.2.4.2.2.3\">𝑛</ci></apply></interval></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p1.8.m8.2c\">[(s^{\\prime}_{1},e^{\\prime}_{1}),(s^{\\prime}_{2},e^{\\prime}_{2})\\dots(s^{\\prime}_{n},e^{\\prime}_{n})]</annotation></semantics></math>.</p>\n</div>\n<div id=\"S3.SS3.p2\" class=\"ltx_para\">\n<p id=\"S3.SS3.p2.1\" class=\"ltx_p\">When the prediction for a certain query contains multiple candidate intervals, we feed them along with the target query into a classifier that is trained on NLQ data to select the optimal candidate <math id=\"S3.SS3.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"v^{*}\" display=\"inline\"><semantics id=\"S3.SS3.p2.1.m1.1a\"><msup id=\"S3.SS3.p2.1.m1.1.1\" xref=\"S3.SS3.p2.1.m1.1.1.cmml\"><mi id=\"S3.SS3.p2.1.m1.1.1.2\" xref=\"S3.SS3.p2.1.m1.1.1.2.cmml\">v</mi><mo id=\"S3.SS3.p2.1.m1.1.1.3\" xref=\"S3.SS3.p2.1.m1.1.1.3.cmml\">∗</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p2.1.m1.1b\"><apply id=\"S3.SS3.p2.1.m1.1.1.cmml\" xref=\"S3.SS3.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p2.1.m1.1.1.1.cmml\" xref=\"S3.SS3.p2.1.m1.1.1\">superscript</csymbol><ci id=\"S3.SS3.p2.1.m1.1.1.2.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.2\">𝑣</ci><times id=\"S3.SS3.p2.1.m1.1.1.3.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.3\"></times></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p2.1.m1.1c\">v^{*}</annotation></semantics></math>. For queries without predictions (<em id=\"S3.SS3.p2.1.1\" class=\"ltx_emph ltx_font_italic\">i.e</em>.<span id=\"S3.SS3.p2.1.2\" class=\"ltx_text\"></span> “NA”), we simply use the original full video. Since we have already retrieved coarse temporal windows, the NLQ task here becomes easier compared to retrieving directly from the original video.</p>\n</div>\n<div id=\"S3.SS3.p3\" class=\"ltx_para\">\n<p id=\"S3.SS3.p3.1\" class=\"ltx_p\">To directly evaluate the effectiveness of our LLM methodology, we do not employ fine-grained interval refinement in our main experiment, and the refinement is only used for the full Ego4D NLQ competition benchmark.</p>\n</div>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para\">\n<p id=\"S4.p1.1\" class=\"ltx_p\">In this section, we evaluate our LifelongMemory framework in real-world egocentric video query tasks.</p>\n</div>\n<figure id=\"S4.T1\" class=\"ltx_table\">\n<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span id=\"S4.T1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Captions</span></td>\n<td id=\"S4.T1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">Count</span></td>\n<td id=\"S4.T1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.2.2.5.1\" class=\"ltx_text ltx_font_bold\">Length</span></td>\n<td id=\"S4.T1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.2.2.6.1\" class=\"ltx_text ltx_font_bold\">LLM</span></td>\n<td id=\"S4.T1.2.2.7\" class=\"ltx_td ltx_align_right ltx_border_tt\"><em id=\"S4.T1.2.2.7.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em></td>\n<td id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_right ltx_border_tt\">\n<em id=\"S4.T1.1.1.1.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em> <math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mo id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\dagger</annotation></semantics></math>\n</td>\n<td id=\"S4.T1.2.2.8\" class=\"ltx_td ltx_align_right ltx_border_tt\"><em id=\"S4.T1.2.2.8.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em></td>\n<td id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">\n<em id=\"S4.T1.2.2.2.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em> <math id=\"S4.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.m1.1a\"><mo id=\"S4.T1.2.2.2.m1.1.1\" xref=\"S4.T1.2.2.2.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.m1.1b\"><ci id=\"S4.T1.2.2.2.m1.1.1.cmml\" xref=\"S4.T1.2.2.2.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.m1.1c\">\\dagger</annotation></semantics></math>\n</td>\n<td id=\"S4.T1.2.2.9\" class=\"ltx_td ltx_align_right ltx_border_tt\"><em id=\"S4.T1.2.2.9.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">NA</em></td>\n</tr>\n<tr id=\"S4.T1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.3.1.1.1\" class=\"ltx_text\">Ego4D</span></td>\n<td id=\"S4.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.3.1.2.1\" class=\"ltx_text\">109.44</span></td>\n<td id=\"S4.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.3.1.3.1\" class=\"ltx_text\">7.95</span></td>\n<td id=\"S4.T1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">GPT4</td>\n<td id=\"S4.T1.2.3.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.3.1.5.1\" class=\"ltx_text ltx_font_bold\">51.73</span></td>\n<td id=\"S4.T1.2.3.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.3.1.6.1\" class=\"ltx_text ltx_font_bold\">53.98</span></td>\n<td id=\"S4.T1.2.3.1.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.3.1.7.1\" class=\"ltx_text ltx_font_bold\">15.99</span></td>\n<td id=\"S4.T1.2.3.1.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.3.1.8.1\" class=\"ltx_text ltx_font_bold\">27.38</span></td>\n<td id=\"S4.T1.2.3.1.9\" class=\"ltx_td ltx_align_right ltx_border_t\">40.29</td>\n</tr>\n<tr id=\"S4.T1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.4.2.1\" class=\"ltx_td ltx_align_center\">GPT3.5</td>\n<td id=\"S4.T1.2.4.2.2\" class=\"ltx_td ltx_align_right\">31.27</td>\n<td id=\"S4.T1.2.4.2.3\" class=\"ltx_td ltx_align_right\">33.15</td>\n<td id=\"S4.T1.2.4.2.4\" class=\"ltx_td ltx_align_right\">0.91</td>\n<td id=\"S4.T1.2.4.2.5\" class=\"ltx_td ltx_align_right\">4.34</td>\n<td id=\"S4.T1.2.4.2.6\" class=\"ltx_td ltx_align_right\">94.13</td>\n</tr>\n<tr id=\"S4.T1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.5.3.1.1\" class=\"ltx_text\">LaViLa</span></td>\n<td id=\"S4.T1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.5.3.2.1\" class=\"ltx_text\">186.33</span></td>\n<td id=\"S4.T1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.5.3.3.1\" class=\"ltx_text\">6.40</span></td>\n<td id=\"S4.T1.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">GPT4</td>\n<td id=\"S4.T1.2.5.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.5.3.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">36.61</span></td>\n<td id=\"S4.T1.2.5.3.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.5.3.6.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">38.35</span></td>\n<td id=\"S4.T1.2.5.3.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.5.3.7.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">9.74</span></td>\n<td id=\"S4.T1.2.5.3.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.2.5.3.8.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">19.22</span></td>\n<td id=\"S4.T1.2.5.3.9\" class=\"ltx_td ltx_align_right ltx_border_t\">47.04</td>\n</tr>\n<tr id=\"S4.T1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.6.4.1\" class=\"ltx_td ltx_align_center\">GPT3.5</td>\n<td id=\"S4.T1.2.6.4.2\" class=\"ltx_td ltx_align_right\">20.47</td>\n<td id=\"S4.T1.2.6.4.3\" class=\"ltx_td ltx_align_right\">22.33</td>\n<td id=\"S4.T1.2.6.4.4\" class=\"ltx_td ltx_align_right\">1.29</td>\n<td id=\"S4.T1.2.6.4.5\" class=\"ltx_td ltx_align_right\">4.78</td>\n<td id=\"S4.T1.2.6.4.6\" class=\"ltx_td ltx_align_right\">89.75</td>\n</tr>\n<tr id=\"S4.T1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.7.5.1.1\" class=\"ltx_text\">LLaVA</span></td>\n<td id=\"S4.T1.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.2.7.5.2.1\" class=\"ltx_text\">250.58</span></td>\n<td id=\"S4.T1.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.2.7.5.3.1\" class=\"ltx_text\">52.52</span></td>\n<td id=\"S4.T1.2.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">GPT4</td>\n<td id=\"S4.T1.2.7.5.5\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">6.42</td>\n<td id=\"S4.T1.2.7.5.6\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">8.79</td>\n<td id=\"S4.T1.2.7.5.7\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">1.50</td>\n<td id=\"S4.T1.2.7.5.8\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">2.71</td>\n<td id=\"S4.T1.2.7.5.9\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">60.92</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T1.9.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 1</span>: </span><span id=\"S4.T1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Main results on NLQ performance using different caption and LLM components. The <span id=\"S4.T1.4.1.1\" class=\"ltx_text ltx_font_bold\">bold</span> number denotes the highest and the <span id=\"S4.T1.4.1.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">underlined</span> the second highest. <math id=\"S4.T1.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T1.4.1.m1.1b\"><mo id=\"S4.T1.4.1.m1.1.1\" xref=\"S4.T1.4.1.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.4.1.m1.1c\"><ci id=\"S4.T1.4.1.m1.1.1.cmml\" xref=\"S4.T1.4.1.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.4.1.m1.1d\">\\dagger</annotation></semantics></math> represents predictions with a confidence level of 3. <em id=\"S4.T1.4.1.3\" class=\"ltx_emph ltx_font_italic\">NA</em> represents the ratio of null predictions, and all other metrics do not include null predictions. Numbers in percentages.</span></figcaption>\n</figure>\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Experiment Setup</h3>\n\n<section id=\"S4.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Dataset.</h4>\n\n<div id=\"S4.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">We evaluate our proposed pipeline on the NLQ annotations from the Ego4D dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>, which includes a wide variety of daily life activities recorded by individuals wearing cameras. The NLQ annotations are from 227 hours of videos, with a total of 19,200 queries spanning 13 query templates. The train/val/test split (60%, 20%, 20%) is composed of disjoint sets of video clips. The average video length is approximately 8.7 minutes, while the average duration of a response window is only 9.3 seconds, representing on average only 2% of the full video.</p>\n</div>\n</section>\n<section id=\"S4.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Evaluation Metrics.</h4>\n\n<div id=\"S4.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">We evaluate different stages of our pipeline using validation and test splits of the Ego4D NLQ dataset. In Stage 2, where we only have coarse-grained predictions from LLM, we calculate <span id=\"S4.SS1.SSS0.Px2.p1.1.1\" class=\"ltx_text ltx_font_bold\">(i)</span> the ratio of predictions that overlap with the ground truth (denoted as <em id=\"S4.SS1.SSS0.Px2.p1.1.2\" class=\"ltx_emph ltx_font_italic\">Overlap</em>), <span id=\"S4.SS1.SSS0.Px2.p1.1.3\" class=\"ltx_text ltx_font_bold\">(ii)</span> and the proportion of predictions where at least one candidate achieves an Intersection Over Union (IOU) greater than 0.3 with the ground truth (denoted as <em id=\"S4.SS1.SSS0.Px2.p1.1.4\" class=\"ltx_emph ltx_font_italic\">IOU*@0.3</em>). During Stage 3, we obtain fine-grained predictions so we can evaluate on the test dataset using the standard NLQ metrics – <em id=\"S4.SS1.SSS0.Px2.p1.1.5\" class=\"ltx_emph ltx_font_italic\">R@1 IOU@0.3</em> and <em id=\"S4.SS1.SSS0.Px2.p1.1.6\" class=\"ltx_emph ltx_font_italic\">R@1 IOU@0.5</em>, which is the recall of top one prediction having IOU with the ground truth larger than the threshold {0.3, 0.5}.</p>\n</div>\n</section>\n<section id=\"S4.SS1.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Sources.</h4>\n\n<div id=\"S4.SS1.SSS0.Px3.p1\" class=\"ltx_para\">\n<ul id=\"S4.I1\" class=\"ltx_itemize\">\n<li id=\"S4.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S4.I1.i1.p1\" class=\"ltx_para\">\n<p id=\"S4.I1.i1.p1.1\" class=\"ltx_p\"><span id=\"S4.I1.i1.p1.1.1\" class=\"ltx_text ltx_font_bold\">LLaVA</span>: LLaVa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> is a multimodal LLM pre-trained on a diverse set of 1.2M publicly available data, including conversational-style question-answering, detailed description, complex reasoning, academic-task-oriented visual question-answering, OCR, and region-level perception. To encourage the LLaVA to generate captions that are relevant to the query while not introducing false positives, we follow the template proposed by the LLaVA-1.5<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> and adopt the prompt <em id=\"S4.I1.i1.p1.1.2\" class=\"ltx_emph ltx_font_italic\">“If there are factual errors in the questions, provide a precise description of the image; if not, proceed answering the question. [queries]”</em>.</p>\n</div>\n</li>\n<li id=\"S4.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S4.I1.i2.p1\" class=\"ltx_para\">\n<p id=\"S4.I1.i2.p1.1\" class=\"ltx_p\"><span id=\"S4.I1.i2.p1.1.1\" class=\"ltx_text ltx_font_bold\">LaViLa</span>: LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite> is a multimodal LLM pre-trained on the video-narration pairs from Ego4D and is thus capable of generating captions that mimic the ground-truth descriptions of the video. Each caption is generated using 4 frames uniformly taken from a two-second video clip.</p>\n</div>\n</li>\n<li id=\"S4.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span> \n<div id=\"S4.I1.i3.p1\" class=\"ltx_para\">\n<p id=\"S4.I1.i3.p1.1\" class=\"ltx_p\"><span id=\"S4.I1.i3.p1.1.1\" class=\"ltx_text ltx_font_bold\">Ego4D Narrations</span>: Ego4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> narrations include written sentence narrations in English from human annotators, describing a diverse set of activities in the dataset. The annotated narrations are fairly dense (on average 13.2 sentences per minute of video), but not as dense as LLaVA and LaViLa captions.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section id=\"S4.SS1.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Digest.</h4>\n\n<div id=\"S4.SS1.SSS0.Px4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px4.p1.2\" class=\"ltx_p\">To generate a concise yet comprehensive log of the egocentric videos,\nwe apply the narrator of LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite> – a video captioning video model pre-trained on massive egocentric datasets including Ego4D – to generate captions every <math id=\"S4.SS1.SSS0.Px4.p1.1.m1.3\" class=\"ltx_Math\" alttext=\"k=\\{1,2,4\\}\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px4.p1.1.m1.3a\"><mrow id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.cmml\"><mi id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.2\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.2.cmml\">k</mi><mo id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.1\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.1.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.2\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.2.1\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.1.cmml\">{</mo><mn id=\"S4.SS1.SSS0.Px4.p1.1.m1.1.1\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.1.1.cmml\">1</mn><mo id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.2.2\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.1.cmml\">,</mo><mn id=\"S4.SS1.SSS0.Px4.p1.1.m1.2.2\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.2.2.cmml\">2</mn><mo id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.2.3\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.1.cmml\">,</mo><mn id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.3\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.3.cmml\">4</mn><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.2.4\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.3b\"><apply id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4\"><eq id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.1.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.1\"></eq><ci id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.2.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.2\">𝑘</ci><set id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.1.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.4.3.2\"><cn type=\"integer\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.1.1\">1</cn><cn type=\"integer\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.2.2.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.2.2\">2</cn><cn type=\"integer\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.3.3.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.1.m1.3.3\">4</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px4.p1.1.m1.3c\">k=\\{1,2,4\\}</annotation></semantics></math> seconds. Each caption is generated using 4 frames uniformly taken from a two-second video clip with a low temperature of 0.1. For the video inputs of the NLQ dataset with an average length of 8 minutes, <math id=\"S4.SS1.SSS0.Px4.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"k=2s\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px4.p1.2.m2.1a\"><mrow id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.2\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml\">k</mi><mo id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.1\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml\"><mn id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.2\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.2.cmml\">2</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.1\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.1.cmml\">​</mo><mi id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.3\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.3.cmml\">s</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px4.p1.2.m2.1b\"><apply id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1\"><eq id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.1\"></eq><ci id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.2\">𝑘</ci><apply id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3\"><times id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.1.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.1\"></times><cn type=\"integer\" id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.2.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.2\">2</cn><ci id=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.3.cmml\" xref=\"S4.SS1.SSS0.Px4.p1.2.m2.1.1.3.3\">𝑠</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px4.p1.2.m2.1c\">k=2s</annotation></semantics></math> will result in around 240 captions with an average length of 7 words. To reduce the context length, we first remove ambiguous captions containing <em id=\"S4.SS1.SSS0.Px4.p1.2.1\" class=\"ltx_emph ltx_font_italic\">“look”</em> and <em id=\"S4.SS1.SSS0.Px4.p1.2.2\" class=\"ltx_emph ltx_font_italic\">“walks around”</em>, which are frequently seen in LaViLa’s generations when the inputs are blurry and noisy. We further filter and merge captions based on their similarity scores with the queries and neighboring captions. We apply the text encoder of LaViLa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">45</span></a>]</cite> to obtain the embedding of captions and queries, and then discard irrelevant captions that have less than 0.3 cosine similarity with all queries. We identify groups of similar consecutive captions by calculating the similarity score with neighboring captions, and merge captions in the same group by querying GPT-3.5 with prompt <em id=\"S4.SS1.SSS0.Px4.p1.2.3\" class=\"ltx_emph ltx_font_italic\">“In this task, you will merge a list of captions into a single, concise caption. Focus on clarity and brevity while ensuring that no critical details are lost in the merging process.”</em>. These preprocessing steps decrease the average caption counts in a clip from 240 to 186.</p>\n</div>\n<figure id=\"S4.T2\" class=\"ltx_table\">\n<table id=\"S4.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S4.T2.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Captions</span></th>\n<th id=\"S4.T2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.2.2.4.1\" class=\"ltx_text ltx_font_bold\">LLM</span></th>\n<th id=\"S4.T2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.2.2.5.1\" class=\"ltx_text ltx_font_bold\">Interval</span></th>\n<th id=\"S4.T2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T2.2.2.6.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em></th>\n<th id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<em id=\"S4.T2.1.1.1.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em><math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mo id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><ci id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">\\dagger</annotation></semantics></math>\n</th>\n<th id=\"S4.T2.2.2.7\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T2.2.2.7.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em></th>\n<th id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<em id=\"S4.T2.2.2.2.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em><math id=\"S4.T2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.m1.1a\"><mo id=\"S4.T2.2.2.2.m1.1.1\" xref=\"S4.T2.2.2.2.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.m1.1b\"><ci id=\"S4.T2.2.2.2.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.m1.1c\">\\dagger</annotation></semantics></math>\n</th>\n<th id=\"S4.T2.2.2.8\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T2.2.2.8.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">NA</em></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.2.3.1.1.1\" class=\"ltx_text\">LaViLa</span></td>\n<td id=\"S4.T2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.2.3.1.2.1\" class=\"ltx_text\">GPT4</span></td>\n<td id=\"S4.T2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">4s</td>\n<td id=\"S4.T2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">33.55</td>\n<td id=\"S4.T2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">36.99</td>\n<td id=\"S4.T2.2.3.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\">6.46</td>\n<td id=\"S4.T2.2.3.1.7\" class=\"ltx_td ltx_align_right ltx_border_t\">14.64</td>\n<td id=\"S4.T2.2.3.1.8\" class=\"ltx_td ltx_align_right ltx_border_t\">52.88</td>\n</tr>\n<tr id=\"S4.T2.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">2s</td>\n<td id=\"S4.T2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">36.61</span></td>\n<td id=\"S4.T2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.4.2.3.1\" class=\"ltx_text ltx_font_bold\">38.35</span></td>\n<td id=\"S4.T2.2.4.2.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T2.2.4.2.4.1\" class=\"ltx_text ltx_font_bold\">9.74</span></td>\n<td id=\"S4.T2.2.4.2.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T2.2.4.2.5.1\" class=\"ltx_text ltx_font_bold\">19.22</span></td>\n<td id=\"S4.T2.2.4.2.6\" class=\"ltx_td ltx_align_right ltx_border_bb\">47.04</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T2.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span id=\"S4.T2.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">NLQ performance using LaViLa + GPT4, with different frame sampling intervals (in seconds).</span></figcaption>\n</figure>\n<figure id=\"S4.T3\" class=\"ltx_table\">\n<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.2.3\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T3.2.2.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.2.2.4.1\" class=\"ltx_text ltx_font_bold\">Count</span></th>\n<th id=\"S4.T3.2.2.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T3.2.2.5.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em></th>\n<th id=\"S4.T3.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<em id=\"S4.T3.1.1.1.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em><math id=\"S4.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.m1.1a\"><mo id=\"S4.T3.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.m1.1c\">\\dagger</annotation></semantics></math>\n</th>\n<th id=\"S4.T3.2.2.6\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T3.2.2.6.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em></th>\n<th id=\"S4.T3.2.2.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<em id=\"S4.T3.2.2.2.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em><math id=\"S4.T3.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\dagger\" display=\"inline\"><semantics id=\"S4.T3.2.2.2.m1.1a\"><mo id=\"S4.T3.2.2.2.m1.1.1\" xref=\"S4.T3.2.2.2.m1.1.1.cmml\">†</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.2.m1.1b\"><ci id=\"S4.T3.2.2.2.m1.1.1.cmml\" xref=\"S4.T3.2.2.2.m1.1.1\">†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.2.m1.1c\">\\dagger</annotation></semantics></math>\n</th>\n<th id=\"S4.T3.2.2.7\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T3.2.2.7.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">NA</em></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">No Digest</th>\n<td id=\"S4.T3.2.3.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">240</td>\n<td id=\"S4.T3.2.3.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">23.71</td>\n<td id=\"S4.T3.2.3.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">23.89</td>\n<td id=\"S4.T3.2.3.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">4.92</td>\n<td id=\"S4.T3.2.3.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\">11.28</td>\n<td id=\"S4.T3.2.3.1.7\" class=\"ltx_td ltx_align_right ltx_border_t\">59.51</td>\n</tr>\n<tr id=\"S4.T3.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">With Digest</th>\n<td id=\"S4.T3.2.4.2.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">186</td>\n<td id=\"S4.T3.2.4.2.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T3.2.4.2.3.1\" class=\"ltx_text ltx_font_bold\">36.61</span></td>\n<td id=\"S4.T3.2.4.2.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T3.2.4.2.4.1\" class=\"ltx_text ltx_font_bold\">38.35</span></td>\n<td id=\"S4.T3.2.4.2.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T3.2.4.2.5.1\" class=\"ltx_text ltx_font_bold\">9.74</span></td>\n<td id=\"S4.T3.2.4.2.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T3.2.4.2.6.1\" class=\"ltx_text ltx_font_bold\">19.22</span></td>\n<td id=\"S4.T3.2.4.2.7\" class=\"ltx_td ltx_align_right ltx_border_bb\">47.04</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T3.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 3</span>: </span><span id=\"S4.T3.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of Caption Digest, using LaViLa + GPT4.</span></figcaption>\n</figure>\n</section>\n<section id=\"S4.SS1.SSS0.Px5\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">NLQ Implementation Details.</h4>\n\n<div id=\"S4.SS1.SSS0.Px5.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px5.p1.8\" class=\"ltx_p\">To select the optimal candidate from multiple predictions from the LLM, we train a classifier on the NLQ train set. To construct a video dataset similar to the real LLM predictions, we randomly shift and scale the ground-truth temporal windows. Specifically, for a ground-truth interval <math id=\"S4.SS1.SSS0.Px5.p1.1.m1.2\" class=\"ltx_Math\" alttext=\"[s,e]\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.1.m1.2a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.2.1\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.1.cmml\">[</mo><mi id=\"S4.SS1.SSS0.Px5.p1.1.m1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.1.1.cmml\">s</mi><mo id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.1.cmml\">,</mo><mi id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.2.cmml\">e</mi><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.1.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.1.m1.2b\"><interval closure=\"closed\" id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.3.2\"><ci id=\"S4.SS1.SSS0.Px5.p1.1.m1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.1.1\">𝑠</ci><ci id=\"S4.SS1.SSS0.Px5.p1.1.m1.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.1.m1.2.2\">𝑒</ci></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.1.m1.2c\">[s,e]</annotation></semantics></math> with center <math id=\"S4.SS1.SSS0.Px5.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"x=(s+e)/2\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.2.m2.1a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.3.cmml\">x</mi><mo id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.2.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.cmml\"><mrow id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.2.cmml\">s</mi><mo id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.1.cmml\">+</mo><mi id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.3.cmml\">e</mi></mrow><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.cmml\">)</mo></mrow><mo id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.2.cmml\">/</mo><mn id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.3.cmml\">2</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.2.m2.1b\"><apply id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1\"><eq id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.2\"></eq><ci id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.3\">𝑥</ci><apply id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1\"><divide id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.2\"></divide><apply id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1\"><plus id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.1\"></plus><ci id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.2\">𝑠</ci><ci id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.1.1.1.3\">𝑒</ci></apply><cn type=\"integer\" id=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.2.m2.1.1.1.3\">2</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.2.m2.1c\">x=(s+e)/2</annotation></semantics></math> and width <math id=\"S4.SS1.SSS0.Px5.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"w=log(e-s)\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.3.m3.1a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.3.cmml\">w</mi><mo id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.2.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.3.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2.cmml\">​</mo><mi id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.4\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.4.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2a\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2.cmml\">​</mo><mi id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.5\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.5.cmml\">g</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2b\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2.cmml\">​</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.2.cmml\">e</mi><mo id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.1.cmml\">−</mo><mi id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.3.cmml\">s</mi></mrow><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1b\"><apply id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1\"><eq id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.2\"></eq><ci id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.3\">𝑤</ci><apply id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1\"><times id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.2\"></times><ci id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.3\">𝑙</ci><ci id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.4.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.4\">𝑜</ci><ci id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.5.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.5\">𝑔</ci><apply id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1\"><minus id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.1\"></minus><ci id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.3.m3.1.1.1.1.1.1.3\">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.3.m3.1c\">w=log(e-s)</annotation></semantics></math>, we sample a shift ratio <math id=\"S4.SS1.SSS0.Px5.p1.4.m4.2\" class=\"ltx_Math\" alttext=\"\\alpha\\sim\\mathcal{N}(0,0.25)\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.4.m4.2a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.2.cmml\">α</mi><mo id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.1.cmml\">∼</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.2.cmml\">𝒩</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.1.cmml\">​</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.2.1\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.1.cmml\">(</mo><mn id=\"S4.SS1.SSS0.Px5.p1.4.m4.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.1.1.cmml\">0</mn><mo id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.1.cmml\">,</mo><mn id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.2.cmml\">0.25</mn><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.1.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2b\"><apply id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3\"><csymbol cd=\"latexml\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.1\">similar-to</csymbol><ci id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.2\">𝛼</ci><apply id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3\"><times id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.1\"></times><ci id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.2\">𝒩</ci><interval closure=\"open\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.3.3.3.2\"><cn type=\"integer\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.1.1\">0</cn><cn type=\"float\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.4.m4.2.2\">0.25</cn></interval></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.4.m4.2c\">\\alpha\\sim\\mathcal{N}(0,0.25)</annotation></semantics></math> and a scale ratio <math id=\"S4.SS1.SSS0.Px5.p1.5.m5.2\" class=\"ltx_Math\" alttext=\"\\beta\\sim log\\mathcal{N}(0,0.25)\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.5.m5.2a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.2.cmml\">β</mi><mo id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.1.cmml\">∼</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.2.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1.cmml\">​</mo><mi id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1a\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1.cmml\">​</mo><mi id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.4\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.4.cmml\">g</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1b\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1.cmml\">​</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.5\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.5.cmml\">𝒩</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1c\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1.cmml\">​</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.2\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.2.1\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.1.cmml\">(</mo><mn id=\"S4.SS1.SSS0.Px5.p1.5.m5.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.1.1.cmml\">0</mn><mo id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.1.cmml\">,</mo><mn id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.2.cmml\">0.25</mn><mo stretchy=\"false\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.1.cmml\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2b\"><apply id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3\"><csymbol cd=\"latexml\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.1\">similar-to</csymbol><ci id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.2\">𝛽</ci><apply id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3\"><times id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.1\"></times><ci id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.2\">𝑙</ci><ci id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.3\">𝑜</ci><ci id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.4.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.4\">𝑔</ci><ci id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.5.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.5\">𝒩</ci><interval closure=\"open\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.3.3.6.2\"><cn type=\"integer\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.1.1\">0</cn><cn type=\"float\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.5.m5.2.2\">0.25</cn></interval></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.5.m5.2c\">\\beta\\sim log\\mathcal{N}(0,0.25)</annotation></semantics></math> and obtain an augmented dataset consisting of <math id=\"S4.SS1.SSS0.Px5.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"w^{\\prime}_{i}=w/\\beta\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.6.m6.1a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.cmml\"><msubsup id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.2.cmml\">w</mi><mi id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.3.cmml\">i</mi><mo id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.3.cmml\">′</mo></msubsup><mo id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.1.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.2.cmml\">w</mi><mo id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.1.cmml\">/</mo><mi id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.3.cmml\">β</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.6.m6.1b\"><apply id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1\"><eq id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.1\"></eq><apply id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2\">subscript</csymbol><apply id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2\">superscript</csymbol><ci id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.2\">𝑤</ci><ci id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.2.3\">′</ci></apply><ci id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.2.3\">𝑖</ci></apply><apply id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3\"><divide id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.1\"></divide><ci id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.2\">𝑤</ci><ci id=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.6.m6.1.1.3.3\">𝛽</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.6.m6.1c\">w^{\\prime}_{i}=w/\\beta</annotation></semantics></math> and <math id=\"S4.SS1.SSS0.Px5.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"x^{\\prime}_{i}=x-\\alpha*w^{\\prime}\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.7.m7.1a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.cmml\"><msubsup id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.2.cmml\">x</mi><mi id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.3.cmml\">i</mi><mo id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.3\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.3.cmml\">′</mo></msubsup><mo id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.1.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.2.cmml\">x</mi><mo id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.1.cmml\">−</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.2.cmml\">α</mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.1\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.1.cmml\">∗</mo><msup id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.2.cmml\">w</mi><mo id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.3.cmml\">′</mo></msup></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.7.m7.1b\"><apply id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1\"><eq id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.1\"></eq><apply id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2\">subscript</csymbol><apply id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2\">superscript</csymbol><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.2\">𝑥</ci><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.2.3\">′</ci></apply><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.2.3\">𝑖</ci></apply><apply id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3\"><minus id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.1\"></minus><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.2\">𝑥</ci><apply id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3\"><times id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.1\"></times><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.2\">𝛼</ci><apply id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3\">superscript</csymbol><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.2\">𝑤</ci><ci id=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.7.m7.1.1.3.3.3.3\">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.7.m7.1c\">x^{\\prime}_{i}=x-\\alpha*w^{\\prime}</annotation></semantics></math> for <math id=\"S4.SS1.SSS0.Px5.p1.8.m8.3\" class=\"ltx_Math\" alttext=\"i=1,\\dots,n\" display=\"inline\"><semantics id=\"S4.SS1.SSS0.Px5.p1.8.m8.3a\"><mrow id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.cmml\"><mi id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.2\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.2.cmml\">i</mi><mo id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.1\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.1.cmml\">=</mo><mrow id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.2\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.1.cmml\"><mn id=\"S4.SS1.SSS0.Px5.p1.8.m8.1.1\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.1.1.cmml\">1</mn><mo id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.2.1\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.1.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S4.SS1.SSS0.Px5.p1.8.m8.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.2.2.cmml\">…</mi><mo id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.2.2\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.1.cmml\">,</mo><mi id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.3\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.3.cmml\">n</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.SSS0.Px5.p1.8.m8.3b\"><apply id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4\"><eq id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.1\"></eq><ci id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.2\">𝑖</ci><list id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.4.3.2\"><cn type=\"integer\" id=\"S4.SS1.SSS0.Px5.p1.8.m8.1.1.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.1.1\">1</cn><ci id=\"S4.SS1.SSS0.Px5.p1.8.m8.2.2.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.2.2\">…</ci><ci id=\"S4.SS1.SSS0.Px5.p1.8.m8.3.3.cmml\" xref=\"S4.SS1.SSS0.Px5.p1.8.m8.3.3\">𝑛</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.SSS0.Px5.p1.8.m8.3c\">i=1,\\dots,n</annotation></semantics></math>. In this way, we obtain augmented annotations and mark those intervals that have IOU with the ground truth larger than 0.5 as positives, while we randomly sample from the rest of the video for negatives that have IOU less than 0.1 with the ground truth to construct a balanced dataset. We utilize video features encoded by InternVideo <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>]</cite> and EgoVLP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>, which are video understanding models pretrained on Ego4D datasets, and adapt the span-based localization network VSLNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">41</span></a>]</cite> to this video classification task, where we replace the localization head with a classification head. We use cross entropy as the training loss and train the classifier for 100 epochs while selecting the model with the highest validation classification accuracy.</p>\n</div>\n<div id=\"S4.SS1.SSS0.Px5.p2\" class=\"ltx_para\">\n<p id=\"S4.SS1.SSS0.Px5.p2.1\" class=\"ltx_p\">After obtaining the optimal candidate temporal windows, we extended them by a window size of 5 seconds to provide more context to the NLQ model and then feed them into the state-of-the-art NLQ model, NaQ++ReLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite>, which is finetuned on the Ego4D NLQ dataset. This gives us fine-grained predicted temporal windows that can reflect the answers to the target queries.</p>\n</div>\n</section>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Main Results</h3>\n\n<figure id=\"S4.F4\" class=\"ltx_figure\"><img src=\"./assets/x4.png\" id=\"S4.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"380\" height=\"467\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span id=\"S4.F4.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Qualitative visualizations. Captions are produced by LaViLa and GPT-4 is used as the LLM engine. LLM interval prediction denoted as <span id=\"S4.F4.5.2.1\" class=\"ltx_text\" style=\"color:#FF8000;\">orange</span> boxes and the ground truth in <span id=\"S4.F4.5.2.2\" class=\"ltx_text\" style=\"color:#00FF00;\">green</span>.</span></figcaption>\n</figure>\n<section id=\"S4.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Captioning Model Choices.</h4>\n\n<div id=\"S4.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px1.p1.1\" class=\"ltx_p\">We compare the performance of LaViLa and LLaVA in our main results in  <a href=\"#S4.T1\" title=\"In 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>. Although LLaVA generates longer captions conditioned on the queries, the performance of LaViLa is significantly better than LLaVA. This indicates the necessity of adopting an egocentric captioning model that focuses on the core activity of the individual. Despite the effectiveness of LaViLa in this task, we identify that LaViLa tends to generate false positive captions as it is finetuned on Ego4D data. We thus evaluate the ground-truth captions provided by the Ego4D Narrations data and observe that it achieves the best performance with significantly fewer captions. This experiment confirms our assumption that an accurate well-crafted set of captions can effectively summarize the information of the camera wearer’s activity in egocentric videos.</p>\n</div>\n</section>\n<section id=\"S4.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Digest.</h4>\n\n<div id=\"S4.SS2.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px2.p1.1\" class=\"ltx_p\">We evaluate the effect of caption digest in <a href=\"#S4.T3\" title=\"In Caption Digest. ‣ 4.1 Experiment Setup ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">3</span></a>. “No Digest” means that we input the raw captions generated from LaViLa into the LLM without any preprocessing steps. “With Digest” represents that we perform the full preprocessing steps as in <a href=\"#S4.SS1\" title=\"4.1 Experiment Setup ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span> <span class=\"ltx_text ltx_ref_tag\">4.1</span></a> where we discard ambiguous and irrelevant captions and use LLM to merge similar ones. This technique significantly improves both metrics by around 10%, indicating that a concise context leads to much better retrieval capability of LLM.</p>\n</div>\n</section>\n<section id=\"S4.SS2.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Caption Sampling Interval.</h4>\n\n<div id=\"S4.SS2.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px3.p1.1\" class=\"ltx_p\">Given the same captioning models and preprocessing process, denser captions lead to higher performance as they provide a richer context for the LLM to scan and digest. Since the response windows are very short (around 9 seconds on average), coarse-grained captioning will result in a loss of information. We test the LaViLa captions that are sampled every 4 seconds and 2 seconds and observe 3% gain in both <em id=\"S4.SS2.SSS0.Px3.p1.1.1\" class=\"ltx_emph ltx_font_italic\">Overlap</em> and <em id=\"S4.SS2.SSS0.Px3.p1.1.2\" class=\"ltx_emph ltx_font_italic\">IoU*@0.3</em> scores, as in  <a href=\"#S4.T2\" title=\"In Caption Digest. ‣ 4.1 Experiment Setup ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n</div>\n</section>\n<section id=\"S4.SS2.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Choices.</h4>\n\n<div id=\"S4.SS2.SSS0.Px4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px4.p1.1\" class=\"ltx_p\">We compare the performance of GPT4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> (<span id=\"S4.SS2.SSS0.Px4.p1.1.1\" class=\"ltx_text ltx_font_smallcaps\">gpt-4-32k</span>) and GPT3.5 (<span id=\"S4.SS2.SSS0.Px4.p1.1.2\" class=\"ltx_text ltx_font_smallcaps\">gpt-3.5-turbo-16k</span>) for different captions. We observe that GPT4 outperforms GPT3.5 significantly in terms of retrieval accuracy. GPT4 is also better than GPT3.5 in producing numerical intervals with other required information in the TSV format as specified in the prompt. GPT3.5 frequently misunderstands the task or outputs predictions in the wrong format, leading to a high NA ratio. We also tried to use open-source LLMs such as LLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">31</span></a>, <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">32</span></a>]</cite> and Vicuna <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>]</cite>, but they fail to provide any meaningful predictions.</p>\n</div>\n</section>\n<section id=\"S4.SS2.SSS0.Px5\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Explanation.</h4>\n\n<div id=\"S4.SS2.SSS0.Px5.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px5.p1.1\" class=\"ltx_p\">We also experiment with different prompts. To encourage LLM reasoning step by step, we provide detailed instructions on how to retrieve the temporal windows and explicitly ask it to provide an explanation for its prediction. This prompt enhancement improves the <em id=\"S4.SS2.SSS0.Px5.p1.1.1\" class=\"ltx_emph ltx_font_italic\">Overlap</em> score by around 4% and <em id=\"S4.SS2.SSS0.Px5.p1.1.2\" class=\"ltx_emph ltx_font_italic\">IoU*@0.3</em> by around 1% (See <a href=\"#S4.T4\" title=\"In Ego4D NLQ Benchmark. ‣ 4.2 Main Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>), and also increases the interpretability of using LLM for the NLQ task in real-world applications.</p>\n</div>\n</section>\n<section id=\"S4.SS2.SSS0.Px6\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Confidence Level. </h4>\n\n<div id=\"S4.SS2.SSS0.Px6.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px6.p1.1\" class=\"ltx_p\">To further boost the accuracy of LLM prediction, we also explicitly ask LLM to add a confidence level to its predictions. We report the accuracy of all confidence levels and the accuracy of the most confident predictions in  <a href=\"#S4.T5\" title=\"In Ego4D NLQ Benchmark. ‣ 4.2 Main Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a> and the relationship between scores and confidence level in  <a href=\"#S4.T5\" title=\"In Ego4D NLQ Benchmark. ‣ 4.2 Main Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>. The increase in confidence scores leads to an increase in accuracy, which further illustrates the reasoning capability of LLM.</p>\n</div>\n</section>\n<section id=\"S4.SS2.SSS0.Px7\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Ego4D NLQ Benchmark.</h4>\n\n<div id=\"S4.SS2.SSS0.Px7.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.SSS0.Px7.p1.1\" class=\"ltx_p\">In <a href=\"#S4.T6\" title=\"In Ego4D NLQ Benchmark. ‣ 4.2 Main Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">6</span></a>, we compare the performance of our method and two other competitive methods on the Ego4D NLQ benchmark: NaQ++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite>, the official baseline for the CVPR 2023 Ego4D NLQ challenge, and GroundNLQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>, the winner of the CVPR 2023 Ego4D NLQ challenge. While our method is not as good as the current state-of-the-art, it outperforms NaQ++ on the official test set (hosted on EvalAI). We also compare the performance of our method on the validation set with LaViLa captions and Ego4D narrations respectively. With LaViLa captions the performance is not as good as the NaQ++ baseline, but the performance surpasses NaQ++ when we use the human-annotated narrations in the Ego4D dataset. This suggests the potential of our method for future improvement as we plug in better captioning models.</p>\n</div>\n<figure id=\"S4.T4\" class=\"ltx_table\">\n<table id=\"S4.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T4.2.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T4.2.1.1.2.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em></th>\n<th id=\"S4.T4.2.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T4.2.1.1.3.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em></th>\n<th id=\"S4.T4.2.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T4.2.1.1.4.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">NA</em></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">No Explanation</th>\n<td id=\"S4.T4.2.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">32.73</td>\n<td id=\"S4.T4.2.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">8.65</td>\n<td id=\"S4.T4.2.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">44.58</td>\n</tr>\n<tr id=\"S4.T4.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">With Explanation</th>\n<td id=\"S4.T4.2.3.2.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T4.2.3.2.2.1\" class=\"ltx_text ltx_font_bold\">36.61</span></td>\n<td id=\"S4.T4.2.3.2.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T4.2.3.2.3.1\" class=\"ltx_text ltx_font_bold\">9.74</span></td>\n<td id=\"S4.T4.2.3.2.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">47.04</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 4</span>: </span><span id=\"S4.T4.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of LLM Explanation, using LaViLa + GPT4.</span></figcaption>\n</figure>\n<figure id=\"S4.T5\" class=\"ltx_table\">\n<table id=\"S4.T5.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T5.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T5.3.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Conf. Level</span></th>\n<th id=\"S4.T5.3.4.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T5.3.4.1.2.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">Overlap</em></th>\n<th id=\"S4.T5.3.4.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><em id=\"S4.T5.3.4.1.3.1\" class=\"ltx_emph ltx_font_bold ltx_font_italic\">IoU*@0.3</em></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<math id=\"S4.T5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\geq\" display=\"inline\"><semantics id=\"S4.T5.1.1.1.m1.1a\"><mo id=\"S4.T5.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.m1.1b\"><geq id=\"S4.T5.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.m1.1c\">\\geq</annotation></semantics></math> 1</th>\n<td id=\"S4.T5.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">36.46</td>\n<td id=\"S4.T5.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">9.63</td>\n</tr>\n<tr id=\"S4.T5.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T5.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<math id=\"S4.T5.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\geq\" display=\"inline\"><semantics id=\"S4.T5.2.2.1.m1.1a\"><mo id=\"S4.T5.2.2.1.m1.1.1\" xref=\"S4.T5.2.2.1.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.2.2.1.m1.1b\"><geq id=\"S4.T5.2.2.1.m1.1.1.cmml\" xref=\"S4.T5.2.2.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.2.2.1.m1.1c\">\\geq</annotation></semantics></math> 2</th>\n<td id=\"S4.T5.2.2.2\" class=\"ltx_td ltx_align_right\">36.49</td>\n<td id=\"S4.T5.2.2.3\" class=\"ltx_td ltx_align_right\">17.52</td>\n</tr>\n<tr id=\"S4.T5.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T5.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<math id=\"S4.T5.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\geq\" display=\"inline\"><semantics id=\"S4.T5.3.3.1.m1.1a\"><mo id=\"S4.T5.3.3.1.m1.1.1\" xref=\"S4.T5.3.3.1.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.3.3.1.m1.1b\"><geq id=\"S4.T5.3.3.1.m1.1.1.cmml\" xref=\"S4.T5.3.3.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.3.3.1.m1.1c\">\\geq</annotation></semantics></math> 3</th>\n<td id=\"S4.T5.3.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T5.3.3.2.1\" class=\"ltx_text ltx_font_bold\">38.20</span></td>\n<td id=\"S4.T5.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T5.3.3.3.1\" class=\"ltx_text ltx_font_bold\">19.06</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 5</span>: </span><span id=\"S4.T5.6.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of confidence level filtering, using LaViLa + GPT4.</span></figcaption>\n</figure>\n<figure id=\"S4.T6\" class=\"ltx_table\">\n<div id=\"S4.T6.2\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:216.8pt;height:79pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-89.3pt,32.5pt) scale(0.548364411482136,0.548364411482136) ;\">\n<table id=\"S4.T6.2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T6.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S4.T6.2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T6.2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Set</span></th>\n<th id=\"S4.T6.2.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T6.2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Mean r@1</span></th>\n<th id=\"S4.T6.2.1.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T6.2.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">r@1</span></th>\n<th id=\"S4.T6.2.1.1.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T6.2.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">r@1</span></th>\n</tr>\n<tr id=\"S4.T6.2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"></th>\n<th id=\"S4.T6.2.1.2.2.2\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n<th id=\"S4.T6.2.1.2.2.3\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n<th id=\"S4.T6.2.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column\">IoU=0.3</th>\n<th id=\"S4.T6.2.1.2.2.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column\">IoU=0.5</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.2.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">NaQ++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>\n</th>\n<td id=\"S4.T6.2.1.3.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">val</td>\n<td id=\"S4.T6.2.1.3.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">20.20</td>\n<td id=\"S4.T6.2.1.3.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">25.00</td>\n<td id=\"S4.T6.2.1.3.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">15.40</td>\n</tr>\n<tr id=\"S4.T6.2.1.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours (LaViLa)</th>\n<td id=\"S4.T6.2.1.4.2.2\" class=\"ltx_td ltx_align_right\">val</td>\n<td id=\"S4.T6.2.1.4.2.3\" class=\"ltx_td ltx_align_right\">19.00</td>\n<td id=\"S4.T6.2.1.4.2.4\" class=\"ltx_td ltx_align_right\">23.40</td>\n<td id=\"S4.T6.2.1.4.2.5\" class=\"ltx_td ltx_align_right\">14.61</td>\n</tr>\n<tr id=\"S4.T6.2.1.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours (Ego4D)</th>\n<td id=\"S4.T6.2.1.5.3.2\" class=\"ltx_td ltx_align_right\">val</td>\n<td id=\"S4.T6.2.1.5.3.3\" class=\"ltx_td ltx_align_right\"><span id=\"S4.T6.2.1.5.3.3.1\" class=\"ltx_text ltx_font_bold\">21.09</span></td>\n<td id=\"S4.T6.2.1.5.3.4\" class=\"ltx_td ltx_align_right\"><span id=\"S4.T6.2.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">26.12</span></td>\n<td id=\"S4.T6.2.1.5.3.5\" class=\"ltx_td ltx_align_right\"><span id=\"S4.T6.2.1.5.3.5.1\" class=\"ltx_text ltx_font_bold\">16.06</span></td>\n</tr>\n<tr id=\"S4.T6.2.1.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">NaQ++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>\n</th>\n<td id=\"S4.T6.2.1.6.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\">test</td>\n<td id=\"S4.T6.2.1.6.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\">17.67</td>\n<td id=\"S4.T6.2.1.6.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\">21.70</td>\n<td id=\"S4.T6.2.1.6.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\">13.64</td>\n</tr>\n<tr id=\"S4.T6.2.1.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours (LaViLa)</th>\n<td id=\"S4.T6.2.1.7.5.2\" class=\"ltx_td ltx_align_right\">test</td>\n<td id=\"S4.T6.2.1.7.5.3\" class=\"ltx_td ltx_align_right\">18.06</td>\n<td id=\"S4.T6.2.1.7.5.4\" class=\"ltx_td ltx_align_right\">22.28</td>\n<td id=\"S4.T6.2.1.7.5.5\" class=\"ltx_td ltx_align_right\">13.84</td>\n</tr>\n<tr id=\"S4.T6.2.1.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">GroundNLQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>\n</th>\n<td id=\"S4.T6.2.1.8.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">test</td>\n<td id=\"S4.T6.2.1.8.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T6.2.1.8.6.3.1\" class=\"ltx_text ltx_font_bold\">21.93</span></td>\n<td id=\"S4.T6.2.1.8.6.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T6.2.1.8.6.4.1\" class=\"ltx_text ltx_font_bold\">25.67</span></td>\n<td id=\"S4.T6.2.1.8.6.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S4.T6.2.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">18.18</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T6.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 6</span>: </span><span id=\"S4.T6.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Ego4D NLQ benchmark results. We use GPT4 as the reasoning LLM in all of these experiments.</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Qualitative Results</h3>\n\n<div id=\"S4.SS3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.p1.1\" class=\"ltx_p\">We visualize our NLQ examples in Figure <a href=\"#S4.F4\" title=\"Figure 4 ‣ 4.2 Main Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We did not use interval refinement and we show that many LLM predictions have high-quality overlaps with the ground-truth window. On the third row, our method has produced correct reasoning localizing the Jenga blocks which is not part of the ground truth. We note that many successful retrievals rely on a good high-quality caption and we expect there can be large room for future improvement in cases where the target objects are misdetected in current captions.</p>\n</div>\n</section>\n<section id=\"S4.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Limitations</h3>\n\n<div id=\"S4.SS4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS4.p1.1\" class=\"ltx_p\">In this section, we analyze the bottleneck of each stage in the pipeline. First, there is inevitable information loss in the captioning stage where we convert long video inputs into a concise textual log. For LaViLa, the captioning stage is independent of the target queries and thus the generated captions might discard details that can be potentially helpful for answering the queries. However, query-conditioned captions generated by LLaVA introduce false positives and thus decrease the retrieval precision. The outstanding performance of the ground-truth captions from Ego4D narrations suggests that an in-domain captioning model can potentially boost the performance. For the LLM stage, we identify that LLM tends to only select the intervals that contain the key events instead of capturing the whole context, which makes the refinement NLQ model in the subsequent stage unable to process the complete answer window. Lastly, we note that the pretrained NLQ model only obtains around 50% r@1 IoU=0.3 for short video inputs that overlap with ground truth. We expect a more powerful video localization model can result in higher NLQ scores.</p>\n</div>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para\">\n<p id=\"S5.p1.1\" class=\"ltx_p\">We propose LifelongMemory, a novel framework that leverages pre-trained LLMs for answering natural language queries in egocentric videos.\nSpecifically, it first converts egocentric videos into textual captions, then prompts an LLM for coarse-grained predictions, and finally uses a pre-trained NLQ model to refine the predictions. Our framework is flexible and we expect it to achieve better performance over time by plugging in updated captioning, LLM, and NLQ modules.\nExperiments show that our proposed method achieves competitive performance on the Ego4D NLQ benchmark compared to the supervised learning baselines. When using the Ego4D narrations, our method achieves much better performance both in terms of overlapping percentage between the predictions and the ground truth temporal windows, and the benchmark results on the Ego4D NLQ validation dataset. These results demonstrate our method’s strong ability in retrieving relevant video segments from comprehensive video captions, and great potential for enhanced performance with better captioning models and LLMs in the future. We also experiment with different design and hyperparameter choices, which produces interesting insights for LLM practitioners.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para\">\n<p id=\"S5.p2.1\" class=\"ltx_p\">Our work is an important proof-of-concept in leveraging pre-trained MLLMs for egocentric video understanding, with a focus on the NLQ task. The proposed framework is a promising direction for solving other egocentric video understanding tasks, including moment queries, action anticipation, object interaction anticipation, among others. A major limitation of our method is that the captions might not include all relevant information present in the video frame. While we can enforce the captioning model to generate multiple captions with a high temperature to alleviate a problem, it increases the cost of running the captioning model and reduces the number of frames the LLM can digest within a limited context window. Despite this limitation, we are able to achieve reasonable performance on the Ego4D dataset which includes very complex and cluttered scenes, thereby making it a competitive baseline for future research in applying LLMs to zero-shot video understanding tasks.</p>\n</div>\n</section>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgement</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">We would like to thank the Microsoft Accelerating Foundation Models Research program for providing cloud compute credits for running some parts of our LLM experiments. This work was also supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.\n\n\n<span id=\"Sx1.p1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"></span></p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\" style=\"font-size:90%;\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib1.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Alayrac et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib1.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Flamingo: a visual language model for few-shot learning.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib1.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Advances in Neural Information Processing Systems</em><span id=\"bib.bib1.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 35:23716–23736, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib2.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Brown et al. [2020]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib2.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Language models are few-shot learners.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib2.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Advances in neural information processing systems</em><span id=\"bib.bib2.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 33:1877–1901, 2020.\n</span>\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib3.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Bubeck et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib3.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sparks of artificial general intelligence: Early experiments with gpt-4.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib3.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2303.12712</em><span id=\"bib.bib3.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib4.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Chen et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nGuo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun Huang, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib4.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Internvideo-ego4d: A pack of champion solutions to ego4d challenges.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib4.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2211.09529</em><span id=\"bib.bib4.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib5.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Chen et al. [2023a]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nGuo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib5.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Videollm: Modeling video sequence with large language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib5.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2305.13292</em><span id=\"bib.bib5.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023a.\n</span>\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib6.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Chen et al. [2023b]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nLiangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib6.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Large language models are visual reasoning coordinators.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib6.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2310.15166</em><span id=\"bib.bib6.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023b.\n</span>\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib7.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Chiang et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib7.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib8.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Chowdhery et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib8.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib8.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Palm: Scaling language modeling with pathways.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib8.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2204.02311</em><span id=\"bib.bib8.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib9.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Driess et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib9.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib9.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Palm-e: An embodied multimodal language model.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib9.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2303.03378</em><span id=\"bib.bib9.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib10.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Du et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib10.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nYilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B Tenenbaum, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib10.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Video language planning.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib10.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2310.10625</em><span id=\"bib.bib10.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib11.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Grauman et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib11.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib11.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Ego4d: Around the world in 3,000 hours of egocentric video.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib11.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib11.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id=\"bib.bib11.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 18995–19012, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib12.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Hou et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib12.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nZhijian Hou, Wanjun Zhong, Lei Ji, Difei Gao, Kun Yan, Wing-Kwong Chan, Chong-Wah Ngo, Zheng Shou, and Nan Duan.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib12.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cone: An efficient coarse-to-fine alignment framework for long video temporal grounding.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib12.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2209.10918</em><span id=\"bib.bib12.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib13.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Hou et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib13.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nZhijian Hou, Lei Ji, Difei Gao, Wanjun Zhong, Kun Yan, Chao Li, Wing-Kwong Chan, Chong-Wah Ngo, Nan Duan, and Mike Zheng Shou.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib13.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Groundnlq @ ego4d natural language queries challenge 2023, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib14.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Lei et al. [2021]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib14.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib14.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Less is more: Clipbert for video-and-language learning via sparse sampling.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib14.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib14.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id=\"bib.bib14.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 7331–7341, 2021.\n</span>\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib15.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Li et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib15.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib15.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib15.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib15.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">International Conference on Machine Learning</em><span id=\"bib.bib15.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 12888–12900. PMLR, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib16.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Li et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib16.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib16.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib16.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2301.12597</em><span id=\"bib.bib16.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib17.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Lin et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib17.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nKevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib17.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Egocentric video-language pretraining@ ego4d challenge 2022.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib17.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2207.01622</em><span id=\"bib.bib17.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib18.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Liu et al. [2023a]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib18.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib18.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Improved baselines with visual instruction tuning.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib18.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2310.03744</em><span id=\"bib.bib18.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023a.\n</span>\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib19.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Liu et al. [2023b]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib19.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib19.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Visual instruction tuning.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib19.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">2023b.\n</span>\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib20.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Liu [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib20.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJerry Liu.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib20.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">LlamaIndex, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib21.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Liu et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib21.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nNaiyuan Liu, Xiaohan Wang, Xiaobo Li, Yi Yang, and Yueting Zhuang.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib21.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Reler@ zju-alibaba submission to the ego4d natural language queries challenge 2022.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib21.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2207.00383</em><span id=\"bib.bib21.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib22.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Ma et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib22.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nChaofan Ma, Yuhuan Yang, Yanfeng Wang, Ya Zhang, and Weidi Xie.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib22.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Open-vocabulary semantic segmentation with frozen vision-language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib22.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2210.15138</em><span id=\"bib.bib22.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib23.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">OpenAI [2023a]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib23.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nOpenAI.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib23.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Gpt-4 technical report, 2023a.\n</span>\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib24.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">OpenAI [2023b]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib24.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nOpenAI.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib24.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Gpt-4v(ision) system card, 2023b.\n</span>\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib25.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Pan et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib25.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJunting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi Wang, Yu Qiao, and Hongsheng Li.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib25.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Retrieving-to-answer: Zero-shot video question answering with frozen large language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib25.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2306.11732</em><span id=\"bib.bib25.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib26.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Qinghong Lin et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib26.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nKevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib26.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Egocentric video-language pretraining.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib26.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv e-prints</em><span id=\"bib.bib26.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, pages arXiv–2206, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib27.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Radford et al. [2021]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib27.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib27.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Learning transferable visual models from natural language supervision.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib27.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib27.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">International conference on machine learning</em><span id=\"bib.bib27.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 8748–8763. PMLR, 2021.\n</span>\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib28.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Ramakrishnan et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib28.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nSanthosh Kumar Ramakrishnan, Ziad Al-Halah, and Kristen Grauman.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib28.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Naq: Leveraging narrations as queries to supervise episodic memory.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib28.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2301.00746</em><span id=\"bib.bib28.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib29.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Shao et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib29.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJiayi Shao, Xiaohan Wang, Ruijie Quan, and Yi Yang.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib29.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Action sensitivity learning for the ego4d episodic memory challenge 2023, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib30.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Tong et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib30.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib30.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib30.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2203.12602</em><span id=\"bib.bib30.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib31.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Touvron et al. [2023a]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib31.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib31.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Llama: Open and efficient foundation language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib31.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2302.13971</em><span id=\"bib.bib31.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023a.\n</span>\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib32.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Touvron et al. [2023b]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib32.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib32.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Llama 2: Open foundation and fine-tuned chat models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib32.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2307.09288</em><span id=\"bib.bib32.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023b.\n</span>\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib33.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Wang et al. [2022a]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib33.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib33.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Git: A generative image-to-text transformer for vision and language, 2022a.\n</span>\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib34.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Wang et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib34.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nShijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and Chen Sun.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib34.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Vamos: Versatile action models for video understanding, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib35.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Wang et al. [2022b]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib35.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nZhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib35.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Language models with image descriptors are strong few-shot video-language learners.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib35.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Advances in Neural Information Processing Systems</em><span id=\"bib.bib35.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 35:8483–8497, 2022b.\n</span>\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib36.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Xiong et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib36.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib36.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib36.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2306.13063</em><span id=\"bib.bib36.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib37.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Yang et al. [2021]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib37.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib37.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Just ask: Learning to answer questions from millions of narrated videos.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib37.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib37.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Proceedings of the IEEE/CVF international conference on computer vision</em><span id=\"bib.bib37.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 1686–1697, 2021.\n</span>\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib38.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Yang et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib38.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib38.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot video question answering via frozen bidirectional language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib38.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Advances in Neural Information Processing Systems</em><span id=\"bib.bib38.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 35:124–141, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib39.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Yang et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib39.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib39.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Mm-react: Prompting chatgpt for multimodal reasoning and action.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib39.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2303.11381</em><span id=\"bib.bib39.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib40.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zeng et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib40.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib40.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Socratic models: Composing zero-shot multimodal reasoning with language.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib40.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv</em><span id=\"bib.bib40.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib41.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zhang et al. [2020]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib41.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nHao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib41.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Span-based localizing network for natural language video localization.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib41.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib41.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span id=\"bib.bib41.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 6543–6554, Online, 2020. Association for Computational Linguistics.\n</span>\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib42.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zhang et al. [2022]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib42.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib42.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Opt: Open pre-trained transformer language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib42.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2205.01068</em><span id=\"bib.bib42.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2022.\n</span>\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib43.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zhang et al. [2023]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib43.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib43.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Multimodal chain-of-thought reasoning in language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib43.9.1\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">arXiv preprint arXiv:2302.00923</em><span id=\"bib.bib43.10.2\" class=\"ltx_text\" style=\"font-size:90%;\">, 2023.\n</span>\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib44.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zhao et al. [2023a]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib44.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nXufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib44.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Chat with the environment: Interactive multimodal perception using large language models, 2023a.\n</span>\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\"><span id=\"bib.bib45.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zhao et al. [2023b]</span></span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib45.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">\nYue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Girdhar.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib45.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Learning video representations from large language models.\n</span>\n</span>\n<span class=\"ltx_bibblock\"><span id=\"bib.bib45.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">In </span><em id=\"bib.bib45.10.2\" class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id=\"bib.bib45.11.3\" class=\"ltx_text\" style=\"font-size:90%;\">, pages 6586–6597, 2023b.\n</span>\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Prompting</h2>\n\n<div id=\"A1.p1\" class=\"ltx_para\">\n<p id=\"A1.p1.1\" class=\"ltx_p\">We provide the complete prompt in <a href=\"#A1.F5\" title=\"In Appendix A Prompting ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">5</span></a>. To reduce the cost of API calls, we avoid passing the same caption list multiple times by including all queries of the same clip in the prompt. We provide an instructive prompt with detailed steps and ask the LLM to produce responses in the TSV format to expedite post-processing. We observe that GPT4 is capable of following the correct format, whereas GPT3.5 frequently generates erroneously formatted outputs which leads to a high NA rate as shown in <a href=\"#S4.T1\" title=\"In 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<figure id=\"A1.F5\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"A1.F5.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"528\" height=\"430\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A1.F5.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span id=\"A1.F5.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">System prompt and user prompt. The text in <span id=\"A1.F5.4.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">blue</span> should be replaced by the queries and captions in the video clip.</span></figcaption>\n</figure>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Comparison of Methods on Different NLQ Query Templates</h2>\n\n<div id=\"A2.p1\" class=\"ltx_para\">\n<p id=\"A2.p1.1\" class=\"ltx_p\">To further understand the strengths and weaknesses of LLMs in the NLQ task, we evaluate the performance by different query templates. As shown in <a href=\"#A2.T7\" title=\"In Appendix B Comparison of Methods on Different NLQ Query Templates ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Table</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a>, our method generally performs better on queries associated with short temporal windows, such as <em id=\"A2.p1.1.1\" class=\"ltx_emph ltx_font_italic\">“Objects: Where is object X before / after event Y?”</em>(6.96s), <em id=\"A2.p1.1.2\" class=\"ltx_emph ltx_font_italic\">“Place: Where did I put X? ”</em>(8.05s) and <em id=\"A2.p1.1.3\" class=\"ltx_emph ltx_font_italic\">“Objects: State of an object”</em>(8.17s), whereas extracting such short intervals from an eight-minute video is usually challenging for traditional video localization methods like NaQ++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite>. This confirms our hypothesis that we can leverage LLM’s remarkable contextual understanding capabilities to extract critical details from a long context.</p>\n</div>\n<figure id=\"A2.T7\" class=\"ltx_table\">\n<div id=\"A2.T7.2\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:496.9pt;height:157.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-63.7pt,20.2pt) scale(0.796029923730721,0.796029923730721) ;\">\n<table id=\"A2.T7.2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T7.2.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T7.2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A2.T7.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Template</span></th>\n<th id=\"A2.T7.2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T7.2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Ours(LaViLa)</span></th>\n<th id=\"A2.T7.2.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T7.2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Ours(Ego4D)</span></th>\n<th id=\"A2.T7.2.1.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A2.T7.2.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">NaQ++<cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T7.2.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_medium\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a><span id=\"A2.T7.2.1.1.1.4.1.2.2\" class=\"ltx_text ltx_font_medium\">]</span></cite></span></th>\n<th id=\"A2.T7.2.1.1.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T7.2.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Count</span></th>\n<th id=\"A2.T7.2.1.1.1.6\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T7.2.1.1.1.6.1\" class=\"ltx_text ltx_font_bold\">Avg Length</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T7.2.1.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Objects: Where is object X before / after event Y?</td>\n<td id=\"A2.T7.2.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">24.47</td>\n<td id=\"A2.T7.2.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"A2.T7.2.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">27.47</span></td>\n<td id=\"A2.T7.2.1.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">25.07</td>\n<td id=\"A2.T7.2.1.2.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">750</td>\n<td id=\"A2.T7.2.1.2.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\">6.96</td>\n</tr>\n<tr id=\"A2.T7.2.1.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Place: Where did I put X?</td>\n<td id=\"A2.T7.2.1.3.2.2\" class=\"ltx_td ltx_align_right\">18.69</td>\n<td id=\"A2.T7.2.1.3.2.3\" class=\"ltx_td ltx_align_right\"><span id=\"A2.T7.2.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">23.59</span></td>\n<td id=\"A2.T7.2.1.3.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\">20.90</td>\n<td id=\"A2.T7.2.1.3.2.5\" class=\"ltx_td ltx_align_right\">725</td>\n<td id=\"A2.T7.2.1.3.2.6\" class=\"ltx_td ltx_align_right\">8.05</td>\n</tr>\n<tr id=\"A2.T7.2.1.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: Where is object X?</td>\n<td id=\"A2.T7.2.1.4.3.2\" class=\"ltx_td ltx_align_right\">8.70</td>\n<td id=\"A2.T7.2.1.4.3.3\" class=\"ltx_td ltx_align_right\"><span id=\"A2.T7.2.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">9.24</span></td>\n<td id=\"A2.T7.2.1.4.3.4\" class=\"ltx_td ltx_align_right ltx_border_r\">8.51</td>\n<td id=\"A2.T7.2.1.4.3.5\" class=\"ltx_td ltx_align_right\">552</td>\n<td id=\"A2.T7.2.1.4.3.6\" class=\"ltx_td ltx_align_right\">11.43</td>\n</tr>\n<tr id=\"A2.T7.2.1.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: What did I put in X?</td>\n<td id=\"A2.T7.2.1.5.4.2\" class=\"ltx_td ltx_align_right\">22.16</td>\n<td id=\"A2.T7.2.1.5.4.3\" class=\"ltx_td ltx_align_right\">22.25</td>\n<td id=\"A2.T7.2.1.5.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"A2.T7.2.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">23.26</span></td>\n<td id=\"A2.T7.2.1.5.4.5\" class=\"ltx_td ltx_align_right\">546</td>\n<td id=\"A2.T7.2.1.5.4.6\" class=\"ltx_td ltx_align_right\">10.99</td>\n</tr>\n<tr id=\"A2.T7.2.1.6.5\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: How many X’s? (quantity question)</td>\n<td id=\"A2.T7.2.1.6.5.2\" class=\"ltx_td ltx_align_right\">26.55</td>\n<td id=\"A2.T7.2.1.6.5.3\" class=\"ltx_td ltx_align_right\">26.17</td>\n<td id=\"A2.T7.2.1.6.5.4\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"A2.T7.2.1.6.5.4.1\" class=\"ltx_text ltx_font_bold\">27.59</span></td>\n<td id=\"A2.T7.2.1.6.5.5\" class=\"ltx_td ltx_align_right\">386</td>\n<td id=\"A2.T7.2.1.6.5.6\" class=\"ltx_td ltx_align_right\">15.54</td>\n</tr>\n<tr id=\"A2.T7.2.1.7.6\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: Objects: In what location did I see object X?</td>\n<td id=\"A2.T7.2.1.7.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"A2.T7.2.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">8.77</span></td>\n<td id=\"A2.T7.2.1.7.6.3\" class=\"ltx_td ltx_align_right\">7.66</td>\n<td id=\"A2.T7.2.1.7.6.4\" class=\"ltx_td ltx_align_right ltx_border_r\">8.50</td>\n<td id=\"A2.T7.2.1.7.6.5\" class=\"ltx_td ltx_align_right\">359</td>\n<td id=\"A2.T7.2.1.7.6.6\" class=\"ltx_td ltx_align_right\">9.15</td>\n</tr>\n<tr id=\"A2.T7.2.1.8.7\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: What X did I Y?</td>\n<td id=\"A2.T7.2.1.8.7.2\" class=\"ltx_td ltx_align_right\">21.57</td>\n<td id=\"A2.T7.2.1.8.7.3\" class=\"ltx_td ltx_align_right\"><span id=\"A2.T7.2.1.8.7.3.1\" class=\"ltx_text ltx_font_bold\">28.57</span></td>\n<td id=\"A2.T7.2.1.8.7.4\" class=\"ltx_td ltx_align_right ltx_border_r\">26.00</td>\n<td id=\"A2.T7.2.1.8.7.5\" class=\"ltx_td ltx_align_right\">350</td>\n<td id=\"A2.T7.2.1.8.7.6\" class=\"ltx_td ltx_align_right\">14.75</td>\n</tr>\n<tr id=\"A2.T7.2.1.9.8\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: What X is Y?</td>\n<td id=\"A2.T7.2.1.9.8.2\" class=\"ltx_td ltx_align_right\">15.81</td>\n<td id=\"A2.T7.2.1.9.8.3\" class=\"ltx_td ltx_align_right\"><span id=\"A2.T7.2.1.9.8.3.1\" class=\"ltx_text ltx_font_bold\">16.27</span></td>\n<td id=\"A2.T7.2.1.9.8.4\" class=\"ltx_td ltx_align_right ltx_border_r\">16.11</td>\n<td id=\"A2.T7.2.1.9.8.5\" class=\"ltx_td ltx_align_right\">332</td>\n<td id=\"A2.T7.2.1.9.8.6\" class=\"ltx_td ltx_align_right\">10.66</td>\n</tr>\n<tr id=\"A2.T7.2.1.10.9\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Objects: State of an object</td>\n<td id=\"A2.T7.2.1.10.9.2\" class=\"ltx_td ltx_align_right\">26.17</td>\n<td id=\"A2.T7.2.1.10.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"A2.T7.2.1.10.9.3.1\" class=\"ltx_text ltx_font_bold\">29.36</span></td>\n<td id=\"A2.T7.2.1.10.9.4\" class=\"ltx_td ltx_align_right ltx_border_r\">27.23</td>\n<td id=\"A2.T7.2.1.10.9.5\" class=\"ltx_td ltx_align_right\">236</td>\n<td id=\"A2.T7.2.1.10.9.6\" class=\"ltx_td ltx_align_right\">8.17</td>\n</tr>\n<tr id=\"A2.T7.2.1.11.10\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">People: Who did I interact with when I did activity X?</td>\n<td id=\"A2.T7.2.1.11.10.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">20.43</td>\n<td id=\"A2.T7.2.1.11.10.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">18.70</td>\n<td id=\"A2.T7.2.1.11.10.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span id=\"A2.T7.2.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\">22.61</span></td>\n<td id=\"A2.T7.2.1.11.10.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">22</td>\n<td id=\"A2.T7.2.1.11.10.6\" class=\"ltx_td ltx_align_right ltx_border_bb\">17.51</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"A2.T7.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Table 7</span>: </span><span id=\"A2.T7.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Ego4D NLQ benchmark results by different templates. The scores are the average of r@1 IoU={0.3,0.5} on the validation set. We use GPT-4 as the reasoning LLM in all of these experiments. The last column represents the average length (in seconds) of the answer windows associated with each query template. The overall average length of answer windows is 9.3 seconds. </span></figcaption>\n</figure>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Preliminary Results on GPT-4V: A comparison of caption and image inputs.</h2>\n\n<div id=\"A3.p1\" class=\"ltx_para\">\n<p id=\"A3.p1.1\" class=\"ltx_p\">Since captions are a high-level abstraction of videos, a large amount of information is lost during the conversion and thus may lead to null or incorrect LLM predictions. On the other hand, a textual summarization of videos can save storage space, allow for longer input, and facilitate the reasoning process. To compare the performance of LLMs with raw video inputs and machine-generated captions, we test the newly released GPT-4V <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite> on the NLQ task. Due to the context length restriction, we extract frames every 15s and feed 33 images on average to the model. GPT-4V performs well at detecting key objects (e.g. mushroom in the second example) when the target is easily identifiable in the input images. However, the performance is highly dependent on the quality of the frame samples. Unfortunately, those randomly selected frames often miss the keyframes due to the low sampling rate, with poor image quality due to the blur and noise in input videos.</p>\n</div>\n<figure id=\"A3.F6\" class=\"ltx_figure\"><img src=\"./assets/x6.png\" id=\"A3.F6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"380\" height=\"233\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F6.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span id=\"A3.F6.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Comparison of <span id=\"A3.F6.5.2.1\" class=\"ltx_text\" style=\"color:#FF8000;\">gpt-4</span> and <span id=\"A3.F6.5.2.2\" class=\"ltx_text\" style=\"color:#0000FF;\">gpt-4-vision-preview</span>. </span></figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"A4\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Multiple Predictions for Ambiguous Queries</h2>\n\n<div id=\"A4.p1\" class=\"ltx_para\">\n<p id=\"A4.p1.1\" class=\"ltx_p\">LLMs generate more than one interval when there are multiple temporal windows that can potentially answer the given query. In the refinement stage, we train a classifier to select the most promising interval (with an accuracy of around 80%) and then feed it to a trained NLQ model. However, we observe that some temporal windows proposed by the LLM seem to be reasonable despite only one ground-truth answer available in Ego4D NLQ annotations. We present two examples in <a href=\"#A4.F7\" title=\"In Appendix D Multiple Predictions for Ambiguous Queries ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">7</span></a> and more video examples in the video in the supplementary material.</p>\n</div>\n<figure id=\"A4.F7\" class=\"ltx_figure\"><img src=\"./assets/x7.png\" id=\"A4.F7.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"361\" height=\"398\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A4.F7.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span id=\"A4.F7.6.2\" class=\"ltx_text\" style=\"font-size:90%;\">Multiple LLM predictions. LLM engine here is GPT-4 and the captioning model is LaViLa. The inputs of the LLM are preprocessed in the <em id=\"A4.F7.6.2.1\" class=\"ltx_emph ltx_font_italic\">Caption Digest</em> step, but we present raw captions here for better visualization of video frames and captions (each caption is generated from a two-second 30fps video clip). LLM prediction in <span id=\"A4.F7.6.2.2\" class=\"ltx_text\" style=\"color:#FF8000;\">orange</span> boxes and the ground truth in <span id=\"A4.F7.6.2.3\" class=\"ltx_text\" style=\"color:#00FF00;\">green</span>.</span></figcaption>\n</figure>\n</section>\n<section id=\"A5\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Additional Qualitative Results</h2>\n\n<div id=\"A5.p1\" class=\"ltx_para\">\n<p id=\"A5.p1.1\" class=\"ltx_p\">We visualize the outputs of the LLM in <a href=\"#A5.F8\" title=\"In Appendix E Additional Qualitative Results ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">8</span></a> as an addition to <a href=\"#S4.F4\" title=\"In 4.2 Main Results ‣ 4 Experiments ‣ LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Figure</span> <span class=\"ltx_text ltx_ref_tag\">4</span></a>. We provide more video examples in the video in the supplementary material. It is worth noting that the machine-generated captions may contain objects that are not present in the video or miss critical information that can potentially answer the target query. Based on the imperfect captions, the LLM is still able to capture the key event and produce high-quality responses. This suggests that a more powerful captioning model will further boost the performance.</p>\n</div>\n<figure id=\"A5.F8\" class=\"ltx_figure\"><img src=\"./assets/x8.png\" id=\"A5.F8.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"361\" height=\"388\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A5.F8.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span id=\"A5.F8.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Qualitative results. Each figure represents a two-second 30fps video clip (which is 60 frames). LLM interval prediction is denoted as <span id=\"A5.F8.5.2.1\" class=\"ltx_text\" style=\"color:#FF8000;\">orange</span> boxes and the ground truth is in <span id=\"A5.F8.5.2.2\" class=\"ltx_text\" style=\"color:#00FF00;\">green</span>. LLM engine here is GPT-4 and the captioning model is LaViLa. To illustrate the reasoning skills of LLMs, we show the raw LLM predictions without any refinement. As shown by abundant qualitative examples, the LLM is able to produce high-quality answers in zero shot.</span></figcaption>\n</figure>\n</section>",
  "css": "",
  "arxiv_id": "2312.05269",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:41.190Z"
}
