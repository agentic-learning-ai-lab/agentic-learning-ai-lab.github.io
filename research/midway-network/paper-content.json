{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\">Animals and humans are able to recognize objects and predict their motion by observing the dynamic world with little to no supervision.\nInspired by this capability, research in deep learning has made significant progress in emulating “learning by observing.”\nPrior work has shown that observing objects through time via video streams can serve as a powerful learning signal <cite class=\"ltx_cite ltx_citemacro_citep\">(Földiák, <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">1991</a>; Wiskott and Sejnowski, <a class=\"ltx_ref\" href=\"#bib.bib80\" title=\"\">2002</a>; Wang and Gupta, <a class=\"ltx_ref\" href=\"#bib.bib75\" title=\"\">2015</a>; Srivastava et al., <a class=\"ltx_ref\" href=\"#bib.bib60\" title=\"\">2015</a>)</cite>.\nOthers have shown that self-supervised learning (SSL) methods can learn strong visual representations from vast amounts of unlabeled data <cite class=\"ltx_cite ltx_citemacro_citep\">(Goyal et al., <a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2023</a>; Fan et al., <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">2025</a>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\">Among a number of perception abilities attained via observation, object recognition and motion understanding are two intertwined core components.\nRecognition allows one to identify the same object across views to establish correspondence; conversely, motion links the same object across spacetime to enable learning of its invariant properties <cite class=\"ltx_cite ltx_citemacro_citep\">(Simonyan and Zisserman, <a class=\"ltx_ref\" href=\"#bib.bib58\" title=\"\">2014</a>; Xu and Wang, <a class=\"ltx_ref\" href=\"#bib.bib85\" title=\"\">2021</a>)</cite>.\nHowever, most prior work on visual SSL has focused on learning representations for <span class=\"ltx_text ltx_font_italic\">either</span> object recognition or motion understanding, but not both in tandem.\nImage SSL methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020b</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">2020</a>; Grill et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2020</a>; Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>; Assran et al., <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2023</a>)</cite> have demonstrated strong capabilities in learning semantic representations, but most operate on iconic, i.e., single-subject, image datasets which are human-curated and additionally lack temporal information to learn motion.\nMore recently, some have proposed performing SSL on natural videos, which depict real-world scenes and can approximate the observational perspective of animals.\nNonetheless, these methods either do not utilize motion transformations for training <cite class=\"ltx_cite ltx_citemacro_citep\">(Gordon et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2020</a>; Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite> or rely on external optical flow networks to incorporate motion as a learning signal <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiong et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2021</a>; Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>.\nOn the other hand, self-supervised methods that focus on learning motion as a pixel-correspondence <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">2019</a>; Jonschkowski et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2020</a>; Luo et al., <a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2021</a>; Stone et al., <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">2021</a>)</cite> or cross-view reconstruction task <cite class=\"ltx_cite ltx_citemacro_citep\">(Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a>)</cite> result in poor semantic representations.\nOnly MC-JEPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2023</a>)</cite> aims to learn both semantic and motion features, but this method still depends on curated, iconic image data for training.\nHow can we jointly learn rich representations for object recognition and motion understanding solely from natural videos?</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"175\" id=\"S1.F1.g1\" src=\"./assets/x1.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n(a) Traditional SSL methods focus on learning representations for object recognition and lean on curated, iconic image data for training.\n(b) Dense SSL methods extend training to natural videos, but either do not utilize motion transformations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gordon et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2020</a>; Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite> for training or rely on external networks to incorporate motion <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiong et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2021</a>; Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>.\n(c) Our proposed Midway Network jointly learns representations of semantics and motion from solely natural videos via latent dynamics modeling. The learned image-level representations can be used towards downstream object recognition and motion understanding tasks.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\">Theories in neuroscience have proposed that animals use internal inverse and forward dynamics models and future sensory prediction, i.e., predictive coding, to perform motor control, planning, and perception <cite class=\"ltx_cite ltx_citemacro_citep\">(Shidara et al., <a class=\"ltx_ref\" href=\"#bib.bib57\" title=\"\">1993</a>; Miall and Wolpert, <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">1996</a>; Wolpert et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">1998</a>; Rao and Ballard, <a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">1999</a>; Friston, <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2005</a>)</cite>.\nWorks in decision making have also suggested using latent dynamics modeling for representation learning, but focus on control and planning tasks in simulated and lab environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Brandfonbrener et al., <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">2023</a>; Schmidt and Jiang, <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">2024</a>; Cui et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>.\nTogether, these studies point to latent dynamics modeling as a natural mechanism for learning useful representations of visual observations and their transformations over time, e.g., motion.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\">Building on this observation, we propose <span class=\"ltx_text ltx_font_bold\">Midway Network</span>, a new SSL architecture that is the first to learn both recognition and motion understanding solely from natural videos, by extending latent dynamics modeling to this domain.\nMidway Network is centered around a <span class=\"ltx_text ltx_font_italic\">midway</span> top-down path, which infers motion latents between video frames via inverse dynamics that are subsequently used to condition the forward predictions.\nWe rely on two design choices in order to better model the complex, multi-object scenes in natural videos.\nFirst, we formulate the forward prediction objective over <span class=\"ltx_text ltx_font_italic\">dense</span> features, rather than global features like in previous works <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite>.\nSecond, Midway Network introduces a hierarchical architecture with backward and lateral layers to refine the motion latents and representations over multiple feature levels, inspired by optical flow networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et al., <a class=\"ltx_ref\" href=\"#bib.bib63\" title=\"\">2018</a>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\">Midway Network shows strong capability of learning image-level representations for object recognition and motion understanding after pretraining on large-scale natural video datasets.\nIn particular, Midway Network outperforms prior SSL methods on optical flow tasks while also achieving competitive performance on semantic segmentation tasks for both BDD100K <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a class=\"ltx_ref\" href=\"#bib.bib88\" title=\"\">2020</a>)</cite> and Walking Tours <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite> pretraining.\nWe also show that Midway Network’s dynamics models can capture high-level correspondence, supported by evidence from our novel analysis method based on forwarded feature perturbation.\nFinally, our ablation studies demonstrate that our hierarchical design components are important for downstream performance.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p6\">\n<p class=\"ltx_p\">To summarize, our contributions are:</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\">We present Midway Network, a novel SSL architecture that is the first to learn rich image-level representations for object recognition and motion understanding solely from natural videos. It leverages a dense forward prediction objective and hierarchical design to better capture the complexity of natural videos.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\">We show that Midway Network achieves strong performance on <span class=\"ltx_text ltx_font_italic\">both</span> optical flow (FlyingThings, MPI-Sintel) and semantic segmentation (BDD100K, CityScapes, WT-Sem, ADE20K) when pretrained on natural video datasets, compared to prior SSL baselines which only attain substantial performance in one of the two tasks.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\">We demonstrate Midway Network’s ability to capture high-level correspondences between video frames with evidence from our novel analysis method based on forwarded feature perturbation.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Work</h2>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Predictive modeling.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">This work builds upon research in predictive modeling from neuroscience and deep learning.\nMany works in neuroscience have explored predictive coding, a theory positing how the brain continuously predicts future sensory inputs with hierarchical networks to perform perception <cite class=\"ltx_cite ltx_citemacro_citep\">(Rao and Ballard, <a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">1999</a>; Rao and Sejnowski, <a class=\"ltx_ref\" href=\"#bib.bib54\" title=\"\">1999</a>; Lee and Mumford, <a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">2003</a>; Friston, <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2005</a>; Summerfield et al., <a class=\"ltx_ref\" href=\"#bib.bib62\" title=\"\">2006</a>)</cite>.\nIn particular, Friston’s theory <cite class=\"ltx_cite ltx_citemacro_citep\">(Friston, <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">2005</a>)</cite> describes how perception may be split into <span class=\"ltx_text ltx_font_italic\">recognition</span>, inferring causes of sensory inputs, which is reminiscent of representation learning and inverse dynamics, and <span class=\"ltx_text ltx_font_italic\">generation</span>, predicting (future) sensory inputs from causes, which is akin to forward dynamics.\nBiological evidence of predictive coding has also been found, such as in monkey neural cells after receptive field excitation <cite class=\"ltx_cite ltx_citemacro_citep\">(Livingstone, <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">1998</a>)</cite> and in functional magnetic resonance imaging data of human subjects following visual stimuli under varying expectation levels <cite class=\"ltx_cite ltx_citemacro_citep\">(Egner et al., <a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">2010</a>)</cite>.\nIn deep learning, prior works such as PredNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Chalasani and Principe, <a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">2013</a>; Lotter et al., <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">2017</a>)</cite> have proposed architectures inspired by predictive coding to perform video prediction.\nMore generally, there has been a line of research in leveraging prediction of future frames in videos as a learning objective <cite class=\"ltx_cite ltx_citemacro_citep\">(Softky, <a class=\"ltx_ref\" href=\"#bib.bib59\" title=\"\">1995</a>; Finn et al., <a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">2016</a>; Villegas et al., <a class=\"ltx_ref\" href=\"#bib.bib70\" title=\"\">2018</a>; Feichtenhofer et al., <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2022</a>)</cite>.\nOthers have developed predictive modeling methods that perform video prediction in latent feature space <cite class=\"ltx_cite ltx_citemacro_citep\">(Vondrick et al., <a class=\"ltx_ref\" href=\"#bib.bib72\" title=\"\">2016</a>; Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">2024</a>)</cite>.\nMidway Network is inspired by these ideas, extending dynamics-conditioned predictive modeling to natural videos with a new hierarchical architecture in order to learn rich representations for object recognition and motion understanding.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Dynamics modeling.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Prior works have suggested that animals use internal inverse and forward dynamics models for motor control and planning <cite class=\"ltx_cite ltx_citemacro_citep\">(Wolpert et al., <a class=\"ltx_ref\" href=\"#bib.bib81\" title=\"\">1995</a>; Miall and Wolpert, <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">1996</a>; Flanagan and Wing, <a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">1997</a>; Wolpert et al., <a class=\"ltx_ref\" href=\"#bib.bib82\" title=\"\">1998</a>; Kitazawa et al., <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">1998</a>; Jordan and Rumelhart, <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">2013</a>)</cite>.\nInverse and forward dynamics have subsequently been used in works like DynaMo <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> to learn latent visual and action representations for robotic manipulation and control tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Brandfonbrener et al., <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">2023</a>; Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">2024</a>; Ye et al., <a class=\"ltx_ref\" href=\"#bib.bib87\" title=\"\">2025</a>)</cite>, but they have only focused on simulated or controlled environments.\nWorld models are a concurrent line of work which learn a latent dynamics model of the environment to enable efficient policy learning and long-horizon planning, but prior works such as Dreamer and V-JEPA 2 have relied on ground truth reward signals or actions <cite class=\"ltx_cite ltx_citemacro_citep\">(Ha and Schmidhuber, <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">2018</a>; Hafner et al., <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">2020</a>; Schwarzer et al., <a class=\"ltx_ref\" href=\"#bib.bib56\" title=\"\">2021</a>; Hu et al., <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">2023</a>; Assran et al., <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">2025</a>)</cite>.\nIn particular, DINO-WM <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib91\" title=\"\">2024</a>)</cite> proposed training a forward dynamics predictor over DINOv2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et al., <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">2023</a>)</cite> patch features, but this method also required access to ground truth actions.\nMore recently, generative models, such as the Genie series, have emerged as a promising approach for learning world models and interactive environments <cite class=\"ltx_cite ltx_citemacro_citep\">(Menapace et al., <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">2022</a>; Yang et al., <a class=\"ltx_ref\" href=\"#bib.bib86\" title=\"\">2024</a>; Parker-Holder et al., <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">2024</a>; Sun et al., <a class=\"ltx_ref\" href=\"#bib.bib64\" title=\"\">2024</a>)</cite>.\nMidway Network utilizes inverse and forward dynamics to tackle a new problem: learning rich image-level representations for recognition and motion understanding solely from natural videos.\nIt leverages <span class=\"ltx_text ltx_font_italic\">dense</span> forward prediction and a new hierarchical refinement architecture to capture the complex, multi-object scenes in this data domain.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Visual self-supervised learning.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">SSL on visual data has enjoyed a long history, from denoising autoencoders <cite class=\"ltx_cite ltx_citemacro_citep\">(Vincent et al., <a class=\"ltx_ref\" href=\"#bib.bib71\" title=\"\">2010</a>; Pathak et al., <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">2016</a>; Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">2020a</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2022</a>)</cite> to joint embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Grill et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2020</a>; Chen et al., <a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">2020b</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">2020</a>; Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>; Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2022</a>)</cite> and joint-embedding predictive <cite class=\"ltx_cite ltx_citemacro_citep\">(Assran et al., <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2023</a>; Garrido et al., <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">2024</a>)</cite> models.\nThese works primarily aim to learn semantic representations from iconic, single-subject images.\nFollowing their success, others have proposed methods to learn from dense, multi-subject images by leveraging losses on local features <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib76\" title=\"\">2021</a>; Xie et al., <a class=\"ltx_ref\" href=\"#bib.bib83\" title=\"\">2021</a>; Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"#bib.bib89\" title=\"\">2023</a>)</cite>.\nWhile prior work uses motion from natural videos to learn visual representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiong et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2021</a>; Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>, these approaches either rely on external supervised flow networks or use motion only to construct training views <cite class=\"ltx_cite ltx_citemacro_citep\">(Jabri et al., <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">2020</a>; Gordon et al., <a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">2020</a>; Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite>.\nIn contrast, our work also jointly learns representations of the motion transformations themselves.\nA separate line of work focuses on learning motion as a cross-view pixel correspondence <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">2019</a>; Jonschkowski et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2020</a>; Luo et al., <a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">2021</a>; Stone et al., <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">2021</a>)</cite> or reconstruction task <cite class=\"ltx_cite ltx_citemacro_citep\">(Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib78\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a>)</cite>; however, the resulting features have poor recognition capability.\nVideo SSL methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2022</a>; Wei et al., <a class=\"ltx_ref\" href=\"#bib.bib77\" title=\"\">2022</a>; Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">2024</a>)</cite> tackle learning clip-level representations for action recognition tasks, whereas Midway Network and our baselines target image-level representations.\nWhile a few video SSL works <cite class=\"ltx_cite ltx_citemacro_citep\">(Qing et al., <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">2022</a>)</cite> also explore hierarchical designs for learning, these hierarchies are only related to the temporal structure of videos for sampling training pairs.\nFinally, MC-JEPA <cite class=\"ltx_cite ltx_citemacro_citep\">(Bardes et al., <a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">2023</a>)</cite> seeks to learn both semantic and motion features, but unlike Midway Network, it still relies on curated, iconic image data (i.e. ImageNet) for training.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Midway Network</h2>\n<figure class=\"ltx_figure\" id=\"S3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"300\" id=\"S3.F2.g1\" src=\"./assets/x2.png\" width=\"640\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">\nMidway Network employs a hierarchical design in which the <span class=\"ltx_text ltx_font_italic\">midway</span> path infers motion latents <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> between source and target features in a top-down manner. Within each level of this hierarchy, backward layers with top-down and lateral connections refine the source features <math alttext=\"z_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m8\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t}^{l}</annotation></semantics></math>. Forward prediction blocks, conditioned on the refined features <math alttext=\"v_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m9\" intent=\":literal\"><semantics><msubsup><mi>v</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">v_{t}^{l}</annotation></semantics></math> and motion latents <math alttext=\"m^{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m10\" intent=\":literal\"><semantics><msup><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding=\"application/x-tex\">m^{l+1}</annotation></semantics></math>, predict the dense target features <math alttext=\"z_{t+1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m11\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t+1}^{l}</annotation></semantics></math>, and the prediction loss <math alttext=\"\\mathcal{L}_{dyn}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F2.m12\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{dyn}</annotation></semantics></math> jointly trains all components at each level.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\">We present Midway Network, a new SSL architecture that uses latent dynamics modeling to learn representations for object recognition and motion understanding solely from natural videos.\nAt the heart of Midway Network is a <span class=\"ltx_text ltx_font_italic\">midway</span> path that infers motion latents to describe the transformation between a source and target video frame.\nThe visual encoder extracts features from the raw video frames, and the backward layers refine these features with lower-level information in a top-down manner.\nThe forward dynamics model, conditioned on the source frame backward features and motion latents, predicts the <span class=\"ltx_text ltx_font_italic\">dense</span> target frame features, and the resulting prediction error is used to jointly train all components of the model.\nMidway Network employs a hierarchical design, where the forward prediction objective is placed at multiple feature levels, and the forward predictions from higher feature levels are used as the input to refine the motion latents at lower levels.\nThe architecture is illustrated in Figure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Midway Network ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the computations for the dense forward prediction objective are summarized in Algorithm <a class=\"ltx_ref\" href=\"#alg1\" title=\"Algorithm 1 ‣ Motion latents via midway path. ‣ 3 Midway Network ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Preliminaries.</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\">The model inputs are pairs of source and target video frames, <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> and <math alttext=\"x_{t+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">x_{t+1}</annotation></semantics></math>.\nFollowing the SSL knowledge distillation paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(Grill et al., <a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">2020</a>; Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>)</cite>, we encode the video frames into features using source and target networks, <math alttext=\"z_{t}=f_{\\theta}(x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{t}=f_{\\theta}(x_{t})</annotation></semantics></math> and <math alttext=\"z_{t+1}=f_{\\tilde{\\theta}}(x_{t+1})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>f</mi><mover accent=\"true\"><mi>θ</mi><mo>~</mo></mover></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{t+1}=f_{\\tilde{\\theta}}(x_{t+1})</annotation></semantics></math>, where <math alttext=\"\\tilde{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>θ</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\theta}</annotation></semantics></math> is updated using an exponential moving average of the student parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mi>θ</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>.\nMidway Network operates at multiple feature levels, so we use the notation that <math alttext=\"z^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">z^{l}</annotation></semantics></math> are the features of the <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>-th level, ordered from lowest level <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> to highest level <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Motion latents via <span class=\"ltx_text ltx_font_italic\">midway</span> path.</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\">The midway path aims to learn motion latents that capture the transformation between observations over time via inverse dynamics.\nSpecifically, the midway inverse dynamics model is a transformer that takes in previous motion latents <math alttext=\"m^{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msup><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding=\"application/x-tex\">m^{l+1}</annotation></semantics></math> and the source and target features <math alttext=\"z_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t}^{l}</annotation></semantics></math> and <math alttext=\"z_{t+1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t+1}^{l}</annotation></semantics></math> as input, and outputs the motion latents <math alttext=\"m^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><msup><mi>m</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">m^{l}</annotation></semantics></math> for the next level.\nThe motion latents accumulate over levels, i.e. <math alttext=\"m^{l}=\\mathrm{midway}(m^{l+1},z_{t}^{l},z_{t+1}^{l})+m^{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>m</mi><mi>l</mi></msup><mo>=</mo><mrow><mrow><mi>midway</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>,</mo><msubsup><mi>z</mi><mi>t</mi><mi>l</mi></msubsup><mo>,</mo><msubsup><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><msup><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">m^{l}=\\mathrm{midway}(m^{l+1},z_{t}^{l},z_{t+1}^{l})+m^{l+1}</annotation></semantics></math>.\nThe initial motion latents are learnable tokens.\nFor every level besides the top level, we use the output of the higher level’s forward prediction, <math alttext=\"\\hat{z}_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{z}_{t}^{l}</annotation></semantics></math>, instead of the features <math alttext=\"z_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t}^{l}</annotation></semantics></math> as input.\nThus, the model learns to refine the motion latents in a top-down manner, conditioned on the higher-level predictions.\nThis design is motivated by how prior optical flow methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et al., <a class=\"ltx_ref\" href=\"#bib.bib63\" title=\"\">2018</a>; Jonschkowski et al., <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">2020</a>)</cite> would use intermediate flow estimates to warp features before computing cost volumes, which would subsequently be used to refine flow predictions at lower levels.</p>\n</div>\n<figure class=\"ltx_figure ltx_minipage ltx_align_middle ltx_align_floatright ltx_framed ltx_framed_top\" id=\"alg1\" style=\"width:433.6pt;\">\n<figcaption class=\"ltx_caption\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text ltx_font_bold\">Algorithm 1</span> </span> Dense forward prediction objective.</figcaption>\n<div class=\"ltx_listing ltx_listing\">\n<div class=\"ltx_listingline\" id=\"alg1.l1\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">1:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\"></span><math alttext=\"m^{L+1}\\leftarrow 0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l1.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathsize=\"0.800em\">m</mi><mrow><mi mathsize=\"0.800em\">L</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mn mathsize=\"0.800em\">0</mn></mrow><annotation encoding=\"application/x-tex\">m^{L+1}\\leftarrow 0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">, </span><math alttext=\"v_{t}^{L}\\leftarrow z_{t}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi mathsize=\"0.800em\">v</mi><mi mathsize=\"0.800em\">t</mi><mi mathsize=\"0.800em\">L</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><msubsup><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">t</mi><mi mathsize=\"0.800em\">L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">v_{t}^{L}\\leftarrow z_{t}^{L}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l2\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">2:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\"></span><math alttext=\"z_{t}\\leftarrow f_{\\theta}(x_{t}),z_{t+1}\\leftarrow f_{\\tilde{\\theta}}(x_{t+1}),\\hat{z}_{t+1}^{L}\\leftarrow z^{L}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">t</mi></msub><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mrow><msub><mi mathsize=\"0.800em\">f</mi><mi mathsize=\"0.800em\">θ</mi></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msub><mi mathsize=\"0.800em\">x</mi><mi mathsize=\"0.800em\">t</mi></msub><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow></mrow><mo mathsize=\"0.800em\">,</mo><mrow><mrow><msub><mi mathsize=\"0.800em\">z</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msub><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mrow><msub><mi mathsize=\"0.800em\">f</mi><mover accent=\"true\"><mi mathsize=\"0.800em\">θ</mi><mo mathsize=\"0.800em\">~</mo></mover></msub><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msub><mi mathsize=\"0.800em\">x</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msub><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow></mrow><mo mathsize=\"0.800em\">,</mo><mrow><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">L</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><msubsup><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">t</mi><mi mathsize=\"0.800em\">L</mi></msubsup></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{t}\\leftarrow f_{\\theta}(x_{t}),z_{t+1}\\leftarrow f_{\\tilde{\\theta}}(x_{t+1}),\\hat{z}_{t+1}^{L}\\leftarrow z^{L}_{t}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l3\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">3:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">for</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"l\\leftarrow L-1\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mrow><mi mathsize=\"0.800em\">L</mi><mo mathsize=\"0.800em\">−</mo><mn mathsize=\"0.800em\">1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">l\\leftarrow L-1</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> to </span><math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">do</span><span class=\"ltx_text\" style=\"font-size:80%;\">:\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l4\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">4:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\">  </span><math alttext=\"m^{l+1}\\leftarrow\\text{midway}(m^{l+2},\\hat{z}_{t+1}^{l+1},z_{t+1}^{l+1})+m^{l+2}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathsize=\"0.800em\">m</mi><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mrow><mrow><mtext mathsize=\"0.800em\">midway</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msup><mi mathsize=\"0.800em\">m</mi><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">2</mn></mrow></msup><mo mathsize=\"0.800em\">,</mo><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msubsup><mo mathsize=\"0.800em\">,</mo><msubsup><mi mathsize=\"0.800em\">z</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msubsup><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow><mo mathsize=\"0.800em\">+</mo><msup><mi mathsize=\"0.800em\">m</mi><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">2</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">m^{l+1}\\leftarrow\\text{midway}(m^{l+2},\\hat{z}_{t+1}^{l+1},z_{t+1}^{l+1})+m^{l+2}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l5\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">5:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\">  </span><math alttext=\"v_{t}^{l}\\leftarrow\\text{backward}(z_{t}^{l},v_{t}^{l+1})\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathsize=\"0.800em\">v</mi><mi mathsize=\"0.800em\">t</mi><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mrow><mtext mathsize=\"0.800em\">backward</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msubsup><mi mathsize=\"0.800em\">z</mi><mi mathsize=\"0.800em\">t</mi><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\">,</mo><msubsup><mi mathsize=\"0.800em\">v</mi><mi mathsize=\"0.800em\">t</mi><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msubsup><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v_{t}^{l}\\leftarrow\\text{backward}(z_{t}^{l},v_{t}^{l+1})</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l6\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">6:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\">  </span><math alttext=\"\\hat{z}_{t+1}^{l}\\leftarrow\\text{predictor}(v_{t}^{l},m^{l+1})\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l6.m1\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mrow><mtext mathsize=\"0.800em\">predictor</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><msubsup><mi mathsize=\"0.800em\">v</mi><mi mathsize=\"0.800em\">t</mi><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\">,</mo><msup><mi mathsize=\"0.800em\">m</mi><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow></msup><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{z}_{t+1}^{l}\\leftarrow\\text{predictor}(v_{t}^{l},m^{l+1})</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l7\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">7:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\">  </span><math alttext=\"\\bar{\\hat{z}}_{t+1}^{l}\\leftarrow\\frac{\\hat{z}_{t+1}^{l}}{||\\hat{z}_{t+1}^{l}||_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m1\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mo mathsize=\"0.800em\">¯</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mfrac><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><msub><mrow><mo maxsize=\"1.420em\" minsize=\"1.420em\" stretchy=\"true\">‖</mo><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo maxsize=\"1.420em\" minsize=\"1.420em\" stretchy=\"true\">‖</mo></mrow><mn mathsize=\"0.800em\">2</mn></msub></mfrac></mrow><annotation encoding=\"application/x-tex\">\\bar{\\hat{z}}_{t+1}^{l}\\leftarrow\\frac{\\hat{z}_{t+1}^{l}}{||\\hat{z}_{t+1}^{l}||_{2}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">, </span><math alttext=\"\\bar{z}_{t+1}^{l}\\leftarrow\\frac{z_{t+1}^{l}}{||z_{t+1}^{l}||_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m2\" intent=\":literal\"><semantics><mrow><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">¯</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><mfrac><msubsup><mi mathsize=\"0.800em\">z</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><msub><mrow><mo maxsize=\"1.420em\" minsize=\"1.420em\" stretchy=\"true\">‖</mo><msubsup><mi mathsize=\"0.800em\">z</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo maxsize=\"1.420em\" minsize=\"1.420em\" stretchy=\"true\">‖</mo></mrow><mn mathsize=\"0.800em\">2</mn></msub></mfrac></mrow><annotation encoding=\"application/x-tex\">\\bar{z}_{t+1}^{l}\\leftarrow\\frac{z_{t+1}^{l}}{||z_{t+1}^{l}||_{2}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l8\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">8:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\">  </span><math alttext=\"\\mathcal{L}_{dyn}^{l}\\leftarrow||\\bar{\\hat{z}}_{t+1}^{l}-\\bar{z}_{t+1}^{l}||_{2}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l8.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">ℒ</mi><mrow><mi mathsize=\"0.800em\">d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathsize=\"0.800em\">y</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathsize=\"0.800em\">n</mi></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\" stretchy=\"false\">←</mo><msubsup><mrow><mo stretchy=\"false\">‖</mo><mrow><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">^</mo></mover><mo mathsize=\"0.800em\">¯</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><mo mathsize=\"0.800em\">−</mo><msubsup><mover accent=\"true\"><mi mathsize=\"0.800em\">z</mi><mo mathsize=\"0.800em\">¯</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">+</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup></mrow><mo stretchy=\"false\">‖</mo></mrow><mn mathsize=\"0.800em\">2</mn><mn mathsize=\"0.800em\">2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{dyn}^{l}\\leftarrow||\\bar{\\hat{z}}_{t+1}^{l}-\\bar{z}_{t+1}^{l}||_{2}^{2}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l9\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">9:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">end</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">for</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l10\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:80%;\">10:</span></span><span class=\"ltx_text\" style=\"font-size:80%;\"></span><math alttext=\"\\mathcal{L}_{dyn}\\leftarrow\\sum\\limits_{l=1}^{L-1}\\mathcal{L}_{dyn}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l10.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">ℒ</mi><mrow><mi mathsize=\"0.800em\">d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathsize=\"0.800em\">y</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathsize=\"0.800em\">n</mi></mrow></msub><mo mathsize=\"0.800em\" rspace=\"0.111em\" stretchy=\"false\">←</mo><mrow><munderover><mo maxsize=\"0.800em\" minsize=\"0.800em\" movablelimits=\"false\" stretchy=\"true\">∑</mo><mrow><mi mathsize=\"0.800em\">l</mi><mo mathsize=\"0.800em\">=</mo><mn mathsize=\"0.800em\">1</mn></mrow><mrow><mi mathsize=\"0.800em\">L</mi><mo mathsize=\"0.800em\">−</mo><mn mathsize=\"0.800em\">1</mn></mrow></munderover><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">ℒ</mi><mrow><mi mathsize=\"0.800em\">d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathsize=\"0.800em\">y</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi mathsize=\"0.800em\">n</mi></mrow><mi mathsize=\"0.800em\">l</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{dyn}\\leftarrow\\sum\\limits_{l=1}^{L-1}\\mathcal{L}_{dyn}^{l}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">.\n</span>\n</div>\n</div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Backward features.</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\">Prior works, from Ladder Networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Valpola, <a class=\"ltx_ref\" href=\"#bib.bib67\" title=\"\">2015</a>)</cite> to PooDLe <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>, have proposed backward layers with top-down and lateral connections to relieve higher-level features of the burden of encoding low-level details.\nIn this work, backward layers are used to refine features in a top-down manner by using lateral connections to incorporate lower-level information.\nSpecifically, the backward layers are transformer blocks that use cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">2022</a>)</cite>, where laterally-connected features <math alttext=\"z_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t}^{l}</annotation></semantics></math> are used as queries that attend to higher-level backward features <math alttext=\"v_{t}^{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>v</mi><mi>t</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation encoding=\"application/x-tex\">v_{t}^{l+1}</annotation></semantics></math>, which serve as keys and values.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Dense forward prediction.</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\">The forward dynamics model is also a transformer that takes in backward features <math alttext=\"v_{t}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>v</mi><mi>t</mi><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">v_{t}^{l}</annotation></semantics></math> and motion latents <math alttext=\"m^{l+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><msup><mi>m</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding=\"application/x-tex\">m^{l+1}</annotation></semantics></math> as input, concatenated along the spatial dimension, and predicts the dense features of the target frame.\nThe dense forward prediction objective is then to minimize the prediction error between the predicted features <math alttext=\"\\hat{z}_{t+1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">\\hat{z}_{t+1}^{l}</annotation></semantics></math>\nand the realized target features <math alttext=\"z_{t+1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">z_{t+1}^{l}</annotation></semantics></math>.\nThe prediction error is the mean squared error between the normalized dense predictions and targets:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathcal{L}^{l}_{dyn}=\\|\\bar{\\hat{z}}_{t+1}^{l}-\\bar{z}_{t+1}^{l}\\|_{2}^{2}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow><mi>l</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">‖</mo><mrow><msubsup><mover accent=\"true\"><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mo>¯</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup><mo>−</mo><msubsup><mover accent=\"true\"><mi>z</mi><mo>¯</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>l</mi></msubsup></mrow><mo stretchy=\"false\">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}^{l}_{dyn}=\\|\\bar{\\hat{z}}_{t+1}^{l}-\\bar{z}_{t+1}^{l}\\|_{2}^{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n</div>\n<figure class=\"ltx_figure ltx_minipage ltx_align_middle ltx_align_floatright\" id=\"S3.F3\" style=\"width:433.6pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"835\" id=\"S3.F3.g1\" src=\"./assets/x3.png\" width=\"814\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Attention layer with gating unit on <math alttext=\"v_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F3.m2\" intent=\":literal\"><semantics><msub><mi>v</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">v_{t}</annotation></semantics></math>.</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS0.SSS0.Px4.p2\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Forward prediction gating.</span>\nIn a standard transformer block, the input token value is always propagated forward due to the residual connection — this biases the computation towards the identity mapping.\nHowever, we would like the forward transformer model to learn whether the object captured by an input token has moved, i.e., if its features can be computed from tokens at <span class=\"ltx_text ltx_font_italic\">other</span> spatial locations, rather than defaulting to the identity location.\nThus, we introduce learnable gating units for the residual connection in the transformer layers of the forward dynamics model.\nThe gating unit is a multi-layer perceptron that learns a vector-wise gating weight between 0 and 1 for the residual connection of each input token of <math alttext=\"v_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p2.m1\" intent=\":literal\"><semantics><msub><mi>v</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">v_{t}</annotation></semantics></math>.\nSpecifically, the transformer block is modified with gating unit <math alttext=\"g\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><mi>g</mi><annotation encoding=\"application/x-tex\">g</annotation></semantics></math> such that the input to the feedforward network, <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p2.m3\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, is computed as:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S3.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"h=g(x)\\cdot x+\\text{Attention}(x).\" class=\"ltx_Math\" display=\"block\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>h</mi><mo>=</mo><mrow><mrow><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.222em\">⋅</mo><mi>x</mi></mrow><mo>+</mo><mrow><mtext>Attention</mtext><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">h=g(x)\\cdot x+\\text{Attention}(x).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">We do not use gating units in the first transformer block to provide sufficient information for initial estimates of attention, nor do we use them for the motion latents <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p2.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> to fully propagate the motion information.\nIn our experiments, we find that the gating units improve semantic feature quality and interpretability of the learned dynamics models, as shown in Section <a class=\"ltx_ref\" href=\"#S4.SS4\" title=\"4.4 Analysis of dynamics ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS0.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Invariance objective.</h4>\n<div class=\"ltx_para\" id=\"S3.SS0.SSS0.Px5.p1\">\n<p class=\"ltx_p\">We utilize an additional joint-embedding invariance objective over smaller crops to encourage the visual encoder to learn semantic features, following PooDLe <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>.\nIn our experiments, we use the DINO <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>)</cite> objective with projection heads on top of the source and target networks.\nThis can be viewed as a form of regularization for the features that are subsequently used in the latent dynamics modeling.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\">We evaluate Midway Network by pretraining on large-scale natural video datasets, BDD100K <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a class=\"ltx_ref\" href=\"#bib.bib88\" title=\"\">2020</a>)</cite> and Walkings Tours (WT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite>, and evaluating the learned image and motion latent representations on downstream semantic segmentation and optical flow tasks.\nIn our experiments, we study whether Midway Network learns good visual features for both object recognition and motion understanding.\nWe further analyze how each component of Midway Network contributes to downstream performance and what information does its dynamics models capture.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Setup</h3>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Pretraining.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px1.p1\">\n<p class=\"ltx_p\">We pretrain Midway Network on two large-scale video datasets from different domains.\n<span class=\"ltx_text ltx_font_bold\">BDD100K</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et al., <a class=\"ltx_ref\" href=\"#bib.bib88\" title=\"\">2020</a>)</cite> is a dataset of 100,000 dashcam driving videos collected in varying weather, lighting, and time-of-day conditions from New York and the San Francisco Bay Area.\nEach video is 40 seconds long at 720p and 30 fps.\nWe pretrain on all 70,000 videos in the train split.\n<span class=\"ltx_text ltx_font_bold\">Walking Tours (WT)</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite> is a dataset of 10 first-person YouTube walking videos collected in various cities of Europe and Asia, with outdoor and indoor scenes, and natural transitions in lighting and location.\nThe videos range from 59 minutes to 2 hours 55 minutes, at 720p and 30 fps.\nWe pretrain on the Venice video following DoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite>’s original setup.<span class=\"ltx_note ltx_role_footnote\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Due to computational constraints, we did not pretrain on all 10 videos.</span></span></span></p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Downstream evaluations.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px2.p1\">\n<p class=\"ltx_p\">We evaluate Midway Network’s pretrained representations on semantic segmentation tasks to gauge object recognition capability.\nFor BDD pretraining, we perform linear and UperNet readout on the BDD and CityScapes <cite class=\"ltx_cite ltx_citemacro_citep\">(Cordts et al., <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">2016</a>)</cite> benchmarks following FlowE <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiong et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2021</a>)</cite>.\nFor WT pretraining, we perform UperNet finetuning on the WT-Sem <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite> and ADE20K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib90\" title=\"\">2017</a>)</cite> benchmarks following DoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite> and PooDLe <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>.\nFor linear readout only, we use the backward layer features following PooDLe.\nWe also evaluate Midway Network on optical flow tasks to assess motion understanding.\nWe follow CroCo v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a>)</cite>’s finetuning evaluation protocol, replacing their binocular decoder with our midway inverse dynamics and forward dynamics models — baselines without binocular components also use the dynamics models, but with randomly initialized weights.\nWe finetune models pretrained on BDD on TartanAir <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib74\" title=\"\">2020</a>)</cite>, MPI-Sintel <cite class=\"ltx_cite ltx_citemacro_citep\">(Butler et al., <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">2012</a>)</cite>, FlyingThings <cite class=\"ltx_cite ltx_citemacro_citep\">(Mayer et al., <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">2016</a>)</cite>, and FlyingChairs <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy et al., <a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">2015</a>)</cite> datasets, and evaluate on the corresponding validation splits of FlyingThings and MPI-Sintel.\nWe report mean intersection-over-union (mIoU) and pixel-level accuracy (Acc) for semantic segmentation, and endpoint error (EPE) for optical flow.\nMore details on evaluation settings are provided in Appendix <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Implementation details ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Baselines.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px3.p1\">\n<p class=\"ltx_p\">We compare Midway Network to iconic image SSL methods (DINO, iBOT <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>; Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib92\" title=\"\">2021</a>)</cite>), multi-object SSL methods (DoRA, PooDLe  <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>), and masked reconstruction methods (CroCo v2, VideoMAE, MAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a>; Tong et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2022</a>; He et al., <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2022</a>)</cite>).\nDoRA uses 8-frame clips for training, VideoMAE uses 16-frame clips, and iBOT and MAE use single frames.\nMidway Network and all other baselines learn from pairs of frames.\nWe also implement a modified version of DynaMo <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a>)</cite> that uses ViT-S as the encoder and includes the DINO invariance objective.\nWe use official implementations to pretrain the baselines on BDD and WT.\nAll baselines are trained on <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution, except for PooDLe in Table <a class=\"ltx_ref\" href=\"#S4.T2\" title=\"Table 2 ‣ BDD100K pretraining. ‣ 4.2 Semantic segmentation and optical flow results ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which uses <math alttext=\"512\\times 1024\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>512</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">512\\times 1024</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS1.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Implementation.</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS0.Px4.p1\">\n<p class=\"ltx_p\">We use ViT-S and ViT-B sized vision transformers for our visual encoders.\nFor the midway inverse dynamics, forward dynamics, and backward models, we use decoder-only transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"#bib.bib68\" title=\"\">2017</a>)</cite>, with the backward layers using cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et al., <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">2022</a>)</cite> blocks.\nWe largely follow the guidelines provided by PooDLe <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite> on data sampling from natural videos.\nSpecifically, we sample pairs of frames <math alttext=\"0.5\\sim 1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>0.5</mn><mo>∼</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0.5\\sim 1</annotation></semantics></math> seconds apart, one per video per epoch for BDD, and 0.5 seconds apart, for all possible pairs per epoch for WT-Venice.\nFor the dense forward prediction objective, we sample larger crops of area range <math alttext=\"[0.2,0.4]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.2</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.2,0.4]</annotation></semantics></math> at the same location for both frames.\nWe take smaller initial crops of area range <math alttext=\"[0.05,0.2]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0.05</mn><mo>,</mo><mn>0.2</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0.05,0.2]</annotation></semantics></math> at the same location for both frames, from which global and local crops are sampled for the DINO joint-embedding objective.\nAll crops are resized to <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution.\nAppendix <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Implementation details ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> provides more details on implementation, compute resources, and comparisons of training cost across the different methods.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Semantic segmentation and optical flow results</h3>\n<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 1</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Semantic segmentation and optical flow evaluations for BDD100K <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution pretraining. Sem. Seg. is conducted with frozen backbone and optical flow is conducted with finetuning. <sup class=\"ltx_sup\">†</sup>DynaMo is modified to use a ViT-S encoder and DINO objective.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:338.1pt;height:68.7pt;vertical-align:-33.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-277.4pt,56.4pt) scale(0.378685821062538,0.378685821062538) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BDD100K Sem. Seg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Cityscapes Sem. Seg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Optical Flow</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Linear</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UperNet</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Linear</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UperNet</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FlyingThings</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MPI-Sintel</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Arch</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ep.</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">PooDLe </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">R50</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">47.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">44.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">59.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">iBOT </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib92\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">800</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">86.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DINO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">VideoMAE </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Tong et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.1</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DoRA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.1</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DynaMo</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">†</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Cui et al., <a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">2024</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">47.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.1</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Midway (enc. only)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">39.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">50.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">92.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DINO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">62.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">94.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.1</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">48.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">91.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">55.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">51.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">92.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.8</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">BDD100K pretraining.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"#S4.T1\" title=\"Table 1 ‣ 4.2 Semantic segmentation and optical flow results ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows results on BDD100K and CityScapes semantic segmentation, and FlyingThings and MPI-Sintel optical flow benchmarks after BDD100K pretraining.\nNotably, Midway Network is the only model to perform well on both semantic segmentation and optical flow tasks overall.\nFor semantic segmentation, Midway Network outperforms all baselines on BDD100K, and its learned visual features also transfer well to CityScapes, where they are competitive with the best-performing baseline, PooDLe, which relies on an external supervised optical flow network.\nNote that even without the backward network, our model achieves 39.2 mIoU and 90.1 Acc on BDD100K linear readout, continuing to outperform the baselines.\nMidway Network also surpasses all baselines’ performance on FlyingThings and MPI-Sintel optical flow.\nAs shown by <span class=\"ltx_text ltx_font_italic\">Midway Network (enc. only)</span>, performance on optical flow drops drastically if we do not initialize the midway inverse and forward dynamics models with the pretrained weights, indicating that the dynamics models have learned features that are useful towards motion estimation.\nWe also demonstrate that Midway Network’s downstream performance also scales with larger model sizes, from ViT-S to ViT-B.\nWhile CroCo v2 edges out Midway Network on optical flow for ViT-B, Midway Network does not suffer the same tradeoff on semantic segmentation performance as CroCo v2.\nFigure <a class=\"ltx_ref\" href=\"#S4.F4\" title=\"Figure 4 ‣ BDD100K pretraining. ‣ 4.2 Semantic segmentation and optical flow results ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and Figure <a class=\"ltx_ref\" href=\"#S4.F5\" title=\"Figure 5 ‣ Walking Tours pretraining. ‣ 4.2 Semantic segmentation and optical flow results ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> compare predicted segmentation masks for BDD100K, and optical flow for FlyingThings and MPI-Sintel, respectively, across different methods.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"215\" id=\"S4.F4.g1\" src=\"./assets/x4.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of BDD semantic segmentation UperNet readout. Midway Network is able to produce cleaner object boundaries, particularly for the cyclist on the right.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Semantic segmentation and optical flow evaluations for WT-Venice <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution pretraining. Sem. Seg. and optical flow are conducted with finetuning. <sup class=\"ltx_sup\">†</sup>PooDLe on <math alttext=\"512\\times 1024\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mn>512</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">512\\times 1024</annotation></semantics></math> resolution pretraining from their original table <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>. <sup class=\"ltx_sup\">*</sup>iBOT results taken from DoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a>)</cite>.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:338.1pt;height:56.3pt;vertical-align:-27.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-231.2pt,38.5pt) scale(0.422384198423206,0.422384198423206) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WT-Sem Sem. Seg.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ADE20K Sem. Seg.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Optical Flow</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column\"></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r\"></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UperNet</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UperNet</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FlyingThings</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MPI-Sintel</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Arch</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ep.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">PooDLe</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">†</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">R50</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">13.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">36.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">iBOT</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">*</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib92\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MAE </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>He et al., <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">VideoMAE </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Tong et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DINO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib78\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DoRA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">85.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.9</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.6</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_paragraph\" id=\"S4.SS2.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Walking Tours pretraining.</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS0.Px2.p1\">\n<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"#S4.T2\" title=\"Table 2 ‣ BDD100K pretraining. ‣ 4.2 Semantic segmentation and optical flow results ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows results on WT-Sem and ADE20K semantic segmentation, and FlyingThings and MPI-Sintel optical flow benchmarks after WT-Venice pretraining.\nAgain, Midway Network is the only method to achieve strong, competitive performance on <span class=\"ltx_text ltx_font_italic\">both</span> semantic segmentation and optical flow tasks.\nNote that PooDLe was pretrained at high resolution (<math alttext=\"512\\times 1024\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>512</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">512\\times 1024</annotation></semantics></math>) and utilized external supervised optical flow networks.\nWe include additional visualizations of predicted segmentation masks and optical flow for WT-Venice pretraining in Appendix <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"207\" id=\"S4.F5.g1\" src=\"./assets/x5.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of FlyingThings and MPI-Sintel optical flow evaluations after finetuning. Midway Network is able to generate more accurate optical flow predictions compared to CroCo v2.\n</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Ablation studies</h3>\n<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 3</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation studies on Midway Network components evaluated on BDD100K semantic segmentation linear readout and MPI-Sintel optical flow finetuning.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:338.1pt;height:77.8pt;vertical-align:-37.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-95.2pt,21.9pt) scale(0.639841488269114,0.639841488269114) ;\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Latent Dynamics</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Backward</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multi-Level</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Refinement</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Gating</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EPE</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">1</span><span class=\"ltx_text\" style=\"font-size:90%;\"> Base model</span>\n</td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">6</span><span class=\"ltx_text\" style=\"font-size:90%;\"> Full model</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">7</span><span class=\"ltx_text\" style=\"font-size:90%;\"> No backward</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">8</span><span class=\"ltx_text\" style=\"font-size:90%;\"> No multi-level</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#808080;\">9</span><span class=\"ltx_text\" style=\"font-size:90%;\"> No refinement</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_border_bb\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.1</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n<p class=\"ltx_p\">We perform a series of ablation studies, shown in Table <a class=\"ltx_ref\" href=\"#S4.T3\" title=\"Table 3 ‣ 4.3 Ablation studies ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, where we cumulatively add components of Midway Network until we reach the full model.\nFor the ablations, we pretrain variants of Midway Network on BDD100K for 100 epochs and evaluate on BDD semantic segmentation with linear readout and on MPI-Sintel optical flow (clean renderings) after finetuning on FlyingChairs and FlyingThings following CroCo V2 and prior optical flow methods.\nFor reference, we run 5 seeds for the full model (row 6), and obtain a standard deviation of 0.06 on mIoU and 0.08 on EPE.\nMore technical details are found in Appendix <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Implementation details ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p2\">\n<p class=\"ltx_p\">First, we find that adding latent dynamics modeling immediately adds a large boost to performance (row 2).\nNext, we observe that the hierarchical structure of the backward network and multi-level learning work together with motion latent refinement to provide further gains on both recognition and motion understanding (row 5).\nFinally, using gating units improves recognition (row 6) as well as visual interpretability of the learned dynamics, as shown in Figure <a class=\"ltx_ref\" href=\"#S4.F6\" title=\"Figure 6 ‣ 4.4 Analysis of dynamics ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nWe also see that removing any of the introduced design components from Midway Network harms performance by a decent margin (rows 7 - 9).\nAdditional ablations on model capacity are shown in Appendix <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional results ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Analysis of dynamics</h3>\n<figure class=\"ltx_figure\" id=\"S4.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"239\" id=\"S4.F6.g1\" src=\"./assets/x6.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Heatmaps from forwarded feature perturbation. Features are perturbed at green squares in Source, which are also depicted in Target at the same location to highlight the motion between frames. Midway Network without gating units exhibits identity bias (bottom right, red border).</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n<p class=\"ltx_p\">To probe the extent to which Midway Network has learned dynamics after pretraining on natural videos, we introduce a new analysis method based on forwarded feature perturbation.\nFirst, we encode a pair of frames to get features <math alttext=\"z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">z_{t}</annotation></semantics></math> and <math alttext=\"z_{t+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">z_{t+1}</annotation></semantics></math> and compute motion latents <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> between them, as usual.\nThen, we sample a random vector <math alttext=\"r\\sim\\mathcal{N}(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>∼</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">𝒩</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">r\\sim\\mathcal{N}(0,1)</annotation></semantics></math> and \"perturb\" a selected spatial feature by associating <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> as a tangent vector to the selected feature in the source frame.\nWe perform forward prediction to propagate the perturbation to the predicted target features’ tangent vectors — the propagation is done via forward mode automatic differentiation.\nThe cosine similarity between the random vector and the tangent vectors of the predicted features then represents the sensitivity of each spatial feature in the target frame to the initial perturbation.\nThis process is repeated <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> times, and the similarity scores are averaged to obtain a final heatmap over the target frame spatial locations.\nIn Figure <a class=\"ltx_ref\" href=\"#S4.F6\" title=\"Figure 6 ‣ 4.4 Analysis of dynamics ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we observe that the highest similarity regions in Target correctly correspond with the initial perturbation locations in Source (green square), indicating that the dynamics models can capture high-level correspondences.\nWe also see that Midway Network without gating units (bottom right, red border) learns an incorrect identity mapping where the highest similarity region is the same location as the initial perturbation.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"447\" id=\"S4.F7.g1\" src=\"./assets/x7.png\" width=\"640\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">High-level tracking using forwarded feature perturbation and/or feature similarity. Midway Network is able to track high-level regions such as the cyclist’s foot (top row, pink square).</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS4.p2\">\n<p class=\"ltx_p\">We may also use forwarded feature perturbation as a form of high-level tracking.\nFirst, for consecutive pairs of frames, we compute perturbation heatmaps over the target spatial features by individually perturbing each spatial feature in the source frame.\nThen, for the first frame of the video, we select an initial location and take the top-5 locations in the next frame with the highest perturbation heatmap scores; from these locations, we select the one with the highest feature similarity.\nThis process repeats with the newly selected location until we have a track across all frames.\nFigure <a class=\"ltx_ref\" href=\"#S4.F7\" title=\"Figure 7 ‣ 4.4 Analysis of dynamics ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows these tracking results in comparison to selecting the next location based on highest feature similarity with DINO <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>)</cite> pretrained on ImageNet (IN1K).\nDespite being trained in latent space, Midway Network is able to roughly track high-level regions over time, whereas the DINO-IN1K feature similarity baseline tracks quickly diverge.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\">Object recognition and motion understanding are complementary aspects of perception, yet most self-supervised methods have focused on learning representations for only one facet.\nWe aim to bridge this gap by extending latent dynamics modeling to the natural video domain.\nIn this work, we propose Midway Network, the first self-supervised learning architecture to learn representations for both recognition and motion solely from natural videos, leveraging an inverse dynamics midway path, a dense forward prediction objective, and a hierarchical structure to capture the complex, multi-object scenes.\nMidway Network learns strong image-level representations for both recognition and motion, and in many cases, outperforms prior approaches on semantic segmentation and optical flow estimation.\nWe have demonstrated that Midway Network can be used across different video datasets and scales well with larger models — training on more diverse data and continuing to scale model capacity could further improve performance.\nAn exciting avenue for future work is to leverage the motion and dynamics captured by Midway Network for real-world planning tasks.\nPossible next steps towards this direction include incorporating action-labeled data and using Midway Network’s forward dynamics predictor within a world modeling framework.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgement</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">We thank members of the NYU Agentic Learning AI Lab for their helpful discussions.\nCH is supported by the DoD NDSEG Fellowship.\nThe work is supported in part by the Institute of Information &amp; Communications Technology Planning Evaluation (IITP) under grant RS-2024-00469482, funded by the Ministry of Science and ICT (MSIT) of the Republic of Korea in connection with the Global AI Frontier Lab International Collaborative Research.\nThe compute is supported by the NYU High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Assran et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nAssran, M., Bardes, A., Fan, D., Garrido, Q., Howes, R., Komeili, M., Muckley, M., Rizvi, A., Roberts, C., Sinha, K., Zholus, A., Arnaud, S., Gejji, A., Martin, A., Robert Hogan, F., Dugas, D., Bojanowski, P., Khalidov, V., Labatut, P., Massa, F., Szafraniec, M., Krishnakumar, K., Li, Y., Ma, X., Chandar, S., Meier, F., LeCun, Y., Rabbat, M., and Ballas, N. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">V-jepa 2: Self-supervised video models enable understanding, prediction and planning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2506.09985</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Assran et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nAssran, M., Duval, Q., Misra, I., Bojanowski, P., Vincent, P., Rabbat, M., LeCun, Y., and Ballas, N. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised learning from images with a joint-embedding predictive architecture.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., Assran, M., and Ballas, N. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Revisiting feature prediction for learning visual representations from video.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Transactions on Machine Learning Research</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Ponce, J., and LeCun, Y. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">VICReg: Variance-invariance-covariance regularization for self-supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bardes et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nBardes, A., Ponce, J., and LeCun, Y. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Mc-jepa: A joint-embedding predictive architecture for self-supervised learning of motion and content features.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2307.12698</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brandfonbrener et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nBrandfonbrener, D., Nachum, O., and Bruna, J. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Inverse dynamics pretraining learns good representations for multitask imitation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Butler et al.,  (2012)</span>\n<span class=\"ltx_bibblock\">\nButler, D. J., Wulff, J., Stanley, G. B., and Black, M. J. (2012).\n\n</span>\n<span class=\"ltx_bibblock\">A naturalistic open source movie for optical flow evaluation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Caron et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nCaron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., and Joulin, A. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Emerging properties in self-supervised vision transformers.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chalasani and Principe,  (2013)</span>\n<span class=\"ltx_bibblock\">\nChalasani, R. and Principe, J. C. (2013).\n\n</span>\n<span class=\"ltx_bibblock\">Deep predictive coding networks.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:1301.3541</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(10)</span>\n<span class=\"ltx_bibblock\">\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020a).\n\n</span>\n<span class=\"ltx_bibblock\">Generative pretraining from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International conference on machine learning</span>, pages 1691–1703. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(11)</span>\n<span class=\"ltx_bibblock\">\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020b).\n\n</span>\n<span class=\"ltx_bibblock\">A simple framework for contrastive learning of visual representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the 37th International Conference on Machine Learning</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nChen, Y., Ge, Y., Li, Y., Ge, Y., Ding, M., Shan, Y., and Liu, X. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Moto: Latent motion token as the bridging language for robot manipulation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2412.04445</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Contributors,  (2020)</span>\n<span class=\"ltx_bibblock\">\nContributors, M. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/open-mmlab/mmsegmentation\" title=\"\">https://github.com/open-mmlab/mmsegmentation</a>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cordts et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nCordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">The cityscapes dataset for semantic urban scene understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cui et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nCui, Z. J., Pan, H., Iyer, A., Haldar, S., and Pinto, L. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Dynamo: In-domain dynamics pretraining for visuo-motor control.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dosovitskiy et al.,  (2015)</span>\n<span class=\"ltx_bibblock\">\nDosovitskiy, A., Fischer, P., Ilg, E., Häusser, P., Hazirbas, C., Golkov, V., Smagt, P. v. d., Cremers, D., and Brox, T. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Flownet: Learning optical flow with convolutional networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Egner et al.,  (2010)</span>\n<span class=\"ltx_bibblock\">\nEgner, T., Monti, J. M., and Summerfield, C. (2010).\n\n</span>\n<span class=\"ltx_bibblock\">Expectation and surprise determine neural population responses in the ventral visual stream.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Neuroscience</span>, 30(49):16601–16608.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fan et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nFan, D., Tong, S., Zhu, J., Sinha, K., Liu, Z., Chen, X., Rabbat, M., Ballas, N., LeCun, Y., Bar, A., et al. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">Scaling language-free visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2504.01017</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Feichtenhofer et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nFeichtenhofer, C., Fan, H., Li, Y., and He, K. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders as spatiotemporal learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Finn et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nFinn, C., Goodfellow, I., and Levine, S. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning for physical interaction through video prediction.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Flanagan and Wing,  (1997)</span>\n<span class=\"ltx_bibblock\">\nFlanagan, J. R. and Wing, A. M. (1997).\n\n</span>\n<span class=\"ltx_bibblock\">The role of internal models in motion planning and control: evidence from grip force adjustments during movements of hand-held loads.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of Neuroscience</span>, 17(4):1519–1528.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Friston,  (2005)</span>\n<span class=\"ltx_bibblock\">\nFriston, K. (2005).\n\n</span>\n<span class=\"ltx_bibblock\">A theory of cortical responses.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Philosophical transactions of the Royal Society of London. Series B, Biological sciences</span>, 360:815–36.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Földiák,  (1991)</span>\n<span class=\"ltx_bibblock\">\nFöldiák, P. (1991).\n\n</span>\n<span class=\"ltx_bibblock\">Learning invariance from transformation sequences.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neural Computation</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Garrido et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nGarrido, Q., Assran, M., Ballas, N., Bardes, A., Najman, L., and LeCun, Y. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Learning and leveraging world models in visual representation learning.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gordon et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nGordon, D., Ehsani, K., Fox, D., and Farhadi, A. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Watching the world go by: Representation learning from unlabeled videos.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Goyal et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nGoyal, P., Duval, Q., Seessel, I., Caron, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Vision models are more robust and fair when pretrained on uncurated images without supervision.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grill et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nGrill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ha and Schmidhuber,  (2018)</span>\n<span class=\"ltx_bibblock\">\nHa, D. and Schmidhuber, J. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">World models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hafner et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Dream to control: Learning behaviors by latent imagination.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hafner et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Learning latent dynamics for planning from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Machine Learning</span>, pages 2555–2565.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nHe, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Masked autoencoders are scalable vision learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 16000–16009.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Momentum contrast for unsupervised visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nHu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., and Corrado, G. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Gaia-1: A generative world model for autonomous driving.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2309.17080</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Huang et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nHuang, Z., Shi, X., Zhang, C., Wang, Q., Cheung, K. C., Qin, H., Dai, J., and Li, H. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">FlowFormer: A transformer architecture for optical flow.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">European Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jabri et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nJabri, A., Owens, A., and Efros, A. A. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Space-time correspondence as a contrastive random walk.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jonschkowski et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nJonschkowski, R., Stone, A., Barron, J., Gordon, A., Konolige, K., and Angelova, A. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">What matters in unsupervised optical flow.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jordan and Rumelhart,  (2013)</span>\n<span class=\"ltx_bibblock\">\nJordan, M. I. and Rumelhart, D. E. (2013).\n\n</span>\n<span class=\"ltx_bibblock\">Forward models: Supervised learning with a distal teacher.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Backpropagation</span>, pages 189–236. Psychology Press.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kitazawa et al.,  (1998)</span>\n<span class=\"ltx_bibblock\">\nKitazawa, S., Kimura, T., and Yin, P.-B. (1998).\n\n</span>\n<span class=\"ltx_bibblock\">Cerebellar complex spikes encode both destinations and errors in arm movements.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature</span>, 392(6675):494–497.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lee and Mumford,  (2003)</span>\n<span class=\"ltx_bibblock\">\nLee, T. S. and Mumford, D. (2003).\n\n</span>\n<span class=\"ltx_bibblock\">Hierarchical bayesian inference in the visual cortex.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of the Optical Society of America A</span>, 20(7):1434–1448.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lin et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nLin, H., Cheng, X., Wu, X., and Shen, D. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Cat: Cross attention in vision transformer.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Multimedia and Expo</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al.,  (2019)</span>\n<span class=\"ltx_bibblock\">\nLiu, P., Lyu, M. R., King, I., and Xu, J. (2019).\n\n</span>\n<span class=\"ltx_bibblock\">Selflow: Self-supervised learning of optical flow.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">CVPR</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Livingstone,  (1998)</span>\n<span class=\"ltx_bibblock\">\nLivingstone, M. S. (1998).\n\n</span>\n<span class=\"ltx_bibblock\">Mechanisms of direction selectivity in macaque v1.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neuron</span>, 20(3):509–526.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lotter et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nLotter, W., Kreiman, G., and Cox, D. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Deep predictive coding networks for video prediction and unsupervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Luo et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nLuo, K., Wang, C., Liu, S., Fan, H., Wang, J., and Sun, J. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Upflow: Upsampling pyramid for unsupervised optical flow learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 1045–1054.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mayer et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nMayer, N., Ilg, E., Häusser, P., Fischer, P., Cremers, D., Dosovitskiy, A., and Brox, T. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Menapace et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nMenapace, W., Lathuilière, S., Siarohin, A., Theobalt, C., Tulyakov, S., Golyanik, V., and Ricci, E. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Playable environments: Video manipulation in space and time.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 3584–3593.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Miall and Wolpert,  (1996)</span>\n<span class=\"ltx_bibblock\">\nMiall, R. C. and Wolpert, D. M. (1996).\n\n</span>\n<span class=\"ltx_bibblock\">Forward models for physiological motor control.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neural networks</span>, 9(8):1265–1279.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Oquab et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nOquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Dinov2: Learning robust visual features without supervision.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2304.07193</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Parker-Holder et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nParker-Holder, J., Ball, P., Bruce, J., Dasagi, V., Holsheimer, K., Kaplanis, C., Moufarek, A., Scully, G., Shar, J., Shi, J., Spencer, S., Yung, J., Dennis, M., Kenjeyev, S., Long, S., Mnih, V., Chan, H., Gazeau, M., Li, B., Pardo, F., Wang, L., Zhang, L., Besse, F., Harley, T., Mitenkova, A., Wang, J., Clune, J., Hassabis, D., Hadsell, R., Bolton, A., Singh, S., and Rocktäschel, T. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Genie 2: A large-scale foundation world model.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Pathak et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nPathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and Efros, A. A. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Context encoders: Feature learning by inpainting.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 2536–2544.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Qing et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nQing, Z., Zhang, S., Huang, Z., Xu, Y., Wang, X., Tang, M., Gao, C., Jin, R., and Sang, N. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Learning from untrimmed videos: Self-supervised video representation learning with hierarchical consistency.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ranftl et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nRanftl, R., Bochkovskiy, A., and Koltun, V. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Vision transformers for dense prediction.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rao and Ballard,  (1999)</span>\n<span class=\"ltx_bibblock\">\nRao, R. and Ballard, D. (1999).\n\n</span>\n<span class=\"ltx_bibblock\">Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature neuroscience</span>, 2:79–87.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rao and Sejnowski,  (1999)</span>\n<span class=\"ltx_bibblock\">\nRao, R. and Sejnowski, T. J. (1999).\n\n</span>\n<span class=\"ltx_bibblock\">Predictive sequence learning in recurrent neocortical circuits.\n\n</span>\n<span class=\"ltx_bibblock\">In Solla, S., Leen, T., and Müller, K., editors, <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, volume 12. MIT Press.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Schmidt and Jiang,  (2024)</span>\n<span class=\"ltx_bibblock\">\nSchmidt, D. and Jiang, M. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Learning to act without actions.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Schwarzer et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nSchwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin, L., Hjelm, R. D., Bachman, P., and Courville, A. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Pretraining representations for data-efficient reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\">In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shidara et al.,  (1993)</span>\n<span class=\"ltx_bibblock\">\nShidara, M., Kawano, K., Gomi, H., and Kawato, M. (1993).\n\n</span>\n<span class=\"ltx_bibblock\">Inverse-dynamics model eye movement control by purkinje cells in the cerebellum.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature</span>, 365(6441):50–52.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Simonyan and Zisserman,  (2014)</span>\n<span class=\"ltx_bibblock\">\nSimonyan, K. and Zisserman, A. (2014).\n\n</span>\n<span class=\"ltx_bibblock\">Two-stream convolutional networks for action recognition in videos.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Softky,  (1995)</span>\n<span class=\"ltx_bibblock\">\nSoftky, W. (1995).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised pixel-prediction.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Srivastava et al.,  (2015)</span>\n<span class=\"ltx_bibblock\">\nSrivastava, N., Mansimov, E., and Salakhudinov, R. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of video representations using lstms.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Machine Learning</span>, pages 843–852. PMLR.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Stone et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nStone, A., Maurer, D., Ayvaci, A., Angelova, A., and Jonschkowski, R. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Smurf: Self-teaching multi-frame unsupervised raft with full-image warping.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 3887–3896.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Summerfield et al.,  (2006)</span>\n<span class=\"ltx_bibblock\">\nSummerfield, C., Egner, T., Greene, M., Koechlin, E., Mangels, J., and Hirsch, J. (2006).\n\n</span>\n<span class=\"ltx_bibblock\">Predictive codes for forthcoming perception in the frontal cortex.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Science</span>, 314(5803):1311–1314.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sun et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nSun, D., Yang, X., Liu, M.-Y., and Kautz, J. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sun et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nSun, Y., Zhou, H., Yuan, L., Sun, J. J., Li, Y., Jia, X., Adam, H., Hariharan, B., Zhao, L., and Liu, T. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Video creation by demonstration.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Teed and Deng,  (2020)</span>\n<span class=\"ltx_bibblock\">\nTeed, Z. and Deng, J. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Raft: Recurrent all-pairs field transforms for optical flow.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tong et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nTong, Z., Song, Y., Wang, J., and Wang, L. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training.\n\n</span>\n<span class=\"ltx_bibblock\">In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K., editors, <span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Valpola,  (2015)</span>\n<span class=\"ltx_bibblock\">\nValpola, H. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">From neural pca to deep unsupervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Advances in Independent Component Analysis and Learning Machines</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vaswani et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Attention is all you need.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Venkataramanan et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nVenkataramanan, S., Rizve, M. N., Carreira, J., Asano, Y. M., and Avrithis, Y. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Is imagenet worth 1 video? learning strong image encoders from 1 long unlabelled video.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Villegas et al.,  (2018)</span>\n<span class=\"ltx_bibblock\">\nVillegas, R., Erhan, D., Lee, H., et al. (2018).\n\n</span>\n<span class=\"ltx_bibblock\">Hierarchical long-term video prediction without supervision.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Machine Learning</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vincent et al.,  (2010)</span>\n<span class=\"ltx_bibblock\">\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A., and Bottou, L. (2010).\n\n</span>\n<span class=\"ltx_bibblock\">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Journal of machine learning research</span>, 11(12).\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vondrick et al.,  (2016)</span>\n<span class=\"ltx_bibblock\">\nVondrick, C., Pirsiavash, H., and Torralba, A. (2016).\n\n</span>\n<span class=\"ltx_bibblock\">Anticipating visual representations from unlabeled video.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib73\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nWang, A. N., Hoang, C., Xiong, Y., LeCun, Y., and Ren, M. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">Poodle: Pooled and dense self-supervised learning from naturalistic videos.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib74\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nWang, W., Zhu, D., Wang, X., Hu, Y., Qiu, Y., Wang, C., Hu, Y., Kapoor, A., and Scherer, S. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Tartanair: A dataset to push the limits of visual slam.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE/RSJ International Conference on Intelligent Robots and Systems</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib75\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang and Gupta,  (2015)</span>\n<span class=\"ltx_bibblock\">\nWang, X. and Gupta, A. (2015).\n\n</span>\n<span class=\"ltx_bibblock\">Unsupervised learning of visual representations using videos.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib76\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nWang, X., Zhang, R., Shen, C., Kong, T., and Li, L. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Dense contrastive learning for self-supervised visual pre-training.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib77\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nWei, C., Fan, H., Xie, S., Wu, C.-Y., Yuille, A., and Feichtenhofer, C. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Masked feature prediction for self-supervised visual pre-training.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib78\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Weinzaepfel et al.,  (2022)</span>\n<span class=\"ltx_bibblock\">\nWeinzaepfel, P., Leroy, V., Lucas, T., Brégier, R., Cabon, Y., Arora, V., Antsfeld, L., Chidlovskii, B., Csurka, G., and Jérôme, R. (2022).\n\n</span>\n<span class=\"ltx_bibblock\">Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">NeurIPS</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib79\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Weinzaepfel et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nWeinzaepfel, P., Lucas, T., Leroy, V., Cabon, Y., Arora, V., Brégier, R., Csurka, G., Antsfeld, L., Chidlovskii, B., and Revaud, J. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">ICCV</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib80\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wiskott and Sejnowski,  (2002)</span>\n<span class=\"ltx_bibblock\">\nWiskott, L. and Sejnowski, T. J. (2002).\n\n</span>\n<span class=\"ltx_bibblock\">Slow feature analysis: Unsupervised learning of invariances.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Neural computation</span>, 14(4):715–770.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib81\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wolpert et al.,  (1995)</span>\n<span class=\"ltx_bibblock\">\nWolpert, D. M., Ghahramani, Z., and Jordan, M. I. (1995).\n\n</span>\n<span class=\"ltx_bibblock\">An internal model for sensorimotor integration.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Science</span>, 269(5232):1880–1882.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib82\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wolpert et al.,  (1998)</span>\n<span class=\"ltx_bibblock\">\nWolpert, D. M., Miall, R. C., and Kawato, M. (1998).\n\n</span>\n<span class=\"ltx_bibblock\">Internal models in the cerebellum.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Trends in cognitive sciences</span>, 2(9):338–347.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib83\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xie et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nXie, Z., Lin, Y., Zhang, Z., Cao, Y., Lin, S., and Hu, H. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib84\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xiong et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nXiong, Y., Ren, M., Zeng, W., and Urtasun, R. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised representation learning from flow equivariance.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Computer Vision</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib85\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu and Wang,  (2021)</span>\n<span class=\"ltx_bibblock\">\nXu, J. and Wang, X. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">Rethinking self-supervised correspondence learning: A video frame-level similarity perspective.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">IEEE International Conference on Computer Vision</span>, pages 10075–10085.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib86\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nYang, S., Du, Y., Ghasemipour, S. K. S., Tompson, J., Kaelbling, L. P., Schuurmans, D., and Abbeel, P. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Learning interactive real-world simulators.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib87\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et al.,  (2025)</span>\n<span class=\"ltx_bibblock\">\nYe, S., Jang, J., Jeon, B., Joo, S. J., Yang, J., Peng, B., Mandlekar, A., Tan, R., Chao, Y.-W., Lin, B. Y., Liden, L., Lee, K., Gao, J., Zettlemoyer, L., Fox, D., and Seo, M. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">Latent action pretraining from videos.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib88\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al.,  (2020)</span>\n<span class=\"ltx_bibblock\">\nYu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., and Darrell, T. (2020).\n\n</span>\n<span class=\"ltx_bibblock\">Bdd100k: A diverse driving dataset for heterogeneous multitask learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib89\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al.,  (2023)</span>\n<span class=\"ltx_bibblock\">\nZhang, S., Zhu, F., Zhao, R., and Yan, J. (2023).\n\n</span>\n<span class=\"ltx_bibblock\">Patch-level contrasting without patch correspondence for accurate and dense contrastive representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib90\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al.,  (2017)</span>\n<span class=\"ltx_bibblock\">\nZhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., and Torralba, A. (2017).\n\n</span>\n<span class=\"ltx_bibblock\">Scene parsing through ade20k dataset.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib91\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al.,  (2024)</span>\n<span class=\"ltx_bibblock\">\nZhou, G., Pan, H., LeCun, Y., and Pinto, L. (2024).\n\n</span>\n<span class=\"ltx_bibblock\">Dino-wm: World models on pre-trained visual features enable zero-shot planning.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib92\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al.,  (2021)</span>\n<span class=\"ltx_bibblock\">\nZhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T. (2021).\n\n</span>\n<span class=\"ltx_bibblock\">ibot: Image bert pre-training with online tokenizer.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">International Conference on Learning Representations</span>.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_appendix\" id=\"Ax1\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n</section>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional results</h2>\n<section class=\"ltx_subsection\" id=\"A1.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Longer pretraining</h3>\n<div class=\"ltx_para\" id=\"A1.SS1.p1\">\n<p class=\"ltx_p\">We provide additional experiments on WT-Venice pretraining below.\nTable <a class=\"ltx_ref\" href=\"#A1.T4\" title=\"Table 4 ‣ A.1 Longer pretraining ‣ Appendix A Additional results ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that Midway Network’s downstream performance continues to improve with longer pretraining.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A1.T4\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 4</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Semantic segmentation and optical flow evaluations for additional experiments on WT-Venice <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m2\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution pretraining. Sem. Seg. and optical flow are conducted with finetuning.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:338.1pt;height:38.2pt;vertical-align:-17.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-101.6pt,11.5pt) scale(0.624550562899885,0.624550562899885) ;\">\n<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WT-Sem Sem. Seg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ADE20K Sem. Seg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Optical Flow</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UperNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UperNet</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FlyingThings</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MPI-Sintel</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Arch</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ep.</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">mIoU</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">Acc</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">86.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.1</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Model capacity ablations</h3>\n<div class=\"ltx_para\" id=\"A1.SS2.p1\">\n<p class=\"ltx_p\">We investigate how the model capacity of Midway Network’s components affects performance, namely the midway path and forward dynamics model, shown in Table <a class=\"ltx_ref\" href=\"#A1.T5\" title=\"Table 5 ‣ A.2 Model capacity ablations ‣ Appendix A Additional results ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nFor reference, Midway Network uses 4 layers and embedding dimension of 192 for the midway path and 4 layers and embedding dimension of 384 for the forward dynamics model.\nReducing capacity of the midway path primarily harms optical flow performance.\nOn the other hand, adding capacity (<math alttext=\"2\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\">×</mo></mrow><annotation encoding=\"application/x-tex\">2\\times</annotation></semantics></math> midway dim) improves EPE and hurts mIoU, likely because the motion latents can capture more information from the paired frames, but consequently, the forward prediction objective is made easier with the increased motion latent size.\nPerformance drops with fewer forward model layers, indicating that having more model capacity for forward prediction is beneficial.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A1.T5\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 5</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablations studies on capacity of Midway Network’s midway path and forward dynamics model evaluated on BDD100K semantic segmentation linear readout and MPI-Sintel optical flow finetuning.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:120.8pt;height:59.8pt;vertical-align:-28.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-37.7pt,18.7pt) scale(0.615792134933756,0.615792134933756) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ablation</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mIoU</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EPE</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Full model</span>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"0.5\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.T5.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.5</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\">×</mo></mrow><annotation encoding=\"application/x-tex\">0.5\\times</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> midway dim</span>\n</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"2\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.T5.m4\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\">×</mo></mrow><annotation encoding=\"application/x-tex\">2\\times</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> midway dim</span>\n</th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1‑layer midway</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2‑layer midway</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1‑layer forward</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2‑layer forward</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.8</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>ADE20K linear readout</h3>\n<div class=\"ltx_para\" id=\"A1.SS3.p1\">\n<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"#A1.T6\" title=\"Table 6 ‣ A.3 ADE20K linear readout ‣ Appendix A Additional results ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows evaluation results for ADE20K semantic segmentation linear readout.\nPerformance trends follow the UperNet finetuning results in Table <a class=\"ltx_ref\" href=\"#S4.T2\" title=\"Table 2 ‣ BDD100K pretraining. ‣ 4.2 Semantic segmentation and optical flow results ‣ 4 Experiments ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAgain, Midway Network is competitive with baselines, PooDLe and DoRA, and furthermore, it does not rely on an external supervised optical flow network and can jointly learn representations for motion understanding.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A1.T6\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 6</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">ADE20K semantic segmentation linear readout evaluations for WT-Venice <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m3\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution pretraining. <sup class=\"ltx_sup\">†</sup>PooDLe on <math alttext=\"512\\times 1024\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m4\" intent=\":literal\"><semantics><mrow><mn>512</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">512\\times 1024</annotation></semantics></math> resolution pretraining from their original table <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:172.5pt;height:37.3pt;vertical-align:-17.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-138.4pt,29.9pt) scale(0.383991652355384,0.383991652355384) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Arch</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ep.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mIoU</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↑</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Acc</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">PooDLe</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">†</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">R50</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MAE </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>He et al., <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">VideoMAE </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Tong et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DINO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib78\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DoRA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">63.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.3</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.4 </span>Optical flow frozen readout</h3>\n<div class=\"ltx_para\" id=\"A1.SS4.p1\">\n<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"#A1.T7\" title=\"Table 7 ‣ A.4 Optical flow frozen readout ‣ Appendix A Additional results ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides evaluation results for optical flow linear readout.\nHere, the backbone parameters of each method are frozen and only the DPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Ranftl et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2021</a>)</cite> head is trained using the same data as the optical flow finetuning experiments.\nMidway Network’s learned representations again achieve strong performance relative to the baselines.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A1.T7\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 7</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Optical flow frozen readout evaluations for BDD100K <math alttext=\"224\\times 224\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m2\" intent=\":literal\"><semantics><mrow><mn>224</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">×</mo><mn>224</mn></mrow><annotation encoding=\"application/x-tex\">224\\times 224</annotation></semantics></math> resolution pretraining.</span></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:258.8pt;height:63.8pt;vertical-align:-30.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-165.8pt,40.9pt) scale(0.438369456836412,0.438369456836412) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FlyingThings</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MPI-Sintel</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Arch</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ep.</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (c)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">↓</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">EPE (f)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">iBOT </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib92\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">800</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DINO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.5</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">VideoMAE </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Tong et al., <a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">11.6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DoRA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Venkataramanan et al., <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">2024</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.6</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Midway (enc. only)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">18.8</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">11.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-S</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DINO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">19.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.2</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">(</span>Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway</span>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">ViT-B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">13.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.9</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Implementation details</h2>\n<div class=\"ltx_para\" id=\"A2.p1\">\n<p class=\"ltx_p\">In this section, we provide additional details on the implementation of Midway Network, the pretraining and evaluation setups, and compute resources used for our experiments.\nThe experiments were implemented using the <span class=\"ltx_text ltx_font_typewriter\">PyTorch</span> framework.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"A2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Architecture</h3>\n<div class=\"ltx_para\" id=\"A2.SS1.p1\">\n<p class=\"ltx_p\">The ViT encoders have 12 feature levels, and we perform the dense forward prediction objective at levels 3, 6, and 9.\nThe midway path infers motion latents with feature inputs at level 12 for the level 9 objective and refines them as described in Section <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Midway Network ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for levels 6 and 3.\nThe midway inverse dynamics model at each level is a 4-block transformer with feature dimension of <math alttext=\"192\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mn>192</mn><annotation encoding=\"application/x-tex\">192</annotation></semantics></math>, with linear projectors to map from and to the original feature dimension.\nWe use 10 learnable tokens for the motion latents.\nThe backward layers are 1-block cross-attention transformers with feature dimension equal to the dimension of the underlying ViT encoder, i.e. <math alttext=\"384\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m2\" intent=\":literal\"><semantics><mn>384</mn><annotation encoding=\"application/x-tex\">384</annotation></semantics></math> for ViT-S and <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m3\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math> for ViT-B.\nThe forward dynamics model at each level is a 4-block transformer with feature dimension equal to the underlying encoder dimension as well.\nThe learnable gating units are placed at all but the first block.\nEach gating unit is a multi-layer perceptron with 1 hidden layer of same dimension as the encoder, GELU activation, and a final sigmoid activation.\nTo bias the initial gating weights towards 1, i.e. the original fully-weighted residual connection, we add a bias of 4 to the input of the sigmoid.</p>\n</div>\n<div class=\"ltx_para\" id=\"A2.SS1.p2\">\n<p class=\"ltx_p\">We follow DINO <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>)</cite> for implementation of the joint-embedding invariance objective, using the same projection heads, centering and sharpening operations, and temperature schedules as described in their paper.\nGiven that we have 2 paired video frames as input, we can sample 2 global crops and 8 local crops from each frame and compute the loss between crops across frames to leverage the natural temporal motion augmentation.\nThe loss is also symmetrical, where we compute the loss for the original frame ordering as well as the reversed ordering.\nWe utilize this setup for the DINO baseline as well for fair comparison.\nThe final loss is an equal-weighted sum of the dense forward prediction loss, averaged over the feature levels, and the joint-embedding invariance loss:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"A2.E3\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathcal{L}=\\frac{1}{L}\\sum_{l=1}^{L}\\mathcal{L}_{dyn}^{l}+\\mathcal{L}_{inv}.\" class=\"ltx_Math\" display=\"block\" id=\"A2.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mo>=</mo><mrow><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">​</mo><mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msubsup><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi></mrow><mi>l</mi></msubsup></mrow></mrow><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">ℒ</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">​</mo><mi>v</mi></mrow></msub></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\frac{1}{L}\\sum_{l=1}^{L}\\mathcal{L}_{dyn}^{l}+\\mathcal{L}_{inv}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Pretraining</h3>\n<div class=\"ltx_para\" id=\"A2.SS2.p1\">\n<p class=\"ltx_p\">We outline the hyperparameters used for pretraining in Table <a class=\"ltx_ref\" href=\"#A2.T8\" title=\"Table 8 ‣ B.2 Pretraining ‣ Appendix B Implementation details ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nThe hyperparameters largely follow the DINO <cite class=\"ltx_cite ltx_citemacro_citep\">(Caron et al., <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">2021</a>)</cite> training recipe.\nWe use the same hyperparameters for BDD100K and Walking Tours pretraining.\nFor BDD100K, we utilize repeat sampling following MAE-st <cite class=\"ltx_cite ltx_citemacro_citep\">(Feichtenhofer et al., <a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">2022</a>)</cite>, which samples <math alttext=\"R=5\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>R</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">R=5</annotation></semantics></math> frames each time a video is seen for faster data loading.\nTherefore, we treat each pass through the dataset as <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> epochs.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A2.T8\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Hyperparameters used for full Midway Network experiments.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Learning rate</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T8.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">5</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">×</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">−</mo><mn mathsize=\"0.900em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Learning rate warmup</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">10 epochs</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Learning rate schedule</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">cosine</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Batch size</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Weight decay</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Weight decay end</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Optimizer</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">AdamW</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Betas</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">(0.9, 0.999)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gradient clip norm</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Drop path rate</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Use FP16</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Yes</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Baselines</h3>\n<div class=\"ltx_para\" id=\"A2.SS3.p1\">\n<p class=\"ltx_p\">We use the official implementations to pretrain the baselines on BDD100K and Walking Tours.\nWe use the released checkpoints for DINO, DoRA, and PooDLe on Walking Tours; semantic segmentation finetuning results for MAE, DINO, DoRA, and PooDLe are also from the original table in PooDLe <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>Evaluation</h3>\n<div class=\"ltx_para\" id=\"A2.SS4.p1\">\n<p class=\"ltx_p\">For the semantic segmentation tasks, we follow the ViT-based setup described in PooDLe <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a class=\"ltx_ref\" href=\"#bib.bib73\" title=\"\">2025</a>)</cite>, based on the <span class=\"ltx_text ltx_font_typewriter\">mmsegmentation</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Contributors, <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">2020</a>)</cite> codebase.\nThe linear and UperNet readout setups for BDD100K and CityScapes were originally from FlowE <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiong et al., <a class=\"ltx_ref\" href=\"#bib.bib84\" title=\"\">2021</a>)</cite>; the UperNet finetuning setup for ADE20K was originally from iBOT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"#bib.bib92\" title=\"\">2021</a>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"A2.SS4.p2\">\n<p class=\"ltx_p\">For the optical flow tasks, we follow the finetuning evaluation setup described in CroCo v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Weinzaepfel et al., <a class=\"ltx_ref\" href=\"#bib.bib79\" title=\"\">2023</a>)</cite> and use their official implementation.\nOur main results follow CroCo v2’s setup for Table 1 from their paper; our ablation studies follow their setup for their Table 11 (“smaller training data”) to match the settings of other optical flow methods.\nThe primary difference is that we replace CroCo v2’s decoder with Midway Network’s midway inverse dynamics and forward dynamics models.\nWe use the following as input to the DPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Ranftl et al., <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">2021</a>)</cite> that outputs the optical flow predictions: dense tokens of encoder feature level 12, dense spatial tokens corresponding to the target frame processed by the midway model at the highest level of the dense objective, dense token prediction of the forward model at the highest objective level, and dense token prediction of the forward model at the lowest objective level.\nFor reference, the midway model processes the dense spatial tokens from the source and target frames alongside the motion latents.\nWe use this architecture for all other baselines besides CroCo v2 with randomly initialized weights, as they do not have binocular components.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.5 </span>Compute and training costs</h3>\n<div class=\"ltx_para\" id=\"A2.SS5.p1\">\n<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"#A2.T9\" title=\"Table 9 ‣ B.5 Compute and training costs ‣ Appendix B Implementation details ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> provides a comparison on training cost in FLOPs per single training example and model size in parameters for Midway Network and the baseline methods.\nMidway Network uses less than half of the FLOPs of prior video data-based learning methods, PooDLe and DoRA.\nThe dynamics networks of Midway Network use more parameters to capture motion information, but avoid costly iterative refinement operations used by prior flow methods such as RAFT <cite class=\"ltx_cite ltx_citemacro_citep\">(Teed and Deng, <a class=\"ltx_ref\" href=\"#bib.bib65\" title=\"\">2020</a>)</cite> and FlowFormer <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">2022</a>)</cite>.\nTable <a class=\"ltx_ref\" href=\"#A2.T10\" title=\"Table 10 ‣ B.5 Compute and training costs ‣ Appendix B Implementation details ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the compute resources used for the experiments.</p>\n</div>\n<figure class=\"ltx_table\" id=\"A2.T9\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Training cost (GLOPs per example) and model size (millions of parameters) of Midway Network and baseline methods.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training cost (GFLOPs)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Parameters (millions)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Midway Network</span>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.8</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7 (encoder), 36.6 (dynamics networks)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">PooDLe</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">202.3</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.5 (encoder), 12.1 (spatial decoder)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">DoRA</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">202.1</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7 (encoder)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CroCo v2</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7 (encoder), 7.2 (decoder)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">DynaMo</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.9</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7 (encoder), 13.0 (dynamics networks)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoMAE</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.6</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.0 (encoder), 2.0 (decoder)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">iBOT</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.3</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7 (encoder)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">DINO</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7 (encoder)</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n<figure class=\"ltx_table\" id=\"A2.T10\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Compute resources and time used for Midway Network experiments.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experiment</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Epochs</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Resources</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Time</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">BDD100K ViT-S pretraining</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2 A100 GPUs</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">66 hours</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">BDD100K ViT-B pretraining</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">8 RTX A6000 GPUs</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">27 hours</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">BDD100K ViT-S ablations</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></th>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">2 A100 GPUs</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">24 hours</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Walking Tours ViT-S pretraining</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4 RTX A6000 GPUs</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">29 hours</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>More visualizations</h2>\n<div class=\"ltx_para\" id=\"A3.p1\">\n<p class=\"ltx_p\">We show additional visualizations of predictions from the semantic segmentation evaluations in Figure <a class=\"ltx_ref\" href=\"#A3.F8\" title=\"Figure 8 ‣ Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for CityScapes, Figure <a class=\"ltx_ref\" href=\"#A3.F8\" title=\"Figure 8 ‣ Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> for WT-Sem, and Figure <a class=\"ltx_ref\" href=\"#A3.F10\" title=\"Figure 10 ‣ Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> for ADE20K, and optical flow evaluations for models pretrained on Walking Tours in Figure <a class=\"ltx_ref\" href=\"#A3.F11\" title=\"Figure 11 ‣ Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"193\" id=\"A3.F8.g1\" src=\"./assets/x8.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of CityScapes semantic segmentation UperNet readout. Midway Network generates cleaner boundaries, particularly for the crossing pedestrians.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"217\" id=\"A3.F9.g1\" src=\"./assets/x9.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of WT-Sem semantic segmentation UperNet finetuning. Midway Network is able to produce reasonable segmentation masks, even in cluttered scenes.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"253\" id=\"A3.F10.g1\" src=\"./assets/x10.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of ADE20K semantic segmentation UperNet finetuning. Midway Network generates more accurate segmentation masks compared to CroCo v2.\n</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"207\" id=\"A3.F11.g1\" src=\"./assets/x11.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Visualization of FlyingThings and MPI-Sintel optical flow evaluations after finetuning for models pretrained on WT-Venice. Midway Network is able to generate more accurate optical flow predictions compared to CroCo v2 and DINO.\n</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"A3.p2\">\n<p class=\"ltx_p\">We also include more examples of the forwarded feature perturbation analysis of Midway Network’s learned dynamics, with heatmaps in Figure <a class=\"ltx_ref\" href=\"#A3.F12\" title=\"Figure 12 ‣ Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and high-level tracking in Figure <a class=\"ltx_ref\" href=\"#A3.F13\" title=\"Figure 13 ‣ Appendix C More visualizations ‣ Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F12\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"239\" id=\"A3.F12.g1\" src=\"./assets/x12.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 12</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">Heatmaps for forwarded feature perturbation in Source (green squares); shown in Target at the same location to highlight motion. The learned dynamics can capture high-level correspondence, such as the right taillight of the black car (bottom left).</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F13\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"450\" id=\"A3.F13.g1\" src=\"./assets/x13.png\" width=\"647\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 13</span>: </span><span class=\"ltx_text\" style=\"font-size:90%;\">High-level tracking using forwarded feature perturbation and/or feature similarity. Midway Network is able to track high-level regions through motion transformations, such as the back of the toddler (top row, pink square).</span></figcaption>\n</figure>\n</section>",
  "css": "",
  "arxiv_id": "2510.05558",
  "source": "arxiv-experimental",
  "generated": "2025-10-18T01:15:15.350Z"
}
