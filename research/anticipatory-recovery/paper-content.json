{
  "html": "<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">Large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">devlin2018bert, </a>; <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">brown2020language, </a>; <a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">touvron2023llama, </a>; <a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">openai2023gpt4, </a>)</cite> have demonstrated remarkable general capabilities in a wide range of natural language tasks.\nDuring the training of LLMs, documents are typically uniformly sampled at random. Due to the large scale of the training set—in contrast to many other domains—LLM training typically occurs in an online fashion: each document is used only once for just one update step without further repetition <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">hoffmann2022training, </a>; <a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">chowdhery2023palm, </a>; <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">xue2023repeat, </a>)</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">Such a training style is in stark contrast with how real world agents like humans acquire new knowledge. In naturalistic settings, the material we are exposed to is structured in time and often repeats in predictable, quasi-cyclic patterns (e.g., a person’s everyday morning routine consists of first taking a shower, then eating breakfast, and finally dressing up).\nHence it is important to understand how existing deep learning methods and architectures perform in this setting.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">Toward the goal of investigating more naturalistic training setups, we study a simplistic setting involving structured training of LLMs: documents are presented cyclically in a fixed sequence and repeated multiple times, just as we humans go through our daily routines. Moreover, to account for the cost of switching among documents (analogous to the mental switching cost between different environments and the waiting cost of obtaining new data), we allow the network to take multiple gradient steps for each document. Compared to standard task-incremental and class-incremental continual learning settings <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">chen2018lifelong </a></cite> which experience each task only once, our cyclic training setting better approximates the quasi-cyclic temporal structure of real-world environments.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"180\" id=\"S1.F1.g1\" src=\"./assets/x1.png\" width=\"691\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S1.F1.2.1.1\" style=\"font-size:90%;\">Figure 1</span>: </span><span class=\"ltx_text\" id=\"S1.F1.3.2\" style=\"font-size:90%;\">(a) Loss curves on document 1 for cyclic and random shuffled fine-tuning on a pre-trained Pythia-1B model. The black circles indicate points just prior to training on the focal document. The inverted-U loss curves within each epoch demonstrate the anticipatory recovery phenomenon. (b) Shift-averaged loss curve for cyclic fine-tuning. (c) Online loss curves for cyclic and random shuffled fine-tuning with prequential evaluation.</span></figcaption>\n</figure>\n<figure class=\"ltx_table\" id=\"S1.T1\">\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.8\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S1.T1.8.9.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.8.9.1.1\"><span class=\"ltx_text\" id=\"S1.T1.8.9.1.1.1\" style=\"font-size:90%;\">Model Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S1.T1.8.9.1.2\"><span class=\"ltx_text\" id=\"S1.T1.8.9.1.2.1\" style=\"font-size:90%;\">410M</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S1.T1.8.9.1.3\"><span class=\"ltx_text\" id=\"S1.T1.8.9.1.3.1\" style=\"font-size:90%;\">1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S1.T1.8.9.1.4\"><span class=\"ltx_text\" id=\"S1.T1.8.9.1.4.1\" style=\"font-size:90%;\">1.4B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S1.T1.8.9.1.5\"><span class=\"ltx_text\" id=\"S1.T1.8.9.1.5.1\" style=\"font-size:90%;\">2.8B</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.4.4.5\"><span class=\"ltx_text\" id=\"S1.T1.4.4.5.1\" style=\"font-size:90%;\">Cyclic</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.1.1\" style=\"font-size:90%;\">1.09 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.1.1.1.1.m1.1\"><semantics id=\"S1.T1.1.1.1.1.m1.1a\"><mo id=\"S1.T1.1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.1.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.1.1.1.1.m1.1d\">±</annotation></semantics></math> 0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.2.2.2.1\" style=\"font-size:90%;\">1.03 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.2.2.2.1.m1.1\"><semantics id=\"S1.T1.2.2.2.1.m1.1a\"><mo id=\"S1.T1.2.2.2.1.m1.1.1\" xref=\"S1.T1.2.2.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.2.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.2.2.2.1.m1.1.1.cmml\" xref=\"S1.T1.2.2.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.2.2.2.1.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.2.2.2.1.m1.1d\">±</annotation></semantics></math> 0.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.3.3.3.1\" style=\"font-size:90%;\">1.14 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.3.3.3.1.m1.1\"><semantics id=\"S1.T1.3.3.3.1.m1.1a\"><mo id=\"S1.T1.3.3.3.1.m1.1.1\" xref=\"S1.T1.3.3.3.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.3.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.3.3.3.1.m1.1.1.cmml\" xref=\"S1.T1.3.3.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.3.3.3.1.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.3.3.3.1.m1.1d\">±</annotation></semantics></math> 0.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.4.4.4.1\" style=\"font-size:90%;\">1.34 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.4.4.4.1.m1.1\"><semantics id=\"S1.T1.4.4.4.1.m1.1a\"><mo id=\"S1.T1.4.4.4.1.m1.1.1\" xref=\"S1.T1.4.4.4.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.4.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.4.4.4.1.m1.1.1.cmml\" xref=\"S1.T1.4.4.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.4.4.4.1.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.4.4.4.1.m1.1d\">±</annotation></semantics></math> 0.06</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.8.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" id=\"S1.T1.8.8.5\"><span class=\"ltx_text\" id=\"S1.T1.8.8.5.1\" style=\"font-size:90%;\">Shuffle</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S1.T1.5.5.1\">\n<span class=\"ltx_text\" id=\"S1.T1.5.5.1.1\" style=\"font-size:90%;\">1.34 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.5.5.1.m1.1\"><semantics id=\"S1.T1.5.5.1.m1.1a\"><mo id=\"S1.T1.5.5.1.m1.1.1\" mathsize=\"90%\" xref=\"S1.T1.5.5.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.5.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.5.5.1.m1.1.1.cmml\" xref=\"S1.T1.5.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.5.5.1.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.5.5.1.m1.1d\">±</annotation></semantics></math><span class=\"ltx_text\" id=\"S1.T1.5.5.1.2\" style=\"font-size:90%;\"> 0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S1.T1.6.6.2\">\n<span class=\"ltx_text\" id=\"S1.T1.6.6.2.1\" style=\"font-size:90%;\">1.51 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.6.6.2.m1.1\"><semantics id=\"S1.T1.6.6.2.m1.1a\"><mo id=\"S1.T1.6.6.2.m1.1.1\" mathsize=\"90%\" xref=\"S1.T1.6.6.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.6.6.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.6.6.2.m1.1.1.cmml\" xref=\"S1.T1.6.6.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.6.6.2.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.6.6.2.m1.1d\">±</annotation></semantics></math><span class=\"ltx_text\" id=\"S1.T1.6.6.2.2\" style=\"font-size:90%;\"> 0.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S1.T1.7.7.3\">\n<span class=\"ltx_text\" id=\"S1.T1.7.7.3.1\" style=\"font-size:90%;\">1.51 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.7.7.3.m1.1\"><semantics id=\"S1.T1.7.7.3.m1.1a\"><mo id=\"S1.T1.7.7.3.m1.1.1\" mathsize=\"90%\" xref=\"S1.T1.7.7.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.7.7.3.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.7.7.3.m1.1.1.cmml\" xref=\"S1.T1.7.7.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.7.7.3.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.7.7.3.m1.1d\">±</annotation></semantics></math><span class=\"ltx_text\" id=\"S1.T1.7.7.3.2\" style=\"font-size:90%;\"> 0.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S1.T1.8.8.4\">\n<span class=\"ltx_text\" id=\"S1.T1.8.8.4.1\" style=\"font-size:90%;\">1.79 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.8.8.4.m1.1\"><semantics id=\"S1.T1.8.8.4.m1.1a\"><mo id=\"S1.T1.8.8.4.m1.1.1\" mathsize=\"90%\" xref=\"S1.T1.8.8.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.8.8.4.m1.1b\"><csymbol cd=\"latexml\" id=\"S1.T1.8.8.4.m1.1.1.cmml\" xref=\"S1.T1.8.8.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.8.8.4.m1.1c\">\\pm</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.8.8.4.m1.1d\">±</annotation></semantics></math><span class=\"ltx_text\" id=\"S1.T1.8.8.4.2\" style=\"font-size:90%;\"> 0.06</span>\n</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Average online loss across epochs 2 to 5 for cyclic fine-tuning and random shuffled fine-tuning.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">Typically, networks exhibit <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p4.1.1\">catastrophic interference</em> (also known as catastrophic forgetting) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">mccloskey1989catastrophic, </a>)</cite> when training on a sequence of tasks: the loss on a given document increases as the training advances to other documents.\nCuriously, we discover that in a structured training environment, LLMs exhibit a remarkable <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p4.1.2\">anticipatory recovery</em> behavior: they\nrecover from the forgetting of one document before seeing it again, multiple steps in the sequence prior to the recurrence of the document (see Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a) and <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b)). It is analogous to a person anticipating to eat breakfast while taking a morning shower, but leaving the thought aside for the rest of the day. Critically, we never present two documents together in context, so the model cannot directly learn sequential relationships between them. Thus, our finding is surprising as there is no explicit memory in LLMs that stores sequential knowledge across context windows, and there is no systematic overlap of content across documents—the behavior emerges from a random document sequence after repeated exposure to that sequence. Furthermore, we demonstrate that, as a result of anticipatory recovery, training with fixed ordering achieves superior performance than random shuffling in the prequential evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">cai2021online, </a>)</cite> setting (see Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c) and Table <a class=\"ltx_ref\" href=\"#S1.T1\" title=\"Table 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). For an agent that continuously acts and learns in the real world, the performance on the upcoming task is what matters, and prequential evaluation measures such performance. This result hints at the practical benefits of structured training.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.1\">Through extensive experiments, we study how different factors in model architecture and training contribute to the anticipatory recovery phenomenon (Section <a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). We show that only large-scale networks exhibit this reawakening of knowledge, and smaller ones exhibit no such behavior (Section <a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). We also show that this phenomenon is not unique to LLMs; some vision models with sufficient width and depth also demonstrate a similar behavior, but LLMs on language modeling tasks exhibit the strongest recovery (Section <a class=\"ltx_ref\" href=\"#S3.SS4\" title=\"3.4 Anticipatory Recovery in Vision Models ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). We offer insights on the training dynamics in sequentially and cyclically structured input data and propose hypotheses for the causes of the behavior (Section <a class=\"ltx_ref\" href=\"#S4\" title=\"4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Data and Experiment Setup</h2>\n<div class=\"ltx_para\" id=\"S2.p1\">\n<p class=\"ltx_p\" id=\"S2.p1.1\">In this section, we describe the models, datasets, and training setups that we use in the subsequent experiments. Additional details are presented in Appendix <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px1.p1.1\">For the LLM experiments, we use Pythia <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">biderman2023pythia, </a>)</cite>, a suite of decoder-only autoregressive language models pre-trained on the Pile dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">gao2020pile, </a>; <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">biderman2022datasheet, </a>)</cite>. We use pre-trained Pythia models ranging from 160M to 2.8B parameters. For the vision experiments, we use pre-trained Image GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">chen2020generative, </a>)</cite> models for causal image modeling and pre-trained vision transformer (ViT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">dosovitskiy2020image, </a>)</cite> and VGG-19 <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">simonyan2014very, </a>)</cite> models for image classification.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px2.p1.1\">For the LLM experiments, we use the CNN/Daily Mail news dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">nallapati2016abstractive, </a>)</cite>. We repurpose it for causal language modeling by discarding the summaries and only using the articles. Importantly, the CNN/Daily Mail dataset is not part of the Pile dataset and hence it is a new domain for the Pythia pre-trained models. We use the same documents for both training and evaluation. Our goal here is not to determine whether a trained model generalizes to new documents, but rather to study the memory for a particular document as a function of position within the training history. For the vision experiments, we use images sampled from CIFAR-10 <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">krizhevsky2009learning, </a>)</cite> and ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">deng2009imagenet, </a>)</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S2.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training Setup.</h4>\n<div class=\"ltx_para\" id=\"S2.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS0.SSS0.Px3.p1.10\">We randomly sample <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.1.m1.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.1.m1.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.1.m1.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.1.m1.1c\">T</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.1.m1.1d\">italic_T</annotation></semantics></math> documents from the dataset. In pre-processing, we truncate each document to the first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.2.m2.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.2.m2.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.2.m2.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1\">𝐶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.2.m2.1c\">C</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.2.m2.1d\">italic_C</annotation></semantics></math> tokens (we refer to <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.3.m3.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.3.m3.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.3.m3.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1\">𝐶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.3.m3.1c\">C</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.3.m3.1d\">italic_C</annotation></semantics></math> as “context length” in subsequent text). We then fine-tune the LLM on each pre-processed sample for <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.4.m4.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.4.m4.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.4.m4.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.4.m4.1c\">M</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.4.m4.1d\">italic_M</annotation></semantics></math> gradient steps (i.e., using a batch size of 1). We refer to the multiple gradient updates of each document as an “episode”. After each episode we evaluate the loss on all <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.5.m5.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.5.m5.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.5.m5.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.5.m5.1c\">T</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.5.m5.1d\">italic_T</annotation></semantics></math> documents. We repeat the training process for <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.6.m6.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.6.m6.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.6.m6.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1\">𝐸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.6.m6.1c\">E</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.6.m6.1d\">italic_E</annotation></semantics></math> epochs, where an epoch consists of one episode of each document in a fixed sequence. We use a vanilla gradient descent optimizer. Unless otherwise stated, the default hyperparameters in the subsequent experiments are <math alttext=\"T=25\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.7.m7.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.2.cmml\">T</mi><mo id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.2\">𝑇</ci><cn id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.3.cmml\" type=\"integer\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1c\">T=25</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1d\">italic_T = 25</annotation></semantics></math>, <math alttext=\"C=256\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.8.m8.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml\">C</mi><mo id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml\">256</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2\">𝐶</ci><cn id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml\" type=\"integer\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3\">256</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1c\">C=256</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1d\">italic_C = 256</annotation></semantics></math>, <math alttext=\"M=10\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.9.m9.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2.cmml\">M</mi><mo id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2\">𝑀</ci><cn id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3.cmml\" type=\"integer\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1c\">M=10</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1d\">italic_M = 10</annotation></semantics></math>, <math alttext=\"E=5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1\"><semantics id=\"S2.SS0.SSS0.Px3.p1.10.m10.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2.cmml\">E</mi><mo id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2\">𝐸</ci><cn id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3.cmml\" type=\"integer\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1c\">E=5</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1d\">italic_E = 5</annotation></semantics></math>. We use the average cross entropy loss (average negative log-likelihood for each token) as our training and evaluation metric.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Emergent Anticipatory Recovery</h2>\n<figure class=\"ltx_figure\" id=\"S3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"163\" id=\"S3.F2.g1\" src=\"./assets/x2.png\" width=\"729\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F2.2.1.1\" style=\"font-size:90%;\">Figure 2</span>: </span><span class=\"ltx_text\" id=\"S3.F2.3.2\" style=\"font-size:90%;\">Effect of model size for (a) pre-trained models and (b) random initializations. In each subfigure, the left shows shift-averaged loss curves and the right shows the recovery score as a function of model size.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\" id=\"S3.p1.1\">In this section, we present our experimental results that reveal the anticipatory recovery phenomenon in cyclic fine-tuning of large language models. We then demonstrate that anticipatory recovery is an emergent behavior that appears only for models with sufficient capacity.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>The Anticipatory Recovery Phenomenon</h3>\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.p1.19\">In this first experiment, we have <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.1.m1.1\"><semantics id=\"S3.SS1.p1.1.m1.1a\"><mrow id=\"S3.SS1.p1.1.m1.1.1\" xref=\"S3.SS1.p1.1.m1.1.1.cmml\"><mi id=\"S3.SS1.p1.1.m1.1.1.2\" xref=\"S3.SS1.p1.1.m1.1.1.2.cmml\">T</mi><mo id=\"S3.SS1.p1.1.m1.1.1.1\" xref=\"S3.SS1.p1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.SS1.p1.1.m1.1.1.3\" xref=\"S3.SS1.p1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.1.m1.1b\"><apply id=\"S3.SS1.p1.1.m1.1.1.cmml\" xref=\"S3.SS1.p1.1.m1.1.1\"><eq id=\"S3.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S3.SS1.p1.1.m1.1.1.1\"></eq><ci id=\"S3.SS1.p1.1.m1.1.1.2.cmml\" xref=\"S3.SS1.p1.1.m1.1.1.2\">𝑇</ci><cn id=\"S3.SS1.p1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.1.m1.1c\">T=100</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.1.m1.1d\">italic_T = 100</annotation></semantics></math> documents, and we do cyclic fine-tuning of a pre-trained Pythia-1B model <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">biderman2023pythia, </a>)</cite> on the documents for <math alttext=\"E=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.2.m2.1\"><semantics id=\"S3.SS1.p1.2.m2.1a\"><mrow id=\"S3.SS1.p1.2.m2.1.1\" xref=\"S3.SS1.p1.2.m2.1.1.cmml\"><mi id=\"S3.SS1.p1.2.m2.1.1.2\" xref=\"S3.SS1.p1.2.m2.1.1.2.cmml\">E</mi><mo id=\"S3.SS1.p1.2.m2.1.1.1\" xref=\"S3.SS1.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"S3.SS1.p1.2.m2.1.1.3\" xref=\"S3.SS1.p1.2.m2.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.2.m2.1b\"><apply id=\"S3.SS1.p1.2.m2.1.1.cmml\" xref=\"S3.SS1.p1.2.m2.1.1\"><eq id=\"S3.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS1.p1.2.m2.1.1.1\"></eq><ci id=\"S3.SS1.p1.2.m2.1.1.2.cmml\" xref=\"S3.SS1.p1.2.m2.1.1.2\">𝐸</ci><cn id=\"S3.SS1.p1.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.2.m2.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.2.m2.1c\">E=5</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.2.m2.1d\">italic_E = 5</annotation></semantics></math> epochs in the same ordering. Both the documents and the ordering are sampled at random beforehand, but kept fixed during the sequential fine-tuning process. We refer to these <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.3.m3.1\"><semantics id=\"S3.SS1.p1.3.m3.1a\"><mi id=\"S3.SS1.p1.3.m3.1.1\" xref=\"S3.SS1.p1.3.m3.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.3.m3.1b\"><ci id=\"S3.SS1.p1.3.m3.1.1.cmml\" xref=\"S3.SS1.p1.3.m3.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.3.m3.1c\">T</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.3.m3.1d\">italic_T</annotation></semantics></math> documents as <math alttext=\"\\bm{x}_{1},\\cdots,\\bm{x}_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.4.m4.3\"><semantics id=\"S3.SS1.p1.4.m4.3a\"><mrow id=\"S3.SS1.p1.4.m4.3.3.2\" xref=\"S3.SS1.p1.4.m4.3.3.3.cmml\"><msub id=\"S3.SS1.p1.4.m4.2.2.1.1\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.cmml\"><mi id=\"S3.SS1.p1.4.m4.2.2.1.1.2\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.4.m4.2.2.1.1.3\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.3.cmml\">1</mn></msub><mo id=\"S3.SS1.p1.4.m4.3.3.2.3\" xref=\"S3.SS1.p1.4.m4.3.3.3.cmml\">,</mo><mi id=\"S3.SS1.p1.4.m4.1.1\" mathvariant=\"normal\" xref=\"S3.SS1.p1.4.m4.1.1.cmml\">⋯</mi><mo id=\"S3.SS1.p1.4.m4.3.3.2.4\" xref=\"S3.SS1.p1.4.m4.3.3.3.cmml\">,</mo><msub id=\"S3.SS1.p1.4.m4.3.3.2.2\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.cmml\"><mi id=\"S3.SS1.p1.4.m4.3.3.2.2.2\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.2.cmml\">𝒙</mi><mi id=\"S3.SS1.p1.4.m4.3.3.2.2.3\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.3.cmml\">T</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.4.m4.3b\"><list id=\"S3.SS1.p1.4.m4.3.3.3.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2\"><apply id=\"S3.SS1.p1.4.m4.2.2.1.1.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.4.m4.2.2.1.1.1.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.4.m4.2.2.1.1.2.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.4.m4.2.2.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p1.4.m4.1.1.cmml\" xref=\"S3.SS1.p1.4.m4.1.1\">⋯</ci><apply id=\"S3.SS1.p1.4.m4.3.3.2.2.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.4.m4.3.3.2.2.1.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2\">subscript</csymbol><ci id=\"S3.SS1.p1.4.m4.3.3.2.2.2.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.2\">𝒙</ci><ci id=\"S3.SS1.p1.4.m4.3.3.2.2.3.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.3\">𝑇</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.4.m4.3c\">\\bm{x}_{1},\\cdots,\\bm{x}_{T}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.4.m4.3d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>. At the start, we fine-tune on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.5.m5.1\"><semantics id=\"S3.SS1.p1.5.m5.1a\"><msub id=\"S3.SS1.p1.5.m5.1.1\" xref=\"S3.SS1.p1.5.m5.1.1.cmml\"><mi id=\"S3.SS1.p1.5.m5.1.1.2\" xref=\"S3.SS1.p1.5.m5.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.5.m5.1.1.3\" xref=\"S3.SS1.p1.5.m5.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.5.m5.1b\"><apply id=\"S3.SS1.p1.5.m5.1.1.cmml\" xref=\"S3.SS1.p1.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.5.m5.1.1.1.cmml\" xref=\"S3.SS1.p1.5.m5.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.5.m5.1.1.2.cmml\" xref=\"S3.SS1.p1.5.m5.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.5.m5.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.5.m5.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.5.m5.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.5.m5.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext=\"M=10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.6.m6.1\"><semantics id=\"S3.SS1.p1.6.m6.1a\"><mrow id=\"S3.SS1.p1.6.m6.1.1\" xref=\"S3.SS1.p1.6.m6.1.1.cmml\"><mi id=\"S3.SS1.p1.6.m6.1.1.2\" xref=\"S3.SS1.p1.6.m6.1.1.2.cmml\">M</mi><mo id=\"S3.SS1.p1.6.m6.1.1.1\" xref=\"S3.SS1.p1.6.m6.1.1.1.cmml\">=</mo><mn id=\"S3.SS1.p1.6.m6.1.1.3\" xref=\"S3.SS1.p1.6.m6.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.6.m6.1b\"><apply id=\"S3.SS1.p1.6.m6.1.1.cmml\" xref=\"S3.SS1.p1.6.m6.1.1\"><eq id=\"S3.SS1.p1.6.m6.1.1.1.cmml\" xref=\"S3.SS1.p1.6.m6.1.1.1\"></eq><ci id=\"S3.SS1.p1.6.m6.1.1.2.cmml\" xref=\"S3.SS1.p1.6.m6.1.1.2\">𝑀</ci><cn id=\"S3.SS1.p1.6.m6.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.6.m6.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.6.m6.1c\">M=10</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.6.m6.1d\">italic_M = 10</annotation></semantics></math> gradient steps, leading to a significant decrease in the model’s loss on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.7.m7.1\"><semantics id=\"S3.SS1.p1.7.m7.1a\"><msub id=\"S3.SS1.p1.7.m7.1.1\" xref=\"S3.SS1.p1.7.m7.1.1.cmml\"><mi id=\"S3.SS1.p1.7.m7.1.1.2\" xref=\"S3.SS1.p1.7.m7.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.7.m7.1.1.3\" xref=\"S3.SS1.p1.7.m7.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.7.m7.1b\"><apply id=\"S3.SS1.p1.7.m7.1.1.cmml\" xref=\"S3.SS1.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.7.m7.1.1.1.cmml\" xref=\"S3.SS1.p1.7.m7.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.7.m7.1.1.2.cmml\" xref=\"S3.SS1.p1.7.m7.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.7.m7.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.7.m7.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.7.m7.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.7.m7.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. As we move away from <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.8.m8.1\"><semantics id=\"S3.SS1.p1.8.m8.1a\"><msub id=\"S3.SS1.p1.8.m8.1.1\" xref=\"S3.SS1.p1.8.m8.1.1.cmml\"><mi id=\"S3.SS1.p1.8.m8.1.1.2\" xref=\"S3.SS1.p1.8.m8.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.8.m8.1.1.3\" xref=\"S3.SS1.p1.8.m8.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.8.m8.1b\"><apply id=\"S3.SS1.p1.8.m8.1.1.cmml\" xref=\"S3.SS1.p1.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.8.m8.1.1.1.cmml\" xref=\"S3.SS1.p1.8.m8.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.8.m8.1.1.2.cmml\" xref=\"S3.SS1.p1.8.m8.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.8.m8.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.8.m8.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.8.m8.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.8.m8.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and fine-tune on other documents, we naturally observe catastrophic interference: the model’s loss on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.9.m9.1\"><semantics id=\"S3.SS1.p1.9.m9.1a\"><msub id=\"S3.SS1.p1.9.m9.1.1\" xref=\"S3.SS1.p1.9.m9.1.1.cmml\"><mi id=\"S3.SS1.p1.9.m9.1.1.2\" xref=\"S3.SS1.p1.9.m9.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.9.m9.1.1.3\" xref=\"S3.SS1.p1.9.m9.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.9.m9.1b\"><apply id=\"S3.SS1.p1.9.m9.1.1.cmml\" xref=\"S3.SS1.p1.9.m9.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.9.m9.1.1.1.cmml\" xref=\"S3.SS1.p1.9.m9.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.9.m9.1.1.2.cmml\" xref=\"S3.SS1.p1.9.m9.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.9.m9.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.9.m9.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.9.m9.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.9.m9.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> gradually increases until we finish fine-tuning on all other documents and return to <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.10.m10.1\"><semantics id=\"S3.SS1.p1.10.m10.1a\"><msub id=\"S3.SS1.p1.10.m10.1.1\" xref=\"S3.SS1.p1.10.m10.1.1.cmml\"><mi id=\"S3.SS1.p1.10.m10.1.1.2\" xref=\"S3.SS1.p1.10.m10.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.10.m10.1.1.3\" xref=\"S3.SS1.p1.10.m10.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.10.m10.1b\"><apply id=\"S3.SS1.p1.10.m10.1.1.cmml\" xref=\"S3.SS1.p1.10.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.10.m10.1.1.1.cmml\" xref=\"S3.SS1.p1.10.m10.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.10.m10.1.1.2.cmml\" xref=\"S3.SS1.p1.10.m10.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.10.m10.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.10.m10.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.10.m10.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.10.m10.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. As we iterate through the same document sequence for a second time, we would normally expect the loss on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.11.m11.1\"><semantics id=\"S3.SS1.p1.11.m11.1a\"><msub id=\"S3.SS1.p1.11.m11.1.1\" xref=\"S3.SS1.p1.11.m11.1.1.cmml\"><mi id=\"S3.SS1.p1.11.m11.1.1.2\" xref=\"S3.SS1.p1.11.m11.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.11.m11.1.1.3\" xref=\"S3.SS1.p1.11.m11.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.11.m11.1b\"><apply id=\"S3.SS1.p1.11.m11.1.1.cmml\" xref=\"S3.SS1.p1.11.m11.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.11.m11.1.1.1.cmml\" xref=\"S3.SS1.p1.11.m11.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.11.m11.1.1.2.cmml\" xref=\"S3.SS1.p1.11.m11.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.11.m11.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.11.m11.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.11.m11.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.11.m11.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> to increase monotonically after the initial decrease. However, Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a) shows that the loss on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.12.m12.1\"><semantics id=\"S3.SS1.p1.12.m12.1a\"><msub id=\"S3.SS1.p1.12.m12.1.1\" xref=\"S3.SS1.p1.12.m12.1.1.cmml\"><mi id=\"S3.SS1.p1.12.m12.1.1.2\" xref=\"S3.SS1.p1.12.m12.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.12.m12.1.1.3\" xref=\"S3.SS1.p1.12.m12.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.12.m12.1b\"><apply id=\"S3.SS1.p1.12.m12.1.1.cmml\" xref=\"S3.SS1.p1.12.m12.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.12.m12.1.1.1.cmml\" xref=\"S3.SS1.p1.12.m12.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.12.m12.1.1.2.cmml\" xref=\"S3.SS1.p1.12.m12.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.12.m12.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.12.m12.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.12.m12.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.12.m12.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> peaks around <math alttext=\"\\bm{x}_{60}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.13.m13.1\"><semantics id=\"S3.SS1.p1.13.m13.1a\"><msub id=\"S3.SS1.p1.13.m13.1.1\" xref=\"S3.SS1.p1.13.m13.1.1.cmml\"><mi id=\"S3.SS1.p1.13.m13.1.1.2\" xref=\"S3.SS1.p1.13.m13.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.13.m13.1.1.3\" xref=\"S3.SS1.p1.13.m13.1.1.3.cmml\">60</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.13.m13.1b\"><apply id=\"S3.SS1.p1.13.m13.1.1.cmml\" xref=\"S3.SS1.p1.13.m13.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.13.m13.1.1.1.cmml\" xref=\"S3.SS1.p1.13.m13.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.13.m13.1.1.2.cmml\" xref=\"S3.SS1.p1.13.m13.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.13.m13.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.13.m13.1.1.3\">60</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.13.m13.1c\">\\bm{x}_{60}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.13.m13.1d\">bold_italic_x start_POSTSUBSCRIPT 60 end_POSTSUBSCRIPT</annotation></semantics></math> and then starts to decrease. Before we return to <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.14.m14.1\"><semantics id=\"S3.SS1.p1.14.m14.1a\"><msub id=\"S3.SS1.p1.14.m14.1.1\" xref=\"S3.SS1.p1.14.m14.1.1.cmml\"><mi id=\"S3.SS1.p1.14.m14.1.1.2\" xref=\"S3.SS1.p1.14.m14.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.14.m14.1.1.3\" xref=\"S3.SS1.p1.14.m14.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.14.m14.1b\"><apply id=\"S3.SS1.p1.14.m14.1.1.cmml\" xref=\"S3.SS1.p1.14.m14.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.14.m14.1.1.1.cmml\" xref=\"S3.SS1.p1.14.m14.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.14.m14.1.1.2.cmml\" xref=\"S3.SS1.p1.14.m14.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.14.m14.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.14.m14.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.14.m14.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.14.m14.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, the model has recovered more than half of its initial forgetting during the second epoch. We refer to this counterintuitive decrease in loss as the <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.p1.19.1\">anticipatory recovery</em> phenomenon. In Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b), we plot the losses for all the documents and re-align them so that <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.15.m15.1\"><semantics id=\"S3.SS1.p1.15.m15.1a\"><mn id=\"S3.SS1.p1.15.m15.1.1\" xref=\"S3.SS1.p1.15.m15.1.1.cmml\">0</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.15.m15.1b\"><cn id=\"S3.SS1.p1.15.m15.1.1.cmml\" type=\"integer\" xref=\"S3.SS1.p1.15.m15.1.1\">0</cn></annotation-xml></semantics></math> on the x-axis refers to the loss on each document <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.16.m16.1\"><semantics id=\"S3.SS1.p1.16.m16.1a\"><mi id=\"S3.SS1.p1.16.m16.1.1\" xref=\"S3.SS1.p1.16.m16.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.16.m16.1b\"><ci id=\"S3.SS1.p1.16.m16.1.1.cmml\" xref=\"S3.SS1.p1.16.m16.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.16.m16.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.16.m16.1d\">italic_t</annotation></semantics></math> immediately before training on it for the first time. The figure confirms that the anticipatory recovery phenomenon exists for not only <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.17.m17.1\"><semantics id=\"S3.SS1.p1.17.m17.1a\"><msub id=\"S3.SS1.p1.17.m17.1.1\" xref=\"S3.SS1.p1.17.m17.1.1.cmml\"><mi id=\"S3.SS1.p1.17.m17.1.1.2\" xref=\"S3.SS1.p1.17.m17.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.17.m17.1.1.3\" xref=\"S3.SS1.p1.17.m17.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.17.m17.1b\"><apply id=\"S3.SS1.p1.17.m17.1.1.cmml\" xref=\"S3.SS1.p1.17.m17.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.17.m17.1.1.1.cmml\" xref=\"S3.SS1.p1.17.m17.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.17.m17.1.1.2.cmml\" xref=\"S3.SS1.p1.17.m17.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.17.m17.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.17.m17.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.17.m17.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.17.m17.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> but all documents. On the other hand, when we randomly shuffle the document order within each epoch (except <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.18.m18.1\"><semantics id=\"S3.SS1.p1.18.m18.1a\"><msub id=\"S3.SS1.p1.18.m18.1.1\" xref=\"S3.SS1.p1.18.m18.1.1.cmml\"><mi id=\"S3.SS1.p1.18.m18.1.1.2\" xref=\"S3.SS1.p1.18.m18.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.18.m18.1.1.3\" xref=\"S3.SS1.p1.18.m18.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.18.m18.1b\"><apply id=\"S3.SS1.p1.18.m18.1.1.cmml\" xref=\"S3.SS1.p1.18.m18.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.18.m18.1.1.1.cmml\" xref=\"S3.SS1.p1.18.m18.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.18.m18.1.1.2.cmml\" xref=\"S3.SS1.p1.18.m18.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.18.m18.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.18.m18.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.18.m18.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.18.m18.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is always the first document), we do not observe such anticipatory recovery behavior, and the loss on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.19.m19.1\"><semantics id=\"S3.SS1.p1.19.m19.1a\"><msub id=\"S3.SS1.p1.19.m19.1.1\" xref=\"S3.SS1.p1.19.m19.1.1.cmml\"><mi id=\"S3.SS1.p1.19.m19.1.1.2\" xref=\"S3.SS1.p1.19.m19.1.1.2.cmml\">𝒙</mi><mn id=\"S3.SS1.p1.19.m19.1.1.3\" xref=\"S3.SS1.p1.19.m19.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.19.m19.1b\"><apply id=\"S3.SS1.p1.19.m19.1.1.cmml\" xref=\"S3.SS1.p1.19.m19.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.19.m19.1.1.1.cmml\" xref=\"S3.SS1.p1.19.m19.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.19.m19.1.1.2.cmml\" xref=\"S3.SS1.p1.19.m19.1.1.2\">𝒙</ci><cn id=\"S3.SS1.p1.19.m19.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p1.19.m19.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.19.m19.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p1.19.m19.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> keeps increasing before we return to it every time.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.p2\">\n<p class=\"ltx_p\" id=\"S3.SS1.p2.11\">To quantify the strength of the anticipatory recovery phenomenon, we define the <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.p2.11.1\">recovery score</em> as the proportion of the initial forgetting during the current epoch that the model recovers before returning to the same document. Mathematically, let the mean (over <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.1.m1.1\"><semantics id=\"S3.SS1.p2.1.m1.1a\"><mi id=\"S3.SS1.p2.1.m1.1.1\" xref=\"S3.SS1.p2.1.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.1.m1.1b\"><ci id=\"S3.SS1.p2.1.m1.1.1.cmml\" xref=\"S3.SS1.p2.1.m1.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.1.m1.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.1.m1.1d\">italic_t</annotation></semantics></math>) of the maximum loss on each document <math alttext=\"\\bm{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.2.m2.1\"><semantics id=\"S3.SS1.p2.2.m2.1a\"><msub id=\"S3.SS1.p2.2.m2.1.1\" xref=\"S3.SS1.p2.2.m2.1.1.cmml\"><mi id=\"S3.SS1.p2.2.m2.1.1.2\" xref=\"S3.SS1.p2.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"S3.SS1.p2.2.m2.1.1.3\" xref=\"S3.SS1.p2.2.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.2.m2.1b\"><apply id=\"S3.SS1.p2.2.m2.1.1.cmml\" xref=\"S3.SS1.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.2.m2.1.1.1.cmml\" xref=\"S3.SS1.p2.2.m2.1.1\">subscript</csymbol><ci id=\"S3.SS1.p2.2.m2.1.1.2.cmml\" xref=\"S3.SS1.p2.2.m2.1.1.2\">𝒙</ci><ci id=\"S3.SS1.p2.2.m2.1.1.3.cmml\" xref=\"S3.SS1.p2.2.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.2.m2.1c\">\\bm{x}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> between the <math alttext=\"n^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.3.m3.1\"><semantics id=\"S3.SS1.p2.3.m3.1a\"><msup id=\"S3.SS1.p2.3.m3.1.1\" xref=\"S3.SS1.p2.3.m3.1.1.cmml\"><mi id=\"S3.SS1.p2.3.m3.1.1.2\" xref=\"S3.SS1.p2.3.m3.1.1.2.cmml\">n</mi><mtext id=\"S3.SS1.p2.3.m3.1.1.3\" xref=\"S3.SS1.p2.3.m3.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.3.m3.1b\"><apply id=\"S3.SS1.p2.3.m3.1.1.cmml\" xref=\"S3.SS1.p2.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.3.m3.1.1.1.cmml\" xref=\"S3.SS1.p2.3.m3.1.1\">superscript</csymbol><ci id=\"S3.SS1.p2.3.m3.1.1.2.cmml\" xref=\"S3.SS1.p2.3.m3.1.1.2\">𝑛</ci><ci id=\"S3.SS1.p2.3.m3.1.1.3a.cmml\" xref=\"S3.SS1.p2.3.m3.1.1.3\"><mtext id=\"S3.SS1.p2.3.m3.1.1.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.3.m3.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.3.m3.1c\">n^{\\text{th}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.3.m3.1d\">italic_n start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext=\"(n+1)^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.4.m4.1\"><semantics id=\"S3.SS1.p2.4.m4.1a\"><msup id=\"S3.SS1.p2.4.m4.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.cmml\"><mrow id=\"S3.SS1.p2.4.m4.1.1.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\"><mo id=\"S3.SS1.p2.4.m4.1.1.1.1.2\" stretchy=\"false\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.4.m4.1.1.1.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\"><mi id=\"S3.SS1.p2.4.m4.1.1.1.1.1.2\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.4.m4.1.1.1.1.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml\">+</mo><mn id=\"S3.SS1.p2.4.m4.1.1.1.1.1.3\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml\">1</mn></mrow><mo id=\"S3.SS1.p2.4.m4.1.1.1.1.3\" stretchy=\"false\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\">)</mo></mrow><mtext id=\"S3.SS1.p2.4.m4.1.1.3\" xref=\"S3.SS1.p2.4.m4.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.4.m4.1b\"><apply id=\"S3.SS1.p2.4.m4.1.1.cmml\" xref=\"S3.SS1.p2.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.4.m4.1.1.2.cmml\" xref=\"S3.SS1.p2.4.m4.1.1\">superscript</csymbol><apply id=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1\"><plus id=\"S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.1\"></plus><ci id=\"S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.2\">𝑛</ci><cn id=\"S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p2.4.m4.1.1.3a.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.3\"><mtext id=\"S3.SS1.p2.4.m4.1.1.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.4.m4.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.4.m4.1c\">(n+1)^{\\text{th}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.4.m4.1d\">( italic_n + 1 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> time we train on that document be <math alttext=\"l_{\\text{max}}(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.5.m5.1\"><semantics id=\"S3.SS1.p2.5.m5.1a\"><mrow id=\"S3.SS1.p2.5.m5.1.2\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\"><msub id=\"S3.SS1.p2.5.m5.1.2.2\" xref=\"S3.SS1.p2.5.m5.1.2.2.cmml\"><mi id=\"S3.SS1.p2.5.m5.1.2.2.2\" xref=\"S3.SS1.p2.5.m5.1.2.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.5.m5.1.2.2.3\" xref=\"S3.SS1.p2.5.m5.1.2.2.3a.cmml\">max</mtext></msub><mo id=\"S3.SS1.p2.5.m5.1.2.1\" xref=\"S3.SS1.p2.5.m5.1.2.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.5.m5.1.2.3.2\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\"><mo id=\"S3.SS1.p2.5.m5.1.2.3.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.5.m5.1.1\" xref=\"S3.SS1.p2.5.m5.1.1.cmml\">n</mi><mo id=\"S3.SS1.p2.5.m5.1.2.3.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.5.m5.1b\"><apply id=\"S3.SS1.p2.5.m5.1.2.cmml\" xref=\"S3.SS1.p2.5.m5.1.2\"><times id=\"S3.SS1.p2.5.m5.1.2.1.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.1\"></times><apply id=\"S3.SS1.p2.5.m5.1.2.2.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.5.m5.1.2.2.1.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2\">subscript</csymbol><ci id=\"S3.SS1.p2.5.m5.1.2.2.2.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2.2\">𝑙</ci><ci id=\"S3.SS1.p2.5.m5.1.2.2.3a.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2.3\"><mtext id=\"S3.SS1.p2.5.m5.1.2.2.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.5.m5.1.2.2.3\">max</mtext></ci></apply><ci id=\"S3.SS1.p2.5.m5.1.1.cmml\" xref=\"S3.SS1.p2.5.m5.1.1\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.5.m5.1c\">l_{\\text{max}}(n)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.5.m5.1d\">italic_l start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_n )</annotation></semantics></math>, right before the <math alttext=\"(n+1)^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.6.m6.1\"><semantics id=\"S3.SS1.p2.6.m6.1a\"><msup id=\"S3.SS1.p2.6.m6.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.cmml\"><mrow id=\"S3.SS1.p2.6.m6.1.1.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\"><mo id=\"S3.SS1.p2.6.m6.1.1.1.1.2\" stretchy=\"false\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.6.m6.1.1.1.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\"><mi id=\"S3.SS1.p2.6.m6.1.1.1.1.1.2\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.6.m6.1.1.1.1.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.1.cmml\">+</mo><mn id=\"S3.SS1.p2.6.m6.1.1.1.1.1.3\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml\">1</mn></mrow><mo id=\"S3.SS1.p2.6.m6.1.1.1.1.3\" stretchy=\"false\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\">)</mo></mrow><mtext id=\"S3.SS1.p2.6.m6.1.1.3\" xref=\"S3.SS1.p2.6.m6.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.6.m6.1b\"><apply id=\"S3.SS1.p2.6.m6.1.1.cmml\" xref=\"S3.SS1.p2.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.6.m6.1.1.2.cmml\" xref=\"S3.SS1.p2.6.m6.1.1\">superscript</csymbol><apply id=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1\"><plus id=\"S3.SS1.p2.6.m6.1.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.1\"></plus><ci id=\"S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.2\">𝑛</ci><cn id=\"S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p2.6.m6.1.1.3a.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.3\"><mtext id=\"S3.SS1.p2.6.m6.1.1.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.6.m6.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.6.m6.1c\">(n+1)^{\\text{th}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.6.m6.1d\">( italic_n + 1 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> time we train on it be <math alttext=\"l_{\\text{before}}(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.7.m7.1\"><semantics id=\"S3.SS1.p2.7.m7.1a\"><mrow id=\"S3.SS1.p2.7.m7.1.2\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\"><msub id=\"S3.SS1.p2.7.m7.1.2.2\" xref=\"S3.SS1.p2.7.m7.1.2.2.cmml\"><mi id=\"S3.SS1.p2.7.m7.1.2.2.2\" xref=\"S3.SS1.p2.7.m7.1.2.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.7.m7.1.2.2.3\" xref=\"S3.SS1.p2.7.m7.1.2.2.3a.cmml\">before</mtext></msub><mo id=\"S3.SS1.p2.7.m7.1.2.1\" xref=\"S3.SS1.p2.7.m7.1.2.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.7.m7.1.2.3.2\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\"><mo id=\"S3.SS1.p2.7.m7.1.2.3.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.7.m7.1.1\" xref=\"S3.SS1.p2.7.m7.1.1.cmml\">n</mi><mo id=\"S3.SS1.p2.7.m7.1.2.3.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.7.m7.1b\"><apply id=\"S3.SS1.p2.7.m7.1.2.cmml\" xref=\"S3.SS1.p2.7.m7.1.2\"><times id=\"S3.SS1.p2.7.m7.1.2.1.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.1\"></times><apply id=\"S3.SS1.p2.7.m7.1.2.2.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.7.m7.1.2.2.1.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2\">subscript</csymbol><ci id=\"S3.SS1.p2.7.m7.1.2.2.2.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2.2\">𝑙</ci><ci id=\"S3.SS1.p2.7.m7.1.2.2.3a.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2.3\"><mtext id=\"S3.SS1.p2.7.m7.1.2.2.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.7.m7.1.2.2.3\">before</mtext></ci></apply><ci id=\"S3.SS1.p2.7.m7.1.1.cmml\" xref=\"S3.SS1.p2.7.m7.1.1\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.7.m7.1c\">l_{\\text{before}}(n)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.7.m7.1d\">italic_l start_POSTSUBSCRIPT before end_POSTSUBSCRIPT ( italic_n )</annotation></semantics></math>, and right after the <math alttext=\"(n+1)^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.8.m8.1\"><semantics id=\"S3.SS1.p2.8.m8.1a\"><msup id=\"S3.SS1.p2.8.m8.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.cmml\"><mrow id=\"S3.SS1.p2.8.m8.1.1.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\"><mo id=\"S3.SS1.p2.8.m8.1.1.1.1.2\" stretchy=\"false\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.8.m8.1.1.1.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\"><mi id=\"S3.SS1.p2.8.m8.1.1.1.1.1.2\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.8.m8.1.1.1.1.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml\">+</mo><mn id=\"S3.SS1.p2.8.m8.1.1.1.1.1.3\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml\">1</mn></mrow><mo id=\"S3.SS1.p2.8.m8.1.1.1.1.3\" stretchy=\"false\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\">)</mo></mrow><mtext id=\"S3.SS1.p2.8.m8.1.1.3\" xref=\"S3.SS1.p2.8.m8.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.8.m8.1b\"><apply id=\"S3.SS1.p2.8.m8.1.1.cmml\" xref=\"S3.SS1.p2.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.8.m8.1.1.2.cmml\" xref=\"S3.SS1.p2.8.m8.1.1\">superscript</csymbol><apply id=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1\"><plus id=\"S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.1\"></plus><ci id=\"S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.2\">𝑛</ci><cn id=\"S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p2.8.m8.1.1.3a.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.3\"><mtext id=\"S3.SS1.p2.8.m8.1.1.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.8.m8.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.8.m8.1c\">(n+1)^{\\text{th}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.8.m8.1d\">( italic_n + 1 ) start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> time we train on it be <math alttext=\"l_{\\text{after}}(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.9.m9.1\"><semantics id=\"S3.SS1.p2.9.m9.1a\"><mrow id=\"S3.SS1.p2.9.m9.1.2\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\"><msub id=\"S3.SS1.p2.9.m9.1.2.2\" xref=\"S3.SS1.p2.9.m9.1.2.2.cmml\"><mi id=\"S3.SS1.p2.9.m9.1.2.2.2\" xref=\"S3.SS1.p2.9.m9.1.2.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.9.m9.1.2.2.3\" xref=\"S3.SS1.p2.9.m9.1.2.2.3a.cmml\">after</mtext></msub><mo id=\"S3.SS1.p2.9.m9.1.2.1\" xref=\"S3.SS1.p2.9.m9.1.2.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.9.m9.1.2.3.2\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\"><mo id=\"S3.SS1.p2.9.m9.1.2.3.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.9.m9.1.1\" xref=\"S3.SS1.p2.9.m9.1.1.cmml\">n</mi><mo id=\"S3.SS1.p2.9.m9.1.2.3.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.9.m9.1b\"><apply id=\"S3.SS1.p2.9.m9.1.2.cmml\" xref=\"S3.SS1.p2.9.m9.1.2\"><times id=\"S3.SS1.p2.9.m9.1.2.1.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.1\"></times><apply id=\"S3.SS1.p2.9.m9.1.2.2.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.9.m9.1.2.2.1.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2\">subscript</csymbol><ci id=\"S3.SS1.p2.9.m9.1.2.2.2.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2.2\">𝑙</ci><ci id=\"S3.SS1.p2.9.m9.1.2.2.3a.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2.3\"><mtext id=\"S3.SS1.p2.9.m9.1.2.2.3.cmml\" mathsize=\"70%\" xref=\"S3.SS1.p2.9.m9.1.2.2.3\">after</mtext></ci></apply><ci id=\"S3.SS1.p2.9.m9.1.1.cmml\" xref=\"S3.SS1.p2.9.m9.1.1\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.9.m9.1c\">l_{\\text{after}}(n)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.9.m9.1d\">italic_l start_POSTSUBSCRIPT after end_POSTSUBSCRIPT ( italic_n )</annotation></semantics></math>. Then we define the recovery score (RS) for epoch <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.10.m10.1\"><semantics id=\"S3.SS1.p2.10.m10.1a\"><mi id=\"S3.SS1.p2.10.m10.1.1\" xref=\"S3.SS1.p2.10.m10.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.10.m10.1b\"><ci id=\"S3.SS1.p2.10.m10.1.1.cmml\" xref=\"S3.SS1.p2.10.m10.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.10.m10.1c\">n</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.10.m10.1d\">italic_n</annotation></semantics></math> to be<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In some cases, a randomly initialized model will produce loss curves that decrease throughout the epoch, because its knowledge is so poor that it enjoys positive generalization among all documents. This yields a misleadingly large recovery score under this definition. We do not include such cases in our experiments so do not bother with more nuanced recovery scores.</span></span></span> <math alttext=\"RS(n)=\\frac{l_{\\text{max}}(n)-l_{\\text{before}}(n)}{l_{\\text{max}}(n)-l_{\\text%\n{after}}(n-1)}.\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.11.m11.6\"><semantics id=\"S3.SS1.p2.11.m11.6a\"><mrow id=\"S3.SS1.p2.11.m11.6.6.1\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.cmml\"><mrow id=\"S3.SS1.p2.11.m11.6.6.1.1\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.cmml\"><mrow id=\"S3.SS1.p2.11.m11.6.6.1.1.2\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.cmml\"><mi id=\"S3.SS1.p2.11.m11.6.6.1.1.2.2\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.2.cmml\">R</mi><mo id=\"S3.SS1.p2.11.m11.6.6.1.1.2.1\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.1.cmml\">⁢</mo><mi id=\"S3.SS1.p2.11.m11.6.6.1.1.2.3\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.3.cmml\">S</mi><mo id=\"S3.SS1.p2.11.m11.6.6.1.1.2.1a\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.11.m11.6.6.1.1.2.4.2\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.cmml\"><mo id=\"S3.SS1.p2.11.m11.6.6.1.1.2.4.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.11.m11.5.5\" xref=\"S3.SS1.p2.11.m11.5.5.cmml\">n</mi><mo id=\"S3.SS1.p2.11.m11.6.6.1.1.2.4.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.cmml\">)</mo></mrow></mrow><mo id=\"S3.SS1.p2.11.m11.6.6.1.1.1\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.1.cmml\">=</mo><mfrac id=\"S3.SS1.p2.11.m11.4.4\" xref=\"S3.SS1.p2.11.m11.4.4.cmml\"><mrow id=\"S3.SS1.p2.11.m11.2.2.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.cmml\"><mrow id=\"S3.SS1.p2.11.m11.2.2.2.4\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.cmml\"><msub id=\"S3.SS1.p2.11.m11.2.2.2.4.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2.cmml\"><mi id=\"S3.SS1.p2.11.m11.2.2.2.4.2.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.11.m11.2.2.2.4.2.3\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2.3a.cmml\">max</mtext></msub><mo id=\"S3.SS1.p2.11.m11.2.2.2.4.1\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.11.m11.2.2.2.4.3.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.cmml\"><mo id=\"S3.SS1.p2.11.m11.2.2.2.4.3.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.cmml\">(</mo><mi id=\"S3.SS1.p2.11.m11.1.1.1.1\" xref=\"S3.SS1.p2.11.m11.1.1.1.1.cmml\">n</mi><mo id=\"S3.SS1.p2.11.m11.2.2.2.4.3.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.cmml\">)</mo></mrow></mrow><mo id=\"S3.SS1.p2.11.m11.2.2.2.3\" xref=\"S3.SS1.p2.11.m11.2.2.2.3.cmml\">−</mo><mrow id=\"S3.SS1.p2.11.m11.2.2.2.5\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.cmml\"><msub id=\"S3.SS1.p2.11.m11.2.2.2.5.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2.cmml\"><mi id=\"S3.SS1.p2.11.m11.2.2.2.5.2.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.11.m11.2.2.2.5.2.3\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2.3a.cmml\">before</mtext></msub><mo id=\"S3.SS1.p2.11.m11.2.2.2.5.1\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.11.m11.2.2.2.5.3.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.cmml\"><mo id=\"S3.SS1.p2.11.m11.2.2.2.5.3.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.cmml\">(</mo><mi id=\"S3.SS1.p2.11.m11.2.2.2.2\" xref=\"S3.SS1.p2.11.m11.2.2.2.2.cmml\">n</mi><mo id=\"S3.SS1.p2.11.m11.2.2.2.5.3.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.cmml\">)</mo></mrow></mrow></mrow><mrow id=\"S3.SS1.p2.11.m11.4.4.4\" xref=\"S3.SS1.p2.11.m11.4.4.4.cmml\"><mrow id=\"S3.SS1.p2.11.m11.4.4.4.4\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.cmml\"><msub id=\"S3.SS1.p2.11.m11.4.4.4.4.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2.cmml\"><mi id=\"S3.SS1.p2.11.m11.4.4.4.4.2.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.11.m11.4.4.4.4.2.3\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2.3a.cmml\">max</mtext></msub><mo id=\"S3.SS1.p2.11.m11.4.4.4.4.1\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.1.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.11.m11.4.4.4.4.3.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.cmml\"><mo id=\"S3.SS1.p2.11.m11.4.4.4.4.3.2.1\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.cmml\">(</mo><mi id=\"S3.SS1.p2.11.m11.3.3.3.1\" xref=\"S3.SS1.p2.11.m11.3.3.3.1.cmml\">n</mi><mo id=\"S3.SS1.p2.11.m11.4.4.4.4.3.2.2\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.cmml\">)</mo></mrow></mrow><mo id=\"S3.SS1.p2.11.m11.4.4.4.3\" xref=\"S3.SS1.p2.11.m11.4.4.4.3.cmml\">−</mo><mrow id=\"S3.SS1.p2.11.m11.4.4.4.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.cmml\"><msub id=\"S3.SS1.p2.11.m11.4.4.4.2.3\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3.cmml\"><mi id=\"S3.SS1.p2.11.m11.4.4.4.2.3.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.11.m11.4.4.4.2.3.3\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3.3a.cmml\">after</mtext></msub><mo id=\"S3.SS1.p2.11.m11.4.4.4.2.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.2.cmml\">⁢</mo><mrow id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.cmml\"><mo id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.2\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.cmml\"><mi id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.2\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.1\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.1.cmml\">−</mo><mn id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.3\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.3.cmml\">1</mn></mrow><mo id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.3\" stretchy=\"false\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.cmml\">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id=\"S3.SS1.p2.11.m11.6.6.1.2\" lspace=\"0em\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.11.m11.6b\"><apply id=\"S3.SS1.p2.11.m11.6.6.1.1.cmml\" xref=\"S3.SS1.p2.11.m11.6.6.1\"><eq id=\"S3.SS1.p2.11.m11.6.6.1.1.1.cmml\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.1\"></eq><apply id=\"S3.SS1.p2.11.m11.6.6.1.1.2.cmml\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2\"><times id=\"S3.SS1.p2.11.m11.6.6.1.1.2.1.cmml\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.1\"></times><ci id=\"S3.SS1.p2.11.m11.6.6.1.1.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.2\">𝑅</ci><ci id=\"S3.SS1.p2.11.m11.6.6.1.1.2.3.cmml\" xref=\"S3.SS1.p2.11.m11.6.6.1.1.2.3\">𝑆</ci><ci id=\"S3.SS1.p2.11.m11.5.5.cmml\" xref=\"S3.SS1.p2.11.m11.5.5\">𝑛</ci></apply><apply id=\"S3.SS1.p2.11.m11.4.4.cmml\" xref=\"S3.SS1.p2.11.m11.4.4\"><divide id=\"S3.SS1.p2.11.m11.4.4.5.cmml\" xref=\"S3.SS1.p2.11.m11.4.4\"></divide><apply id=\"S3.SS1.p2.11.m11.2.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2\"><minus id=\"S3.SS1.p2.11.m11.2.2.2.3.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.3\"></minus><apply id=\"S3.SS1.p2.11.m11.2.2.2.4.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.4\"><times id=\"S3.SS1.p2.11.m11.2.2.2.4.1.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.1\"></times><apply id=\"S3.SS1.p2.11.m11.2.2.2.4.2.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.11.m11.2.2.2.4.2.1.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2\">subscript</csymbol><ci id=\"S3.SS1.p2.11.m11.2.2.2.4.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2.2\">𝑙</ci><ci id=\"S3.SS1.p2.11.m11.2.2.2.4.2.3a.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2.3\"><mtext id=\"S3.SS1.p2.11.m11.2.2.2.4.2.3.cmml\" mathsize=\"50%\" xref=\"S3.SS1.p2.11.m11.2.2.2.4.2.3\">max</mtext></ci></apply><ci id=\"S3.SS1.p2.11.m11.1.1.1.1.cmml\" xref=\"S3.SS1.p2.11.m11.1.1.1.1\">𝑛</ci></apply><apply id=\"S3.SS1.p2.11.m11.2.2.2.5.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.5\"><times id=\"S3.SS1.p2.11.m11.2.2.2.5.1.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.1\"></times><apply id=\"S3.SS1.p2.11.m11.2.2.2.5.2.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.11.m11.2.2.2.5.2.1.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2\">subscript</csymbol><ci id=\"S3.SS1.p2.11.m11.2.2.2.5.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2.2\">𝑙</ci><ci id=\"S3.SS1.p2.11.m11.2.2.2.5.2.3a.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2.3\"><mtext id=\"S3.SS1.p2.11.m11.2.2.2.5.2.3.cmml\" mathsize=\"50%\" xref=\"S3.SS1.p2.11.m11.2.2.2.5.2.3\">before</mtext></ci></apply><ci id=\"S3.SS1.p2.11.m11.2.2.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.2.2.2.2\">𝑛</ci></apply></apply><apply id=\"S3.SS1.p2.11.m11.4.4.4.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4\"><minus id=\"S3.SS1.p2.11.m11.4.4.4.3.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.3\"></minus><apply id=\"S3.SS1.p2.11.m11.4.4.4.4.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.4\"><times id=\"S3.SS1.p2.11.m11.4.4.4.4.1.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.1\"></times><apply id=\"S3.SS1.p2.11.m11.4.4.4.4.2.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.11.m11.4.4.4.4.2.1.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2\">subscript</csymbol><ci id=\"S3.SS1.p2.11.m11.4.4.4.4.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2.2\">𝑙</ci><ci id=\"S3.SS1.p2.11.m11.4.4.4.4.2.3a.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2.3\"><mtext id=\"S3.SS1.p2.11.m11.4.4.4.4.2.3.cmml\" mathsize=\"50%\" xref=\"S3.SS1.p2.11.m11.4.4.4.4.2.3\">max</mtext></ci></apply><ci id=\"S3.SS1.p2.11.m11.3.3.3.1.cmml\" xref=\"S3.SS1.p2.11.m11.3.3.3.1\">𝑛</ci></apply><apply id=\"S3.SS1.p2.11.m11.4.4.4.2.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2\"><times id=\"S3.SS1.p2.11.m11.4.4.4.2.2.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.2\"></times><apply id=\"S3.SS1.p2.11.m11.4.4.4.2.3.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.11.m11.4.4.4.2.3.1.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3\">subscript</csymbol><ci id=\"S3.SS1.p2.11.m11.4.4.4.2.3.2.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3.2\">𝑙</ci><ci id=\"S3.SS1.p2.11.m11.4.4.4.2.3.3a.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3.3\"><mtext id=\"S3.SS1.p2.11.m11.4.4.4.2.3.3.cmml\" mathsize=\"50%\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.3.3\">after</mtext></ci></apply><apply id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1\"><minus id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.1.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.1\"></minus><ci id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.2.cmml\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.2\">𝑛</ci><cn id=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS1.p2.11.m11.4.4.4.2.1.1.1.3\">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.11.m11.6c\">RS(n)=\\frac{l_{\\text{max}}(n)-l_{\\text{before}}(n)}{l_{\\text{max}}(n)-l_{\\text%\n{after}}(n-1)}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS1.p2.11.m11.6d\">italic_R italic_S ( italic_n ) = divide start_ARG italic_l start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_n ) - italic_l start_POSTSUBSCRIPT before end_POSTSUBSCRIPT ( italic_n ) end_ARG start_ARG italic_l start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_n ) - italic_l start_POSTSUBSCRIPT after end_POSTSUBSCRIPT ( italic_n - 1 ) end_ARG .</annotation></semantics></math>\nIn the following subsections we compute the recovery scores for different model sizes and training\nhyperparameters (Figures <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"#S3.F5\" title=\"Figure 5 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) to investigate their effects on the anticipatory recovery phenomenon.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Anticipatory Recovery is an Emergent Behavior</h3>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">To study how the model size affects the amount of anticipatory recovery, we repeat this experiment with pre-trained Pythia models <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">biderman2023pythia, </a>)</cite> of sizes 160M, 410M, 1.4B, and 2.8B. We plot the average loss curves as well as the recovery score for epoch 4 in Figure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a)\n. We observe that larger models clearly demonstrate stronger anticipatory recovery. The sharp increase of average recovery score from the 160M model to the 410M model indicates that anticipatory recovery is an emergent behavior.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Anticipatory Recovery in Randomly Initialized Models.</h4>\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px1.p1.1\">To study whether anticipatory recovery is a result of pre-training, we repeat the experiments on randomly initialized models of different sizes, and plot the loss curves and average recovery scores in\nFigure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b). We follow the model initialization recipe of <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">biderman2023pythia </a></cite>. From the loss curves for the 410M and 1B models, especially in the last epoch, we see that the anticipation phenomenon also exists in randomly initialized LLMs. We observe that the anticipation effect is not as strong as in the pre-trained models. The effect of model size still holds: larger models clearly demonstrate stronger recovery.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS2.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Effects of Model Width and Depth.</h4>\n<div class=\"ltx_para\" id=\"S3.SS2.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.SSS0.Px2.p1.1\">To further study the effect of model width and depth on the anticipatory recovery phenomenon beyond the model hyperparameters in the Pythia suite, we take a Pythia-1B model and vary the width (size of token embedding) and depth (number of transformer blocks) of the model and plot the average loss curves for cyclic training from random initializations in Figure <a class=\"ltx_ref\" href=\"#S3.F4\" title=\"Figure 4 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The original Pythia-1B model has token embedding of size 2048 and 16 transformer blocks. We observe that the model needs sufficient width (at least 512) and depth (at least 8 transformer blocks) to exhibit noticeable recovery, confirming that it is an emergent behavior contingent on model size.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F4\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"S3.F4.1\" style=\"width:208.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"343\" id=\"S3.F4.1.g1\" src=\"./assets/x3.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F4.1.1.1.1\" style=\"font-size:90%;\">Figure 3</span>: </span><span class=\"ltx_text\" id=\"S3.F4.1.2.2\" style=\"font-size:90%;\">Models trained from scratch with (a) different width (token embedding size) and (b) different depth (number of transformer blocks). </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"S3.F4.6\" style=\"width:208.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"345\" id=\"S3.F4.2.g1\" src=\"./assets/x4.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F4.6.5.3.1\" style=\"font-size:90%;\">Figure 4</span>: </span><span class=\"ltx_text\" id=\"S3.F4.6.4.2\" style=\"font-size:90%;\">Effect of data randomization strength. (a) Random masking with probability up to <math alttext=\"0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F4.5.3.1.m1.1\"><semantics id=\"S3.F4.5.3.1.m1.1b\"><mn id=\"S3.F4.5.3.1.m1.1.1\" xref=\"S3.F4.5.3.1.m1.1.1.cmml\">0.3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F4.5.3.1.m1.1c\"><cn id=\"S3.F4.5.3.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.F4.5.3.1.m1.1.1\">0.3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F4.5.3.1.m1.1d\">0.3</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.F4.5.3.1.m1.1e\">0.3</annotation></semantics></math>; (b) Random shift of context window up to <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F4.6.4.2.m2.1\"><semantics id=\"S3.F4.6.4.2.m2.1b\"><mn id=\"S3.F4.6.4.2.m2.1.1\" xref=\"S3.F4.6.4.2.m2.1.1.cmml\">128</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F4.6.4.2.m2.1c\"><cn id=\"S3.F4.6.4.2.m2.1.1.cmml\" type=\"integer\" xref=\"S3.F4.6.4.2.m2.1.1\">128</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F4.6.4.2.m2.1d\">128</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.F4.6.4.2.m2.1e\">128</annotation></semantics></math> tokens.</span></figcaption>\n</figure>\n</div>\n</div>\n</figure>\n<figure class=\"ltx_figure\" id=\"S3.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"360\" id=\"S3.F5.1.g1\" src=\"./assets/x5.png\" width=\"813\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F5.3.1.1\" style=\"font-size:90%;\">Figure 5</span>: </span><span class=\"ltx_text\" id=\"S3.F5.4.2\" style=\"font-size:90%;\">Effects of (a) number of documents (b) number of gradient steps (c) context length and (d) number of frozen blocks.</span></figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>Other Influential Factors</h3>\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.p1.1\">In this section we discuss the effect of other training hyperparameters on the anticipatory recovery phenomenon.\nWe also include additional experiment details in Appendix <a class=\"ltx_ref\" href=\"#A1.SS1\" title=\"A.1 LLM Experiments ‣ Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and additional results in Appendix <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Number of Tasks.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px1.p1.1\">Figure <a class=\"ltx_ref\" href=\"#S3.F5\" title=\"Figure 5 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) plots the loss curves for different number of documents (<math alttext=\"T\\in~{}\\{10,25,50,100,200\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.1.m1.5\"><semantics id=\"S3.SS3.SSS0.Px1.p1.1.m1.5a\"><mrow id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.cmml\"><mi id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.2\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.2.cmml\">T</mi><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.1\" rspace=\"0.608em\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.1.cmml\">∈</mo><mrow id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\"><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2.1\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml\">10</mn><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2.2\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.1.m1.2.2\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.2.2.cmml\">25</mn><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2.3\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.1.m1.3.3\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.3.3.cmml\">50</mn><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2.4\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.1.m1.4.4\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.4.4.cmml\">100</mn><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2.5\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.5\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.5.cmml\">200</mn><mo id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2.6\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px1.p1.1.m1.5b\"><apply id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6\"><in id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.1.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.1\"></in><ci id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.2.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.2\">𝑇</ci><set id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.1.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.6.3.2\"><cn id=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1\">10</cn><cn id=\"S3.SS3.SSS0.Px1.p1.1.m1.2.2.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.2.2\">25</cn><cn id=\"S3.SS3.SSS0.Px1.p1.1.m1.3.3.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.3.3\">50</cn><cn id=\"S3.SS3.SSS0.Px1.p1.1.m1.4.4.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.4.4\">100</cn><cn id=\"S3.SS3.SSS0.Px1.p1.1.m1.5.5.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.5.5\">200</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px1.p1.1.m1.5c\">T\\in~{}\\{10,25,50,100,200\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px1.p1.1.m1.5d\">italic_T ∈ { 10 , 25 , 50 , 100 , 200 }</annotation></semantics></math>). We observe clear recovery for all the curves, suggesting that the model can “memorize” a certain task transition even after training on 200 other tasks.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Number of Gradient Steps.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px2.p1.3\">Figure <a class=\"ltx_ref\" href=\"#S3.F5\" title=\"Figure 5 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) plots training curves with different numbers of gradient steps taken on each document (<math alttext=\"M\\in\\{1,5,10,20\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.4\"><semantics id=\"S3.SS3.SSS0.Px2.p1.1.m1.4a\"><mrow id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.cmml\"><mi id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.2\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.2.cmml\">M</mi><mo id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.1\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.1.cmml\">∈</mo><mrow id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\"><mo id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2.1\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml\">1</mn><mo id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2.2\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px2.p1.1.m1.2.2\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.2.2.cmml\">5</mn><mo id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2.3\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px2.p1.1.m1.3.3\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.3.3.cmml\">10</mn><mo id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2.4\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.4\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.4.cmml\">20</mn><mo id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2.5\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.4b\"><apply id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5\"><in id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.1\"></in><ci id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.2.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.2\">𝑀</ci><set id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.5.3.2\"><cn id=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1\">1</cn><cn id=\"S3.SS3.SSS0.Px2.p1.1.m1.2.2.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.2.2\">5</cn><cn id=\"S3.SS3.SSS0.Px2.p1.1.m1.3.3.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.3.3\">10</cn><cn id=\"S3.SS3.SSS0.Px2.p1.1.m1.4.4.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.4.4\">20</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.4c\">M\\in\\{1,5,10,20\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.4d\">italic_M ∈ { 1 , 5 , 10 , 20 }</annotation></semantics></math>). More gradient steps in general leads to a higher recovery score, although in Appendix <a class=\"ltx_ref\" href=\"#A2.SS2\" title=\"B.2 One-step Gradient Descent with Larger Learning Rate ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a> and <a class=\"ltx_ref\" href=\"#A2.SS8\" title=\"B.8 Effect of Number of Gradient Steps with Inverse Learning Rate Scaling ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">B.8</span></a> we show that slight anticipation is still observed for <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1\"><semantics id=\"S3.SS3.SSS0.Px2.p1.2.m2.1a\"><mn id=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1b\"><cn id=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1c\">1</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1d\">1</annotation></semantics></math> gradient step if we use a larger learning rate, and that the anticipation effect is stronger when the same total update is divided among more gradient steps by scaling the learning rate inversely with <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1\"><semantics id=\"S3.SS3.SSS0.Px2.p1.3.m3.1a\"><mi id=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1b\"><ci id=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1c\">M</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1d\">italic_M</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Context Length.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px3.p1.1\">Figure <a class=\"ltx_ref\" href=\"#S3.F5\" title=\"Figure 5 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(c) plots the loss curves for different context lengths (<math alttext=\"C\\in\\{128,256,512,1024\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4\"><semantics id=\"S3.SS3.SSS0.Px3.p1.1.m1.4a\"><mrow id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.cmml\"><mi id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2.cmml\">C</mi><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1.cmml\">∈</mo><mrow id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\"><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.1\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml\">128</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2.cmml\">256</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.3\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3.cmml\">512</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.4\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4.cmml\">1024</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.5\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4b\"><apply id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5\"><in id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1\"></in><ci id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2\">𝐶</ci><set id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2\"><cn id=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1\">128</cn><cn id=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2\">256</cn><cn id=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3\">512</cn><cn id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4\">1024</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4c\">C\\in\\{128,256,512,1024\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4d\">italic_C ∈ { 128 , 256 , 512 , 1024 }</annotation></semantics></math>). Documents are padded to the same length with padding tokens if they are shorter than the specified context length. With the same number of gradient steps, larger context length is correlated with lower recovery score. This suggests that sufficient training on each task is necessary, and for longer input context it takes more gradient descent steps to memorize the task.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Number of Frozen Blocks.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px4.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px4.p1.2\">We experimented with freezing the first <math alttext=\"B\\in\\{4,6,8,10,12\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5\"><semantics id=\"S3.SS3.SSS0.Px4.p1.1.m1.5a\"><mrow id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.cmml\"><mi id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2.cmml\">B</mi><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1.cmml\">∈</mo><mrow id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\"><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.1\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml\">4</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2.cmml\">6</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.3\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3.cmml\">8</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.4\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4.cmml\">10</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.5\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5.cmml\">12</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.6\" stretchy=\"false\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5b\"><apply id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6\"><in id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1\"></in><ci id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2\">𝐵</ci><set id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2\"><cn id=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1\">4</cn><cn id=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2\">6</cn><cn id=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3\">8</cn><cn id=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4\">10</cn><cn id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5\">12</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5c\">B\\in\\{4,6,8,10,12\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5d\">italic_B ∈ { 4 , 6 , 8 , 10 , 12 }</annotation></semantics></math> transformer blocks in the pre-trained Pythia-1B model and tune only the last <math alttext=\"16-B\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1\"><semantics id=\"S3.SS3.SSS0.Px4.p1.2.m2.1a\"><mrow id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml\"><mn id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml\">16</mn><mo id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1.cmml\">−</mo><mi id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml\">B</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1b\"><apply id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1\"><minus id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1\"></minus><cn id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2\">16</cn><ci id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3\">𝐵</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1c\">16-B</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1d\">16 - italic_B</annotation></semantics></math> blocks. Loss curves are plotted in\nFigure <a class=\"ltx_ref\" href=\"#S3.F5\" title=\"Figure 5 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(d)\n. More frozen transformer blocks is correlated with lower recovery score. This observation is consistent with section <a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and confirm that the model needs sufficient depth to exhibit anticipatory recovery even with a frozen pre-trained deep representation.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px5\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Optimizer.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px5.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px5.p1.1\">In addition to the gradient descent optimizer, we experimented with the Adam optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">kingma2014adam, </a>)</cite>. Loss curves are plotted in Figure <a class=\"ltx_ref\" href=\"#S3.F6\" title=\"Figure 6 ‣ Summary. ‣ 3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We reset the optimizer state for each document. Results show that Adam, which is a stronger optimizer, further facilitates anticipatory recovery for both randomly initialized and pre-trained models.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px6\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data Randomness.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px6.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px6.p1.1\">In realistic sequential learning setting the data points might be slightly different for each repetition (e.g. different descriptions of the same concept, different perspectives of the same object), leading to stochasticity in the optimization process. To explore sequential cyclic training with data randomness, we design the following two training settings:\n(1) we randomly mask a subset of the tokens in the input, and (2) we randomly shift the “window” of <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px6.p1.1.m1.1\"><semantics id=\"S3.SS3.SSS0.Px6.p1.1.m1.1a\"><mi id=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px6.p1.1.m1.1b\"><ci id=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1\">𝐶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px6.p1.1.m1.1c\">C</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS3.SSS0.Px6.p1.1.m1.1d\">italic_C</annotation></semantics></math> tokens used for training. The resulting loss curves are plotted in Figure <a class=\"ltx_ref\" href=\"#S3.F4\" title=\"Figure 4 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that, while anticipatory recovery is generally weaker when there is more variation in each data point, the recovery still clearly exists.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS0.Px7\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Summary.</h4>\n<div class=\"ltx_para\" id=\"S3.SS3.SSS0.Px7.p1\">\n<p class=\"ltx_p\" id=\"S3.SS3.SSS0.Px7.p1.1\">The experiment results in this subsection suggest that the model’s ability to fit on each task is crucial for the strength of anticipatory recovery. With a larger number of gradient steps, shorter context length, more learnable layers, and a better optimizer, the model is more capable of fitting to the focal task, and those factors also correlate with larger recovery score. We also confirmed that anticipatory recovery exists for long task sequences and slightly augmented data points within each episode, and again these factors that make learning harder also reduce anticipatory recovery.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.SS3.SSS0.Px7.2\">\n<div class=\"ltx_block\" id=\"S3.SS3.SSS0.Px7.2.3\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"S3.F6\" style=\"width:208.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"152\" id=\"S3.SS3.SSS0.Px7.1.1.g1\" src=\"./assets/x6.png\" width=\"369\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F6.2.1.1\" style=\"font-size:90%;\">Figure 6</span>: </span><span class=\"ltx_text\" id=\"S3.F6.3.2\" style=\"font-size:90%;\">Comparison between Adam and vanilla gradient descent on (a) randomly initialized and (b) pre-trained Pythia-1B models with cyclic training.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"S3.F7\" style=\"width:208.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"152\" id=\"S3.SS3.SSS0.Px7.2.2.g1\" src=\"./assets/x7.png\" width=\"374\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S3.F7.2.1.1\" style=\"font-size:90%;\">Figure 7</span>: </span><span class=\"ltx_text\" id=\"S3.F7.3.2\" style=\"font-size:90%;\">Results for cyclic training on (a) Causal image modeling with Image GPT. (b) image classification with vision transformers and VGG networks.</span></figcaption>\n</figure>\n</div>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>Anticipatory Recovery in Vision Models</h3>\n<div class=\"ltx_para\" id=\"S3.SS4.p1\">\n<p class=\"ltx_p\" id=\"S3.SS4.p1.1\">To examine the generality of the anticipatory recovery phenomenon, in this subsection we explore the sequential cyclic training setting on two tasks in computer vision: causal image modeling and image classification. More detailed experiment setup is available in Appendix <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS4.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Causal Image Modeling.</h4>\n<div class=\"ltx_para\" id=\"S3.SS4.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS4.SSS0.Px1.p1.2\">Similar to the LLM experiments, we fine-tune a pre-trained Image GPT model <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">chen2020generative </a></cite> on each sampled image from CIFAR-10 <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">krizhevsky2009learning, </a>)</cite> for <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1\"><semantics id=\"S3.SS4.SSS0.Px1.p1.1.m1.1a\"><mi id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1b\"><ci id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1c\">M</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1d\">italic_M</annotation></semantics></math> gradient steps, and repeat <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px1.p1.2.m2.1\"><semantics id=\"S3.SS4.SSS0.Px1.p1.2.m2.1a\"><mi id=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1\" xref=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px1.p1.2.m2.1b\"><ci id=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1.cmml\" xref=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1\">𝐸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px1.p1.2.m2.1c\">E</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.SSS0.Px1.p1.2.m2.1d\">italic_E</annotation></semantics></math> epochs with a fixed order of the images. The resulting loss curves are shown in Figure <a class=\"ltx_ref\" href=\"#S3.F7\" title=\"Figure 7 ‣ Summary. ‣ 3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a). The results show that the anticipatory recovery phenomenon also exists for sequential cyclic training of image modeling in addition to language modeling.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS4.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Image Classification.</h4>\n<div class=\"ltx_para\" id=\"S3.SS4.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS4.SSS0.Px2.p1.2\">For each experiment, we randomly sample 800 images from Imagenet <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">deng2009imagenet, </a>)</cite> and divide them into 25 batches of 32 images each. We fine-tune a pre-trained vision transformer (ViT) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">dosovitskiy2020image, </a>)</cite> and VGG-19 <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">simonyan2014very, </a>)</cite> model on each batch for <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.1.m1.1\"><semantics id=\"S3.SS4.SSS0.Px2.p1.1.m1.1a\"><mi id=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1\" xref=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px2.p1.1.m1.1b\"><ci id=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px2.p1.1.m1.1c\">M</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.SSS0.Px2.p1.1.m1.1d\">italic_M</annotation></semantics></math> gradient steps and repeat <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.2.m2.1\"><semantics id=\"S3.SS4.SSS0.Px2.p1.2.m2.1a\"><mi id=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1\" xref=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px2.p1.2.m2.1b\"><ci id=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1.cmml\" xref=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1\">𝐸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px2.p1.2.m2.1c\">E</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.SSS0.Px2.p1.2.m2.1d\">italic_E</annotation></semantics></math> epochs with a fixed order of the batches. The resulting loss curves are plotted in Figure <a class=\"ltx_ref\" href=\"#S3.F7\" title=\"Figure 7 ‣ Summary. ‣ 3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(b). Results show that both the transformer ViT and the convolutional VGG exhibit anticipatory recovery in cyclic training.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS4.SSS0.Px2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS4.SSS0.Px2.p2.1\">By these experiments we confirm that anticipatory recovery occurs not only for LLMs but also for at least some of the widespread image classification models and non-transformer architectures.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.5 </span>Online Loss Evaluation</h3>\n<div class=\"ltx_para\" id=\"S3.SS5.p1\">\n<p class=\"ltx_p\" id=\"S3.SS5.p1.4\">We compare the performance of training with fixed ordering and random shuffling of each epoch in the prequential evaluation setting with <math alttext=\"T=100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.1.m1.1\"><semantics id=\"S3.SS5.p1.1.m1.1a\"><mrow id=\"S3.SS5.p1.1.m1.1.1\" xref=\"S3.SS5.p1.1.m1.1.1.cmml\"><mi id=\"S3.SS5.p1.1.m1.1.1.2\" xref=\"S3.SS5.p1.1.m1.1.1.2.cmml\">T</mi><mo id=\"S3.SS5.p1.1.m1.1.1.1\" xref=\"S3.SS5.p1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.SS5.p1.1.m1.1.1.3\" xref=\"S3.SS5.p1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS5.p1.1.m1.1b\"><apply id=\"S3.SS5.p1.1.m1.1.1.cmml\" xref=\"S3.SS5.p1.1.m1.1.1\"><eq id=\"S3.SS5.p1.1.m1.1.1.1.cmml\" xref=\"S3.SS5.p1.1.m1.1.1.1\"></eq><ci id=\"S3.SS5.p1.1.m1.1.1.2.cmml\" xref=\"S3.SS5.p1.1.m1.1.1.2\">𝑇</ci><cn id=\"S3.SS5.p1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS5.p1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS5.p1.1.m1.1c\">T=100</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS5.p1.1.m1.1d\">italic_T = 100</annotation></semantics></math> documents. Prequential evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">cai2021online, </a>)</cite> measures online performance by evaluating the model on the document it is about to be trained on, which is equivalent to evaluating the training loss of each batch. For the random shuffling condition, document 1 is always trained first in each epoch, and the other 99 documents are presented in a different random order in each epoch. In this experiment we set <math alttext=\"C=512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.2.m2.1\"><semantics id=\"S3.SS5.p1.2.m2.1a\"><mrow id=\"S3.SS5.p1.2.m2.1.1\" xref=\"S3.SS5.p1.2.m2.1.1.cmml\"><mi id=\"S3.SS5.p1.2.m2.1.1.2\" xref=\"S3.SS5.p1.2.m2.1.1.2.cmml\">C</mi><mo id=\"S3.SS5.p1.2.m2.1.1.1\" xref=\"S3.SS5.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"S3.SS5.p1.2.m2.1.1.3\" xref=\"S3.SS5.p1.2.m2.1.1.3.cmml\">512</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS5.p1.2.m2.1b\"><apply id=\"S3.SS5.p1.2.m2.1.1.cmml\" xref=\"S3.SS5.p1.2.m2.1.1\"><eq id=\"S3.SS5.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS5.p1.2.m2.1.1.1\"></eq><ci id=\"S3.SS5.p1.2.m2.1.1.2.cmml\" xref=\"S3.SS5.p1.2.m2.1.1.2\">𝐶</ci><cn id=\"S3.SS5.p1.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS5.p1.2.m2.1.1.3\">512</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS5.p1.2.m2.1c\">C=512</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS5.p1.2.m2.1d\">italic_C = 512</annotation></semantics></math>, <math alttext=\"M=10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.3.m3.1\"><semantics id=\"S3.SS5.p1.3.m3.1a\"><mrow id=\"S3.SS5.p1.3.m3.1.1\" xref=\"S3.SS5.p1.3.m3.1.1.cmml\"><mi id=\"S3.SS5.p1.3.m3.1.1.2\" xref=\"S3.SS5.p1.3.m3.1.1.2.cmml\">M</mi><mo id=\"S3.SS5.p1.3.m3.1.1.1\" xref=\"S3.SS5.p1.3.m3.1.1.1.cmml\">=</mo><mn id=\"S3.SS5.p1.3.m3.1.1.3\" xref=\"S3.SS5.p1.3.m3.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS5.p1.3.m3.1b\"><apply id=\"S3.SS5.p1.3.m3.1.1.cmml\" xref=\"S3.SS5.p1.3.m3.1.1\"><eq id=\"S3.SS5.p1.3.m3.1.1.1.cmml\" xref=\"S3.SS5.p1.3.m3.1.1.1\"></eq><ci id=\"S3.SS5.p1.3.m3.1.1.2.cmml\" xref=\"S3.SS5.p1.3.m3.1.1.2\">𝑀</ci><cn id=\"S3.SS5.p1.3.m3.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS5.p1.3.m3.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS5.p1.3.m3.1c\">M=10</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS5.p1.3.m3.1d\">italic_M = 10</annotation></semantics></math>, <math alttext=\"E=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.4.m4.1\"><semantics id=\"S3.SS5.p1.4.m4.1a\"><mrow id=\"S3.SS5.p1.4.m4.1.1\" xref=\"S3.SS5.p1.4.m4.1.1.cmml\"><mi id=\"S3.SS5.p1.4.m4.1.1.2\" xref=\"S3.SS5.p1.4.m4.1.1.2.cmml\">E</mi><mo id=\"S3.SS5.p1.4.m4.1.1.1\" xref=\"S3.SS5.p1.4.m4.1.1.1.cmml\">=</mo><mn id=\"S3.SS5.p1.4.m4.1.1.3\" xref=\"S3.SS5.p1.4.m4.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS5.p1.4.m4.1b\"><apply id=\"S3.SS5.p1.4.m4.1.1.cmml\" xref=\"S3.SS5.p1.4.m4.1.1\"><eq id=\"S3.SS5.p1.4.m4.1.1.1.cmml\" xref=\"S3.SS5.p1.4.m4.1.1.1\"></eq><ci id=\"S3.SS5.p1.4.m4.1.1.2.cmml\" xref=\"S3.SS5.p1.4.m4.1.1.2\">𝐸</ci><cn id=\"S3.SS5.p1.4.m4.1.1.3.cmml\" type=\"integer\" xref=\"S3.SS5.p1.4.m4.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS5.p1.4.m4.1c\">E=5</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS5.p1.4.m4.1d\">italic_E = 5</annotation></semantics></math> and the error bars are based on 10 different seeds.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS5.p2\">\n<p class=\"ltx_p\" id=\"S3.SS5.p2.1\">The resulting loss curves are plotted in Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c), and the average loss throughout each run is summarized in Table <a class=\"ltx_ref\" href=\"#S1.T1\" title=\"Table 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We exclude epoch 1 when computing the average loss since all documents are new to the model for both the cyclic training and random shuffling condition. We observe that training with fixed ordering is superior to random shuffling in the prequential evaluation setting across all 4 pre-trained Pythia models of different sizes, due to the structure in the data stream. The results suggest practical potential of structured training.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Understanding Cyclic Training Dynamics</h2>\n<figure class=\"ltx_figure\" id=\"S4.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"158\" id=\"S4.F8.g1\" src=\"./assets/x8.png\" width=\"765\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F8.8.4.1\" style=\"font-size:90%;\">Figure 8</span>: </span><span class=\"ltx_text\" id=\"S4.F8.6.3\" style=\"font-size:90%;\">Heat map visualizations for (a) cosine similarities between the gradient vectors of the attention layer in transformer block 12 of the model for each task; (b) loss recoveries for training on task <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F8.4.1.m1.1\"><semantics id=\"S4.F8.4.1.m1.1b\"><msub id=\"S4.F8.4.1.m1.1.1\" xref=\"S4.F8.4.1.m1.1.1.cmml\"><mi id=\"S4.F8.4.1.m1.1.1.2\" xref=\"S4.F8.4.1.m1.1.1.2.cmml\">𝒙</mi><mi id=\"S4.F8.4.1.m1.1.1.3\" xref=\"S4.F8.4.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F8.4.1.m1.1c\"><apply id=\"S4.F8.4.1.m1.1.1.cmml\" xref=\"S4.F8.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F8.4.1.m1.1.1.1.cmml\" xref=\"S4.F8.4.1.m1.1.1\">subscript</csymbol><ci id=\"S4.F8.4.1.m1.1.1.2.cmml\" xref=\"S4.F8.4.1.m1.1.1.2\">𝒙</ci><ci id=\"S4.F8.4.1.m1.1.1.3.cmml\" xref=\"S4.F8.4.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F8.4.1.m1.1d\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F8.4.1.m1.1e\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (y-axis) and evaluating on task <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F8.5.2.m2.1\"><semantics id=\"S4.F8.5.2.m2.1b\"><msub id=\"S4.F8.5.2.m2.1.1\" xref=\"S4.F8.5.2.m2.1.1.cmml\"><mi id=\"S4.F8.5.2.m2.1.1.2\" xref=\"S4.F8.5.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"S4.F8.5.2.m2.1.1.3\" xref=\"S4.F8.5.2.m2.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F8.5.2.m2.1c\"><apply id=\"S4.F8.5.2.m2.1.1.cmml\" xref=\"S4.F8.5.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F8.5.2.m2.1.1.1.cmml\" xref=\"S4.F8.5.2.m2.1.1\">subscript</csymbol><ci id=\"S4.F8.5.2.m2.1.1.2.cmml\" xref=\"S4.F8.5.2.m2.1.1.2\">𝒙</ci><ci id=\"S4.F8.5.2.m2.1.1.3.cmml\" xref=\"S4.F8.5.2.m2.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F8.5.2.m2.1d\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F8.5.2.m2.1e\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> (x-axis); (c) cosine similarities between the flattened model weight residuals at each point in training; (d) cosine similarities between the last layer activations for document <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F8.6.3.m3.1\"><semantics id=\"S4.F8.6.3.m3.1b\"><msub id=\"S4.F8.6.3.m3.1.1\" xref=\"S4.F8.6.3.m3.1.1.cmml\"><mi id=\"S4.F8.6.3.m3.1.1.2\" xref=\"S4.F8.6.3.m3.1.1.2.cmml\">𝒙</mi><mn id=\"S4.F8.6.3.m3.1.1.3\" xref=\"S4.F8.6.3.m3.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F8.6.3.m3.1c\"><apply id=\"S4.F8.6.3.m3.1.1.cmml\" xref=\"S4.F8.6.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F8.6.3.m3.1.1.1.cmml\" xref=\"S4.F8.6.3.m3.1.1\">subscript</csymbol><ci id=\"S4.F8.6.3.m3.1.1.2.cmml\" xref=\"S4.F8.6.3.m3.1.1.2\">𝒙</ci><cn id=\"S4.F8.6.3.m3.1.1.3.cmml\" type=\"integer\" xref=\"S4.F8.6.3.m3.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F8.6.3.m3.1d\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F8.6.3.m3.1e\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> at each point in training.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\" id=\"S4.p1.2\">An important general question about anticipatory recovery is whether it is due to some causal mechanism relating the dynamics of model parameters to the training sequence, or whether it is more correlational in that adjacent tasks come to be represented more similarly by the model. We found initial evidence for the latter hypothesis in experiments locally reversing the task sequence (e.g., showing that <math alttext=\"\\bm{x}_{t+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.1.m1.1\"><semantics id=\"S4.p1.1.m1.1a\"><msub id=\"S4.p1.1.m1.1.1\" xref=\"S4.p1.1.m1.1.1.cmml\"><mi id=\"S4.p1.1.m1.1.1.2\" xref=\"S4.p1.1.m1.1.1.2.cmml\">𝒙</mi><mrow id=\"S4.p1.1.m1.1.1.3\" xref=\"S4.p1.1.m1.1.1.3.cmml\"><mi id=\"S4.p1.1.m1.1.1.3.2\" xref=\"S4.p1.1.m1.1.1.3.2.cmml\">t</mi><mo id=\"S4.p1.1.m1.1.1.3.1\" xref=\"S4.p1.1.m1.1.1.3.1.cmml\">+</mo><mn id=\"S4.p1.1.m1.1.1.3.3\" xref=\"S4.p1.1.m1.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.p1.1.m1.1b\"><apply id=\"S4.p1.1.m1.1.1.cmml\" xref=\"S4.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.p1.1.m1.1.1.1.cmml\" xref=\"S4.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.p1.1.m1.1.1.2.cmml\" xref=\"S4.p1.1.m1.1.1.2\">𝒙</ci><apply id=\"S4.p1.1.m1.1.1.3.cmml\" xref=\"S4.p1.1.m1.1.1.3\"><plus id=\"S4.p1.1.m1.1.1.3.1.cmml\" xref=\"S4.p1.1.m1.1.1.3.1\"></plus><ci id=\"S4.p1.1.m1.1.1.3.2.cmml\" xref=\"S4.p1.1.m1.1.1.3.2\">𝑡</ci><cn id=\"S4.p1.1.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.p1.1.m1.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.p1.1.m1.1c\">\\bm{x}_{t+1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.p1.1.m1.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT</annotation></semantics></math> primes <math alttext=\"\\bm{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.2.m2.1\"><semantics id=\"S4.p1.2.m2.1a\"><msub id=\"S4.p1.2.m2.1.1\" xref=\"S4.p1.2.m2.1.1.cmml\"><mi id=\"S4.p1.2.m2.1.1.2\" xref=\"S4.p1.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"S4.p1.2.m2.1.1.3\" xref=\"S4.p1.2.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.p1.2.m2.1b\"><apply id=\"S4.p1.2.m2.1.1.cmml\" xref=\"S4.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.p1.2.m2.1.1.1.cmml\" xref=\"S4.p1.2.m2.1.1\">subscript</csymbol><ci id=\"S4.p1.2.m2.1.1.2.cmml\" xref=\"S4.p1.2.m2.1.1.2\">𝒙</ci><ci id=\"S4.p1.2.m2.1.1.3.cmml\" xref=\"S4.p1.2.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.p1.2.m2.1c\">\\bm{x}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.p1.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> nearly as much as vice versa). To further test this learned similarity hypothesis, we explore the relationships between tasks and the model’s loss gradients, weights, and activations across training history. The results enable us to better understand the dynamics of cyclic training. Unless otherwise stated, all visualizations in this section use the 410M model and default hyperparameters.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Temporal Structure of Gradients</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.p1.1\">We first explore the similarities of gradient information between documents during the training process. Our goal is to test the hypothesis that anticipatory recovery is mediated by increased similarity between gradients of proximal documents in our training sequence.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p2\">\n<p class=\"ltx_p\" id=\"S4.SS1.p2.20\">We do cyclic training for 4 epochs and compute the gradient of each document at the attention layer of transformer block 12 at the conclusion of training. In Figure <a class=\"ltx_ref\" href=\"#S4.F8\" title=\"Figure 8 ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(a), we plot the cosine similarities between these gradient vectors of each document. Results show that the gradients have mostly positive cosine similarities (except for the last document, on which the model has just been trained).\nTo our surprise, the gradient similarities are highest near the center of the heat map rather than peaking along the diagonal. That is, the gradient similarity between documents <math alttext=\"\\bm{x}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.1.m1.1\"><semantics id=\"S4.SS1.p2.1.m1.1a\"><msub id=\"S4.SS1.p2.1.m1.1.1\" xref=\"S4.SS1.p2.1.m1.1.1.cmml\"><mi id=\"S4.SS1.p2.1.m1.1.1.2\" xref=\"S4.SS1.p2.1.m1.1.1.2.cmml\">𝒙</mi><mrow id=\"S4.SS1.p2.1.m1.1.1.3\" xref=\"S4.SS1.p2.1.m1.1.1.3.cmml\"><mi id=\"S4.SS1.p2.1.m1.1.1.3.2\" xref=\"S4.SS1.p2.1.m1.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.1.m1.1.1.3.1\" xref=\"S4.SS1.p2.1.m1.1.1.3.1.cmml\">−</mo><mn id=\"S4.SS1.p2.1.m1.1.1.3.3\" xref=\"S4.SS1.p2.1.m1.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.1.m1.1b\"><apply id=\"S4.SS1.p2.1.m1.1.1.cmml\" xref=\"S4.SS1.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.1.m1.1.1.1.cmml\" xref=\"S4.SS1.p2.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.1.m1.1.1.2.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.2\">𝒙</ci><apply id=\"S4.SS1.p2.1.m1.1.1.3.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.3\"><minus id=\"S4.SS1.p2.1.m1.1.1.3.1.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.3.1\"></minus><ci id=\"S4.SS1.p2.1.m1.1.1.3.2.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.3.2\">𝑡</ci><cn id=\"S4.SS1.p2.1.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.SS1.p2.1.m1.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.1.m1.1c\">\\bm{x}_{t-1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.1.m1.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"\\bm{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.2.m2.1\"><semantics id=\"S4.SS1.p2.2.m2.1a\"><msub id=\"S4.SS1.p2.2.m2.1.1\" xref=\"S4.SS1.p2.2.m2.1.1.cmml\"><mi id=\"S4.SS1.p2.2.m2.1.1.2\" xref=\"S4.SS1.p2.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.2.m2.1.1.3\" xref=\"S4.SS1.p2.2.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.2.m2.1b\"><apply id=\"S4.SS1.p2.2.m2.1.1.cmml\" xref=\"S4.SS1.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.2.m2.1.1.1.cmml\" xref=\"S4.SS1.p2.2.m2.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.2.m2.1.1.2.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.2.m2.1.1.3.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.2.m2.1c\">\\bm{x}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> depends on where we are in the cycle. This result suggests an additional layer to the anticipatory recovery phenomenon: Recovery for document <math alttext=\"\\bm{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.3.m3.1\"><semantics id=\"S4.SS1.p2.3.m3.1a\"><msub id=\"S4.SS1.p2.3.m3.1.1\" xref=\"S4.SS1.p2.3.m3.1.1.cmml\"><mi id=\"S4.SS1.p2.3.m3.1.1.2\" xref=\"S4.SS1.p2.3.m3.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.3.m3.1.1.3\" xref=\"S4.SS1.p2.3.m3.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.3.m3.1b\"><apply id=\"S4.SS1.p2.3.m3.1.1.cmml\" xref=\"S4.SS1.p2.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.3.m3.1.1.1.cmml\" xref=\"S4.SS1.p2.3.m3.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.3.m3.1.1.2.cmml\" xref=\"S4.SS1.p2.3.m3.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.3.m3.1.1.3.cmml\" xref=\"S4.SS1.p2.3.m3.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.3.m3.1c\">\\bm{x}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.3.m3.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is greatest from training on document <math alttext=\"\\bm{x}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.4.m4.1\"><semantics id=\"S4.SS1.p2.4.m4.1a\"><msub id=\"S4.SS1.p2.4.m4.1.1\" xref=\"S4.SS1.p2.4.m4.1.1.cmml\"><mi id=\"S4.SS1.p2.4.m4.1.1.2\" xref=\"S4.SS1.p2.4.m4.1.1.2.cmml\">𝒙</mi><mrow id=\"S4.SS1.p2.4.m4.1.1.3\" xref=\"S4.SS1.p2.4.m4.1.1.3.cmml\"><mi id=\"S4.SS1.p2.4.m4.1.1.3.2\" xref=\"S4.SS1.p2.4.m4.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.4.m4.1.1.3.1\" xref=\"S4.SS1.p2.4.m4.1.1.3.1.cmml\">−</mo><mn id=\"S4.SS1.p2.4.m4.1.1.3.3\" xref=\"S4.SS1.p2.4.m4.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.4.m4.1b\"><apply id=\"S4.SS1.p2.4.m4.1.1.cmml\" xref=\"S4.SS1.p2.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.4.m4.1.1.1.cmml\" xref=\"S4.SS1.p2.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.4.m4.1.1.2.cmml\" xref=\"S4.SS1.p2.4.m4.1.1.2\">𝒙</ci><apply id=\"S4.SS1.p2.4.m4.1.1.3.cmml\" xref=\"S4.SS1.p2.4.m4.1.1.3\"><minus id=\"S4.SS1.p2.4.m4.1.1.3.1.cmml\" xref=\"S4.SS1.p2.4.m4.1.1.3.1\"></minus><ci id=\"S4.SS1.p2.4.m4.1.1.3.2.cmml\" xref=\"S4.SS1.p2.4.m4.1.1.3.2\">𝑡</ci><cn id=\"S4.SS1.p2.4.m4.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.SS1.p2.4.m4.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.4.m4.1c\">\\bm{x}_{t-1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.4.m4.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, but the strength of the potential facilitation between <math alttext=\"\\bm{x}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.5.m5.1\"><semantics id=\"S4.SS1.p2.5.m5.1a\"><msub id=\"S4.SS1.p2.5.m5.1.1\" xref=\"S4.SS1.p2.5.m5.1.1.cmml\"><mi id=\"S4.SS1.p2.5.m5.1.1.2\" xref=\"S4.SS1.p2.5.m5.1.1.2.cmml\">𝒙</mi><mrow id=\"S4.SS1.p2.5.m5.1.1.3\" xref=\"S4.SS1.p2.5.m5.1.1.3.cmml\"><mi id=\"S4.SS1.p2.5.m5.1.1.3.2\" xref=\"S4.SS1.p2.5.m5.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.5.m5.1.1.3.1\" xref=\"S4.SS1.p2.5.m5.1.1.3.1.cmml\">−</mo><mn id=\"S4.SS1.p2.5.m5.1.1.3.3\" xref=\"S4.SS1.p2.5.m5.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.5.m5.1b\"><apply id=\"S4.SS1.p2.5.m5.1.1.cmml\" xref=\"S4.SS1.p2.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.5.m5.1.1.1.cmml\" xref=\"S4.SS1.p2.5.m5.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.5.m5.1.1.2.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.2\">𝒙</ci><apply id=\"S4.SS1.p2.5.m5.1.1.3.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3\"><minus id=\"S4.SS1.p2.5.m5.1.1.3.1.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3.1\"></minus><ci id=\"S4.SS1.p2.5.m5.1.1.3.2.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3.2\">𝑡</ci><cn id=\"S4.SS1.p2.5.m5.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.SS1.p2.5.m5.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.5.m5.1c\">\\bm{x}_{t-1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.5.m5.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"\\bm{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.6.m6.1\"><semantics id=\"S4.SS1.p2.6.m6.1a\"><msub id=\"S4.SS1.p2.6.m6.1.1\" xref=\"S4.SS1.p2.6.m6.1.1.cmml\"><mi id=\"S4.SS1.p2.6.m6.1.1.2\" xref=\"S4.SS1.p2.6.m6.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.6.m6.1.1.3\" xref=\"S4.SS1.p2.6.m6.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.6.m6.1b\"><apply id=\"S4.SS1.p2.6.m6.1.1.cmml\" xref=\"S4.SS1.p2.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.6.m6.1.1.1.cmml\" xref=\"S4.SS1.p2.6.m6.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.6.m6.1.1.2.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.6.m6.1.1.3.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.6.m6.1c\">\\bm{x}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.6.m6.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is actually greatest after we train for another <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.7.m7.1\"><semantics id=\"S4.SS1.p2.7.m7.1a\"><mi id=\"S4.SS1.p2.7.m7.1.1\" xref=\"S4.SS1.p2.7.m7.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.7.m7.1b\"><ci id=\"S4.SS1.p2.7.m7.1.1.cmml\" xref=\"S4.SS1.p2.7.m7.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.7.m7.1c\">b</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.7.m7.1d\">italic_b</annotation></semantics></math> documents (for some small number <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.8.m8.1\"><semantics id=\"S4.SS1.p2.8.m8.1a\"><mi id=\"S4.SS1.p2.8.m8.1.1\" xref=\"S4.SS1.p2.8.m8.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.8.m8.1b\"><ci id=\"S4.SS1.p2.8.m8.1.1.cmml\" xref=\"S4.SS1.p2.8.m8.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.8.m8.1c\">b</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.8.m8.1d\">italic_b</annotation></semantics></math>).\nWe verify this by computing the pairwise recovery: we take the model checkpoint after 4 epochs of cyclic training, and then for each pair of documents <math alttext=\"(\\bm{x}_{i},\\bm{x}_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.9.m9.2\"><semantics id=\"S4.SS1.p2.9.m9.2a\"><mrow id=\"S4.SS1.p2.9.m9.2.2.2\" xref=\"S4.SS1.p2.9.m9.2.2.3.cmml\"><mo id=\"S4.SS1.p2.9.m9.2.2.2.3\" stretchy=\"false\" xref=\"S4.SS1.p2.9.m9.2.2.3.cmml\">(</mo><msub id=\"S4.SS1.p2.9.m9.1.1.1.1\" xref=\"S4.SS1.p2.9.m9.1.1.1.1.cmml\"><mi id=\"S4.SS1.p2.9.m9.1.1.1.1.2\" xref=\"S4.SS1.p2.9.m9.1.1.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.9.m9.1.1.1.1.3\" xref=\"S4.SS1.p2.9.m9.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S4.SS1.p2.9.m9.2.2.2.4\" xref=\"S4.SS1.p2.9.m9.2.2.3.cmml\">,</mo><msub id=\"S4.SS1.p2.9.m9.2.2.2.2\" xref=\"S4.SS1.p2.9.m9.2.2.2.2.cmml\"><mi id=\"S4.SS1.p2.9.m9.2.2.2.2.2\" xref=\"S4.SS1.p2.9.m9.2.2.2.2.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.9.m9.2.2.2.2.3\" xref=\"S4.SS1.p2.9.m9.2.2.2.2.3.cmml\">j</mi></msub><mo id=\"S4.SS1.p2.9.m9.2.2.2.5\" stretchy=\"false\" xref=\"S4.SS1.p2.9.m9.2.2.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.9.m9.2b\"><interval closure=\"open\" id=\"S4.SS1.p2.9.m9.2.2.3.cmml\" xref=\"S4.SS1.p2.9.m9.2.2.2\"><apply id=\"S4.SS1.p2.9.m9.1.1.1.1.cmml\" xref=\"S4.SS1.p2.9.m9.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.9.m9.1.1.1.1.1.cmml\" xref=\"S4.SS1.p2.9.m9.1.1.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.9.m9.1.1.1.1.2.cmml\" xref=\"S4.SS1.p2.9.m9.1.1.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.9.m9.1.1.1.1.3.cmml\" xref=\"S4.SS1.p2.9.m9.1.1.1.1.3\">𝑖</ci></apply><apply id=\"S4.SS1.p2.9.m9.2.2.2.2.cmml\" xref=\"S4.SS1.p2.9.m9.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.9.m9.2.2.2.2.1.cmml\" xref=\"S4.SS1.p2.9.m9.2.2.2.2\">subscript</csymbol><ci id=\"S4.SS1.p2.9.m9.2.2.2.2.2.cmml\" xref=\"S4.SS1.p2.9.m9.2.2.2.2.2\">𝒙</ci><ci id=\"S4.SS1.p2.9.m9.2.2.2.2.3.cmml\" xref=\"S4.SS1.p2.9.m9.2.2.2.2.3\">𝑗</ci></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.9.m9.2c\">(\\bm{x}_{i},\\bm{x}_{j})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.9.m9.2d\">( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math>, we do <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.10.m10.1\"><semantics id=\"S4.SS1.p2.10.m10.1a\"><mi id=\"S4.SS1.p2.10.m10.1.1\" xref=\"S4.SS1.p2.10.m10.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.10.m10.1b\"><ci id=\"S4.SS1.p2.10.m10.1.1.cmml\" xref=\"S4.SS1.p2.10.m10.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.10.m10.1c\">M</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.10.m10.1d\">italic_M</annotation></semantics></math> gradient updates on <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.11.m11.1\"><semantics id=\"S4.SS1.p2.11.m11.1a\"><msub id=\"S4.SS1.p2.11.m11.1.1\" xref=\"S4.SS1.p2.11.m11.1.1.cmml\"><mi id=\"S4.SS1.p2.11.m11.1.1.2\" xref=\"S4.SS1.p2.11.m11.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.11.m11.1.1.3\" xref=\"S4.SS1.p2.11.m11.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.11.m11.1b\"><apply id=\"S4.SS1.p2.11.m11.1.1.cmml\" xref=\"S4.SS1.p2.11.m11.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.11.m11.1.1.1.cmml\" xref=\"S4.SS1.p2.11.m11.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.11.m11.1.1.2.cmml\" xref=\"S4.SS1.p2.11.m11.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.11.m11.1.1.3.cmml\" xref=\"S4.SS1.p2.11.m11.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.11.m11.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.11.m11.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and compute the difference in the loss of <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.12.m12.1\"><semantics id=\"S4.SS1.p2.12.m12.1a\"><msub id=\"S4.SS1.p2.12.m12.1.1\" xref=\"S4.SS1.p2.12.m12.1.1.cmml\"><mi id=\"S4.SS1.p2.12.m12.1.1.2\" xref=\"S4.SS1.p2.12.m12.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.12.m12.1.1.3\" xref=\"S4.SS1.p2.12.m12.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.12.m12.1b\"><apply id=\"S4.SS1.p2.12.m12.1.1.cmml\" xref=\"S4.SS1.p2.12.m12.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.12.m12.1.1.1.cmml\" xref=\"S4.SS1.p2.12.m12.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.12.m12.1.1.2.cmml\" xref=\"S4.SS1.p2.12.m12.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.12.m12.1.1.3.cmml\" xref=\"S4.SS1.p2.12.m12.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.12.m12.1c\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.12.m12.1d\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> before and after these gradient updates. We plot these pairwise loss recoveries in Figure <a class=\"ltx_ref\" href=\"#S4.F8\" title=\"Figure 8 ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(b). Results confirm that the amount of recovery on document <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.13.m13.1\"><semantics id=\"S4.SS1.p2.13.m13.1a\"><msub id=\"S4.SS1.p2.13.m13.1.1\" xref=\"S4.SS1.p2.13.m13.1.1.cmml\"><mi id=\"S4.SS1.p2.13.m13.1.1.2\" xref=\"S4.SS1.p2.13.m13.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.13.m13.1.1.3\" xref=\"S4.SS1.p2.13.m13.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.13.m13.1b\"><apply id=\"S4.SS1.p2.13.m13.1.1.cmml\" xref=\"S4.SS1.p2.13.m13.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.13.m13.1.1.1.cmml\" xref=\"S4.SS1.p2.13.m13.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.13.m13.1.1.2.cmml\" xref=\"S4.SS1.p2.13.m13.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.13.m13.1.1.3.cmml\" xref=\"S4.SS1.p2.13.m13.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.13.m13.1c\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.13.m13.1d\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is highest when the model checkpoint is taken from roughly <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.14.m14.1\"><semantics id=\"S4.SS1.p2.14.m14.1a\"><mi id=\"S4.SS1.p2.14.m14.1.1\" xref=\"S4.SS1.p2.14.m14.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.14.m14.1b\"><ci id=\"S4.SS1.p2.14.m14.1.1.cmml\" xref=\"S4.SS1.p2.14.m14.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.14.m14.1c\">b</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.14.m14.1d\">italic_b</annotation></semantics></math> documents before or after document <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.15.m15.1\"><semantics id=\"S4.SS1.p2.15.m15.1a\"><msub id=\"S4.SS1.p2.15.m15.1.1\" xref=\"S4.SS1.p2.15.m15.1.1.cmml\"><mi id=\"S4.SS1.p2.15.m15.1.1.2\" xref=\"S4.SS1.p2.15.m15.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.15.m15.1.1.3\" xref=\"S4.SS1.p2.15.m15.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.15.m15.1b\"><apply id=\"S4.SS1.p2.15.m15.1.1.cmml\" xref=\"S4.SS1.p2.15.m15.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.15.m15.1.1.1.cmml\" xref=\"S4.SS1.p2.15.m15.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.15.m15.1.1.2.cmml\" xref=\"S4.SS1.p2.15.m15.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.15.m15.1.1.3.cmml\" xref=\"S4.SS1.p2.15.m15.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.15.m15.1c\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.15.m15.1d\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> in cyclic training and then fine-tuned on a proximal document <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.16.m16.1\"><semantics id=\"S4.SS1.p2.16.m16.1a\"><msub id=\"S4.SS1.p2.16.m16.1.1\" xref=\"S4.SS1.p2.16.m16.1.1.cmml\"><mi id=\"S4.SS1.p2.16.m16.1.1.2\" xref=\"S4.SS1.p2.16.m16.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.16.m16.1.1.3\" xref=\"S4.SS1.p2.16.m16.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.16.m16.1b\"><apply id=\"S4.SS1.p2.16.m16.1.1.cmml\" xref=\"S4.SS1.p2.16.m16.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.16.m16.1.1.1.cmml\" xref=\"S4.SS1.p2.16.m16.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.16.m16.1.1.2.cmml\" xref=\"S4.SS1.p2.16.m16.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.16.m16.1.1.3.cmml\" xref=\"S4.SS1.p2.16.m16.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.16.m16.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.16.m16.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in the sequence. The fact that this pairwise loss recovery matrix is roughly symmetric also suggests that the anticipatory recovery phenomenon approximately exhibits task symmetry: gradient updates on document <math alttext=\"\\bm{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.17.m17.1\"><semantics id=\"S4.SS1.p2.17.m17.1a\"><msub id=\"S4.SS1.p2.17.m17.1.1\" xref=\"S4.SS1.p2.17.m17.1.1.cmml\"><mi id=\"S4.SS1.p2.17.m17.1.1.2\" xref=\"S4.SS1.p2.17.m17.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS1.p2.17.m17.1.1.3\" xref=\"S4.SS1.p2.17.m17.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.17.m17.1b\"><apply id=\"S4.SS1.p2.17.m17.1.1.cmml\" xref=\"S4.SS1.p2.17.m17.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.17.m17.1.1.1.cmml\" xref=\"S4.SS1.p2.17.m17.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.17.m17.1.1.2.cmml\" xref=\"S4.SS1.p2.17.m17.1.1.2\">𝒙</ci><ci id=\"S4.SS1.p2.17.m17.1.1.3.cmml\" xref=\"S4.SS1.p2.17.m17.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.17.m17.1c\">\\bm{x}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.17.m17.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> decrease the loss for document <math alttext=\"\\bm{x}_{t+k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.18.m18.1\"><semantics id=\"S4.SS1.p2.18.m18.1a\"><msub id=\"S4.SS1.p2.18.m18.1.1\" xref=\"S4.SS1.p2.18.m18.1.1.cmml\"><mi id=\"S4.SS1.p2.18.m18.1.1.2\" xref=\"S4.SS1.p2.18.m18.1.1.2.cmml\">𝒙</mi><mrow id=\"S4.SS1.p2.18.m18.1.1.3\" xref=\"S4.SS1.p2.18.m18.1.1.3.cmml\"><mi id=\"S4.SS1.p2.18.m18.1.1.3.2\" xref=\"S4.SS1.p2.18.m18.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.18.m18.1.1.3.1\" xref=\"S4.SS1.p2.18.m18.1.1.3.1.cmml\">+</mo><mi id=\"S4.SS1.p2.18.m18.1.1.3.3\" xref=\"S4.SS1.p2.18.m18.1.1.3.3.cmml\">k</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.18.m18.1b\"><apply id=\"S4.SS1.p2.18.m18.1.1.cmml\" xref=\"S4.SS1.p2.18.m18.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.18.m18.1.1.1.cmml\" xref=\"S4.SS1.p2.18.m18.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.18.m18.1.1.2.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.2\">𝒙</ci><apply id=\"S4.SS1.p2.18.m18.1.1.3.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3\"><plus id=\"S4.SS1.p2.18.m18.1.1.3.1.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3.1\"></plus><ci id=\"S4.SS1.p2.18.m18.1.1.3.2.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3.2\">𝑡</ci><ci id=\"S4.SS1.p2.18.m18.1.1.3.3.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3.3\">𝑘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.18.m18.1c\">\\bm{x}_{t+k}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.18.m18.1d\">bold_italic_x start_POSTSUBSCRIPT italic_t + italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for small integers <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.19.m19.1\"><semantics id=\"S4.SS1.p2.19.m19.1a\"><mi id=\"S4.SS1.p2.19.m19.1.1\" xref=\"S4.SS1.p2.19.m19.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.19.m19.1b\"><ci id=\"S4.SS1.p2.19.m19.1.1.cmml\" xref=\"S4.SS1.p2.19.m19.1.1\">𝑘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.19.m19.1c\">k</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.19.m19.1d\">italic_k</annotation></semantics></math>, and vice versa. We provide additional visualizations for <math alttext=\"T=50,100,200\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.20.m20.3\"><semantics id=\"S4.SS1.p2.20.m20.3a\"><mrow id=\"S4.SS1.p2.20.m20.3.4\" xref=\"S4.SS1.p2.20.m20.3.4.cmml\"><mi id=\"S4.SS1.p2.20.m20.3.4.2\" xref=\"S4.SS1.p2.20.m20.3.4.2.cmml\">T</mi><mo id=\"S4.SS1.p2.20.m20.3.4.1\" xref=\"S4.SS1.p2.20.m20.3.4.1.cmml\">=</mo><mrow id=\"S4.SS1.p2.20.m20.3.4.3.2\" xref=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\"><mn id=\"S4.SS1.p2.20.m20.1.1\" xref=\"S4.SS1.p2.20.m20.1.1.cmml\">50</mn><mo id=\"S4.SS1.p2.20.m20.3.4.3.2.1\" xref=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\">,</mo><mn id=\"S4.SS1.p2.20.m20.2.2\" xref=\"S4.SS1.p2.20.m20.2.2.cmml\">100</mn><mo id=\"S4.SS1.p2.20.m20.3.4.3.2.2\" xref=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\">,</mo><mn id=\"S4.SS1.p2.20.m20.3.3\" xref=\"S4.SS1.p2.20.m20.3.3.cmml\">200</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.20.m20.3b\"><apply id=\"S4.SS1.p2.20.m20.3.4.cmml\" xref=\"S4.SS1.p2.20.m20.3.4\"><eq id=\"S4.SS1.p2.20.m20.3.4.1.cmml\" xref=\"S4.SS1.p2.20.m20.3.4.1\"></eq><ci id=\"S4.SS1.p2.20.m20.3.4.2.cmml\" xref=\"S4.SS1.p2.20.m20.3.4.2\">𝑇</ci><list id=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\" xref=\"S4.SS1.p2.20.m20.3.4.3.2\"><cn id=\"S4.SS1.p2.20.m20.1.1.cmml\" type=\"integer\" xref=\"S4.SS1.p2.20.m20.1.1\">50</cn><cn id=\"S4.SS1.p2.20.m20.2.2.cmml\" type=\"integer\" xref=\"S4.SS1.p2.20.m20.2.2\">100</cn><cn id=\"S4.SS1.p2.20.m20.3.3.cmml\" type=\"integer\" xref=\"S4.SS1.p2.20.m20.3.3\">200</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.20.m20.3c\">T=50,100,200</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS1.p2.20.m20.3d\">italic_T = 50 , 100 , 200</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Additional Visualizations ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span> <span class=\"ltx_text ltx_ref_tag\">C</span></a> and a more detailed description for this phenomenon.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.SS1.2\">\n<div class=\"ltx_block\" id=\"S4.SS1.2.2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_bottom\" id=\"S4.F9\" style=\"width:138.8pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" height=\"114\" id=\"S4.SS1.1.1.g1\" src=\"./assets/x9.png\" width=\"109\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F9.2.1.1\" style=\"font-size:90%;\">Figure 9</span>: </span><span class=\"ltx_text\" id=\"S4.F9.3.2\" style=\"font-size:90%;\">Top three PCA components of last layer weights in the first three epochs.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_bottom\" id=\"S4.F10\" style=\"width:277.5pt;\">\n<div class=\"ltx_block ltx_figure_panel\" id=\"S4.F10.3\">\n<figure class=\"ltx_figure\" id=\"S4.F10.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"131\" id=\"S4.F10.sf1.g1\" src=\"./assets/x10.png\" width=\"169\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F10.sf1.4.1.1\" style=\"font-size:90%;\">(a)</span> </span><math alttext=\"f_{i}(\\bm{w})=\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F10.sf1.2.m1.1\"><semantics id=\"S4.F10.sf1.2.m1.1b\"><mrow id=\"S4.F10.sf1.2.m1.1.2\" xref=\"S4.F10.sf1.2.m1.1.2.cmml\"><mrow id=\"S4.F10.sf1.2.m1.1.2.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\"><msub id=\"S4.F10.sf1.2.m1.1.2.2.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.cmml\"><mi id=\"S4.F10.sf1.2.m1.1.2.2.2.2\" mathsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.F10.sf1.2.m1.1.2.2.2.3\" mathsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"S4.F10.sf1.2.m1.1.2.2.1\" xref=\"S4.F10.sf1.2.m1.1.2.2.1.cmml\">⁢</mo><mrow id=\"S4.F10.sf1.2.m1.1.2.2.3.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\"><mo id=\"S4.F10.sf1.2.m1.1.2.2.3.2.1\" maxsize=\"90%\" minsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\">(</mo><mi id=\"S4.F10.sf1.2.m1.1.1\" mathsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.1.cmml\">𝒘</mi><mo id=\"S4.F10.sf1.2.m1.1.2.2.3.2.2\" maxsize=\"90%\" minsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.F10.sf1.2.m1.1.2.1\" mathsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.2.1.cmml\">=</mo><mi id=\"S4.F10.sf1.2.m1.1.2.3\" mathsize=\"90%\" xref=\"S4.F10.sf1.2.m1.1.2.3.cmml\">𝒘</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F10.sf1.2.m1.1c\"><apply id=\"S4.F10.sf1.2.m1.1.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2\"><eq id=\"S4.F10.sf1.2.m1.1.2.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.1\"></eq><apply id=\"S4.F10.sf1.2.m1.1.2.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2\"><times id=\"S4.F10.sf1.2.m1.1.2.2.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.1\"></times><apply id=\"S4.F10.sf1.2.m1.1.2.2.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.F10.sf1.2.m1.1.2.2.2.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2\">subscript</csymbol><ci id=\"S4.F10.sf1.2.m1.1.2.2.2.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.2\">𝑓</ci><ci id=\"S4.F10.sf1.2.m1.1.2.2.2.3.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.3\">𝑖</ci></apply><ci id=\"S4.F10.sf1.2.m1.1.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.1\">𝒘</ci></apply><ci id=\"S4.F10.sf1.2.m1.1.2.3.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.3\">𝒘</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F10.sf1.2.m1.1d\">f_{i}(\\bm{w})=\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F10.sf1.2.m1.1e\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_w</annotation></semantics></math><span class=\"ltx_text\" id=\"S4.F10.sf1.5.2\" style=\"font-size:90%;\"> </span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_align_center\" id=\"S4.F10.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"131\" id=\"S4.F10.sf2.g1\" src=\"./assets/x11.png\" width=\"169\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F10.sf2.4.1.1\" style=\"font-size:90%;\">(b)</span> </span><math alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F10.sf2.2.m1.1\"><semantics id=\"S4.F10.sf2.2.m1.1b\"><mrow id=\"S4.F10.sf2.2.m1.1.2\" xref=\"S4.F10.sf2.2.m1.1.2.cmml\"><mrow id=\"S4.F10.sf2.2.m1.1.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\"><msub id=\"S4.F10.sf2.2.m1.1.2.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.cmml\"><mi id=\"S4.F10.sf2.2.m1.1.2.2.2.2\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.F10.sf2.2.m1.1.2.2.2.3\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"S4.F10.sf2.2.m1.1.2.2.1\" xref=\"S4.F10.sf2.2.m1.1.2.2.1.cmml\">⁢</mo><mrow id=\"S4.F10.sf2.2.m1.1.2.2.3.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\"><mo id=\"S4.F10.sf2.2.m1.1.2.2.3.2.1\" maxsize=\"90%\" minsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\">(</mo><mi id=\"S4.F10.sf2.2.m1.1.1\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.1.cmml\">𝒘</mi><mo id=\"S4.F10.sf2.2.m1.1.2.2.3.2.2\" maxsize=\"90%\" minsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.F10.sf2.2.m1.1.2.1\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.1.cmml\">=</mo><mrow id=\"S4.F10.sf2.2.m1.1.2.3\" xref=\"S4.F10.sf2.2.m1.1.2.3.cmml\"><msub id=\"S4.F10.sf2.2.m1.1.2.3.2\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.cmml\"><mi id=\"S4.F10.sf2.2.m1.1.2.3.2.2\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.2.cmml\">𝒚</mi><mi id=\"S4.F10.sf2.2.m1.1.2.3.2.3\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"S4.F10.sf2.2.m1.1.2.3.1\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.3.1.cmml\">−</mo><mi id=\"S4.F10.sf2.2.m1.1.2.3.3\" mathsize=\"90%\" xref=\"S4.F10.sf2.2.m1.1.2.3.3.cmml\">𝒘</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F10.sf2.2.m1.1c\"><apply id=\"S4.F10.sf2.2.m1.1.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2\"><eq id=\"S4.F10.sf2.2.m1.1.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.1\"></eq><apply id=\"S4.F10.sf2.2.m1.1.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2\"><times id=\"S4.F10.sf2.2.m1.1.2.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.1\"></times><apply id=\"S4.F10.sf2.2.m1.1.2.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.F10.sf2.2.m1.1.2.2.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2\">subscript</csymbol><ci id=\"S4.F10.sf2.2.m1.1.2.2.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.2\">𝑓</ci><ci id=\"S4.F10.sf2.2.m1.1.2.2.2.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.3\">𝑖</ci></apply><ci id=\"S4.F10.sf2.2.m1.1.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.1\">𝒘</ci></apply><apply id=\"S4.F10.sf2.2.m1.1.2.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3\"><minus id=\"S4.F10.sf2.2.m1.1.2.3.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.1\"></minus><apply id=\"S4.F10.sf2.2.m1.1.2.3.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.F10.sf2.2.m1.1.2.3.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2\">subscript</csymbol><ci id=\"S4.F10.sf2.2.m1.1.2.3.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.2\">𝒚</ci><ci id=\"S4.F10.sf2.2.m1.1.2.3.2.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.3\">𝑖</ci></apply><ci id=\"S4.F10.sf2.2.m1.1.2.3.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.3\">𝒘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F10.sf2.2.m1.1d\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F10.sf2.2.m1.1e\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_italic_w</annotation></semantics></math><span class=\"ltx_text\" id=\"S4.F10.sf2.5.2\" style=\"font-size:90%;\"> </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F10.5.2.1\" style=\"font-size:90%;\">Figure 10</span>: </span><span class=\"ltx_text\" id=\"S4.F10.2.1\" style=\"font-size:90%;\">Loss curve for task 1 in computational toy model, with different <math alttext=\"f_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F10.2.1.m1.1\"><semantics id=\"S4.F10.2.1.m1.1b\"><msub id=\"S4.F10.2.1.m1.1.1\" xref=\"S4.F10.2.1.m1.1.1.cmml\"><mi id=\"S4.F10.2.1.m1.1.1.2\" xref=\"S4.F10.2.1.m1.1.1.2.cmml\">f</mi><mi id=\"S4.F10.2.1.m1.1.1.3\" xref=\"S4.F10.2.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F10.2.1.m1.1c\"><apply id=\"S4.F10.2.1.m1.1.1.cmml\" xref=\"S4.F10.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F10.2.1.m1.1.1.1.cmml\" xref=\"S4.F10.2.1.m1.1.1\">subscript</csymbol><ci id=\"S4.F10.2.1.m1.1.1.2.cmml\" xref=\"S4.F10.2.1.m1.1.1.2\">𝑓</ci><ci id=\"S4.F10.2.1.m1.1.1.3.cmml\" xref=\"S4.F10.2.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F10.2.1.m1.1d\">f_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F10.2.1.m1.1e\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. More experiment details in Appendix <a class=\"ltx_ref\" href=\"#A1.SS5\" title=\"A.5 Computational Toy Model ‣ Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A.5</span></a>.</span></figcaption>\n</figure>\n</div>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S4.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"153\" id=\"S4.SS1.3.g1\" src=\"./assets/x12.png\" width=\"789\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"S4.F11.6.3.1\" style=\"font-size:90%;\">Figure 11</span>: </span><span class=\"ltx_text\" id=\"S4.F11.4.2\" style=\"font-size:90%;\">Visualization of PCA embeddings of the projected data points (<math alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F11.3.1.m1.1\"><semantics id=\"S4.F11.3.1.m1.1b\"><mrow id=\"S4.F11.3.1.m1.1.1\" xref=\"S4.F11.3.1.m1.1.1.cmml\"><msubsup id=\"S4.F11.3.1.m1.1.1.3\" xref=\"S4.F11.3.1.m1.1.1.3.cmml\"><mi id=\"S4.F11.3.1.m1.1.1.3.2.2\" xref=\"S4.F11.3.1.m1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.F11.3.1.m1.1.1.3.2.3\" xref=\"S4.F11.3.1.m1.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.F11.3.1.m1.1.1.3.3\" xref=\"S4.F11.3.1.m1.1.1.3.3.cmml\"><mo id=\"S4.F11.3.1.m1.1.1.3.3b\" xref=\"S4.F11.3.1.m1.1.1.3.3.cmml\">−</mo><mn id=\"S4.F11.3.1.m1.1.1.3.3.2\" xref=\"S4.F11.3.1.m1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.F11.3.1.m1.1.1.2\" xref=\"S4.F11.3.1.m1.1.1.2.cmml\">⁢</mo><mrow id=\"S4.F11.3.1.m1.1.1.1.1\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\"><mo id=\"S4.F11.3.1.m1.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.F11.3.1.m1.1.1.1.1.1\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\"><mi id=\"S4.F11.3.1.m1.1.1.1.1.1.2\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.F11.3.1.m1.1.1.1.1.1.1\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.F11.3.1.m1.1.1.1.1.1.3\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.cmml\"><mi id=\"S4.F11.3.1.m1.1.1.1.1.1.3.2\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.F11.3.1.m1.1.1.1.1.1.3.3\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.F11.3.1.m1.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F11.3.1.m1.1c\"><apply id=\"S4.F11.3.1.m1.1.1.cmml\" xref=\"S4.F11.3.1.m1.1.1\"><times id=\"S4.F11.3.1.m1.1.1.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.2\"></times><apply id=\"S4.F11.3.1.m1.1.1.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.F11.3.1.m1.1.1.3.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\">superscript</csymbol><apply id=\"S4.F11.3.1.m1.1.1.3.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.F11.3.1.m1.1.1.3.2.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.F11.3.1.m1.1.1.3.2.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.2.2\">𝑓</ci><ci id=\"S4.F11.3.1.m1.1.1.3.2.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.2.3\">𝑖</ci></apply><apply id=\"S4.F11.3.1.m1.1.1.3.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.3\"><minus id=\"S4.F11.3.1.m1.1.1.3.3.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.3\"></minus><cn id=\"S4.F11.3.1.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.F11.3.1.m1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1\"><times id=\"S4.F11.3.1.m1.1.1.1.1.1.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.1\"></times><ci id=\"S4.F11.3.1.m1.1.1.1.1.1.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.F11.3.1.m1.1.1.1.1.1.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.F11.3.1.m1.1.1.1.1.1.3.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.F11.3.1.m1.1.1.1.1.1.3.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.2\">𝒙</ci><ci id=\"S4.F11.3.1.m1.1.1.1.1.1.3.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.3\">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F11.3.1.m1.1d\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F11.3.1.m1.1e\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F11.4.2.m2.1\"><semantics id=\"S4.F11.4.2.m2.1b\"><mrow id=\"S4.F11.4.2.m2.1.2\" xref=\"S4.F11.4.2.m2.1.2.cmml\"><mrow id=\"S4.F11.4.2.m2.1.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\"><msub id=\"S4.F11.4.2.m2.1.2.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.2.cmml\"><mi id=\"S4.F11.4.2.m2.1.2.2.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.F11.4.2.m2.1.2.2.2.3\" xref=\"S4.F11.4.2.m2.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"S4.F11.4.2.m2.1.2.2.1\" xref=\"S4.F11.4.2.m2.1.2.2.1.cmml\">⁢</mo><mrow id=\"S4.F11.4.2.m2.1.2.2.3.2\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\"><mo id=\"S4.F11.4.2.m2.1.2.2.3.2.1\" stretchy=\"false\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\">(</mo><mi id=\"S4.F11.4.2.m2.1.1\" xref=\"S4.F11.4.2.m2.1.1.cmml\">𝒘</mi><mo id=\"S4.F11.4.2.m2.1.2.2.3.2.2\" stretchy=\"false\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.F11.4.2.m2.1.2.1\" xref=\"S4.F11.4.2.m2.1.2.1.cmml\">=</mo><mrow id=\"S4.F11.4.2.m2.1.2.3\" xref=\"S4.F11.4.2.m2.1.2.3.cmml\"><msub id=\"S4.F11.4.2.m2.1.2.3.2\" xref=\"S4.F11.4.2.m2.1.2.3.2.cmml\"><mi id=\"S4.F11.4.2.m2.1.2.3.2.2\" xref=\"S4.F11.4.2.m2.1.2.3.2.2.cmml\">𝒚</mi><mi id=\"S4.F11.4.2.m2.1.2.3.2.3\" xref=\"S4.F11.4.2.m2.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"S4.F11.4.2.m2.1.2.3.1\" xref=\"S4.F11.4.2.m2.1.2.3.1.cmml\">−</mo><mi id=\"S4.F11.4.2.m2.1.2.3.3\" xref=\"S4.F11.4.2.m2.1.2.3.3.cmml\">𝒘</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F11.4.2.m2.1c\"><apply id=\"S4.F11.4.2.m2.1.2.cmml\" xref=\"S4.F11.4.2.m2.1.2\"><eq id=\"S4.F11.4.2.m2.1.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.1\"></eq><apply id=\"S4.F11.4.2.m2.1.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.2\"><times id=\"S4.F11.4.2.m2.1.2.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.1\"></times><apply id=\"S4.F11.4.2.m2.1.2.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.F11.4.2.m2.1.2.2.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2\">subscript</csymbol><ci id=\"S4.F11.4.2.m2.1.2.2.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2.2\">𝑓</ci><ci id=\"S4.F11.4.2.m2.1.2.2.2.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2.3\">𝑖</ci></apply><ci id=\"S4.F11.4.2.m2.1.1.cmml\" xref=\"S4.F11.4.2.m2.1.1\">𝒘</ci></apply><apply id=\"S4.F11.4.2.m2.1.2.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.3\"><minus id=\"S4.F11.4.2.m2.1.2.3.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.1\"></minus><apply id=\"S4.F11.4.2.m2.1.2.3.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.F11.4.2.m2.1.2.3.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2\">subscript</csymbol><ci id=\"S4.F11.4.2.m2.1.2.3.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2.2\">𝒚</ci><ci id=\"S4.F11.4.2.m2.1.2.3.2.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2.3\">𝑖</ci></apply><ci id=\"S4.F11.4.2.m2.1.2.3.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.3\">𝒘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F11.4.2.m2.1d\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F11.4.2.m2.1e\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_italic_w</annotation></semantics></math>) in the toy model throughout training. Epoch 0 refers to the model before any training.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Temporal Structure of Model Weights</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.p1.2\">We explore the structure of model weights along the optimization trajectory of cyclic training. We flatten and concatenate the model weight vectors after fine-tuning on each document. However, the cosine similarities between these raw model weight vectors are all very close to 1 without obvious structure, due to the proximity of model weights along the same optimization trajectory and numerical instability in huge weight vectors. To resolve these issues, we instead explore the structure of “weight residuals.” We compute the weight residuals by subtracting the average of weights in a window of length <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.1.m1.1\"><semantics id=\"S4.SS2.p1.1.m1.1a\"><mi id=\"S4.SS2.p1.1.m1.1.1\" xref=\"S4.SS2.p1.1.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.1.m1.1b\"><ci id=\"S4.SS2.p1.1.m1.1.1.cmml\" xref=\"S4.SS2.p1.1.m1.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.1.m1.1c\">T</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS2.p1.1.m1.1d\">italic_T</annotation></semantics></math> centered at the current document from the current weight, i.e. <math alttext=\"w_{\\text{res}}(t)=w(t)-\\frac{1}{T}\\sum_{n=t-T/2}^{t+T/2}w(n)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.2.m2.3\"><semantics id=\"S4.SS2.p1.2.m2.3a\"><mrow id=\"S4.SS2.p1.2.m2.3.4\" xref=\"S4.SS2.p1.2.m2.3.4.cmml\"><mrow id=\"S4.SS2.p1.2.m2.3.4.2\" xref=\"S4.SS2.p1.2.m2.3.4.2.cmml\"><msub id=\"S4.SS2.p1.2.m2.3.4.2.2\" xref=\"S4.SS2.p1.2.m2.3.4.2.2.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.2.2.2\" xref=\"S4.SS2.p1.2.m2.3.4.2.2.2.cmml\">w</mi><mtext id=\"S4.SS2.p1.2.m2.3.4.2.2.3\" xref=\"S4.SS2.p1.2.m2.3.4.2.2.3a.cmml\">res</mtext></msub><mo id=\"S4.SS2.p1.2.m2.3.4.2.1\" xref=\"S4.SS2.p1.2.m2.3.4.2.1.cmml\">⁢</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.2.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.2.cmml\"><mo id=\"S4.SS2.p1.2.m2.3.4.2.3.2.1\" stretchy=\"false\" xref=\"S4.SS2.p1.2.m2.3.4.2.cmml\">(</mo><mi id=\"S4.SS2.p1.2.m2.1.1\" xref=\"S4.SS2.p1.2.m2.1.1.cmml\">t</mi><mo id=\"S4.SS2.p1.2.m2.3.4.2.3.2.2\" stretchy=\"false\" xref=\"S4.SS2.p1.2.m2.3.4.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS2.p1.2.m2.3.4.1\" xref=\"S4.SS2.p1.2.m2.3.4.1.cmml\">=</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.cmml\"><mrow id=\"S4.SS2.p1.2.m2.3.4.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.2.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.2.cmml\">w</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.2.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.1.cmml\">⁢</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.2.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.cmml\"><mo id=\"S4.SS2.p1.2.m2.3.4.3.2.3.2.1\" stretchy=\"false\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.cmml\">(</mo><mi id=\"S4.SS2.p1.2.m2.2.2\" xref=\"S4.SS2.p1.2.m2.2.2.cmml\">t</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.2.3.2.2\" stretchy=\"false\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS2.p1.2.m2.3.4.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.1.cmml\">−</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.cmml\"><mfrac id=\"S4.SS2.p1.2.m2.3.4.3.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2.cmml\"><mn id=\"S4.SS2.p1.2.m2.3.4.3.3.2.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2.2.cmml\">1</mn><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.2.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2.3.cmml\">T</mi></mfrac><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.1.cmml\">⁢</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.cmml\"><msubsup id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.cmml\"><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.2.cmml\">∑</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.2.cmml\">n</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.1.cmml\">=</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.2.cmml\">t</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.1.cmml\">−</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.2.cmml\">T</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.1.cmml\">/</mo><mn id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.3.cmml\">2</mn></mrow></mrow></mrow><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.2.cmml\">t</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.1.cmml\">+</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.2.cmml\">T</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.1.cmml\">/</mo><mn id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.3\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.3.cmml\">2</mn></mrow></mrow></msubsup><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.cmml\"><mi id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.2.cmml\">w</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.1\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.1.cmml\">⁢</mo><mrow id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.3.2\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.cmml\"><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.3.2.1\" stretchy=\"false\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.cmml\">(</mo><mi id=\"S4.SS2.p1.2.m2.3.3\" xref=\"S4.SS2.p1.2.m2.3.3.cmml\">n</mi><mo id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.3.2.2\" stretchy=\"false\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.cmml\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.2.m2.3b\"><apply id=\"S4.SS2.p1.2.m2.3.4.cmml\" xref=\"S4.SS2.p1.2.m2.3.4\"><eq id=\"S4.SS2.p1.2.m2.3.4.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.1\"></eq><apply id=\"S4.SS2.p1.2.m2.3.4.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.2\"><times id=\"S4.SS2.p1.2.m2.3.4.2.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.2.1\"></times><apply id=\"S4.SS2.p1.2.m2.3.4.2.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.2.m2.3.4.2.2.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.2.2\">subscript</csymbol><ci id=\"S4.SS2.p1.2.m2.3.4.2.2.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.2.2.2\">𝑤</ci><ci id=\"S4.SS2.p1.2.m2.3.4.2.2.3a.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.2.2.3\"><mtext id=\"S4.SS2.p1.2.m2.3.4.2.2.3.cmml\" mathsize=\"70%\" xref=\"S4.SS2.p1.2.m2.3.4.2.2.3\">res</mtext></ci></apply><ci id=\"S4.SS2.p1.2.m2.1.1.cmml\" xref=\"S4.SS2.p1.2.m2.1.1\">𝑡</ci></apply><apply id=\"S4.SS2.p1.2.m2.3.4.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3\"><minus id=\"S4.SS2.p1.2.m2.3.4.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.1\"></minus><apply id=\"S4.SS2.p1.2.m2.3.4.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.2\"><times id=\"S4.SS2.p1.2.m2.3.4.3.2.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.1\"></times><ci id=\"S4.SS2.p1.2.m2.3.4.3.2.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.2.2\">𝑤</ci><ci id=\"S4.SS2.p1.2.m2.2.2.cmml\" xref=\"S4.SS2.p1.2.m2.2.2\">𝑡</ci></apply><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3\"><times id=\"S4.SS2.p1.2.m2.3.4.3.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.1\"></times><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2\"><divide id=\"S4.SS2.p1.2.m2.3.4.3.3.2.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2\"></divide><cn id=\"S4.SS2.p1.2.m2.3.4.3.3.2.2.cmml\" type=\"integer\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2.2\">1</cn><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.2.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.2.3\">𝑇</ci></apply><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3\"><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1\">superscript</csymbol><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1\">subscript</csymbol><sum id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.2\"></sum><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3\"><eq id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.1\"></eq><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.2\">𝑛</ci><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3\"><minus id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.1\"></minus><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.2\">𝑡</ci><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3\"><divide id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.1\"></divide><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.2\">𝑇</ci><cn id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.2.3.3.3.3\">2</cn></apply></apply></apply></apply><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3\"><plus id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.1\"></plus><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.2\">𝑡</ci><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3\"><divide id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.1\"></divide><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.2\">𝑇</ci><cn id=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.1.3.3.3\">2</cn></apply></apply></apply><apply id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2\"><times id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.1.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.1\"></times><ci id=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.2.cmml\" xref=\"S4.SS2.p1.2.m2.3.4.3.3.3.2.2\">𝑤</ci><ci id=\"S4.SS2.p1.2.m2.3.3.cmml\" xref=\"S4.SS2.p1.2.m2.3.3\">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.2.m2.3c\">w_{\\text{res}}(t)=w(t)-\\frac{1}{T}\\sum_{n=t-T/2}^{t+T/2}w(n)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS2.p1.2.m2.3d\">italic_w start_POSTSUBSCRIPT res end_POSTSUBSCRIPT ( italic_t ) = italic_w ( italic_t ) - divide start_ARG 1 end_ARG start_ARG italic_T end_ARG ∑ start_POSTSUBSCRIPT italic_n = italic_t - italic_T / 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + italic_T / 2 end_POSTSUPERSCRIPT italic_w ( italic_n )</annotation></semantics></math>.\nThis removes the shared components along the optimization trajectory and allows us to focus on the model weight updates for each document. Figure <a class=\"ltx_ref\" href=\"#S4.F8\" title=\"Figure 8 ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(c) visualizes a heat map of the cosine similarity between each pair of weight residuals from the second epoch to the fourth epoch. The visualization shows a cyclic structure in the weight residuals, as equidistant bright stripes that align with the training epochs. Furthermore, each stripe spans several documents, suggesting the similarity of weight residuals for proximal documents.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.p2\">\n<p class=\"ltx_p\" id=\"S4.SS2.p2.1\">In addition to cosine similarities between weights, we explore visualizing the weights in a lower-dimensional space with Principle Component Analysis (PCA). We compute the top three PCs of the flattened last-layer weight vector (the output word embedding layer) for the Pythia-1B model, and plot its trajectory in Figure <a class=\"ltx_ref\" href=\"#S4.F9\" title=\"Figure 9 ‣ 4.1 Temporal Structure of Gradients ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The plot exhibits a clear helical structure that gradually converges. We believe this is highly relevant to anticipatory recovery: right before revisiting a task, the projected model weights in the helix move closer to the point corresponding to the previous appearance of that task, leading to anticipatory recovery on the loss of that task. As we go on with cyclic training, the model also exhibits less forgetting and gradually converges to a solution that achieves low loss on all tasks. It is important to note that the helical structure of the weight trajectory is not an obviously necessary consequence of the cyclical training. Cyclical training could be expected to yield a repeating pattern, but the facts that the tasks come to be organized in a circle that respects their ordering and that the trajectory goes through one full revolution per epoch (rather than some other arc length) are nontrivial and seem to be essential for anticipatory recovery.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Temporal Structure of Activations</h3>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.p1.4\">In addition to gradients and weights, we visualize the trajectory of activations on a single document during the course of cyclic training. We do cyclic training for three epochs and save model checkpoints after fine-tuning on each document. We then compute the model activations before the output word embedding layer for document <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.1.m1.1\"><semantics id=\"S4.SS3.p1.1.m1.1a\"><msub id=\"S4.SS3.p1.1.m1.1.1\" xref=\"S4.SS3.p1.1.m1.1.1.cmml\"><mi id=\"S4.SS3.p1.1.m1.1.1.2\" xref=\"S4.SS3.p1.1.m1.1.1.2.cmml\">𝒙</mi><mn id=\"S4.SS3.p1.1.m1.1.1.3\" xref=\"S4.SS3.p1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.1.m1.1b\"><apply id=\"S4.SS3.p1.1.m1.1.1.cmml\" xref=\"S4.SS3.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.1.m1.1.1.1.cmml\" xref=\"S4.SS3.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.1.m1.1.1.2.cmml\" xref=\"S4.SS3.p1.1.m1.1.1.2\">𝒙</ci><cn id=\"S4.SS3.p1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS3.p1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.1.m1.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS3.p1.1.m1.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> on each model checkpoint, and plot the cosine similarities between the flattened activation vectors in Figure <a class=\"ltx_ref\" href=\"#S4.F8\" title=\"Figure 8 ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(d). From the plot we can clearly observe the blocked pattern wherein the similarity between the activations become progressively higher across each epoch of cyclic training. This pattern suggests that every time we train on document <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.2.m2.1\"><semantics id=\"S4.SS3.p1.2.m2.1a\"><msub id=\"S4.SS3.p1.2.m2.1.1\" xref=\"S4.SS3.p1.2.m2.1.1.cmml\"><mi id=\"S4.SS3.p1.2.m2.1.1.2\" xref=\"S4.SS3.p1.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS3.p1.2.m2.1.1.3\" xref=\"S4.SS3.p1.2.m2.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.2.m2.1b\"><apply id=\"S4.SS3.p1.2.m2.1.1.cmml\" xref=\"S4.SS3.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.2.m2.1.1.1.cmml\" xref=\"S4.SS3.p1.2.m2.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.2.m2.1.1.2.cmml\" xref=\"S4.SS3.p1.2.m2.1.1.2\">𝒙</ci><ci id=\"S4.SS3.p1.2.m2.1.1.3.cmml\" xref=\"S4.SS3.p1.2.m2.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.2.m2.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS3.p1.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, the internal representation of <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.3.m3.1\"><semantics id=\"S4.SS3.p1.3.m3.1a\"><msub id=\"S4.SS3.p1.3.m3.1.1\" xref=\"S4.SS3.p1.3.m3.1.1.cmml\"><mi id=\"S4.SS3.p1.3.m3.1.1.2\" xref=\"S4.SS3.p1.3.m3.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS3.p1.3.m3.1.1.3\" xref=\"S4.SS3.p1.3.m3.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.3.m3.1b\"><apply id=\"S4.SS3.p1.3.m3.1.1.cmml\" xref=\"S4.SS3.p1.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS3.p1.3.m3.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS3.p1.3.m3.1.1.2\">𝒙</ci><ci id=\"S4.SS3.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS3.p1.3.m3.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.3.m3.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS3.p1.3.m3.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in the model is more resistant to gradient updates on other documents <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.4.m4.1\"><semantics id=\"S4.SS3.p1.4.m4.1a\"><msub id=\"S4.SS3.p1.4.m4.1.1\" xref=\"S4.SS3.p1.4.m4.1.1.cmml\"><mi id=\"S4.SS3.p1.4.m4.1.1.2\" xref=\"S4.SS3.p1.4.m4.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS3.p1.4.m4.1.1.3\" xref=\"S4.SS3.p1.4.m4.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.4.m4.1b\"><apply id=\"S4.SS3.p1.4.m4.1.1.cmml\" xref=\"S4.SS3.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.4.m4.1.1.1.cmml\" xref=\"S4.SS3.p1.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.4.m4.1.1.2.cmml\" xref=\"S4.SS3.p1.4.m4.1.1.2\">𝒙</ci><ci id=\"S4.SS3.p1.4.m4.1.1.3.cmml\" xref=\"S4.SS3.p1.4.m4.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.4.m4.1c\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS3.p1.4.m4.1d\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Computational Toy Model</h3>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS4.p1.10\">To further understand the essential mechanism that yields anticipatory recovery, we design a minimalist “toy” simulation experiment.\nIn this toy simulation, each task (formerly, document) <math alttext=\"i\\in\\{1,\\cdots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.1.m1.3\"><semantics id=\"S4.SS4.p1.1.m1.3a\"><mrow id=\"S4.SS4.p1.1.m1.3.4\" xref=\"S4.SS4.p1.1.m1.3.4.cmml\"><mi id=\"S4.SS4.p1.1.m1.3.4.2\" xref=\"S4.SS4.p1.1.m1.3.4.2.cmml\">i</mi><mo id=\"S4.SS4.p1.1.m1.3.4.1\" xref=\"S4.SS4.p1.1.m1.3.4.1.cmml\">∈</mo><mrow id=\"S4.SS4.p1.1.m1.3.4.3.2\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\"><mo id=\"S4.SS4.p1.1.m1.3.4.3.2.1\" stretchy=\"false\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">{</mo><mn id=\"S4.SS4.p1.1.m1.1.1\" xref=\"S4.SS4.p1.1.m1.1.1.cmml\">1</mn><mo id=\"S4.SS4.p1.1.m1.3.4.3.2.2\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">,</mo><mi id=\"S4.SS4.p1.1.m1.2.2\" mathvariant=\"normal\" xref=\"S4.SS4.p1.1.m1.2.2.cmml\">⋯</mi><mo id=\"S4.SS4.p1.1.m1.3.4.3.2.3\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">,</mo><mi id=\"S4.SS4.p1.1.m1.3.3\" xref=\"S4.SS4.p1.1.m1.3.3.cmml\">T</mi><mo id=\"S4.SS4.p1.1.m1.3.4.3.2.4\" stretchy=\"false\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.1.m1.3b\"><apply id=\"S4.SS4.p1.1.m1.3.4.cmml\" xref=\"S4.SS4.p1.1.m1.3.4\"><in id=\"S4.SS4.p1.1.m1.3.4.1.cmml\" xref=\"S4.SS4.p1.1.m1.3.4.1\"></in><ci id=\"S4.SS4.p1.1.m1.3.4.2.cmml\" xref=\"S4.SS4.p1.1.m1.3.4.2\">𝑖</ci><set id=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\" xref=\"S4.SS4.p1.1.m1.3.4.3.2\"><cn id=\"S4.SS4.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"S4.SS4.p1.1.m1.1.1\">1</cn><ci id=\"S4.SS4.p1.1.m1.2.2.cmml\" xref=\"S4.SS4.p1.1.m1.2.2\">⋯</ci><ci id=\"S4.SS4.p1.1.m1.3.3.cmml\" xref=\"S4.SS4.p1.1.m1.3.3\">𝑇</ci></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.1.m1.3c\">i\\in\\{1,\\cdots,T\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.1.m1.3d\">italic_i ∈ { 1 , ⋯ , italic_T }</annotation></semantics></math> is described by a single data point, <math alttext=\"\\bm{x}_{1},\\cdots,\\bm{x}_{T}\\in\\mathbb{R}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.2.m2.3\"><semantics id=\"S4.SS4.p1.2.m2.3a\"><mrow id=\"S4.SS4.p1.2.m2.3.3\" xref=\"S4.SS4.p1.2.m2.3.3.cmml\"><mrow id=\"S4.SS4.p1.2.m2.3.3.2.2\" xref=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\"><msub id=\"S4.SS4.p1.2.m2.2.2.1.1.1\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.cmml\"><mi id=\"S4.SS4.p1.2.m2.2.2.1.1.1.2\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.2.cmml\">𝒙</mi><mn id=\"S4.SS4.p1.2.m2.2.2.1.1.1.3\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.3.cmml\">1</mn></msub><mo id=\"S4.SS4.p1.2.m2.3.3.2.2.3\" xref=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\">,</mo><mi id=\"S4.SS4.p1.2.m2.1.1\" mathvariant=\"normal\" xref=\"S4.SS4.p1.2.m2.1.1.cmml\">⋯</mi><mo id=\"S4.SS4.p1.2.m2.3.3.2.2.4\" xref=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\">,</mo><msub id=\"S4.SS4.p1.2.m2.3.3.2.2.2\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.cmml\"><mi id=\"S4.SS4.p1.2.m2.3.3.2.2.2.2\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p1.2.m2.3.3.2.2.2.3\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.3.cmml\">T</mi></msub></mrow><mo id=\"S4.SS4.p1.2.m2.3.3.3\" xref=\"S4.SS4.p1.2.m2.3.3.3.cmml\">∈</mo><msup id=\"S4.SS4.p1.2.m2.3.3.4\" xref=\"S4.SS4.p1.2.m2.3.3.4.cmml\"><mi id=\"S4.SS4.p1.2.m2.3.3.4.2\" xref=\"S4.SS4.p1.2.m2.3.3.4.2.cmml\">ℝ</mi><mi id=\"S4.SS4.p1.2.m2.3.3.4.3\" xref=\"S4.SS4.p1.2.m2.3.3.4.3.cmml\">N</mi></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.2.m2.3b\"><apply id=\"S4.SS4.p1.2.m2.3.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3\"><in id=\"S4.SS4.p1.2.m2.3.3.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.3\"></in><list id=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2\"><apply id=\"S4.SS4.p1.2.m2.2.2.1.1.1.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.2.m2.2.2.1.1.1.1.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.2.m2.2.2.1.1.1.2.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.2\">𝒙</ci><cn id=\"S4.SS4.p1.2.m2.2.2.1.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.3\">1</cn></apply><ci id=\"S4.SS4.p1.2.m2.1.1.cmml\" xref=\"S4.SS4.p1.2.m2.1.1\">⋯</ci><apply id=\"S4.SS4.p1.2.m2.3.3.2.2.2.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.2.m2.3.3.2.2.2.1.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2\">subscript</csymbol><ci id=\"S4.SS4.p1.2.m2.3.3.2.2.2.2.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.2\">𝒙</ci><ci id=\"S4.SS4.p1.2.m2.3.3.2.2.2.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.3\">𝑇</ci></apply></list><apply id=\"S4.SS4.p1.2.m2.3.3.4.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.2.m2.3.3.4.1.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4\">superscript</csymbol><ci id=\"S4.SS4.p1.2.m2.3.3.4.2.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4.2\">ℝ</ci><ci id=\"S4.SS4.p1.2.m2.3.3.4.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4.3\">𝑁</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.2.m2.3c\">\\bm{x}_{1},\\cdots,\\bm{x}_{T}\\in\\mathbb{R}^{N}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.2.m2.3d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. We assume a learnable linear embedding <math alttext=\"\\bm{P}\\in\\mathbb{R}^{M\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.3.m3.1\"><semantics id=\"S4.SS4.p1.3.m3.1a\"><mrow id=\"S4.SS4.p1.3.m3.1.1\" xref=\"S4.SS4.p1.3.m3.1.1.cmml\"><mi id=\"S4.SS4.p1.3.m3.1.1.2\" xref=\"S4.SS4.p1.3.m3.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p1.3.m3.1.1.1\" xref=\"S4.SS4.p1.3.m3.1.1.1.cmml\">∈</mo><msup id=\"S4.SS4.p1.3.m3.1.1.3\" xref=\"S4.SS4.p1.3.m3.1.1.3.cmml\"><mi id=\"S4.SS4.p1.3.m3.1.1.3.2\" xref=\"S4.SS4.p1.3.m3.1.1.3.2.cmml\">ℝ</mi><mrow id=\"S4.SS4.p1.3.m3.1.1.3.3\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.cmml\"><mi id=\"S4.SS4.p1.3.m3.1.1.3.3.2\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.2.cmml\">M</mi><mo id=\"S4.SS4.p1.3.m3.1.1.3.3.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.1.cmml\">×</mo><mi id=\"S4.SS4.p1.3.m3.1.1.3.3.3\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.3.cmml\">N</mi></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.3.m3.1b\"><apply id=\"S4.SS4.p1.3.m3.1.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1\"><in id=\"S4.SS4.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.1\"></in><ci id=\"S4.SS4.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.3.m3.1.1.3.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3\">superscript</csymbol><ci id=\"S4.SS4.p1.3.m3.1.1.3.2.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.2\">ℝ</ci><apply id=\"S4.SS4.p1.3.m3.1.1.3.3.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3\"><times id=\"S4.SS4.p1.3.m3.1.1.3.3.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.1\"></times><ci id=\"S4.SS4.p1.3.m3.1.1.3.3.2.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.2\">𝑀</ci><ci id=\"S4.SS4.p1.3.m3.1.1.3.3.3.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.3\">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.3.m3.1c\">\\bm{P}\\in\\mathbb{R}^{M\\times N}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.3.m3.1d\">bold_italic_P ∈ blackboard_R start_POSTSUPERSCRIPT italic_M × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> that projects each <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.4.m4.1\"><semantics id=\"S4.SS4.p1.4.m4.1a\"><msub id=\"S4.SS4.p1.4.m4.1.1\" xref=\"S4.SS4.p1.4.m4.1.1.cmml\"><mi id=\"S4.SS4.p1.4.m4.1.1.2\" xref=\"S4.SS4.p1.4.m4.1.1.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p1.4.m4.1.1.3\" xref=\"S4.SS4.p1.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.4.m4.1b\"><apply id=\"S4.SS4.p1.4.m4.1.1.cmml\" xref=\"S4.SS4.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.4.m4.1.1.1.cmml\" xref=\"S4.SS4.p1.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.4.m4.1.1.2.cmml\" xref=\"S4.SS4.p1.4.m4.1.1.2\">𝒙</ci><ci id=\"S4.SS4.p1.4.m4.1.1.3.cmml\" xref=\"S4.SS4.p1.4.m4.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.4.m4.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.4.m4.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> into an <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.5.m5.1\"><semantics id=\"S4.SS4.p1.5.m5.1a\"><mi id=\"S4.SS4.p1.5.m5.1.1\" xref=\"S4.SS4.p1.5.m5.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.5.m5.1b\"><ci id=\"S4.SS4.p1.5.m5.1.1.cmml\" xref=\"S4.SS4.p1.5.m5.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.5.m5.1c\">M</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.5.m5.1d\">italic_M</annotation></semantics></math>-dimensional embedding space. We also assume a learnable vector <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.6.m6.1\"><semantics id=\"S4.SS4.p1.6.m6.1a\"><mi id=\"S4.SS4.p1.6.m6.1.1\" xref=\"S4.SS4.p1.6.m6.1.1.cmml\">w</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.6.m6.1b\"><ci id=\"S4.SS4.p1.6.m6.1.1.cmml\" xref=\"S4.SS4.p1.6.m6.1.1\">𝑤</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.6.m6.1c\">w</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.6.m6.1d\">italic_w</annotation></semantics></math> and task-specific mappings <math alttext=\"f_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.7.m7.1\"><semantics id=\"S4.SS4.p1.7.m7.1a\"><msub id=\"S4.SS4.p1.7.m7.1.1\" xref=\"S4.SS4.p1.7.m7.1.1.cmml\"><mi id=\"S4.SS4.p1.7.m7.1.1.2\" xref=\"S4.SS4.p1.7.m7.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p1.7.m7.1.1.3\" xref=\"S4.SS4.p1.7.m7.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.7.m7.1b\"><apply id=\"S4.SS4.p1.7.m7.1.1.cmml\" xref=\"S4.SS4.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.7.m7.1.1.1.cmml\" xref=\"S4.SS4.p1.7.m7.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.7.m7.1.1.2.cmml\" xref=\"S4.SS4.p1.7.m7.1.1.2\">𝑓</ci><ci id=\"S4.SS4.p1.7.m7.1.1.3.cmml\" xref=\"S4.SS4.p1.7.m7.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.7.m7.1c\">f_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.7.m7.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext=\"f_{i}(w)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.8.m8.1\"><semantics id=\"S4.SS4.p1.8.m8.1a\"><mrow id=\"S4.SS4.p1.8.m8.1.2\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\"><msub id=\"S4.SS4.p1.8.m8.1.2.2\" xref=\"S4.SS4.p1.8.m8.1.2.2.cmml\"><mi id=\"S4.SS4.p1.8.m8.1.2.2.2\" xref=\"S4.SS4.p1.8.m8.1.2.2.2.cmml\">f</mi><mi id=\"S4.SS4.p1.8.m8.1.2.2.3\" xref=\"S4.SS4.p1.8.m8.1.2.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p1.8.m8.1.2.1\" xref=\"S4.SS4.p1.8.m8.1.2.1.cmml\">⁢</mo><mrow id=\"S4.SS4.p1.8.m8.1.2.3.2\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\"><mo id=\"S4.SS4.p1.8.m8.1.2.3.2.1\" stretchy=\"false\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\">(</mo><mi id=\"S4.SS4.p1.8.m8.1.1\" xref=\"S4.SS4.p1.8.m8.1.1.cmml\">w</mi><mo id=\"S4.SS4.p1.8.m8.1.2.3.2.2\" stretchy=\"false\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.8.m8.1b\"><apply id=\"S4.SS4.p1.8.m8.1.2.cmml\" xref=\"S4.SS4.p1.8.m8.1.2\"><times id=\"S4.SS4.p1.8.m8.1.2.1.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.1\"></times><apply id=\"S4.SS4.p1.8.m8.1.2.2.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.8.m8.1.2.2.1.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2\">subscript</csymbol><ci id=\"S4.SS4.p1.8.m8.1.2.2.2.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2.2\">𝑓</ci><ci id=\"S4.SS4.p1.8.m8.1.2.2.3.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2.3\">𝑖</ci></apply><ci id=\"S4.SS4.p1.8.m8.1.1.cmml\" xref=\"S4.SS4.p1.8.m8.1.1\">𝑤</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.8.m8.1c\">f_{i}(w)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.8.m8.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_w )</annotation></semantics></math> is the target for task <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.9.m9.1\"><semantics id=\"S4.SS4.p1.9.m9.1a\"><mi id=\"S4.SS4.p1.9.m9.1.1\" xref=\"S4.SS4.p1.9.m9.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.9.m9.1b\"><ci id=\"S4.SS4.p1.9.m9.1.1.cmml\" xref=\"S4.SS4.p1.9.m9.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.9.m9.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.9.m9.1d\">italic_i</annotation></semantics></math> in the same embedding space. We require each <math alttext=\"f_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.10.m10.1\"><semantics id=\"S4.SS4.p1.10.m10.1a\"><msub id=\"S4.SS4.p1.10.m10.1.1\" xref=\"S4.SS4.p1.10.m10.1.1.cmml\"><mi id=\"S4.SS4.p1.10.m10.1.1.2\" xref=\"S4.SS4.p1.10.m10.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p1.10.m10.1.1.3\" xref=\"S4.SS4.p1.10.m10.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.10.m10.1b\"><apply id=\"S4.SS4.p1.10.m10.1.1.cmml\" xref=\"S4.SS4.p1.10.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.10.m10.1.1.1.cmml\" xref=\"S4.SS4.p1.10.m10.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.10.m10.1.1.2.cmml\" xref=\"S4.SS4.p1.10.m10.1.1.2\">𝑓</ci><ci id=\"S4.SS4.p1.10.m10.1.1.3.cmml\" xref=\"S4.SS4.p1.10.m10.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.10.m10.1c\">f_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p1.10.m10.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to be invertible as a simplifying assumption.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p2\">\n<p class=\"ltx_p\" id=\"S4.SS4.p2.5\">We define the loss for task <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.1.m1.1\"><semantics id=\"S4.SS4.p2.1.m1.1a\"><mi id=\"S4.SS4.p2.1.m1.1.1\" xref=\"S4.SS4.p2.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.1.m1.1b\"><ci id=\"S4.SS4.p2.1.m1.1.1.cmml\" xref=\"S4.SS4.p2.1.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.1.m1.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.1.m1.1d\">italic_i</annotation></semantics></math> as <math alttext=\"\\ell_{i}(\\bm{P},\\bm{w})=\\frac{1}{2}\\lVert\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w})\\rVert^%\n{2}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.2.m2.4\"><semantics id=\"S4.SS4.p2.2.m2.4a\"><mrow id=\"S4.SS4.p2.2.m2.4.4\" xref=\"S4.SS4.p2.2.m2.4.4.cmml\"><mrow id=\"S4.SS4.p2.2.m2.4.4.3\" xref=\"S4.SS4.p2.2.m2.4.4.3.cmml\"><msub id=\"S4.SS4.p2.2.m2.4.4.3.2\" xref=\"S4.SS4.p2.2.m2.4.4.3.2.cmml\"><mi id=\"S4.SS4.p2.2.m2.4.4.3.2.2\" mathvariant=\"normal\" xref=\"S4.SS4.p2.2.m2.4.4.3.2.2.cmml\">ℓ</mi><mi id=\"S4.SS4.p2.2.m2.4.4.3.2.3\" xref=\"S4.SS4.p2.2.m2.4.4.3.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p2.2.m2.4.4.3.1\" xref=\"S4.SS4.p2.2.m2.4.4.3.1.cmml\">⁢</mo><mrow id=\"S4.SS4.p2.2.m2.4.4.3.3.2\" xref=\"S4.SS4.p2.2.m2.4.4.3.3.1.cmml\"><mo id=\"S4.SS4.p2.2.m2.4.4.3.3.2.1\" stretchy=\"false\" xref=\"S4.SS4.p2.2.m2.4.4.3.3.1.cmml\">(</mo><mi id=\"S4.SS4.p2.2.m2.1.1\" xref=\"S4.SS4.p2.2.m2.1.1.cmml\">𝑷</mi><mo id=\"S4.SS4.p2.2.m2.4.4.3.3.2.2\" xref=\"S4.SS4.p2.2.m2.4.4.3.3.1.cmml\">,</mo><mi id=\"S4.SS4.p2.2.m2.2.2\" xref=\"S4.SS4.p2.2.m2.2.2.cmml\">𝒘</mi><mo id=\"S4.SS4.p2.2.m2.4.4.3.3.2.3\" stretchy=\"false\" xref=\"S4.SS4.p2.2.m2.4.4.3.3.1.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p2.2.m2.4.4.2\" xref=\"S4.SS4.p2.2.m2.4.4.2.cmml\">=</mo><mrow id=\"S4.SS4.p2.2.m2.4.4.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.cmml\"><mfrac id=\"S4.SS4.p2.2.m2.4.4.1.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.3.cmml\"><mn id=\"S4.SS4.p2.2.m2.4.4.1.3.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.3.2.cmml\">1</mn><mn id=\"S4.SS4.p2.2.m2.4.4.1.3.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.3.3.cmml\">2</mn></mfrac><mo id=\"S4.SS4.p2.2.m2.4.4.1.2\" lspace=\"0em\" xref=\"S4.SS4.p2.2.m2.4.4.1.2.cmml\">⁢</mo><msubsup id=\"S4.SS4.p2.2.m2.4.4.1.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.cmml\"><mrow id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.2.cmml\"><mo fence=\"true\" id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.2\" rspace=\"0em\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.2.1.cmml\">∥</mo><mrow id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.cmml\"><mrow id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.cmml\"><mi id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.1.cmml\">⁢</mo><msub id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.cmml\"><mi id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.1.cmml\">−</mo><mrow id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.cmml\"><msub id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.cmml\"><mi id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.1\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.1.cmml\">⁢</mo><mrow id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.cmml\"><mo id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.3.2.1\" stretchy=\"false\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.cmml\">(</mo><mi id=\"S4.SS4.p2.2.m2.3.3\" xref=\"S4.SS4.p2.2.m2.3.3.cmml\">𝒘</mi><mo id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.3.2.2\" stretchy=\"false\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo fence=\"true\" id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.3\" lspace=\"0em\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.2.1.cmml\">∥</mo></mrow><mn id=\"S4.SS4.p2.2.m2.4.4.1.1.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.3.cmml\">2</mn><mn id=\"S4.SS4.p2.2.m2.4.4.1.1.1.3\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.3.cmml\">2</mn></msubsup></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.2.m2.4b\"><apply id=\"S4.SS4.p2.2.m2.4.4.cmml\" xref=\"S4.SS4.p2.2.m2.4.4\"><eq id=\"S4.SS4.p2.2.m2.4.4.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.2\"></eq><apply id=\"S4.SS4.p2.2.m2.4.4.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3\"><times id=\"S4.SS4.p2.2.m2.4.4.3.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3.1\"></times><apply id=\"S4.SS4.p2.2.m2.4.4.3.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p2.2.m2.4.4.3.2.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3.2\">subscript</csymbol><ci id=\"S4.SS4.p2.2.m2.4.4.3.2.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3.2.2\">ℓ</ci><ci id=\"S4.SS4.p2.2.m2.4.4.3.2.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3.2.3\">𝑖</ci></apply><interval closure=\"open\" id=\"S4.SS4.p2.2.m2.4.4.3.3.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.3.3.2\"><ci id=\"S4.SS4.p2.2.m2.1.1.cmml\" xref=\"S4.SS4.p2.2.m2.1.1\">𝑷</ci><ci id=\"S4.SS4.p2.2.m2.2.2.cmml\" xref=\"S4.SS4.p2.2.m2.2.2\">𝒘</ci></interval></apply><apply id=\"S4.SS4.p2.2.m2.4.4.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1\"><times id=\"S4.SS4.p2.2.m2.4.4.1.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.2\"></times><apply id=\"S4.SS4.p2.2.m2.4.4.1.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.3\"><divide id=\"S4.SS4.p2.2.m2.4.4.1.3.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.3\"></divide><cn id=\"S4.SS4.p2.2.m2.4.4.1.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p2.2.m2.4.4.1.3.2\">1</cn><cn id=\"S4.SS4.p2.2.m2.4.4.1.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p2.2.m2.4.4.1.3.3\">2</cn></apply><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p2.2.m2.4.4.1.1.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1\">subscript</csymbol><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p2.2.m2.4.4.1.1.1.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1\">superscript</csymbol><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.2.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.2\">delimited-∥∥</csymbol><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1\"><minus id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.1\"></minus><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2\"><times id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.1\"></times><ci id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.2\">𝑷</ci><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3\">subscript</csymbol><ci id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.2\">𝒙</ci><ci id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.2.3.3\">𝑖</ci></apply></apply><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3\"><times id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.1\"></times><apply id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2\">subscript</csymbol><ci id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.2\">𝑓</ci><ci id=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.1.1.1.3.2.3\">𝑖</ci></apply><ci id=\"S4.SS4.p2.2.m2.3.3.cmml\" xref=\"S4.SS4.p2.2.m2.3.3\">𝒘</ci></apply></apply></apply><cn id=\"S4.SS4.p2.2.m2.4.4.1.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.1.3\">2</cn></apply><cn id=\"S4.SS4.p2.2.m2.4.4.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS4.p2.2.m2.4.4.1.1.3\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.2.m2.4c\">\\ell_{i}(\\bm{P},\\bm{w})=\\frac{1}{2}\\lVert\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w})\\rVert^%\n{2}_{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.2.m2.4d\">roman_ℓ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_P , bold_italic_w ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>.\nJust as when training a deep net, we assume here that representation learning occurs slowly, and that one training step for task <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.3.m3.1\"><semantics id=\"S4.SS4.p2.3.m3.1a\"><mi id=\"S4.SS4.p2.3.m3.1.1\" xref=\"S4.SS4.p2.3.m3.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.3.m3.1b\"><ci id=\"S4.SS4.p2.3.m3.1.1.cmml\" xref=\"S4.SS4.p2.3.m3.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.3.m3.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.3.m3.1d\">italic_i</annotation></semantics></math> involves a single gradient update of <math alttext=\"\\bm{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.4.m4.1\"><semantics id=\"S4.SS4.p2.4.m4.1a\"><mi id=\"S4.SS4.p2.4.m4.1.1\" xref=\"S4.SS4.p2.4.m4.1.1.cmml\">𝑷</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.4.m4.1b\"><ci id=\"S4.SS4.p2.4.m4.1.1.cmml\" xref=\"S4.SS4.p2.4.m4.1.1\">𝑷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.4.m4.1c\">\\bm{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.4.m4.1d\">bold_italic_P</annotation></semantics></math> with step size <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.5.m5.1\"><semantics id=\"S4.SS4.p2.5.m5.1a\"><mi id=\"S4.SS4.p2.5.m5.1.1\" xref=\"S4.SS4.p2.5.m5.1.1.cmml\">α</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.5.m5.1b\"><ci id=\"S4.SS4.p2.5.m5.1.1.cmml\" xref=\"S4.SS4.p2.5.m5.1.1\">𝛼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.5.m5.1c\">\\alpha</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.5.m5.1d\">italic_α</annotation></semantics></math>:</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Ax1.EGx1\">\n<tbody id=\"S4.E1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\bm{P}\\leftarrow\\bm{P}-\\alpha(\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w}))\\bm{%\nx}_{i}^{\\top}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E1.m1.2\"><semantics id=\"S4.E1.m1.2a\"><mrow id=\"S4.E1.m1.2.2.1\" xref=\"S4.E1.m1.2.2.1.1.cmml\"><mrow id=\"S4.E1.m1.2.2.1.1\" xref=\"S4.E1.m1.2.2.1.1.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.3\" xref=\"S4.E1.m1.2.2.1.1.3.cmml\">𝑷</mi><mo id=\"S4.E1.m1.2.2.1.1.2\" stretchy=\"false\" xref=\"S4.E1.m1.2.2.1.1.2.cmml\">←</mo><mrow id=\"S4.E1.m1.2.2.1.1.1\" xref=\"S4.E1.m1.2.2.1.1.1.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.1.3\" xref=\"S4.E1.m1.2.2.1.1.1.3.cmml\">𝑷</mi><mo id=\"S4.E1.m1.2.2.1.1.1.2\" xref=\"S4.E1.m1.2.2.1.1.1.2.cmml\">−</mo><mrow id=\"S4.E1.m1.2.2.1.1.1.1\" xref=\"S4.E1.m1.2.2.1.1.1.1.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.1.1.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.3.cmml\">α</mi><mo id=\"S4.E1.m1.2.2.1.1.1.1.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S4.E1.m1.2.2.1.1.1.1.1.1\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml\"><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml\"><mrow id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml\">𝑷</mi><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.1\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml\">⁢</mo><msub id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.2.cmml\">𝒙</mi><mi id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.1\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml\">−</mo><mrow id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml\"><msub id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml\">i</mi></msub><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.1\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml\">⁢</mo><mrow id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml\"><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.1\" stretchy=\"false\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml\">(</mo><mi id=\"S4.E1.m1.1.1\" xref=\"S4.E1.m1.1.1.cmml\">𝒘</mi><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.2\" stretchy=\"false\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S4.E1.m1.2.2.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml\">)</mo></mrow><mo id=\"S4.E1.m1.2.2.1.1.1.1.2a\" xref=\"S4.E1.m1.2.2.1.1.1.1.2.cmml\">⁢</mo><msubsup id=\"S4.E1.m1.2.2.1.1.1.1.4\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.cmml\"><mi id=\"S4.E1.m1.2.2.1.1.1.1.4.2.2\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.2.2.cmml\">𝒙</mi><mi id=\"S4.E1.m1.2.2.1.1.1.1.4.2.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.2.3.cmml\">i</mi><mo id=\"S4.E1.m1.2.2.1.1.1.1.4.3\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.3.cmml\">⊤</mo></msubsup></mrow></mrow></mrow><mo id=\"S4.E1.m1.2.2.1.2\" lspace=\"0em\" xref=\"S4.E1.m1.2.2.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E1.m1.2b\"><apply id=\"S4.E1.m1.2.2.1.1.cmml\" xref=\"S4.E1.m1.2.2.1\"><ci id=\"S4.E1.m1.2.2.1.1.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.2\">←</ci><ci id=\"S4.E1.m1.2.2.1.1.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.3\">𝑷</ci><apply id=\"S4.E1.m1.2.2.1.1.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1\"><minus id=\"S4.E1.m1.2.2.1.1.1.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.2\"></minus><ci id=\"S4.E1.m1.2.2.1.1.1.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.3\">𝑷</ci><apply id=\"S4.E1.m1.2.2.1.1.1.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1\"><times id=\"S4.E1.m1.2.2.1.1.1.1.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.2\"></times><ci id=\"S4.E1.m1.2.2.1.1.1.1.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.3\">𝛼</ci><apply id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1\"><minus id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.1\"></minus><apply id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2\"><times id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.1\"></times><ci id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.2\">𝑷</ci><apply id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3\">subscript</csymbol><ci id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.2\">𝒙</ci><ci id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.2.3.3\">𝑖</ci></apply></apply><apply id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3\"><times id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.1\"></times><apply id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2\">subscript</csymbol><ci id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2\">𝑓</ci><ci id=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3\">𝑖</ci></apply><ci id=\"S4.E1.m1.1.1.cmml\" xref=\"S4.E1.m1.1.1\">𝒘</ci></apply></apply><apply id=\"S4.E1.m1.2.2.1.1.1.1.4.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S4.E1.m1.2.2.1.1.1.1.4.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4\">superscript</csymbol><apply id=\"S4.E1.m1.2.2.1.1.1.1.4.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S4.E1.m1.2.2.1.1.1.1.4.2.1.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4\">subscript</csymbol><ci id=\"S4.E1.m1.2.2.1.1.1.1.4.2.2.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.2.2\">𝒙</ci><ci id=\"S4.E1.m1.2.2.1.1.1.1.4.2.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.2.3\">𝑖</ci></apply><csymbol cd=\"latexml\" id=\"S4.E1.m1.2.2.1.1.1.1.4.3.cmml\" xref=\"S4.E1.m1.2.2.1.1.1.1.4.3\">top</csymbol></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E1.m1.2c\">\\displaystyle\\bm{P}\\leftarrow\\bm{P}-\\alpha(\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w}))\\bm{%\nx}_{i}^{\\top}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.E1.m1.2d\">bold_italic_P ← bold_italic_P - italic_α ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) ) bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT .</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S4.SS4.p2.8\">In contrast, at each training step, <math alttext=\"\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.6.m1.1\"><semantics id=\"S4.SS4.p2.6.m1.1a\"><mi id=\"S4.SS4.p2.6.m1.1.1\" xref=\"S4.SS4.p2.6.m1.1.1.cmml\">𝒘</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.6.m1.1b\"><ci id=\"S4.SS4.p2.6.m1.1.1.cmml\" xref=\"S4.SS4.p2.6.m1.1.1\">𝒘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.6.m1.1c\">\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.6.m1.1d\">bold_italic_w</annotation></semantics></math>, analogous to the fast-adapting weights in a neural network, can be rapidly tuned to solve for task <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.7.m2.1\"><semantics id=\"S4.SS4.p2.7.m2.1a\"><mi id=\"S4.SS4.p2.7.m2.1.1\" xref=\"S4.SS4.p2.7.m2.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.7.m2.1b\"><ci id=\"S4.SS4.p2.7.m2.1.1.cmml\" xref=\"S4.SS4.p2.7.m2.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.7.m2.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.7.m2.1d\">italic_i</annotation></semantics></math>, yielding the loss minimizer conditional on <math alttext=\"\\bm{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.8.m3.1\"><semantics id=\"S4.SS4.p2.8.m3.1a\"><mi id=\"S4.SS4.p2.8.m3.1.1\" xref=\"S4.SS4.p2.8.m3.1.1.cmml\">𝑷</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.8.m3.1b\"><ci id=\"S4.SS4.p2.8.m3.1.1.cmml\" xref=\"S4.SS4.p2.8.m3.1.1\">𝑷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.8.m3.1c\">\\bm{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p2.8.m3.1d\">bold_italic_P</annotation></semantics></math>:</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"Ax1.EGx2\">\n<tbody id=\"S4.E2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\bm{w}\\leftarrow f_{i}^{-1}(\\bm{P}\\bm{x}_{i}).\" class=\"ltx_Math\" display=\"inline\" id=\"S4.E2.m1.1\"><semantics id=\"S4.E2.m1.1a\"><mrow id=\"S4.E2.m1.1.1.1\" xref=\"S4.E2.m1.1.1.1.1.cmml\"><mrow id=\"S4.E2.m1.1.1.1.1\" xref=\"S4.E2.m1.1.1.1.1.cmml\"><mi id=\"S4.E2.m1.1.1.1.1.3\" xref=\"S4.E2.m1.1.1.1.1.3.cmml\">𝒘</mi><mo id=\"S4.E2.m1.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.E2.m1.1.1.1.1.2.cmml\">←</mo><mrow id=\"S4.E2.m1.1.1.1.1.1\" xref=\"S4.E2.m1.1.1.1.1.1.cmml\"><msubsup id=\"S4.E2.m1.1.1.1.1.1.3\" xref=\"S4.E2.m1.1.1.1.1.1.3.cmml\"><mi id=\"S4.E2.m1.1.1.1.1.1.3.2.2\" xref=\"S4.E2.m1.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.E2.m1.1.1.1.1.1.3.2.3\" xref=\"S4.E2.m1.1.1.1.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.E2.m1.1.1.1.1.1.3.3\" xref=\"S4.E2.m1.1.1.1.1.1.3.3.cmml\"><mo id=\"S4.E2.m1.1.1.1.1.1.3.3a\" xref=\"S4.E2.m1.1.1.1.1.1.3.3.cmml\">−</mo><mn id=\"S4.E2.m1.1.1.1.1.1.3.3.2\" xref=\"S4.E2.m1.1.1.1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.E2.m1.1.1.1.1.1.2\" xref=\"S4.E2.m1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S4.E2.m1.1.1.1.1.1.1.1\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S4.E2.m1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.E2.m1.1.1.1.1.1.1.1.1\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S4.E2.m1.1.1.1.1.1.1.1.1.2\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.E2.m1.1.1.1.1.1.1.1.1.1\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.2\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.3\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.E2.m1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S4.E2.m1.1.1.1.2\" lspace=\"0em\" xref=\"S4.E2.m1.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E2.m1.1b\"><apply id=\"S4.E2.m1.1.1.1.1.cmml\" xref=\"S4.E2.m1.1.1.1\"><ci id=\"S4.E2.m1.1.1.1.1.2.cmml\" xref=\"S4.E2.m1.1.1.1.1.2\">←</ci><ci id=\"S4.E2.m1.1.1.1.1.3.cmml\" xref=\"S4.E2.m1.1.1.1.1.3\">𝒘</ci><apply id=\"S4.E2.m1.1.1.1.1.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1\"><times id=\"S4.E2.m1.1.1.1.1.1.2.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.2\"></times><apply id=\"S4.E2.m1.1.1.1.1.1.3.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E2.m1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3\">superscript</csymbol><apply id=\"S4.E2.m1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E2.m1.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.E2.m1.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3.2.2\">𝑓</ci><ci id=\"S4.E2.m1.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3.2.3\">𝑖</ci></apply><apply id=\"S4.E2.m1.1.1.1.1.1.3.3.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3.3\"><minus id=\"S4.E2.m1.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.3.3\"></minus><cn id=\"S4.E2.m1.1.1.1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.E2.m1.1.1.1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.E2.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1\"><times id=\"S4.E2.m1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.1\"></times><ci id=\"S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.2\">𝒙</ci><ci id=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S4.E2.m1.1.1.1.1.1.1.1.1.3.3\">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E2.m1.1c\">\\displaystyle\\bm{w}\\leftarrow f_{i}^{-1}(\\bm{P}\\bm{x}_{i}).</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.E2.m1.1d\">bold_italic_w ← italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p3\">\n<p class=\"ltx_p\" id=\"S4.SS4.p3.14\">As in our main experiments, we sequentially optimize each <math alttext=\"\\ell_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.1.m1.1\"><semantics id=\"S4.SS4.p3.1.m1.1a\"><msub id=\"S4.SS4.p3.1.m1.1.1\" xref=\"S4.SS4.p3.1.m1.1.1.cmml\"><mi id=\"S4.SS4.p3.1.m1.1.1.2\" mathvariant=\"normal\" xref=\"S4.SS4.p3.1.m1.1.1.2.cmml\">ℓ</mi><mi id=\"S4.SS4.p3.1.m1.1.1.3\" xref=\"S4.SS4.p3.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.1.m1.1b\"><apply id=\"S4.SS4.p3.1.m1.1.1.cmml\" xref=\"S4.SS4.p3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.1.m1.1.1.1.cmml\" xref=\"S4.SS4.p3.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS4.p3.1.m1.1.1.2.cmml\" xref=\"S4.SS4.p3.1.m1.1.1.2\">ℓ</ci><ci id=\"S4.SS4.p3.1.m1.1.1.3.cmml\" xref=\"S4.SS4.p3.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.1.m1.1c\">\\ell_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.1.m1.1d\">roman_ℓ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as we iterate through the sequence of tasks.\nIn each training step, we first update <math alttext=\"\\bm{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.2.m2.1\"><semantics id=\"S4.SS4.p3.2.m2.1a\"><mi id=\"S4.SS4.p3.2.m2.1.1\" xref=\"S4.SS4.p3.2.m2.1.1.cmml\">𝑷</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.2.m2.1b\"><ci id=\"S4.SS4.p3.2.m2.1.1.cmml\" xref=\"S4.SS4.p3.2.m2.1.1\">𝑷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.2.m2.1c\">\\bm{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.2.m2.1d\">bold_italic_P</annotation></semantics></math> and then solve for <math alttext=\"\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.3.m3.1\"><semantics id=\"S4.SS4.p3.3.m3.1a\"><mi id=\"S4.SS4.p3.3.m3.1.1\" xref=\"S4.SS4.p3.3.m3.1.1.cmml\">𝒘</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.3.m3.1b\"><ci id=\"S4.SS4.p3.3.m3.1.1.cmml\" xref=\"S4.SS4.p3.3.m3.1.1\">𝒘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.3.m3.1c\">\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.3.m3.1d\">bold_italic_w</annotation></semantics></math> given the update to <math alttext=\"\\bm{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.4.m4.1\"><semantics id=\"S4.SS4.p3.4.m4.1a\"><mi id=\"S4.SS4.p3.4.m4.1.1\" xref=\"S4.SS4.p3.4.m4.1.1.cmml\">𝑷</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.4.m4.1b\"><ci id=\"S4.SS4.p3.4.m4.1.1.cmml\" xref=\"S4.SS4.p3.4.m4.1.1\">𝑷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.4.m4.1c\">\\bm{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.4.m4.1d\">bold_italic_P</annotation></semantics></math>.\nUpdating <math alttext=\"\\bm{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.5.m5.1\"><semantics id=\"S4.SS4.p3.5.m5.1a\"><mi id=\"S4.SS4.p3.5.m5.1.1\" xref=\"S4.SS4.p3.5.m5.1.1.cmml\">𝑷</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.5.m5.1b\"><ci id=\"S4.SS4.p3.5.m5.1.1.cmml\" xref=\"S4.SS4.p3.5.m5.1.1\">𝑷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.5.m5.1c\">\\bm{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.5.m5.1d\">bold_italic_P</annotation></semantics></math> approximately reduces the distance between\n<math alttext=\"\\bm{P}\\bm{x}_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.6.m6.1\"><semantics id=\"S4.SS4.p3.6.m6.1a\"><mrow id=\"S4.SS4.p3.6.m6.1.1\" xref=\"S4.SS4.p3.6.m6.1.1.cmml\"><mi id=\"S4.SS4.p3.6.m6.1.1.2\" xref=\"S4.SS4.p3.6.m6.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p3.6.m6.1.1.1\" xref=\"S4.SS4.p3.6.m6.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p3.6.m6.1.1.3\" xref=\"S4.SS4.p3.6.m6.1.1.3.cmml\"><mi id=\"S4.SS4.p3.6.m6.1.1.3.2\" xref=\"S4.SS4.p3.6.m6.1.1.3.2.cmml\">𝒙</mi><mrow id=\"S4.SS4.p3.6.m6.1.1.3.3\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.6.m6.1.1.3.3.2\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.6.m6.1.1.3.3.1\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.6.m6.1.1.3.3.3\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.6.m6.1b\"><apply id=\"S4.SS4.p3.6.m6.1.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1\"><times id=\"S4.SS4.p3.6.m6.1.1.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.1\"></times><ci id=\"S4.SS4.p3.6.m6.1.1.2.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p3.6.m6.1.1.3.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.6.m6.1.1.3.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.6.m6.1.1.3.2.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.2\">𝒙</ci><apply id=\"S4.SS4.p3.6.m6.1.1.3.3.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3\"><plus id=\"S4.SS4.p3.6.m6.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.6.m6.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.2\">𝑖</ci><cn id=\"S4.SS4.p3.6.m6.1.1.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.3\">1</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.6.m6.1c\">\\bm{P}\\bm{x}_{i+1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.6.m6.1d\">bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"f_{i+1}(f_{i}^{-1}(\\bm{P}\\bm{x}_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.7.m7.1\"><semantics id=\"S4.SS4.p3.7.m7.1a\"><mrow id=\"S4.SS4.p3.7.m7.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.cmml\"><msub id=\"S4.SS4.p3.7.m7.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.3.2.cmml\">f</mi><mrow id=\"S4.SS4.p3.7.m7.1.1.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.3.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.7.m7.1.1.3.3.1\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.7.m7.1.1.3.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.3.cmml\">1</mn></mrow></msub><mo id=\"S4.SS4.p3.7.m7.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\"><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\"><msubsup id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3a\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.cmml\">−</mo><mn id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.7.m7.1b\"><apply id=\"S4.SS4.p3.7.m7.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1\"><times id=\"S4.SS4.p3.7.m7.1.1.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.2\"></times><apply id=\"S4.SS4.p3.7.m7.1.1.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.7.m7.1.1.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.2\">𝑓</ci><apply id=\"S4.SS4.p3.7.m7.1.1.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3\"><plus id=\"S4.SS4.p3.7.m7.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.7.m7.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.2\">𝑖</ci><cn id=\"S4.SS4.p3.7.m7.1.1.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.3\">1</cn></apply></apply><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1\"><times id=\"S4.SS4.p3.7.m7.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.2\"></times><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2\">𝑓</ci><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3\">𝑖</ci></apply><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3\"><minus id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3\"></minus><cn id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1\"><times id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2\">𝒙</ci><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3\">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.7.m7.1c\">f_{i+1}(f_{i}^{-1}(\\bm{P}\\bm{x}_{i}))</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.7.m7.1d\">italic_f start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )</annotation></semantics></math>, which entails reducing the upper bound of <math alttext=\"||f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})-f_{i}^{-1}(\\bm{P}\\bm{x}_{i})||\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.8.m8.1\"><semantics id=\"S4.SS4.p3.8.m8.1a\"><mrow id=\"S4.SS4.p3.8.m8.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.2.cmml\"><mo id=\"S4.SS4.p3.8.m8.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.8.m8.1.1.2.1.cmml\">‖</mo><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.cmml\"><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.cmml\"><msubsup id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.2.cmml\">f</mi><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.3.cmml\">1</mn></mrow><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3a\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.cmml\">−</mo><mn id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.2.cmml\">𝒙</mi><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.3.cmml\">−</mo><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.cmml\"><msubsup id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.cmml\"><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3a\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.cmml\">−</mo><mn id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.2.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.cmml\"><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.2\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.3\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S4.SS4.p3.8.m8.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.8.m8.1.1.2.1.cmml\">‖</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.8.m8.1b\"><apply id=\"S4.SS4.p3.8.m8.1.1.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1\"><csymbol cd=\"latexml\" id=\"S4.SS4.p3.8.m8.1.1.2.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.2\">norm</csymbol><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1\"><minus id=\"S4.SS4.p3.8.m8.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.3\"></minus><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1\"><times id=\"S4.SS4.p3.8.m8.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.2\"></times><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.2\">𝑓</ci><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3\"><plus id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.1\"></plus><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.2\">𝑖</ci><cn id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.2.3.3\">1</cn></apply></apply><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3\"><minus id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3\"></minus><cn id=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1\"><times id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.2\">𝒙</ci><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3\"><plus id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.2\">𝑖</ci><cn id=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.1.1.1.1.3.3.3\">1</cn></apply></apply></apply></apply><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2\"><times id=\"S4.SS4.p3.8.m8.1.1.1.1.2.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.2\"></times><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3\">superscript</csymbol><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3\">subscript</csymbol><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.2\">𝑓</ci><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.2.3\">𝑖</ci></apply><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3\"><minus id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3\"></minus><cn id=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1\"><times id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.1\"></times><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.2\">𝒙</ci><ci id=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.1.1.2.1.1.1.3.3\">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.8.m8.1c\">||f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})-f_{i}^{-1}(\\bm{P}\\bm{x}_{i})||</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.8.m8.1d\">| | italic_f start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) - italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | |</annotation></semantics></math>, assuming that each <math alttext=\"f_{i}^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.9.m9.1\"><semantics id=\"S4.SS4.p3.9.m9.1a\"><msubsup id=\"S4.SS4.p3.9.m9.1.1\" xref=\"S4.SS4.p3.9.m9.1.1.cmml\"><mi id=\"S4.SS4.p3.9.m9.1.1.2.2\" xref=\"S4.SS4.p3.9.m9.1.1.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.9.m9.1.1.2.3\" xref=\"S4.SS4.p3.9.m9.1.1.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.9.m9.1.1.3\" xref=\"S4.SS4.p3.9.m9.1.1.3.cmml\"><mo id=\"S4.SS4.p3.9.m9.1.1.3a\" xref=\"S4.SS4.p3.9.m9.1.1.3.cmml\">−</mo><mn id=\"S4.SS4.p3.9.m9.1.1.3.2\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.cmml\">1</mn></mrow></msubsup><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.9.m9.1b\"><apply id=\"S4.SS4.p3.9.m9.1.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.9.m9.1.1.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1\">superscript</csymbol><apply id=\"S4.SS4.p3.9.m9.1.1.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.9.m9.1.1.2.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1\">subscript</csymbol><ci id=\"S4.SS4.p3.9.m9.1.1.2.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.2.2\">𝑓</ci><ci id=\"S4.SS4.p3.9.m9.1.1.2.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.2.3\">𝑖</ci></apply><apply id=\"S4.SS4.p3.9.m9.1.1.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3\"><minus id=\"S4.SS4.p3.9.m9.1.1.3.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3\"></minus><cn id=\"S4.SS4.p3.9.m9.1.1.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p3.9.m9.1.1.3.2\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.9.m9.1c\">f_{i}^{-1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.9.m9.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> is Lipschitz continuous.\nAs a result of this optimization objective, the model will evolve along the optimization trajectory such that the <math alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.10.m10.1\"><semantics id=\"S4.SS4.p3.10.m10.1a\"><mrow id=\"S4.SS4.p3.10.m10.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.cmml\"><msubsup id=\"S4.SS4.p3.10.m10.1.1.3\" xref=\"S4.SS4.p3.10.m10.1.1.3.cmml\"><mi id=\"S4.SS4.p3.10.m10.1.1.3.2.2\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.10.m10.1.1.3.2.3\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.10.m10.1.1.3.3\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.10.m10.1.1.3.3a\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.cmml\">−</mo><mn id=\"S4.SS4.p3.10.m10.1.1.3.3.2\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.SS4.p3.10.m10.1.1.2\" xref=\"S4.SS4.p3.10.m10.1.1.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p3.10.m10.1.1.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\"><mo id=\"S4.SS4.p3.10.m10.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.10.m10.1.1.1.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.10.m10.1.1.1.1.1.2\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p3.10.m10.1.1.1.1.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.SS4.p3.10.m10.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.10.m10.1b\"><apply id=\"S4.SS4.p3.10.m10.1.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1\"><times id=\"S4.SS4.p3.10.m10.1.1.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.2\"></times><apply id=\"S4.SS4.p3.10.m10.1.1.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.10.m10.1.1.3.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.10.m10.1.1.3.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.10.m10.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.10.m10.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.2\">𝑓</ci><ci id=\"S4.SS4.p3.10.m10.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.3\">𝑖</ci></apply><apply id=\"S4.SS4.p3.10.m10.1.1.3.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.3\"><minus id=\"S4.SS4.p3.10.m10.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.3\"></minus><cn id=\"S4.SS4.p3.10.m10.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1\"><times id=\"S4.SS4.p3.10.m10.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.10.m10.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2\">𝒙</ci><ci id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3\">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.10.m10.1c\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.10.m10.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> for all tasks <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.11.m11.1\"><semantics id=\"S4.SS4.p3.11.m11.1a\"><mi id=\"S4.SS4.p3.11.m11.1.1\" xref=\"S4.SS4.p3.11.m11.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.11.m11.1b\"><ci id=\"S4.SS4.p3.11.m11.1.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.11.m11.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.11.m11.1d\">italic_i</annotation></semantics></math> gradually form a circular pattern. This gives an intuitive explanation on the anticipatory recovery phenomenon, since updaing <math alttext=\"\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.12.m12.1\"><semantics id=\"S4.SS4.p3.12.m12.1a\"><mi id=\"S4.SS4.p3.12.m12.1.1\" xref=\"S4.SS4.p3.12.m12.1.1.cmml\">𝒘</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.12.m12.1b\"><ci id=\"S4.SS4.p3.12.m12.1.1.cmml\" xref=\"S4.SS4.p3.12.m12.1.1\">𝒘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.12.m12.1c\">\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.12.m12.1d\">bold_italic_w</annotation></semantics></math> according to equation <a class=\"ltx_ref\" href=\"#S4.E2\" title=\"Equation 2 ‣ 4.4 Computational Toy Model ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> will also bring it closer to <math alttext=\"f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.13.m13.1\"><semantics id=\"S4.SS4.p3.13.m13.1a\"><mrow id=\"S4.SS4.p3.13.m13.1.1\" xref=\"S4.SS4.p3.13.m13.1.1.cmml\"><msubsup id=\"S4.SS4.p3.13.m13.1.1.3\" xref=\"S4.SS4.p3.13.m13.1.1.3.cmml\"><mi id=\"S4.SS4.p3.13.m13.1.1.3.2.2\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.2.cmml\">f</mi><mrow id=\"S4.SS4.p3.13.m13.1.1.3.2.3\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.cmml\"><mi id=\"S4.SS4.p3.13.m13.1.1.3.2.3.2\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.13.m13.1.1.3.2.3.1\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.13.m13.1.1.3.2.3.3\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.3.cmml\">1</mn></mrow><mrow id=\"S4.SS4.p3.13.m13.1.1.3.3\" xref=\"S4.SS4.p3.13.m13.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.13.m13.1.1.3.3a\" xref=\"S4.SS4.p3.13.m13.1.1.3.3.cmml\">−</mo><mn id=\"S4.SS4.p3.13.m13.1.1.3.3.2\" xref=\"S4.SS4.p3.13.m13.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.SS4.p3.13.m13.1.1.2\" xref=\"S4.SS4.p3.13.m13.1.1.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p3.13.m13.1.1.1.1\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.cmml\"><mo id=\"S4.SS4.p3.13.m13.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.13.m13.1.1.1.1.1\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.13.m13.1.1.1.1.1.2\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p3.13.m13.1.1.1.1.1.1\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.2.cmml\">𝒙</mi><mrow id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.1\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.3\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><mo id=\"S4.SS4.p3.13.m13.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.13.m13.1b\"><apply id=\"S4.SS4.p3.13.m13.1.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1\"><times id=\"S4.SS4.p3.13.m13.1.1.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.2\"></times><apply id=\"S4.SS4.p3.13.m13.1.1.3.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.13.m13.1.1.3.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.13.m13.1.1.3.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.13.m13.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.13.m13.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.2\">𝑓</ci><apply id=\"S4.SS4.p3.13.m13.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3\"><plus id=\"S4.SS4.p3.13.m13.1.1.3.2.3.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.1\"></plus><ci id=\"S4.SS4.p3.13.m13.1.1.3.2.3.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.2\">𝑖</ci><cn id=\"S4.SS4.p3.13.m13.1.1.3.2.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.13.m13.1.1.3.2.3.3\">1</cn></apply></apply><apply id=\"S4.SS4.p3.13.m13.1.1.3.3.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3.3\"><minus id=\"S4.SS4.p3.13.m13.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.3.3\"></minus><cn id=\"S4.SS4.p3.13.m13.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p3.13.m13.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.13.m13.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1\"><times id=\"S4.SS4.p3.13.m13.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.13.m13.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.2\">𝒙</ci><apply id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3\"><plus id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.2\">𝑖</ci><cn id=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.13.m13.1.1.1.1.1.3.3.3\">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.13.m13.1c\">f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.13.m13.1d\">italic_f start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT )</annotation></semantics></math>, thus reducing the loss on task <math alttext=\"i+1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.14.m14.1\"><semantics id=\"S4.SS4.p3.14.m14.1a\"><mrow id=\"S4.SS4.p3.14.m14.1.1\" xref=\"S4.SS4.p3.14.m14.1.1.cmml\"><mi id=\"S4.SS4.p3.14.m14.1.1.2\" xref=\"S4.SS4.p3.14.m14.1.1.2.cmml\">i</mi><mo id=\"S4.SS4.p3.14.m14.1.1.1\" xref=\"S4.SS4.p3.14.m14.1.1.1.cmml\">+</mo><mn id=\"S4.SS4.p3.14.m14.1.1.3\" xref=\"S4.SS4.p3.14.m14.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.14.m14.1b\"><apply id=\"S4.SS4.p3.14.m14.1.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1\"><plus id=\"S4.SS4.p3.14.m14.1.1.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1\"></plus><ci id=\"S4.SS4.p3.14.m14.1.1.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.2\">𝑖</ci><cn id=\"S4.SS4.p3.14.m14.1.1.3.cmml\" type=\"integer\" xref=\"S4.SS4.p3.14.m14.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.14.m14.1c\">i+1</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p3.14.m14.1d\">italic_i + 1</annotation></semantics></math> and exhibits anticipatory recovery.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p4\">\n<p class=\"ltx_p\" id=\"S4.SS4.p4.6\">We experimented with two very simple choices of <math alttext=\"f_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.1.m1.1\"><semantics id=\"S4.SS4.p4.1.m1.1a\"><msub id=\"S4.SS4.p4.1.m1.1.1\" xref=\"S4.SS4.p4.1.m1.1.1.cmml\"><mi id=\"S4.SS4.p4.1.m1.1.1.2\" xref=\"S4.SS4.p4.1.m1.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p4.1.m1.1.1.3\" xref=\"S4.SS4.p4.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.1.m1.1b\"><apply id=\"S4.SS4.p4.1.m1.1.1.cmml\" xref=\"S4.SS4.p4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.1.m1.1.1.1.cmml\" xref=\"S4.SS4.p4.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS4.p4.1.m1.1.1.2.cmml\" xref=\"S4.SS4.p4.1.m1.1.1.2\">𝑓</ci><ci id=\"S4.SS4.p4.1.m1.1.1.3.cmml\" xref=\"S4.SS4.p4.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.1.m1.1c\">f_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p4.1.m1.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>: <math alttext=\"f_{i}(\\bm{w})=\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.2.m2.1\"><semantics id=\"S4.SS4.p4.2.m2.1a\"><mrow id=\"S4.SS4.p4.2.m2.1.2\" xref=\"S4.SS4.p4.2.m2.1.2.cmml\"><mrow id=\"S4.SS4.p4.2.m2.1.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\"><msub id=\"S4.SS4.p4.2.m2.1.2.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.cmml\"><mi id=\"S4.SS4.p4.2.m2.1.2.2.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.SS4.p4.2.m2.1.2.2.2.3\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p4.2.m2.1.2.2.1\" xref=\"S4.SS4.p4.2.m2.1.2.2.1.cmml\">⁢</mo><mrow id=\"S4.SS4.p4.2.m2.1.2.2.3.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\"><mo id=\"S4.SS4.p4.2.m2.1.2.2.3.2.1\" stretchy=\"false\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\">(</mo><mi id=\"S4.SS4.p4.2.m2.1.1\" xref=\"S4.SS4.p4.2.m2.1.1.cmml\">𝒘</mi><mo id=\"S4.SS4.p4.2.m2.1.2.2.3.2.2\" stretchy=\"false\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p4.2.m2.1.2.1\" xref=\"S4.SS4.p4.2.m2.1.2.1.cmml\">=</mo><mi id=\"S4.SS4.p4.2.m2.1.2.3\" xref=\"S4.SS4.p4.2.m2.1.2.3.cmml\">𝒘</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.2.m2.1b\"><apply id=\"S4.SS4.p4.2.m2.1.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2\"><eq id=\"S4.SS4.p4.2.m2.1.2.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.1\"></eq><apply id=\"S4.SS4.p4.2.m2.1.2.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2\"><times id=\"S4.SS4.p4.2.m2.1.2.2.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.1\"></times><apply id=\"S4.SS4.p4.2.m2.1.2.2.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.2.m2.1.2.2.2.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2\">subscript</csymbol><ci id=\"S4.SS4.p4.2.m2.1.2.2.2.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.2\">𝑓</ci><ci id=\"S4.SS4.p4.2.m2.1.2.2.2.3.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.3\">𝑖</ci></apply><ci id=\"S4.SS4.p4.2.m2.1.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.1\">𝒘</ci></apply><ci id=\"S4.SS4.p4.2.m2.1.2.3.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.3\">𝒘</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.2.m2.1c\">f_{i}(\\bm{w})=\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p4.2.m2.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_w</annotation></semantics></math> and <math alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.3.m3.1\"><semantics id=\"S4.SS4.p4.3.m3.1a\"><mrow id=\"S4.SS4.p4.3.m3.1.2\" xref=\"S4.SS4.p4.3.m3.1.2.cmml\"><mrow id=\"S4.SS4.p4.3.m3.1.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\"><msub id=\"S4.SS4.p4.3.m3.1.2.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.cmml\"><mi id=\"S4.SS4.p4.3.m3.1.2.2.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.SS4.p4.3.m3.1.2.2.2.3\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p4.3.m3.1.2.2.1\" xref=\"S4.SS4.p4.3.m3.1.2.2.1.cmml\">⁢</mo><mrow id=\"S4.SS4.p4.3.m3.1.2.2.3.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\"><mo id=\"S4.SS4.p4.3.m3.1.2.2.3.2.1\" stretchy=\"false\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\">(</mo><mi id=\"S4.SS4.p4.3.m3.1.1\" xref=\"S4.SS4.p4.3.m3.1.1.cmml\">𝒘</mi><mo id=\"S4.SS4.p4.3.m3.1.2.2.3.2.2\" stretchy=\"false\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p4.3.m3.1.2.1\" xref=\"S4.SS4.p4.3.m3.1.2.1.cmml\">=</mo><mrow id=\"S4.SS4.p4.3.m3.1.2.3\" xref=\"S4.SS4.p4.3.m3.1.2.3.cmml\"><msub id=\"S4.SS4.p4.3.m3.1.2.3.2\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.cmml\"><mi id=\"S4.SS4.p4.3.m3.1.2.3.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.2.cmml\">𝒚</mi><mi id=\"S4.SS4.p4.3.m3.1.2.3.2.3\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p4.3.m3.1.2.3.1\" xref=\"S4.SS4.p4.3.m3.1.2.3.1.cmml\">−</mo><mi id=\"S4.SS4.p4.3.m3.1.2.3.3\" xref=\"S4.SS4.p4.3.m3.1.2.3.3.cmml\">𝒘</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.3.m3.1b\"><apply id=\"S4.SS4.p4.3.m3.1.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2\"><eq id=\"S4.SS4.p4.3.m3.1.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.1\"></eq><apply id=\"S4.SS4.p4.3.m3.1.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2\"><times id=\"S4.SS4.p4.3.m3.1.2.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.1\"></times><apply id=\"S4.SS4.p4.3.m3.1.2.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.3.m3.1.2.2.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2\">subscript</csymbol><ci id=\"S4.SS4.p4.3.m3.1.2.2.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.2\">𝑓</ci><ci id=\"S4.SS4.p4.3.m3.1.2.2.2.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.3\">𝑖</ci></apply><ci id=\"S4.SS4.p4.3.m3.1.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.1\">𝒘</ci></apply><apply id=\"S4.SS4.p4.3.m3.1.2.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3\"><minus id=\"S4.SS4.p4.3.m3.1.2.3.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.1\"></minus><apply id=\"S4.SS4.p4.3.m3.1.2.3.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.3.m3.1.2.3.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2\">subscript</csymbol><ci id=\"S4.SS4.p4.3.m3.1.2.3.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.2\">𝒚</ci><ci id=\"S4.SS4.p4.3.m3.1.2.3.2.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.3\">𝑖</ci></apply><ci id=\"S4.SS4.p4.3.m3.1.2.3.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.3\">𝒘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.3.m3.1c\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p4.3.m3.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_italic_w</annotation></semantics></math> for some task-dependent targets <math alttext=\"\\bm{y}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.4.m4.1\"><semantics id=\"S4.SS4.p4.4.m4.1a\"><msub id=\"S4.SS4.p4.4.m4.1.1\" xref=\"S4.SS4.p4.4.m4.1.1.cmml\"><mi id=\"S4.SS4.p4.4.m4.1.1.2\" xref=\"S4.SS4.p4.4.m4.1.1.2.cmml\">𝒚</mi><mi id=\"S4.SS4.p4.4.m4.1.1.3\" xref=\"S4.SS4.p4.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.4.m4.1b\"><apply id=\"S4.SS4.p4.4.m4.1.1.cmml\" xref=\"S4.SS4.p4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.4.m4.1.1.1.cmml\" xref=\"S4.SS4.p4.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS4.p4.4.m4.1.1.2.cmml\" xref=\"S4.SS4.p4.4.m4.1.1.2\">𝒚</ci><ci id=\"S4.SS4.p4.4.m4.1.1.3.cmml\" xref=\"S4.SS4.p4.4.m4.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.4.m4.1c\">\\bm{y}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p4.4.m4.1d\">bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. We follow the same order over tasks—<math alttext=\"1,\\cdots,T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.5.m5.3\"><semantics id=\"S4.SS4.p4.5.m5.3a\"><mrow id=\"S4.SS4.p4.5.m5.3.4.2\" xref=\"S4.SS4.p4.5.m5.3.4.1.cmml\"><mn id=\"S4.SS4.p4.5.m5.1.1\" xref=\"S4.SS4.p4.5.m5.1.1.cmml\">1</mn><mo id=\"S4.SS4.p4.5.m5.3.4.2.1\" xref=\"S4.SS4.p4.5.m5.3.4.1.cmml\">,</mo><mi id=\"S4.SS4.p4.5.m5.2.2\" mathvariant=\"normal\" xref=\"S4.SS4.p4.5.m5.2.2.cmml\">⋯</mi><mo id=\"S4.SS4.p4.5.m5.3.4.2.2\" xref=\"S4.SS4.p4.5.m5.3.4.1.cmml\">,</mo><mi id=\"S4.SS4.p4.5.m5.3.3\" xref=\"S4.SS4.p4.5.m5.3.3.cmml\">T</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.5.m5.3b\"><list id=\"S4.SS4.p4.5.m5.3.4.1.cmml\" xref=\"S4.SS4.p4.5.m5.3.4.2\"><cn id=\"S4.SS4.p4.5.m5.1.1.cmml\" type=\"integer\" xref=\"S4.SS4.p4.5.m5.1.1\">1</cn><ci id=\"S4.SS4.p4.5.m5.2.2.cmml\" xref=\"S4.SS4.p4.5.m5.2.2\">⋯</ci><ci id=\"S4.SS4.p4.5.m5.3.3.cmml\" xref=\"S4.SS4.p4.5.m5.3.3\">𝑇</ci></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.5.m5.3c\">1,\\cdots,T</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p4.5.m5.3d\">1 , ⋯ , italic_T</annotation></semantics></math>—for multiple epochs of training. The resulting loss curves are shown in Figure <a class=\"ltx_ref\" href=\"#S4.F10\" title=\"Figure 10 ‣ 4.1 Temporal Structure of Gradients ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, which exhibits very similar anticipatory recovery trajectory as the full-blown LLM experiment. Visualizations of the 2-dimensional PCA embeddings for <math alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p4.6.m6.1\"><semantics id=\"S4.SS4.p4.6.m6.1a\"><mrow id=\"S4.SS4.p4.6.m6.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.cmml\"><msubsup id=\"S4.SS4.p4.6.m6.1.1.3\" xref=\"S4.SS4.p4.6.m6.1.1.3.cmml\"><mi id=\"S4.SS4.p4.6.m6.1.1.3.2.2\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p4.6.m6.1.1.3.2.3\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p4.6.m6.1.1.3.3\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.cmml\"><mo id=\"S4.SS4.p4.6.m6.1.1.3.3a\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.cmml\">−</mo><mn id=\"S4.SS4.p4.6.m6.1.1.3.3.2\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo id=\"S4.SS4.p4.6.m6.1.1.2\" xref=\"S4.SS4.p4.6.m6.1.1.2.cmml\">⁢</mo><mrow id=\"S4.SS4.p4.6.m6.1.1.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\"><mo id=\"S4.SS4.p4.6.m6.1.1.1.1.2\" stretchy=\"false\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p4.6.m6.1.1.1.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p4.6.m6.1.1.1.1.1.2\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.2.cmml\">𝑷</mi><mo id=\"S4.SS4.p4.6.m6.1.1.1.1.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.SS4.p4.6.m6.1.1.1.1.3\" stretchy=\"false\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.6.m6.1b\"><apply id=\"S4.SS4.p4.6.m6.1.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1\"><times id=\"S4.SS4.p4.6.m6.1.1.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.2\"></times><apply id=\"S4.SS4.p4.6.m6.1.1.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.6.m6.1.1.3.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p4.6.m6.1.1.3.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.6.m6.1.1.3.2.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p4.6.m6.1.1.3.2.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.2\">𝑓</ci><ci id=\"S4.SS4.p4.6.m6.1.1.3.2.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.3\">𝑖</ci></apply><apply id=\"S4.SS4.p4.6.m6.1.1.3.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.3\"><minus id=\"S4.SS4.p4.6.m6.1.1.3.3.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.3\"></minus><cn id=\"S4.SS4.p4.6.m6.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1\"><times id=\"S4.SS4.p4.6.m6.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p4.6.m6.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.2\">𝑷</ci><apply id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2\">𝒙</ci><ci id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3\">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.6.m6.1c\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p4.6.m6.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> in the second experiment are shown in Figure <a class=\"ltx_ref\" href=\"#S4.F11\" title=\"Figure 11 ‣ 4.1 Temporal Structure of Gradients ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, which confirms our analysis that they gradually self-organize into a cyclic structure.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p5\">\n<p class=\"ltx_p\" id=\"S4.SS4.p5.3\">There are two potential reasons large overparameterized networks might produce the anticipatory recovery in a way analogous to the toy simulation. First, for larger networks, it is more likely that the network can develop task-specific parameters that quickly adapt to and memorize new input data, corresponding to Equation <a class=\"ltx_ref\" href=\"#S4.E2\" title=\"Equation 2 ‣ 4.4 Computational Toy Model ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. And when the fast memorization is achieved, the gradient descent dynamics of the slow weights push the representations of the two adjacent tasks (<math alttext=\"P\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p5.1.m1.1\"><semantics id=\"S4.SS4.p5.1.m1.1a\"><mrow id=\"S4.SS4.p5.1.m1.1.1\" xref=\"S4.SS4.p5.1.m1.1.1.cmml\"><mi id=\"S4.SS4.p5.1.m1.1.1.2\" xref=\"S4.SS4.p5.1.m1.1.1.2.cmml\">P</mi><mo id=\"S4.SS4.p5.1.m1.1.1.1\" xref=\"S4.SS4.p5.1.m1.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p5.1.m1.1.1.3\" xref=\"S4.SS4.p5.1.m1.1.1.3.cmml\"><mi id=\"S4.SS4.p5.1.m1.1.1.3.2\" xref=\"S4.SS4.p5.1.m1.1.1.3.2.cmml\">𝒙</mi><mi id=\"S4.SS4.p5.1.m1.1.1.3.3\" xref=\"S4.SS4.p5.1.m1.1.1.3.3.cmml\">i</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p5.1.m1.1b\"><apply id=\"S4.SS4.p5.1.m1.1.1.cmml\" xref=\"S4.SS4.p5.1.m1.1.1\"><times id=\"S4.SS4.p5.1.m1.1.1.1.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.1\"></times><ci id=\"S4.SS4.p5.1.m1.1.1.2.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.2\">𝑃</ci><apply id=\"S4.SS4.p5.1.m1.1.1.3.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p5.1.m1.1.1.3.1.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p5.1.m1.1.1.3.2.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3.2\">𝒙</ci><ci id=\"S4.SS4.p5.1.m1.1.1.3.3.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3.3\">𝑖</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p5.1.m1.1c\">P\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p5.1.m1.1d\">italic_P bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"P\\bm{x}_{i+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p5.2.m2.1\"><semantics id=\"S4.SS4.p5.2.m2.1a\"><mrow id=\"S4.SS4.p5.2.m2.1.1\" xref=\"S4.SS4.p5.2.m2.1.1.cmml\"><mi id=\"S4.SS4.p5.2.m2.1.1.2\" xref=\"S4.SS4.p5.2.m2.1.1.2.cmml\">P</mi><mo id=\"S4.SS4.p5.2.m2.1.1.1\" xref=\"S4.SS4.p5.2.m2.1.1.1.cmml\">⁢</mo><msub id=\"S4.SS4.p5.2.m2.1.1.3\" xref=\"S4.SS4.p5.2.m2.1.1.3.cmml\"><mi id=\"S4.SS4.p5.2.m2.1.1.3.2\" xref=\"S4.SS4.p5.2.m2.1.1.3.2.cmml\">𝒙</mi><mrow id=\"S4.SS4.p5.2.m2.1.1.3.3\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.cmml\"><mi id=\"S4.SS4.p5.2.m2.1.1.3.3.2\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p5.2.m2.1.1.3.3.1\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p5.2.m2.1.1.3.3.3\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p5.2.m2.1b\"><apply id=\"S4.SS4.p5.2.m2.1.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1\"><times id=\"S4.SS4.p5.2.m2.1.1.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.1\"></times><ci id=\"S4.SS4.p5.2.m2.1.1.2.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.2\">𝑃</ci><apply id=\"S4.SS4.p5.2.m2.1.1.3.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p5.2.m2.1.1.3.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p5.2.m2.1.1.3.2.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.2\">𝒙</ci><apply id=\"S4.SS4.p5.2.m2.1.1.3.3.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3\"><plus id=\"S4.SS4.p5.2.m2.1.1.3.3.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p5.2.m2.1.1.3.3.2.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.2\">𝑖</ci><cn id=\"S4.SS4.p5.2.m2.1.1.3.3.3.cmml\" type=\"integer\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.3\">1</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p5.2.m2.1c\">P\\bm{x}_{i+1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p5.2.m2.1d\">italic_P bold_italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math>) closer when <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p5.3.m3.1\"><semantics id=\"S4.SS4.p5.3.m3.1a\"><mi id=\"S4.SS4.p5.3.m3.1.1\" xref=\"S4.SS4.p5.3.m3.1.1.cmml\">f</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p5.3.m3.1b\"><ci id=\"S4.SS4.p5.3.m3.1.1.cmml\" xref=\"S4.SS4.p5.3.m3.1.1\">𝑓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p5.3.m3.1c\">f</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.p5.3.m3.1d\">italic_f</annotation></semantics></math> is an identity function, according to Equation <a class=\"ltx_ref\" href=\"#S4.E1\" title=\"Equation 1 ‣ 4.4 Computational Toy Model ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. This effect can be seen in earlier LLM experiments (Figure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), where larger models achieve significantly lower losses within a few gradient update steps. Second, larger networks have more learning capacity to map the features of two adjacent tasks closer. In our linear projection model, anticipatory recovery keeps growing over many epochs, whereas the anticipatory effect is already at the strongest within 2 or 3 epochs in LLM experiments. Moreover, all data points are randomly generated in the toy model, which makes it easier to separate and map their representations according to a temporal structure than real-world data. In contrast, real-world data could require more representation capacity since data points are noisy and correlated.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.5 </span>Summary</h3>\n<div class=\"ltx_para\" id=\"S4.SS5.p1\">\n<p class=\"ltx_p\" id=\"S4.SS5.p1.2\">In this section, we visualized model weight dynamics with heatmaps and we showed model activations and gradients during cyclic training. We discussed the special temporal structure that is exhibited in these heat maps. We also plotted the pairwise degree of recovery for fine-tuning on document <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.1.m1.1\"><semantics id=\"S4.SS5.p1.1.m1.1a\"><mi id=\"S4.SS5.p1.1.m1.1.1\" xref=\"S4.SS5.p1.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS5.p1.1.m1.1b\"><ci id=\"S4.SS5.p1.1.m1.1.1.cmml\" xref=\"S4.SS5.p1.1.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS5.p1.1.m1.1c\">i</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS5.p1.1.m1.1d\">italic_i</annotation></semantics></math> and evaluating on document <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.2.m2.1\"><semantics id=\"S4.SS5.p1.2.m2.1a\"><mi id=\"S4.SS5.p1.2.m2.1.1\" xref=\"S4.SS5.p1.2.m2.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS5.p1.2.m2.1b\"><ci id=\"S4.SS5.p1.2.m2.1.1.cmml\" xref=\"S4.SS5.p1.2.m2.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS5.p1.2.m2.1c\">j</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS5.p1.2.m2.1d\">italic_j</annotation></semantics></math>, as well as the change of distance between fine-tuned model weights on different tasks. The results suggest that after we train on a document, the model’s representation of that document becomes less sensitive to gradient updates on other documents. Finally, we showed a simple toy experiment that demonstrates a similar anticipatory recovery phenomenon in its loss curve, and discuss its connections to neural network training dynamics through the lens of task-specific and task-general parameters. Overall, these results shed some light on the dynamics of cyclic training.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Related Work</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\" id=\"S5.p1.1\">In this section we discuss the most relevant prior works to this paper. Please refer to Appendix <a class=\"ltx_ref\" href=\"#A4\" title=\"Appendix D Additional Related Work ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for additional related work.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Cyclic and Structured Training.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS0.SSS0.Px1.p1.1\">Prior theoretical works have studied convergence rates, under various assumptions, for the training setup where the data points are shuffled only once and that order is reused for all epochs <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">ahn2020sgd, </a>; <a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">gurbuzbalaban2019convergence, </a>; <a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">mishchenko2020random, </a>; <a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">safran2020good, </a>)</cite>. On the empirical side, <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">xu2022stochastic </a></cite> found that shuffling the data only once in the beginning can achieve a convergence rate comparable to shuffling every epoch.\nThe training setup is equivalent to our cyclic training setup, but our research examines the loss on each task throughout the training cycle and discovers the anticipatory recovery effect. We also extend it to multiple gradient update steps on each data point.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Online Learning.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS0.SSS0.Px2.p1.1\">Online learning deals with the setting where the tasks come from an online sequential stream. One of the simplest algorithms in online learning is follow-the-leader <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">hannan1957approximation, </a>)</cite>, which stores all previous data from the stream and minimizes the total loss. It has strong performance guarantees but is computationally very expensive, and it also might not be feasible to store all the past data. Many subsequent works have developed cheaper algorithms under different assumptions <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">zinkevich2003online, </a>; <a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">cesa2006prediction, </a>; <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">shalev2012online, </a>)</cite>. Many recent works also explore the connection between online learning and meta-learning or continual learning <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">denevi2019learning, </a>; <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">finn2019online, </a>; <a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">denevi2019online, </a>; <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">javed2019meta, </a>; <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">fini2020online, </a>; <a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">ren2020wandering, </a>; <a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">wang2021wanderlust, </a>)</cite>. The cyclic training setting that we explore in this research can be considered as a special case of the online learning setting where the data stream has a cyclic repetition structure. We employ multiple steps of online gradient descent <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">biehl1995learning, </a>)</cite> on each document from the stream and study the training dynamics of over-parameterized neural networks.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Catastrophic Interference.</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"S5.SS0.SSS0.Px3.p1.1\">When transitioning between tasks sequentially, neural networks often experience “catastrophic interference” <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">mccloskey1989catastrophic, </a>)</cite>, marked by a significant drop in performance on previously learned tasks. Numerous algorithms have been proposed to mitigate catastrophic forgetting, focusing on general approaches including parameter regularization <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">kirkpatrick2017overcoming, </a>; <a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">zenke2017continual, </a>; <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">aljundi2018memory, </a>)</cite>, data replay <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">rebuffi2017icarl, </a>; <a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">rolnick2019experience, </a>; <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">chaudhry2019tiny, </a>)</cite>, knowledge distillation <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">hinton2015distilling, </a>; <a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">li2017learning, </a>; <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">buzzega2020dark, </a>; <a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">madaan2023heterogeneous, </a>)</cite>, and architectural isolation and expansion <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">yoon2017lifelong, </a>; <a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">serra2018overcoming, </a>; <a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">gurbuz2022nispa, </a>; <a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">kang2022forget, </a>)</cite>. Our work extends interleaved training <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">mayo2023multitask, </a>)</cite> to a larger number of tasks, specifically investigating the emergent anticipatory recovery phenomenon in cyclic training.\nThis finding adds to the above literature by demonstrating a new mechanism by which large networks can avoid or recover from catastrophic interference.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S5.SS0.SSS0.Px4\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Continual Learning</h4>\n<div class=\"ltx_para\" id=\"S5.SS0.SSS0.Px4.p1\">\n<p class=\"ltx_p\" id=\"S5.SS0.SSS0.Px4.p1.1\">Continual learning <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">chen2018lifelong </a>; <a class=\"ltx_ref\" href=\"#bib.bib53\" title=\"\">madotto2021continual </a>; <a class=\"ltx_ref\" href=\"#bib.bib54\" title=\"\">qin2022LFPT5 </a>; <a class=\"ltx_ref\" href=\"#bib.bib55\" title=\"\">razdaibiedina2023progressive </a></cite> addresses a simplified setup where a model sequentially learns a set of tasks without revision. Recently, there have been debates over the practicality of continual learning setups. Studies like <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"#bib.bib56\" title=\"\">davidson2020sequential </a></cite> have shown that as networks learn more tasks, they improve in learning speed and reduce forgetting. In large models, studies suggest that pre-trained vision classifiers can undertake continual learning with ease, by either freezing or fine-tuning representations <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib57\" title=\"\">janson2022simple, </a>; <a class=\"ltx_ref\" href=\"#bib.bib58\" title=\"\">lee2023pre, </a>; <a class=\"ltx_ref\" href=\"#bib.bib59\" title=\"\">fini2022self, </a>)</cite>. In the language domain, research also suggests that LLMs exhibit emerging continual learning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib60\" title=\"\">scialom2022fine, </a>; <a class=\"ltx_ref\" href=\"#bib.bib61\" title=\"\">ke2022continual, </a>)</cite>.\nNevertheless, it is uncommon in real environments for tasks to occur only once yet for an agent to need to retain them.\nUnlike prior literature on continual learning, our research uniquely focuses on sequential learning environments with cyclic repetition.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Discussion and Limitations</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n<p class=\"ltx_p\" id=\"S6.p1.1\">In this work, we explored the training dynamics of overparametrized neural networks, especially LLMs, in sequential cyclic fine-tuning, where a finite set of documents are presented in the same order within each epoch. We demonstrated the remarkable phenomenon of anticipatory recovery—networks recover from the initial forgetting before seeing the same document again. The effect holds across many different network instances and training hyperparameters. This phenomenon is a sharp contrast with the well known phenomenon of catastrophic interference, where forgetting increases monotonically as a network is trained on a sequence of different documents.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.p2\">\n<p class=\"ltx_p\" id=\"S6.p2.1\">We showed that anticipatory recovery occurs only when the network has sufficient width and depth and when it is well fitted to each document before moving to the next. Visualizations of model weights, model activations, and gradients exhibit clear temporal structure, which provide insights on the underlying mechanisms of anticipatory recovery.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.p3\">\n<p class=\"ltx_p\" id=\"S6.p3.1\">Our research indicates that there is value in exploring naturalistic task sequences within continual learning, where tasks interleave in statistically regular patterns. This approach could expand the field’s current focus on learning and retaining new tasks to also consider how effectively previously encountered tasks are re-learned when they reappear. With the anticipatory recovery phenomenon, we discovered a mechanism in which ML models can do surprisingly better than expected on prequential evaluation. By analyzing the different factors of model pre-training and fine-tuning that moderate this phenomenon, our experiments provide a promising first step toward leveraging structured training with agents in realistic environments.</p>\n</div>\n<section class=\"ltx_paragraph\" id=\"S6.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Limitations.</h4>\n<div class=\"ltx_para\" id=\"S6.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"S6.SS0.SSS0.Px1.p1.1\">The cyclic training setup investigated in this work is distinct from the IID training setting assumed in the vast majority of the machine learning literature. It accounts for task repetition and task switching costs, which are critical components of the learning experience of humans and other real world agents. However, our current setup is still highly simplified. Future research could investigate the emerging training dynamics of neural networks in different types of structured environments, such as multiscale temporal dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"#bib.bib62\" title=\"\">jones2023learning, </a>)</cite>, from both theoretical and empirical perspectives.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.SS0.SSS0.Px1.p2\">\n<p class=\"ltx_p\" id=\"S6.SS0.SSS0.Px1.p2.1\">The mathematical foundation of anticipatory recovery also requires further investigation. Although our computational toy model reproduces the anticipatory recovery phenomenon, it does not explain why the effect is stronger in LLMs and autoregressive tasks than in other types of architecture or learning objectives. In terms of the empirical experiments, the ablation studies are only run in a single setting. Future research could run the experiments in more settings to reach more conclusive results and investigate possible alternative theoretical explanations to the anticipatory recovery phenomenon.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgment</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\" id=\"Sx1.p1.1\">We thank the Microsoft Accelerating Foundation Models Research program for providing Azure cloud compute credits. We thank members of the NYU Agentic Learning AI Lab for helpful discussions. The compute was supported\nby the NYU High Performance Computing resources, services, and staff expertise. MJ was supported by NSF grant 2020-906.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(1)</span>\n<span class=\"ltx_bibblock\">\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n</span>\n<span class=\"ltx_bibblock\">BERT: Pre-training of deep bidirectional transformers for language understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib1.1.1\">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(2)</span>\n<span class=\"ltx_bibblock\">\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Language models are few-shot learners.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib2.1.1\">Advances in Neural Information Processing Systems</span>, 33:1877–1901, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(3)</span>\n<span class=\"ltx_bibblock\">\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n\n</span>\n<span class=\"ltx_bibblock\">LLaMA: Open and efficient foundation language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib3.1.1\">arXiv preprint arXiv:2302.13971</span>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(4)</span>\n<span class=\"ltx_bibblock\">\nOpenAI.\n\n</span>\n<span class=\"ltx_bibblock\">GPT-4 technical report, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(5)</span>\n<span class=\"ltx_bibblock\">\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Training compute-optimal large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib5.1.1\">arXiv preprint arXiv:2203.15556</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(6)</span>\n<span class=\"ltx_bibblock\">\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.\n\n</span>\n<span class=\"ltx_bibblock\">PaLM: Scaling language modeling with pathways.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib6.1.1\">Journal of Machine Learning Research</span>, 24(240):1–113, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(7)</span>\n<span class=\"ltx_bibblock\">\nFuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.\n\n</span>\n<span class=\"ltx_bibblock\">To repeat or not to repeat: Insights from scaling llm under token-crisis.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib7.1.1\">Advances in Neural Information Processing Systems</span>, 36, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(8)</span>\n<span class=\"ltx_bibblock\">\nZhiyuan Chen and Bing Liu.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib8.1.1\">Lifelong Machine Learning</span>, volume 1.\n\n</span>\n<span class=\"ltx_bibblock\">Springer, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(9)</span>\n<span class=\"ltx_bibblock\">\nMichael McCloskey and Neal J Cohen.\n\n</span>\n<span class=\"ltx_bibblock\">Catastrophic interference in connectionist networks: The sequential learning problem.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib9.1.1\">Psychology of Learning and Motivation</span>, volume 24, pages 109–165. Elsevier, 1989.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(10)</span>\n<span class=\"ltx_bibblock\">\nZhipeng Cai, Ozan Sener, and Vladlen Koltun.\n\n</span>\n<span class=\"ltx_bibblock\">Online continual learning with natural distribution shifts: An empirical study with visual data.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib10.1.1\">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 8281–8290, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(11)</span>\n<span class=\"ltx_bibblock\">\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Pythia: A suite for analyzing large language models across training and scaling.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib11.1.1\">International Conference on Machine Learning</span>, pages 2397–2430. PMLR, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(12)</span>\n<span class=\"ltx_bibblock\">\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.\n\n</span>\n<span class=\"ltx_bibblock\">The Pile: An 800GB dataset of diverse text for language modeling.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib12.1.1\">arXiv preprint arXiv:2101.00027</span>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(13)</span>\n<span class=\"ltx_bibblock\">\nStella Biderman, Kieran Bicheno, and Leo Gao.\n\n</span>\n<span class=\"ltx_bibblock\">Datasheet for the Pile.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib13.1.1\">arXiv preprint arXiv:2201.07311</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(14)</span>\n<span class=\"ltx_bibblock\">\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\n\n</span>\n<span class=\"ltx_bibblock\">Generative pretraining from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib14.1.1\">International Conference on Machine Learning</span>, pages 1691–1703. PMLR, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(15)</span>\n<span class=\"ltx_bibblock\">\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 16x16 words: Transformers for image recognition at scale.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib15.1.1\">International Conference on Learning Representations</span>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(16)</span>\n<span class=\"ltx_bibblock\">\nKaren Simonyan and Andrew Zisserman.\n\n</span>\n<span class=\"ltx_bibblock\">Very deep convolutional networks for large-scale image recognition.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib16.1.1\">International Conference on Learning Representations</span>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(17)</span>\n<span class=\"ltx_bibblock\">\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Gulçehre, and Bing Xiang.\n\n</span>\n<span class=\"ltx_bibblock\">Abstractive text summarization using sequence-to-sequence rnns and beyond.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib17.1.1\">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</span>, pages 280–290, 2016.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(18)</span>\n<span class=\"ltx_bibblock\">\nAlex Krizhevsky, Geoffrey Hinton, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Learning multiple layers of features from tiny images.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib18.1.1\">University of Toronto</span>, 2009.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(19)</span>\n<span class=\"ltx_bibblock\">\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\n\n</span>\n<span class=\"ltx_bibblock\">Imagenet: A large-scale hierarchical image database.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib19.1.1\">2009 IEEE conference on Computer Vision and Pattern Recognition</span>, pages 248–255. IEEE, 2009.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(20)</span>\n<span class=\"ltx_bibblock\">\nDiederik P. Kingma and Jimmy Ba.\n\n</span>\n<span class=\"ltx_bibblock\">Adam: A method for stochastic optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib20.1.1\">International Conference on Learning Representations</span>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(21)</span>\n<span class=\"ltx_bibblock\">\nKwangjun Ahn, Chulhee Yun, and Suvrit Sra.\n\n</span>\n<span class=\"ltx_bibblock\">SGD with shuffling: Optimal rates without component convexity and large epoch requirements.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib21.1.1\">Advances in Neural Information Processing Systems</span>, 33:17526–17535, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(22)</span>\n<span class=\"ltx_bibblock\">\nM Gurbuzbalaban, Asu Ozdaglar, and Pablo A Parrilo.\n\n</span>\n<span class=\"ltx_bibblock\">Convergence rate of incremental gradient and incremental newton methods.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib22.1.1\">SIAM Journal on Optimization</span>, 29(4):2542–2565, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(23)</span>\n<span class=\"ltx_bibblock\">\nKonstantin Mishchenko, Ahmed Khaled, and Peter Richtárik.\n\n</span>\n<span class=\"ltx_bibblock\">Random reshuffling: Simple analysis with vast improvements.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib23.1.1\">Advances in Neural Information Processing Systems</span>, 33:17309–17320, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(24)</span>\n<span class=\"ltx_bibblock\">\nItay Safran and Ohad Shamir.\n\n</span>\n<span class=\"ltx_bibblock\">How good is SGD with random shuffling?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib24.1.1\">Conference on Learning Theory</span>, pages 3250–3284. PMLR, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(25)</span>\n<span class=\"ltx_bibblock\">\nLijie Xu, Shuang Qiu, Binhang Yuan, Jiawei Jiang, Cedric Renggli, Shaoduo Gan, Kaan Kara, Guoliang Li, Ji Liu, Wentao Wu, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Stochastic gradient descent without full data shuffle.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib25.1.1\">arXiv preprint arXiv:2206.05830</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(26)</span>\n<span class=\"ltx_bibblock\">\nJames Hannan.\n\n</span>\n<span class=\"ltx_bibblock\">Approximation to Bayes risk in repeated play.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib26.1.1\">Contributions to the Theory of Games</span>, 3:97–139, 1957.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(27)</span>\n<span class=\"ltx_bibblock\">\nMartin Zinkevich.\n\n</span>\n<span class=\"ltx_bibblock\">Online convex programming and generalized infinitesimal gradient ascent.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib27.1.1\">International Conference on Machine Learning</span>, pages 928–936, 2003.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(28)</span>\n<span class=\"ltx_bibblock\">\nNicolo Cesa-Bianchi and Gábor Lugosi.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib28.1.1\">Prediction, learning, and games</span>.\n\n</span>\n<span class=\"ltx_bibblock\">Cambridge University Press, 2006.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(29)</span>\n<span class=\"ltx_bibblock\">\nShai Shalev-Shwartz et al.\n\n</span>\n<span class=\"ltx_bibblock\">Online learning and online convex optimization.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib29.1.1\">Foundations and Trends® in Machine Learning</span>, 4(2):107–194, 2012.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(30)</span>\n<span class=\"ltx_bibblock\">\nGiulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil.\n\n</span>\n<span class=\"ltx_bibblock\">Learning-to-learn stochastic gradient descent with biased regularization.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib30.1.1\">International Conference on Machine Learning</span>, pages 1566–1575. PMLR, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(31)</span>\n<span class=\"ltx_bibblock\">\nChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.\n\n</span>\n<span class=\"ltx_bibblock\">Online meta-learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib31.1.1\">International Conference on Machine Learning</span>, pages 1920–1930. PMLR, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(32)</span>\n<span class=\"ltx_bibblock\">\nGiulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil.\n\n</span>\n<span class=\"ltx_bibblock\">Online-within-online meta-learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib32.1.1\">Advances in Neural Information Processing Systems</span>, 32, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(33)</span>\n<span class=\"ltx_bibblock\">\nKhurram Javed and Martha White.\n\n</span>\n<span class=\"ltx_bibblock\">Meta-learning representations for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib33.1.1\">Advances in Neural Information Processing Systems</span>, 32, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(34)</span>\n<span class=\"ltx_bibblock\">\nEnrico Fini, Stéphane Lathuiliere, Enver Sangineto, Moin Nabi, and Elisa Ricci.\n\n</span>\n<span class=\"ltx_bibblock\">Online continual learning under extreme memory constraints.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib34.1.1\">Proceedings of the European Conference on Computer Vision</span>, pages 720–735. Springer, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(35)</span>\n<span class=\"ltx_bibblock\">\nMengye Ren, Michael Louis Iuzzolino, Michael Curtis Mozer, and Richard S. Zemel.\n\n</span>\n<span class=\"ltx_bibblock\">Wandering within a world: Online contextualized few-shot learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib35.1.1\">International Conference on Learning Representations</span>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(36)</span>\n<span class=\"ltx_bibblock\">\nJianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta.\n\n</span>\n<span class=\"ltx_bibblock\">Wanderlust: Online continual object detection in the real world.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib36.1.1\">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 10829–10838, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(37)</span>\n<span class=\"ltx_bibblock\">\nMichael Biehl and Holm Schwarze.\n\n</span>\n<span class=\"ltx_bibblock\">Learning by on-line gradient descent.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib37.1.1\">Journal of Physics A: Mathematical and general</span>, 28(3):643, 1995.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(38)</span>\n<span class=\"ltx_bibblock\">\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Overcoming catastrophic forgetting in neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib38.1.1\">Proceedings of the National Academy of Sciences</span>, 114(13):3521–3526, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(39)</span>\n<span class=\"ltx_bibblock\">\nFriedemann Zenke, Ben Poole, and Surya Ganguli.\n\n</span>\n<span class=\"ltx_bibblock\">Continual learning through synaptic intelligence.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib39.1.1\">International Conference on Machine Learning</span>, pages 3987–3995. PMLR, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(40)</span>\n<span class=\"ltx_bibblock\">\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.\n\n</span>\n<span class=\"ltx_bibblock\">Memory aware synapses: Learning what (not) to forget.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib40.1.1\">Proceedings of the European Conference on Computer Vision</span>, pages 139–154, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(41)</span>\n<span class=\"ltx_bibblock\">\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\n</span>\n<span class=\"ltx_bibblock\">iCaRL: Incremental classifier and representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib41.1.1\">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</span>, pages 2001–2010, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(42)</span>\n<span class=\"ltx_bibblock\">\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne.\n\n</span>\n<span class=\"ltx_bibblock\">Experience replay for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib42.1.1\">Advances in Neural Information Processing Systems</span>, 32, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(43)</span>\n<span class=\"ltx_bibblock\">\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato.\n\n</span>\n<span class=\"ltx_bibblock\">On tiny episodic memories in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib43.1.1\">arXiv preprint arXiv:1902.10486</span>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(44)</span>\n<span class=\"ltx_bibblock\">\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean.\n\n</span>\n<span class=\"ltx_bibblock\">Distilling the knowledge in a neural network.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib44.1.1\">arXiv preprint arXiv:1503.02531</span>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(45)</span>\n<span class=\"ltx_bibblock\">\nZhizhong Li and Derek Hoiem.\n\n</span>\n<span class=\"ltx_bibblock\">Learning without forgetting.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib45.1.1\">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 40(12):2935–2947, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(46)</span>\n<span class=\"ltx_bibblock\">\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.\n\n</span>\n<span class=\"ltx_bibblock\">Dark experience for general continual learning: a strong, simple baseline.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib46.1.1\">Advances in Neural Information Processing Systems</span>, 33:15920–15930, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(47)</span>\n<span class=\"ltx_bibblock\">\nDivyam Madaan, Hongxu Yin, Wonmin Byeon, Jan Kautz, and Pavlo Molchanov.\n\n</span>\n<span class=\"ltx_bibblock\">Heterogeneous continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib47.1.1\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 15985–15995, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(48)</span>\n<span class=\"ltx_bibblock\">\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang.\n\n</span>\n<span class=\"ltx_bibblock\">Lifelong learning with dynamically expandable networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib48.1.1\">International Conference on Learning Representations</span>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(49)</span>\n<span class=\"ltx_bibblock\">\nJoan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.\n\n</span>\n<span class=\"ltx_bibblock\">Overcoming catastrophic forgetting with hard attention to the task.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib49.1.1\">International Conference on Machine Learning</span>, pages 4548–4557. PMLR, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(50)</span>\n<span class=\"ltx_bibblock\">\nMustafa B Gurbuz and Constantine Dovrolis.\n\n</span>\n<span class=\"ltx_bibblock\">NISPA: Neuro-inspired stability-plasticity adaptation for continual learning in sparse networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib50.1.1\">International Conference on Machine Learning</span>, pages 8157–8174. PMLR, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(51)</span>\n<span class=\"ltx_bibblock\">\nHaeyong Kang, Rusty John Lloyd Mina, Sultan Rizky Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung Ju Hwang, and Chang D Yoo.\n\n</span>\n<span class=\"ltx_bibblock\">Forget-free continual learning with winning subnetworks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib51.1.1\">International Conference on Machine Learning</span>, pages 10734–10750. PMLR, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(52)</span>\n<span class=\"ltx_bibblock\">\nDavid Mayo, Tyler Scott, Mengye Ren, Gamaleldin Elsayed, Katherine Hermann, Matt Jones, and Michael Mozer.\n\n</span>\n<span class=\"ltx_bibblock\">Multitask learning via interleaving: A neural network investigation.\n\n</span>\n<span class=\"ltx_bibblock\">In M. Goldwater, F. K. Anggoro, B. K. Hayes, and D. C. Ong, editors, <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib52.1.1\">Proceedings of the 45th Annual Conference of the Cognitive Science Society</span>, volume 45, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(53)</span>\n<span class=\"ltx_bibblock\">\nAndrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Pascale Fung, and Zhiguang Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Continual learning in task-oriented dialogue systems.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib53.1.1\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 7452–7467, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(54)</span>\n<span class=\"ltx_bibblock\">\nChengwei Qin and Shafiq R. Joty.\n\n</span>\n<span class=\"ltx_bibblock\">LFPT5: A unified framework for lifelong few-shot language learning based on prompt tuning of T5.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib54.1.1\">International Conference on Learning Representations</span>. OpenReview.net, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(55)</span>\n<span class=\"ltx_bibblock\">\nAnastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi.\n\n</span>\n<span class=\"ltx_bibblock\">Progressive prompts: Continual learning for language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib55.1.1\">International Conference on Learning Representations</span>. OpenReview.net, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(56)</span>\n<span class=\"ltx_bibblock\">\nGuy Davidson and Michael C Mozer.\n\n</span>\n<span class=\"ltx_bibblock\">Sequential mastery of multiple visual tasks: Networks naturally learn to learn and forget to forget.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib56.1.1\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</span>, pages 9282–9293, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(57)</span>\n<span class=\"ltx_bibblock\">\nPaul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.\n\n</span>\n<span class=\"ltx_bibblock\">A simple baseline that questions the use of pretrained-models in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib57.1.1\">NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(58)</span>\n<span class=\"ltx_bibblock\">\nKuan-Ying Lee, Yuanyi Zhong, and Yu-Xiong Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Do pre-trained models benefit equally in continual learning?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib58.1.1\">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span>, pages 6485–6493, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(59)</span>\n<span class=\"ltx_bibblock\">\nEnrico Fini, Victor G Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised models are continual learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib59.1.1\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 9621–9630, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(60)</span>\n<span class=\"ltx_bibblock\">\nThomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan.\n\n</span>\n<span class=\"ltx_bibblock\">Fine-tuned language models are continual learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib60.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>, pages 6107–6122, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(61)</span>\n<span class=\"ltx_bibblock\">\nZixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.\n\n</span>\n<span class=\"ltx_bibblock\">Continual pre-training of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib61.1.1\">International Conference on Learning Representations</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(62)</span>\n<span class=\"ltx_bibblock\">\nMatt Jones, Tyler R. Scott, Mengye Ren, Gamaleldin Fathy Elsayed, Katherine L. Hermann, David Mayo, and Michael Curtis Mozer.\n\n</span>\n<span class=\"ltx_bibblock\">Learning in temporally structured environments.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib62.1.1\">International Conference on Learning Representations</span>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(63)</span>\n<span class=\"ltx_bibblock\">\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\n\n</span>\n<span class=\"ltx_bibblock\">Transformers: State-of-the-art natural language processing.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib63.1.1\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</span>, pages 38–45, Online, October 2020. Association for Computational Linguistics.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(64)</span>\n<span class=\"ltx_bibblock\">\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Language models are unsupervised multitask learners.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib64.1.1\">OpenAI blog</span>, 1(8):9, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(65)</span>\n<span class=\"ltx_bibblock\">\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.\n\n</span>\n<span class=\"ltx_bibblock\">Pointer sentinel mixture models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib65.1.1\">5th International Conference on Learning Representations, Toulon, France, April 24-26, 2017, Conference Track Proceedings</span>, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(66)</span>\n<span class=\"ltx_bibblock\">\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.\n\n</span>\n<span class=\"ltx_bibblock\">Attention is all you need.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib66.1.1\">Advances in Neural Information Processing Systems</span>, 30, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(67)</span>\n<span class=\"ltx_bibblock\">\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Emergent abilities of large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib67.1.1\">Transactions on Machine Learning Research</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(68)</span>\n<span class=\"ltx_bibblock\">\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Chain-of-thought prompting elicits reasoning in large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib68.1.1\">Advances in Neural Information Processing Systems</span>, 35:24824–24837, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(69)</span>\n<span class=\"ltx_bibblock\">\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Predictability and surprise in large generative models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib69.1.1\">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</span>, pages 1747–1764, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(70)</span>\n<span class=\"ltx_bibblock\">\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib70.1.1\">arXiv preprint arXiv:2206.04615</span>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(71)</span>\n<span class=\"ltx_bibblock\">\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Quantifying memorization across neural language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib71.1.1\">International Conference on Learning Representations</span>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(72)</span>\n<span class=\"ltx_bibblock\">\nA Emin Orhan.\n\n</span>\n<span class=\"ltx_bibblock\">Recognition, recall, and retention of few-shot memories in large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib72.1.1\">arXiv preprint arXiv:2303.17557</span>, 2023.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Experiment Details</h2>\n<section class=\"ltx_subsection\" id=\"A1.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>LLM Experiments</h3>\n<div class=\"ltx_para\" id=\"A1.SS1.p1\">\n<p class=\"ltx_p\" id=\"A1.SS1.p1.2\">We use the Huggingface Transformers Library <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib63\" title=\"\">63</a>]</cite> for fine-tuning the LLMs. The learning rate <math alttext=\"0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.1.m1.1\"><semantics id=\"A1.SS1.p1.1.m1.1a\"><mn id=\"A1.SS1.p1.1.m1.1.1\" xref=\"A1.SS1.p1.1.m1.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS1.p1.1.m1.1b\"><cn id=\"A1.SS1.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"A1.SS1.p1.1.m1.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS1.p1.1.m1.1c\">0.001</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS1.p1.1.m1.1d\">0.001</annotation></semantics></math> for vanilla gradient descent and <math alttext=\"0.00001\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS1.p1.2.m2.1\"><semantics id=\"A1.SS1.p1.2.m2.1a\"><mn id=\"A1.SS1.p1.2.m2.1.1\" xref=\"A1.SS1.p1.2.m2.1.1.cmml\">0.00001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS1.p1.2.m2.1b\"><cn id=\"A1.SS1.p1.2.m2.1.1.cmml\" type=\"float\" xref=\"A1.SS1.p1.2.m2.1.1\">0.00001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS1.p1.2.m2.1c\">0.00001</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS1.p1.2.m2.1d\">0.00001</annotation></semantics></math> for Adam. For all experiments we run 3 to 5 trials with different random seeds, except the results in Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> which are based on 20 seeds. The shaded area in the figures denotes standard deviation among trials and documents (for shift-averaged loss curves).</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Causal Image Modeling Experiments</h3>\n<section class=\"ltx_paragraph\" id=\"A1.SS2.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models</h4>\n<div class=\"ltx_para\" id=\"A1.SS2.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"A1.SS2.SSS0.Px1.p1.1\">Image GPT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">14</a>]</cite> is a GPT-2-like model trained to predict the next pixel value in an image. It is pre-trained on the Imagenet dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">19</a>]</cite> resized to 32x32. The Image GPT authors provide three pre-trained models of different sizes. In our experiments, we use the Image GPT-small and Image GPT-medium models.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A1.SS2.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets</h4>\n<div class=\"ltx_para\" id=\"A1.SS2.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"A1.SS2.SSS0.Px2.p1.1\">We use the CIFAR-10 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">18</a>]</cite> dataset for fine-tuning. For tokenization, the pixel RGB values are categorized into 512 pre-determined clusters with the nearest-neighbor classifier, as in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">14</a>]</cite>. After pre-processing, each image is transformed to a sequence of length 1024, with code book of size 512.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A1.SS2.SSS0.Px3\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training Setup</h4>\n<div class=\"ltx_para\" id=\"A1.SS2.SSS0.Px3.p1\">\n<p class=\"ltx_p\" id=\"A1.SS2.SSS0.Px3.p1.4\">We did not manage to sequentially fine-tune the model stably with the dropout layers, so the dropout layers are turned off during the Image GPT fine-tuning experiments. We use the Adam optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">20</a>]</cite> with learning rate <math alttext=\"0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px3.p1.1.m1.1\"><semantics id=\"A1.SS2.SSS0.Px3.p1.1.m1.1a\"><mn id=\"A1.SS2.SSS0.Px3.p1.1.m1.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.1.m1.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS2.SSS0.Px3.p1.1.m1.1b\"><cn id=\"A1.SS2.SSS0.Px3.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"A1.SS2.SSS0.Px3.p1.1.m1.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS2.SSS0.Px3.p1.1.m1.1c\">0.001</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS2.SSS0.Px3.p1.1.m1.1d\">0.001</annotation></semantics></math>. The default hyperparameters in the experiments are <math alttext=\"T=25\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px3.p1.2.m2.1\"><semantics id=\"A1.SS2.SSS0.Px3.p1.2.m2.1a\"><mrow id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.cmml\"><mi id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.2\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml\">T</mi><mo id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.3\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS2.SSS0.Px3.p1.2.m2.1b\"><apply id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1\"><eq id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.1\"></eq><ci id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.2\">𝑇</ci><cn id=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"A1.SS2.SSS0.Px3.p1.2.m2.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS2.SSS0.Px3.p1.2.m2.1c\">T=25</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS2.SSS0.Px3.p1.2.m2.1d\">italic_T = 25</annotation></semantics></math> images, <math alttext=\"M=10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px3.p1.3.m3.1\"><semantics id=\"A1.SS2.SSS0.Px3.p1.3.m3.1a\"><mrow id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.cmml\"><mi id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.2\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml\">M</mi><mo id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml\">=</mo><mn id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.3\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS2.SSS0.Px3.p1.3.m3.1b\"><apply id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1\"><eq id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.1\"></eq><ci id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.2\">𝑀</ci><cn id=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml\" type=\"integer\" xref=\"A1.SS2.SSS0.Px3.p1.3.m3.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS2.SSS0.Px3.p1.3.m3.1c\">M=10</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS2.SSS0.Px3.p1.3.m3.1d\">italic_M = 10</annotation></semantics></math> gradient update steps, <math alttext=\"E=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS2.SSS0.Px3.p1.4.m4.1\"><semantics id=\"A1.SS2.SSS0.Px3.p1.4.m4.1a\"><mrow id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.cmml\"><mi id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.2\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.2.cmml\">E</mi><mo id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.1\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.1.cmml\">=</mo><mn id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.3\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS2.SSS0.Px3.p1.4.m4.1b\"><apply id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1\"><eq id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.1.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.1\"></eq><ci id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.2.cmml\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.2\">𝐸</ci><cn id=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.3.cmml\" type=\"integer\" xref=\"A1.SS2.SSS0.Px3.p1.4.m4.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS2.SSS0.Px3.p1.4.m4.1c\">E=5</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS2.SSS0.Px3.p1.4.m4.1d\">italic_E = 5</annotation></semantics></math> epochs. Same as the LLM experiments, we use the average cross-entropy loss as our evaluation metric.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>Image Classification Experiments</h3>\n<div class=\"ltx_para\" id=\"A1.SS3.p1\">\n<p class=\"ltx_p\" id=\"A1.SS3.p1.2\">The images are resized to 256x256 followed by a center crop of 224x224. We use the Adam optimizer with learning rate <math alttext=\"0.0001\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.1.m1.1\"><semantics id=\"A1.SS3.p1.1.m1.1a\"><mn id=\"A1.SS3.p1.1.m1.1.1\" xref=\"A1.SS3.p1.1.m1.1.1.cmml\">0.0001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.1.m1.1b\"><cn id=\"A1.SS3.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"A1.SS3.p1.1.m1.1.1\">0.0001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.1.m1.1c\">0.0001</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS3.p1.1.m1.1d\">0.0001</annotation></semantics></math> for <math alttext=\"M=10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS3.p1.2.m2.1\"><semantics id=\"A1.SS3.p1.2.m2.1a\"><mrow id=\"A1.SS3.p1.2.m2.1.1\" xref=\"A1.SS3.p1.2.m2.1.1.cmml\"><mi id=\"A1.SS3.p1.2.m2.1.1.2\" xref=\"A1.SS3.p1.2.m2.1.1.2.cmml\">M</mi><mo id=\"A1.SS3.p1.2.m2.1.1.1\" xref=\"A1.SS3.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"A1.SS3.p1.2.m2.1.1.3\" xref=\"A1.SS3.p1.2.m2.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.2.m2.1b\"><apply id=\"A1.SS3.p1.2.m2.1.1.cmml\" xref=\"A1.SS3.p1.2.m2.1.1\"><eq id=\"A1.SS3.p1.2.m2.1.1.1.cmml\" xref=\"A1.SS3.p1.2.m2.1.1.1\"></eq><ci id=\"A1.SS3.p1.2.m2.1.1.2.cmml\" xref=\"A1.SS3.p1.2.m2.1.1.2\">𝑀</ci><cn id=\"A1.SS3.p1.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"A1.SS3.p1.2.m2.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.2.m2.1c\">M=10</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS3.p1.2.m2.1d\">italic_M = 10</annotation></semantics></math> gradient steps on each batch of images.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.4 </span>Shift-averaged Loss Calculation</h3>\n<div class=\"ltx_para\" id=\"A1.SS4.p1\">\n<p class=\"ltx_p\" id=\"A1.SS4.p1.1\">The shift-averaged loss curves plotted in Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b) are calculated by replicating Figure <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a) on each document in the training sequence, re-aligning these curves so that 0 on the x-axis always represents the moment before the first occurrence of the focal document, and average them. For example, if the length of the sequence is 50, then for training epoch 0.5 on the x-axis, we take the loss of document 1 after training on document 25; the loss of document 2 after training on document 26; …; the loss of document 50 after training on document 24 of the next epoch; and average these losses. Subsequent figures (Figure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>-<a class=\"ltx_ref\" href=\"#S3.F7\" title=\"Figure 7 ‣ Summary. ‣ 3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, <a class=\"ltx_ref\" href=\"#A2.F12\" title=\"Figure 12 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>-<a class=\"ltx_ref\" href=\"#A2.F19\" title=\"Figure 19 ‣ B.7 Effect of Inserting Random Documents after the Repeating Sequence ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">19</span></a>) are plotted with the same approach.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.5 </span>Computational Toy Model</h3>\n<div class=\"ltx_para\" id=\"A1.SS5.p1\">\n<p class=\"ltx_p\" id=\"A1.SS5.p1.14\">For Figure <a class=\"ltx_ref\" href=\"#S4.F10.sf1\" title=\"Figure 10(a) ‣ Figure 10 ‣ 4.1 Temporal Structure of Gradients ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">10(a)</span></a>, we pick <math alttext=\"f_{i}(\\bm{w})=\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.1.m1.1\"><semantics id=\"A1.SS5.p1.1.m1.1a\"><mrow id=\"A1.SS5.p1.1.m1.1.2\" xref=\"A1.SS5.p1.1.m1.1.2.cmml\"><mrow id=\"A1.SS5.p1.1.m1.1.2.2\" xref=\"A1.SS5.p1.1.m1.1.2.2.cmml\"><msub id=\"A1.SS5.p1.1.m1.1.2.2.2\" xref=\"A1.SS5.p1.1.m1.1.2.2.2.cmml\"><mi id=\"A1.SS5.p1.1.m1.1.2.2.2.2\" xref=\"A1.SS5.p1.1.m1.1.2.2.2.2.cmml\">f</mi><mi id=\"A1.SS5.p1.1.m1.1.2.2.2.3\" xref=\"A1.SS5.p1.1.m1.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"A1.SS5.p1.1.m1.1.2.2.1\" xref=\"A1.SS5.p1.1.m1.1.2.2.1.cmml\">⁢</mo><mrow id=\"A1.SS5.p1.1.m1.1.2.2.3.2\" xref=\"A1.SS5.p1.1.m1.1.2.2.cmml\"><mo id=\"A1.SS5.p1.1.m1.1.2.2.3.2.1\" stretchy=\"false\" xref=\"A1.SS5.p1.1.m1.1.2.2.cmml\">(</mo><mi id=\"A1.SS5.p1.1.m1.1.1\" xref=\"A1.SS5.p1.1.m1.1.1.cmml\">𝒘</mi><mo id=\"A1.SS5.p1.1.m1.1.2.2.3.2.2\" stretchy=\"false\" xref=\"A1.SS5.p1.1.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"A1.SS5.p1.1.m1.1.2.1\" xref=\"A1.SS5.p1.1.m1.1.2.1.cmml\">=</mo><mi id=\"A1.SS5.p1.1.m1.1.2.3\" xref=\"A1.SS5.p1.1.m1.1.2.3.cmml\">𝒘</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.1.m1.1b\"><apply id=\"A1.SS5.p1.1.m1.1.2.cmml\" xref=\"A1.SS5.p1.1.m1.1.2\"><eq id=\"A1.SS5.p1.1.m1.1.2.1.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.1\"></eq><apply id=\"A1.SS5.p1.1.m1.1.2.2.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.2\"><times id=\"A1.SS5.p1.1.m1.1.2.2.1.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.2.1\"></times><apply id=\"A1.SS5.p1.1.m1.1.2.2.2.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.1.m1.1.2.2.2.1.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.2.2\">subscript</csymbol><ci id=\"A1.SS5.p1.1.m1.1.2.2.2.2.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.2.2.2\">𝑓</ci><ci id=\"A1.SS5.p1.1.m1.1.2.2.2.3.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.2.2.3\">𝑖</ci></apply><ci id=\"A1.SS5.p1.1.m1.1.1.cmml\" xref=\"A1.SS5.p1.1.m1.1.1\">𝒘</ci></apply><ci id=\"A1.SS5.p1.1.m1.1.2.3.cmml\" xref=\"A1.SS5.p1.1.m1.1.2.3\">𝒘</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.1.m1.1c\">f_{i}(\\bm{w})=\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.1.m1.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_w</annotation></semantics></math>, and each data point <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.2.m2.1\"><semantics id=\"A1.SS5.p1.2.m2.1a\"><msub id=\"A1.SS5.p1.2.m2.1.1\" xref=\"A1.SS5.p1.2.m2.1.1.cmml\"><mi id=\"A1.SS5.p1.2.m2.1.1.2\" xref=\"A1.SS5.p1.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"A1.SS5.p1.2.m2.1.1.3\" xref=\"A1.SS5.p1.2.m2.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.2.m2.1b\"><apply id=\"A1.SS5.p1.2.m2.1.1.cmml\" xref=\"A1.SS5.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.2.m2.1.1.1.cmml\" xref=\"A1.SS5.p1.2.m2.1.1\">subscript</csymbol><ci id=\"A1.SS5.p1.2.m2.1.1.2.cmml\" xref=\"A1.SS5.p1.2.m2.1.1.2\">𝒙</ci><ci id=\"A1.SS5.p1.2.m2.1.1.3.cmml\" xref=\"A1.SS5.p1.2.m2.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.2.m2.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.3.m3.1\"><semantics id=\"A1.SS5.p1.3.m3.1a\"><mi id=\"A1.SS5.p1.3.m3.1.1\" xref=\"A1.SS5.p1.3.m3.1.1.cmml\">𝒘</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.3.m3.1b\"><ci id=\"A1.SS5.p1.3.m3.1.1.cmml\" xref=\"A1.SS5.p1.3.m3.1.1\">𝒘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.3.m3.1c\">\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.3.m3.1d\">bold_italic_w</annotation></semantics></math> is a vector of length <math alttext=\"N=M=1000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.4.m4.1\"><semantics id=\"A1.SS5.p1.4.m4.1a\"><mrow id=\"A1.SS5.p1.4.m4.1.1\" xref=\"A1.SS5.p1.4.m4.1.1.cmml\"><mi id=\"A1.SS5.p1.4.m4.1.1.2\" xref=\"A1.SS5.p1.4.m4.1.1.2.cmml\">N</mi><mo id=\"A1.SS5.p1.4.m4.1.1.3\" xref=\"A1.SS5.p1.4.m4.1.1.3.cmml\">=</mo><mi id=\"A1.SS5.p1.4.m4.1.1.4\" xref=\"A1.SS5.p1.4.m4.1.1.4.cmml\">M</mi><mo id=\"A1.SS5.p1.4.m4.1.1.5\" xref=\"A1.SS5.p1.4.m4.1.1.5.cmml\">=</mo><mn id=\"A1.SS5.p1.4.m4.1.1.6\" xref=\"A1.SS5.p1.4.m4.1.1.6.cmml\">1000</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.4.m4.1b\"><apply id=\"A1.SS5.p1.4.m4.1.1.cmml\" xref=\"A1.SS5.p1.4.m4.1.1\"><and id=\"A1.SS5.p1.4.m4.1.1a.cmml\" xref=\"A1.SS5.p1.4.m4.1.1\"></and><apply id=\"A1.SS5.p1.4.m4.1.1b.cmml\" xref=\"A1.SS5.p1.4.m4.1.1\"><eq id=\"A1.SS5.p1.4.m4.1.1.3.cmml\" xref=\"A1.SS5.p1.4.m4.1.1.3\"></eq><ci id=\"A1.SS5.p1.4.m4.1.1.2.cmml\" xref=\"A1.SS5.p1.4.m4.1.1.2\">𝑁</ci><ci id=\"A1.SS5.p1.4.m4.1.1.4.cmml\" xref=\"A1.SS5.p1.4.m4.1.1.4\">𝑀</ci></apply><apply id=\"A1.SS5.p1.4.m4.1.1c.cmml\" xref=\"A1.SS5.p1.4.m4.1.1\"><eq id=\"A1.SS5.p1.4.m4.1.1.5.cmml\" xref=\"A1.SS5.p1.4.m4.1.1.5\"></eq><share href=\"#A1.SS5.p1.4.m4.1.1.4.cmml\" id=\"A1.SS5.p1.4.m4.1.1d.cmml\" xref=\"A1.SS5.p1.4.m4.1.1\"></share><cn id=\"A1.SS5.p1.4.m4.1.1.6.cmml\" type=\"integer\" xref=\"A1.SS5.p1.4.m4.1.1.6\">1000</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.4.m4.1c\">N=M=1000</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.4.m4.1d\">italic_N = italic_M = 1000</annotation></semantics></math>. We have <math alttext=\"T=25\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.5.m5.1\"><semantics id=\"A1.SS5.p1.5.m5.1a\"><mrow id=\"A1.SS5.p1.5.m5.1.1\" xref=\"A1.SS5.p1.5.m5.1.1.cmml\"><mi id=\"A1.SS5.p1.5.m5.1.1.2\" xref=\"A1.SS5.p1.5.m5.1.1.2.cmml\">T</mi><mo id=\"A1.SS5.p1.5.m5.1.1.1\" xref=\"A1.SS5.p1.5.m5.1.1.1.cmml\">=</mo><mn id=\"A1.SS5.p1.5.m5.1.1.3\" xref=\"A1.SS5.p1.5.m5.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.5.m5.1b\"><apply id=\"A1.SS5.p1.5.m5.1.1.cmml\" xref=\"A1.SS5.p1.5.m5.1.1\"><eq id=\"A1.SS5.p1.5.m5.1.1.1.cmml\" xref=\"A1.SS5.p1.5.m5.1.1.1\"></eq><ci id=\"A1.SS5.p1.5.m5.1.1.2.cmml\" xref=\"A1.SS5.p1.5.m5.1.1.2\">𝑇</ci><cn id=\"A1.SS5.p1.5.m5.1.1.3.cmml\" type=\"integer\" xref=\"A1.SS5.p1.5.m5.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.5.m5.1c\">T=25</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.5.m5.1d\">italic_T = 25</annotation></semantics></math> data points and use the vanilla gradient descent optimizer with learning rate 0.01. The projection matrix is initialized with every entry sampled independently from <math alttext=\"\\mathcal{N}(0,1/N^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.6.m6.2\"><semantics id=\"A1.SS5.p1.6.m6.2a\"><mrow id=\"A1.SS5.p1.6.m6.2.2\" xref=\"A1.SS5.p1.6.m6.2.2.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"A1.SS5.p1.6.m6.2.2.3\" xref=\"A1.SS5.p1.6.m6.2.2.3.cmml\">𝒩</mi><mo id=\"A1.SS5.p1.6.m6.2.2.2\" xref=\"A1.SS5.p1.6.m6.2.2.2.cmml\">⁢</mo><mrow id=\"A1.SS5.p1.6.m6.2.2.1.1\" xref=\"A1.SS5.p1.6.m6.2.2.1.2.cmml\"><mo id=\"A1.SS5.p1.6.m6.2.2.1.1.2\" stretchy=\"false\" xref=\"A1.SS5.p1.6.m6.2.2.1.2.cmml\">(</mo><mn id=\"A1.SS5.p1.6.m6.1.1\" xref=\"A1.SS5.p1.6.m6.1.1.cmml\">0</mn><mo id=\"A1.SS5.p1.6.m6.2.2.1.1.3\" xref=\"A1.SS5.p1.6.m6.2.2.1.2.cmml\">,</mo><mrow id=\"A1.SS5.p1.6.m6.2.2.1.1.1\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.cmml\"><mn id=\"A1.SS5.p1.6.m6.2.2.1.1.1.2\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.2.cmml\">1</mn><mo id=\"A1.SS5.p1.6.m6.2.2.1.1.1.1\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.1.cmml\">/</mo><msup id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.cmml\"><mi id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.2\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.2.cmml\">N</mi><mn id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.3\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.3.cmml\">2</mn></msup></mrow><mo id=\"A1.SS5.p1.6.m6.2.2.1.1.4\" stretchy=\"false\" xref=\"A1.SS5.p1.6.m6.2.2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.6.m6.2b\"><apply id=\"A1.SS5.p1.6.m6.2.2.cmml\" xref=\"A1.SS5.p1.6.m6.2.2\"><times id=\"A1.SS5.p1.6.m6.2.2.2.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.2\"></times><ci id=\"A1.SS5.p1.6.m6.2.2.3.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.3\">𝒩</ci><interval closure=\"open\" id=\"A1.SS5.p1.6.m6.2.2.1.2.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.1.1\"><cn id=\"A1.SS5.p1.6.m6.1.1.cmml\" type=\"integer\" xref=\"A1.SS5.p1.6.m6.1.1\">0</cn><apply id=\"A1.SS5.p1.6.m6.2.2.1.1.1.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1\"><divide id=\"A1.SS5.p1.6.m6.2.2.1.1.1.1.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.1\"></divide><cn id=\"A1.SS5.p1.6.m6.2.2.1.1.1.2.cmml\" type=\"integer\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.2\">1</cn><apply id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.1.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3\">superscript</csymbol><ci id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.2.cmml\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.2\">𝑁</ci><cn id=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.3.cmml\" type=\"integer\" xref=\"A1.SS5.p1.6.m6.2.2.1.1.1.3.3\">2</cn></apply></apply></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.6.m6.2c\">\\mathcal{N}(0,1/N^{2})</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.6.m6.2d\">caligraphic_N ( 0 , 1 / italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>. Each entry of the data points <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.7.m7.1\"><semantics id=\"A1.SS5.p1.7.m7.1a\"><msub id=\"A1.SS5.p1.7.m7.1.1\" xref=\"A1.SS5.p1.7.m7.1.1.cmml\"><mi id=\"A1.SS5.p1.7.m7.1.1.2\" xref=\"A1.SS5.p1.7.m7.1.1.2.cmml\">𝒙</mi><mi id=\"A1.SS5.p1.7.m7.1.1.3\" xref=\"A1.SS5.p1.7.m7.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.7.m7.1b\"><apply id=\"A1.SS5.p1.7.m7.1.1.cmml\" xref=\"A1.SS5.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.7.m7.1.1.1.cmml\" xref=\"A1.SS5.p1.7.m7.1.1\">subscript</csymbol><ci id=\"A1.SS5.p1.7.m7.1.1.2.cmml\" xref=\"A1.SS5.p1.7.m7.1.1.2\">𝒙</ci><ci id=\"A1.SS5.p1.7.m7.1.1.3.cmml\" xref=\"A1.SS5.p1.7.m7.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.7.m7.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.7.m7.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.8.m8.1\"><semantics id=\"A1.SS5.p1.8.m8.1a\"><mi id=\"A1.SS5.p1.8.m8.1.1\" xref=\"A1.SS5.p1.8.m8.1.1.cmml\">𝒘</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.8.m8.1b\"><ci id=\"A1.SS5.p1.8.m8.1.1.cmml\" xref=\"A1.SS5.p1.8.m8.1.1\">𝒘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.8.m8.1c\">\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.8.m8.1d\">bold_italic_w</annotation></semantics></math> is sampled independently from <math alttext=\"{\\rm Unif}(-1,1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.9.m9.2\"><semantics id=\"A1.SS5.p1.9.m9.2a\"><mrow id=\"A1.SS5.p1.9.m9.2.2\" xref=\"A1.SS5.p1.9.m9.2.2.cmml\"><mi id=\"A1.SS5.p1.9.m9.2.2.3\" xref=\"A1.SS5.p1.9.m9.2.2.3.cmml\">Unif</mi><mo id=\"A1.SS5.p1.9.m9.2.2.2\" xref=\"A1.SS5.p1.9.m9.2.2.2.cmml\">⁢</mo><mrow id=\"A1.SS5.p1.9.m9.2.2.1.1\" xref=\"A1.SS5.p1.9.m9.2.2.1.2.cmml\"><mo id=\"A1.SS5.p1.9.m9.2.2.1.1.2\" stretchy=\"false\" xref=\"A1.SS5.p1.9.m9.2.2.1.2.cmml\">(</mo><mrow id=\"A1.SS5.p1.9.m9.2.2.1.1.1\" xref=\"A1.SS5.p1.9.m9.2.2.1.1.1.cmml\"><mo id=\"A1.SS5.p1.9.m9.2.2.1.1.1a\" xref=\"A1.SS5.p1.9.m9.2.2.1.1.1.cmml\">−</mo><mn id=\"A1.SS5.p1.9.m9.2.2.1.1.1.2\" xref=\"A1.SS5.p1.9.m9.2.2.1.1.1.2.cmml\">1</mn></mrow><mo id=\"A1.SS5.p1.9.m9.2.2.1.1.3\" xref=\"A1.SS5.p1.9.m9.2.2.1.2.cmml\">,</mo><mn id=\"A1.SS5.p1.9.m9.1.1\" xref=\"A1.SS5.p1.9.m9.1.1.cmml\">1</mn><mo id=\"A1.SS5.p1.9.m9.2.2.1.1.4\" stretchy=\"false\" xref=\"A1.SS5.p1.9.m9.2.2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.9.m9.2b\"><apply id=\"A1.SS5.p1.9.m9.2.2.cmml\" xref=\"A1.SS5.p1.9.m9.2.2\"><times id=\"A1.SS5.p1.9.m9.2.2.2.cmml\" xref=\"A1.SS5.p1.9.m9.2.2.2\"></times><ci id=\"A1.SS5.p1.9.m9.2.2.3.cmml\" xref=\"A1.SS5.p1.9.m9.2.2.3\">Unif</ci><interval closure=\"open\" id=\"A1.SS5.p1.9.m9.2.2.1.2.cmml\" xref=\"A1.SS5.p1.9.m9.2.2.1.1\"><apply id=\"A1.SS5.p1.9.m9.2.2.1.1.1.cmml\" xref=\"A1.SS5.p1.9.m9.2.2.1.1.1\"><minus id=\"A1.SS5.p1.9.m9.2.2.1.1.1.1.cmml\" xref=\"A1.SS5.p1.9.m9.2.2.1.1.1\"></minus><cn id=\"A1.SS5.p1.9.m9.2.2.1.1.1.2.cmml\" type=\"integer\" xref=\"A1.SS5.p1.9.m9.2.2.1.1.1.2\">1</cn></apply><cn id=\"A1.SS5.p1.9.m9.1.1.cmml\" type=\"integer\" xref=\"A1.SS5.p1.9.m9.1.1\">1</cn></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.9.m9.2c\">{\\rm Unif}(-1,1)</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.9.m9.2d\">roman_Unif ( - 1 , 1 )</annotation></semantics></math>. For Figure <a class=\"ltx_ref\" href=\"#S4.F10.sf2\" title=\"Figure 10(b) ‣ Figure 10 ‣ 4.1 Temporal Structure of Gradients ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">10(b)</span></a>, we pick <math alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.10.m10.1\"><semantics id=\"A1.SS5.p1.10.m10.1a\"><mrow id=\"A1.SS5.p1.10.m10.1.2\" xref=\"A1.SS5.p1.10.m10.1.2.cmml\"><mrow id=\"A1.SS5.p1.10.m10.1.2.2\" xref=\"A1.SS5.p1.10.m10.1.2.2.cmml\"><msub id=\"A1.SS5.p1.10.m10.1.2.2.2\" xref=\"A1.SS5.p1.10.m10.1.2.2.2.cmml\"><mi id=\"A1.SS5.p1.10.m10.1.2.2.2.2\" xref=\"A1.SS5.p1.10.m10.1.2.2.2.2.cmml\">f</mi><mi id=\"A1.SS5.p1.10.m10.1.2.2.2.3\" xref=\"A1.SS5.p1.10.m10.1.2.2.2.3.cmml\">i</mi></msub><mo id=\"A1.SS5.p1.10.m10.1.2.2.1\" xref=\"A1.SS5.p1.10.m10.1.2.2.1.cmml\">⁢</mo><mrow id=\"A1.SS5.p1.10.m10.1.2.2.3.2\" xref=\"A1.SS5.p1.10.m10.1.2.2.cmml\"><mo id=\"A1.SS5.p1.10.m10.1.2.2.3.2.1\" stretchy=\"false\" xref=\"A1.SS5.p1.10.m10.1.2.2.cmml\">(</mo><mi id=\"A1.SS5.p1.10.m10.1.1\" xref=\"A1.SS5.p1.10.m10.1.1.cmml\">𝒘</mi><mo id=\"A1.SS5.p1.10.m10.1.2.2.3.2.2\" stretchy=\"false\" xref=\"A1.SS5.p1.10.m10.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"A1.SS5.p1.10.m10.1.2.1\" xref=\"A1.SS5.p1.10.m10.1.2.1.cmml\">=</mo><mrow id=\"A1.SS5.p1.10.m10.1.2.3\" xref=\"A1.SS5.p1.10.m10.1.2.3.cmml\"><msub id=\"A1.SS5.p1.10.m10.1.2.3.2\" xref=\"A1.SS5.p1.10.m10.1.2.3.2.cmml\"><mi id=\"A1.SS5.p1.10.m10.1.2.3.2.2\" xref=\"A1.SS5.p1.10.m10.1.2.3.2.2.cmml\">𝒚</mi><mi id=\"A1.SS5.p1.10.m10.1.2.3.2.3\" xref=\"A1.SS5.p1.10.m10.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"A1.SS5.p1.10.m10.1.2.3.1\" xref=\"A1.SS5.p1.10.m10.1.2.3.1.cmml\">−</mo><mi id=\"A1.SS5.p1.10.m10.1.2.3.3\" xref=\"A1.SS5.p1.10.m10.1.2.3.3.cmml\">𝒘</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.10.m10.1b\"><apply id=\"A1.SS5.p1.10.m10.1.2.cmml\" xref=\"A1.SS5.p1.10.m10.1.2\"><eq id=\"A1.SS5.p1.10.m10.1.2.1.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.1\"></eq><apply id=\"A1.SS5.p1.10.m10.1.2.2.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.2\"><times id=\"A1.SS5.p1.10.m10.1.2.2.1.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.2.1\"></times><apply id=\"A1.SS5.p1.10.m10.1.2.2.2.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.10.m10.1.2.2.2.1.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.2.2\">subscript</csymbol><ci id=\"A1.SS5.p1.10.m10.1.2.2.2.2.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.2.2.2\">𝑓</ci><ci id=\"A1.SS5.p1.10.m10.1.2.2.2.3.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.2.2.3\">𝑖</ci></apply><ci id=\"A1.SS5.p1.10.m10.1.1.cmml\" xref=\"A1.SS5.p1.10.m10.1.1\">𝒘</ci></apply><apply id=\"A1.SS5.p1.10.m10.1.2.3.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3\"><minus id=\"A1.SS5.p1.10.m10.1.2.3.1.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3.1\"></minus><apply id=\"A1.SS5.p1.10.m10.1.2.3.2.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.10.m10.1.2.3.2.1.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3.2\">subscript</csymbol><ci id=\"A1.SS5.p1.10.m10.1.2.3.2.2.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3.2.2\">𝒚</ci><ci id=\"A1.SS5.p1.10.m10.1.2.3.2.3.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3.2.3\">𝑖</ci></apply><ci id=\"A1.SS5.p1.10.m10.1.2.3.3.cmml\" xref=\"A1.SS5.p1.10.m10.1.2.3.3\">𝒘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.10.m10.1c\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.10.m10.1d\">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_w ) = bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - bold_italic_w</annotation></semantics></math>, <math alttext=\"N=M=100\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.11.m11.1\"><semantics id=\"A1.SS5.p1.11.m11.1a\"><mrow id=\"A1.SS5.p1.11.m11.1.1\" xref=\"A1.SS5.p1.11.m11.1.1.cmml\"><mi id=\"A1.SS5.p1.11.m11.1.1.2\" xref=\"A1.SS5.p1.11.m11.1.1.2.cmml\">N</mi><mo id=\"A1.SS5.p1.11.m11.1.1.3\" xref=\"A1.SS5.p1.11.m11.1.1.3.cmml\">=</mo><mi id=\"A1.SS5.p1.11.m11.1.1.4\" xref=\"A1.SS5.p1.11.m11.1.1.4.cmml\">M</mi><mo id=\"A1.SS5.p1.11.m11.1.1.5\" xref=\"A1.SS5.p1.11.m11.1.1.5.cmml\">=</mo><mn id=\"A1.SS5.p1.11.m11.1.1.6\" xref=\"A1.SS5.p1.11.m11.1.1.6.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.11.m11.1b\"><apply id=\"A1.SS5.p1.11.m11.1.1.cmml\" xref=\"A1.SS5.p1.11.m11.1.1\"><and id=\"A1.SS5.p1.11.m11.1.1a.cmml\" xref=\"A1.SS5.p1.11.m11.1.1\"></and><apply id=\"A1.SS5.p1.11.m11.1.1b.cmml\" xref=\"A1.SS5.p1.11.m11.1.1\"><eq id=\"A1.SS5.p1.11.m11.1.1.3.cmml\" xref=\"A1.SS5.p1.11.m11.1.1.3\"></eq><ci id=\"A1.SS5.p1.11.m11.1.1.2.cmml\" xref=\"A1.SS5.p1.11.m11.1.1.2\">𝑁</ci><ci id=\"A1.SS5.p1.11.m11.1.1.4.cmml\" xref=\"A1.SS5.p1.11.m11.1.1.4\">𝑀</ci></apply><apply id=\"A1.SS5.p1.11.m11.1.1c.cmml\" xref=\"A1.SS5.p1.11.m11.1.1\"><eq id=\"A1.SS5.p1.11.m11.1.1.5.cmml\" xref=\"A1.SS5.p1.11.m11.1.1.5\"></eq><share href=\"#A1.SS5.p1.11.m11.1.1.4.cmml\" id=\"A1.SS5.p1.11.m11.1.1d.cmml\" xref=\"A1.SS5.p1.11.m11.1.1\"></share><cn id=\"A1.SS5.p1.11.m11.1.1.6.cmml\" type=\"integer\" xref=\"A1.SS5.p1.11.m11.1.1.6\">100</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.11.m11.1c\">N=M=100</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.11.m11.1d\">italic_N = italic_M = 100</annotation></semantics></math>, <math alttext=\"T=25\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.12.m12.1\"><semantics id=\"A1.SS5.p1.12.m12.1a\"><mrow id=\"A1.SS5.p1.12.m12.1.1\" xref=\"A1.SS5.p1.12.m12.1.1.cmml\"><mi id=\"A1.SS5.p1.12.m12.1.1.2\" xref=\"A1.SS5.p1.12.m12.1.1.2.cmml\">T</mi><mo id=\"A1.SS5.p1.12.m12.1.1.1\" xref=\"A1.SS5.p1.12.m12.1.1.1.cmml\">=</mo><mn id=\"A1.SS5.p1.12.m12.1.1.3\" xref=\"A1.SS5.p1.12.m12.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.12.m12.1b\"><apply id=\"A1.SS5.p1.12.m12.1.1.cmml\" xref=\"A1.SS5.p1.12.m12.1.1\"><eq id=\"A1.SS5.p1.12.m12.1.1.1.cmml\" xref=\"A1.SS5.p1.12.m12.1.1.1\"></eq><ci id=\"A1.SS5.p1.12.m12.1.1.2.cmml\" xref=\"A1.SS5.p1.12.m12.1.1.2\">𝑇</ci><cn id=\"A1.SS5.p1.12.m12.1.1.3.cmml\" type=\"integer\" xref=\"A1.SS5.p1.12.m12.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.12.m12.1c\">T=25</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.12.m12.1d\">italic_T = 25</annotation></semantics></math>, and learning rate 0.01. Each entry of <math alttext=\"\\bm{y}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.13.m13.1\"><semantics id=\"A1.SS5.p1.13.m13.1a\"><msub id=\"A1.SS5.p1.13.m13.1.1\" xref=\"A1.SS5.p1.13.m13.1.1.cmml\"><mi id=\"A1.SS5.p1.13.m13.1.1.2\" xref=\"A1.SS5.p1.13.m13.1.1.2.cmml\">𝒚</mi><mi id=\"A1.SS5.p1.13.m13.1.1.3\" xref=\"A1.SS5.p1.13.m13.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.13.m13.1b\"><apply id=\"A1.SS5.p1.13.m13.1.1.cmml\" xref=\"A1.SS5.p1.13.m13.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.SS5.p1.13.m13.1.1.1.cmml\" xref=\"A1.SS5.p1.13.m13.1.1\">subscript</csymbol><ci id=\"A1.SS5.p1.13.m13.1.1.2.cmml\" xref=\"A1.SS5.p1.13.m13.1.1.2\">𝒚</ci><ci id=\"A1.SS5.p1.13.m13.1.1.3.cmml\" xref=\"A1.SS5.p1.13.m13.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.13.m13.1c\">\\bm{y}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.13.m13.1d\">bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is also sampled independently from <math alttext=\"{\\rm Unif}(-1,1)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS5.p1.14.m14.2\"><semantics id=\"A1.SS5.p1.14.m14.2a\"><mrow id=\"A1.SS5.p1.14.m14.2.2\" xref=\"A1.SS5.p1.14.m14.2.2.cmml\"><mi id=\"A1.SS5.p1.14.m14.2.2.3\" xref=\"A1.SS5.p1.14.m14.2.2.3.cmml\">Unif</mi><mo id=\"A1.SS5.p1.14.m14.2.2.2\" xref=\"A1.SS5.p1.14.m14.2.2.2.cmml\">⁢</mo><mrow id=\"A1.SS5.p1.14.m14.2.2.1.1\" xref=\"A1.SS5.p1.14.m14.2.2.1.2.cmml\"><mo id=\"A1.SS5.p1.14.m14.2.2.1.1.2\" stretchy=\"false\" xref=\"A1.SS5.p1.14.m14.2.2.1.2.cmml\">(</mo><mrow id=\"A1.SS5.p1.14.m14.2.2.1.1.1\" xref=\"A1.SS5.p1.14.m14.2.2.1.1.1.cmml\"><mo id=\"A1.SS5.p1.14.m14.2.2.1.1.1a\" xref=\"A1.SS5.p1.14.m14.2.2.1.1.1.cmml\">−</mo><mn id=\"A1.SS5.p1.14.m14.2.2.1.1.1.2\" xref=\"A1.SS5.p1.14.m14.2.2.1.1.1.2.cmml\">1</mn></mrow><mo id=\"A1.SS5.p1.14.m14.2.2.1.1.3\" xref=\"A1.SS5.p1.14.m14.2.2.1.2.cmml\">,</mo><mn id=\"A1.SS5.p1.14.m14.1.1\" xref=\"A1.SS5.p1.14.m14.1.1.cmml\">1</mn><mo id=\"A1.SS5.p1.14.m14.2.2.1.1.4\" stretchy=\"false\" xref=\"A1.SS5.p1.14.m14.2.2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS5.p1.14.m14.2b\"><apply id=\"A1.SS5.p1.14.m14.2.2.cmml\" xref=\"A1.SS5.p1.14.m14.2.2\"><times id=\"A1.SS5.p1.14.m14.2.2.2.cmml\" xref=\"A1.SS5.p1.14.m14.2.2.2\"></times><ci id=\"A1.SS5.p1.14.m14.2.2.3.cmml\" xref=\"A1.SS5.p1.14.m14.2.2.3\">Unif</ci><interval closure=\"open\" id=\"A1.SS5.p1.14.m14.2.2.1.2.cmml\" xref=\"A1.SS5.p1.14.m14.2.2.1.1\"><apply id=\"A1.SS5.p1.14.m14.2.2.1.1.1.cmml\" xref=\"A1.SS5.p1.14.m14.2.2.1.1.1\"><minus id=\"A1.SS5.p1.14.m14.2.2.1.1.1.1.cmml\" xref=\"A1.SS5.p1.14.m14.2.2.1.1.1\"></minus><cn id=\"A1.SS5.p1.14.m14.2.2.1.1.1.2.cmml\" type=\"integer\" xref=\"A1.SS5.p1.14.m14.2.2.1.1.1.2\">1</cn></apply><cn id=\"A1.SS5.p1.14.m14.1.1.cmml\" type=\"integer\" xref=\"A1.SS5.p1.14.m14.1.1\">1</cn></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS5.p1.14.m14.2c\">{\\rm Unif}(-1,1)</annotation><annotation encoding=\"application/x-llamapun\" id=\"A1.SS5.p1.14.m14.2d\">roman_Unif ( - 1 , 1 )</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A1.SS6\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.6 </span>Compute Resources</h3>\n<div class=\"ltx_para\" id=\"A1.SS6.p1\">\n<p class=\"ltx_p\" id=\"A1.SS6.p1.1\">Each experiment presented in the paper is run with one NVIDIA A100 GPU, 2 CPUs, and 32GB of RAM. The training time highly depends on the hyperparameter choices, especially model size and number of gradient steps. The longest fine-tuning experiment with 20 gradient steps per episode on the Pythia-1B model takes roughly 30 minutes under this setup. The minimal compute resource requirement needed to reproduce the experiments with a Pythia-1B model is one GPU with 16GB of GPU memory.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Experiment Results</h2>\n<figure class=\"ltx_figure\" id=\"A2.4\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div class=\"ltx_block ltx_figure_panel\" id=\"A2.4.5\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F12\" style=\"width:104.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"643\" id=\"A2.1.1.g1\" src=\"./assets/x13.png\" width=\"822\"/>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F12.2.1.1\" style=\"font-size:90%;\">Figure 12</span>: </span><span class=\"ltx_text\" id=\"A2.F12.3.2\" style=\"font-size:90%;\">Effect of partial document shuffling.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F13\" style=\"width:104.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"643\" id=\"A2.2.2.g1\" src=\"./assets/x14.png\" width=\"822\"/>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F13.2.1.1\" style=\"font-size:90%;\">Figure 13</span>: </span><span class=\"ltx_text\" id=\"A2.F13.3.2\" style=\"font-size:90%;\">Effect of learning rate in 1-step GD.</span></figcaption>\n</figure>\n</div>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div class=\"ltx_block ltx_figure_panel\" id=\"A2.4.6\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F14\" style=\"width:104.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"624\" id=\"A2.3.3.g1\" src=\"./assets/x15.png\" width=\"822\"/>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F14.2.1.1\" style=\"font-size:90%;\">Figure 14</span>: </span><span class=\"ltx_text\" id=\"A2.F14.3.2\" style=\"font-size:90%;\">Effect of number of attention heads.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F15\" style=\"width:104.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"624\" id=\"A2.4.4.g1\" src=\"./assets/x16.png\" width=\"822\"/>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F15.2.1.1\" style=\"font-size:90%;\">Figure 15</span>: </span><span class=\"ltx_text\" id=\"A2.F15.3.2\" style=\"font-size:90%;\">Effect of model initialization.</span></figcaption>\n</figure>\n</div>\n</div>\n</div>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" id=\"A2.5.1\" style=\"width:203.8pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"344\" id=\"A2.5.1.g1\" src=\"./assets/x17.png\" width=\"821\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.5.1.4.1.1\" style=\"font-size:90%;\">Figure 16</span>: </span><span class=\"ltx_text\" id=\"A2.5.1.5.2\" style=\"font-size:90%;\">(Left) Effect of pre-training steps. The full pre-training process is 143k steps. (Right) Recovery scores for models with different pre-training steps.</span></figcaption>\n<div class=\"ltx_block ltx_figure_panel\" id=\"A2.5.1.6\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F17\" style=\"width:108.4pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"643\" id=\"A2.6.2.g1\" src=\"./assets/x18.png\" width=\"822\"/>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F17.2.1.1\" style=\"font-size:90%;\">Figure 17</span>: </span><span class=\"ltx_text\" id=\"A2.F17.3.2\" style=\"font-size:90%;\">Loss curve for cosine learning rate schedule.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F18\" style=\"width:108.4pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"643\" id=\"A2.7.3.g1\" src=\"./assets/x19.png\" width=\"822\"/>\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F18.2.1.1\" style=\"font-size:90%;\">Figure 18</span>: </span><span class=\"ltx_text\" id=\"A2.F18.3.2\" style=\"font-size:90%;\">Effect of inserting random documents after the repeating sequence.</span></figcaption>\n</figure>\n</div>\n</figure>\n<section class=\"ltx_subsection\" id=\"A2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Partial Random Shuffling</h3>\n<div class=\"ltx_para\" id=\"A2.SS1.p1\">\n<p class=\"ltx_p\" id=\"A2.SS1.p1.8\">Throughout the paper we have been focusing on the setting where the document ordering is sampled once and stay fixed for all epochs. What if we only fix the first and the last document, and shuffle the documents in between? We experimented with shuffling the documents from document <math alttext=\"\\bm{x}_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.1.m1.1\"><semantics id=\"A2.SS1.p1.1.m1.1a\"><msub id=\"A2.SS1.p1.1.m1.1.1\" xref=\"A2.SS1.p1.1.m1.1.1.cmml\"><mi id=\"A2.SS1.p1.1.m1.1.1.2\" xref=\"A2.SS1.p1.1.m1.1.1.2.cmml\">𝒙</mi><mn id=\"A2.SS1.p1.1.m1.1.1.3\" xref=\"A2.SS1.p1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.1.m1.1b\"><apply id=\"A2.SS1.p1.1.m1.1.1.cmml\" xref=\"A2.SS1.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.1.m1.1.1.1.cmml\" xref=\"A2.SS1.p1.1.m1.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.1.m1.1.1.2.cmml\" xref=\"A2.SS1.p1.1.m1.1.1.2\">𝒙</ci><cn id=\"A2.SS1.p1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.1.m1.1c\">\\bm{x}_{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.1.m1.1d\">bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> through <math alttext=\"\\bm{x}_{N}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.2.m2.1\"><semantics id=\"A2.SS1.p1.2.m2.1a\"><msub id=\"A2.SS1.p1.2.m2.1.1\" xref=\"A2.SS1.p1.2.m2.1.1.cmml\"><mi id=\"A2.SS1.p1.2.m2.1.1.2\" xref=\"A2.SS1.p1.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"A2.SS1.p1.2.m2.1.1.3\" xref=\"A2.SS1.p1.2.m2.1.1.3.cmml\">N</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.2.m2.1b\"><apply id=\"A2.SS1.p1.2.m2.1.1.cmml\" xref=\"A2.SS1.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.2.m2.1.1.1.cmml\" xref=\"A2.SS1.p1.2.m2.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.2.m2.1.1.2.cmml\" xref=\"A2.SS1.p1.2.m2.1.1.2\">𝒙</ci><ci id=\"A2.SS1.p1.2.m2.1.1.3.cmml\" xref=\"A2.SS1.p1.2.m2.1.1.3\">𝑁</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.2.m2.1c\">\\bm{x}_{N}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math> for <math alttext=\"N\\in\\{8,16,24\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.3.m3.3\"><semantics id=\"A2.SS1.p1.3.m3.3a\"><mrow id=\"A2.SS1.p1.3.m3.3.4\" xref=\"A2.SS1.p1.3.m3.3.4.cmml\"><mi id=\"A2.SS1.p1.3.m3.3.4.2\" xref=\"A2.SS1.p1.3.m3.3.4.2.cmml\">N</mi><mo id=\"A2.SS1.p1.3.m3.3.4.1\" xref=\"A2.SS1.p1.3.m3.3.4.1.cmml\">∈</mo><mrow id=\"A2.SS1.p1.3.m3.3.4.3.2\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\"><mo id=\"A2.SS1.p1.3.m3.3.4.3.2.1\" stretchy=\"false\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">{</mo><mn id=\"A2.SS1.p1.3.m3.1.1\" xref=\"A2.SS1.p1.3.m3.1.1.cmml\">8</mn><mo id=\"A2.SS1.p1.3.m3.3.4.3.2.2\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS1.p1.3.m3.2.2\" xref=\"A2.SS1.p1.3.m3.2.2.cmml\">16</mn><mo id=\"A2.SS1.p1.3.m3.3.4.3.2.3\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS1.p1.3.m3.3.3\" xref=\"A2.SS1.p1.3.m3.3.3.cmml\">24</mn><mo id=\"A2.SS1.p1.3.m3.3.4.3.2.4\" stretchy=\"false\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.3.m3.3b\"><apply id=\"A2.SS1.p1.3.m3.3.4.cmml\" xref=\"A2.SS1.p1.3.m3.3.4\"><in id=\"A2.SS1.p1.3.m3.3.4.1.cmml\" xref=\"A2.SS1.p1.3.m3.3.4.1\"></in><ci id=\"A2.SS1.p1.3.m3.3.4.2.cmml\" xref=\"A2.SS1.p1.3.m3.3.4.2\">𝑁</ci><set id=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\" xref=\"A2.SS1.p1.3.m3.3.4.3.2\"><cn id=\"A2.SS1.p1.3.m3.1.1.cmml\" type=\"integer\" xref=\"A2.SS1.p1.3.m3.1.1\">8</cn><cn id=\"A2.SS1.p1.3.m3.2.2.cmml\" type=\"integer\" xref=\"A2.SS1.p1.3.m3.2.2\">16</cn><cn id=\"A2.SS1.p1.3.m3.3.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.3.m3.3.3\">24</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.3.m3.3c\">N\\in\\{8,16,24\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.3.m3.3d\">italic_N ∈ { 8 , 16 , 24 }</annotation></semantics></math> every epoch. In Figure <a class=\"ltx_ref\" href=\"#A2.F12\" title=\"Figure 12 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> we plot the loss curves for document <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.4.m4.1\"><semantics id=\"A2.SS1.p1.4.m4.1a\"><msub id=\"A2.SS1.p1.4.m4.1.1\" xref=\"A2.SS1.p1.4.m4.1.1.cmml\"><mi id=\"A2.SS1.p1.4.m4.1.1.2\" xref=\"A2.SS1.p1.4.m4.1.1.2.cmml\">𝒙</mi><mn id=\"A2.SS1.p1.4.m4.1.1.3\" xref=\"A2.SS1.p1.4.m4.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.4.m4.1b\"><apply id=\"A2.SS1.p1.4.m4.1.1.cmml\" xref=\"A2.SS1.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.4.m4.1.1.1.cmml\" xref=\"A2.SS1.p1.4.m4.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.4.m4.1.1.2.cmml\" xref=\"A2.SS1.p1.4.m4.1.1.2\">𝒙</ci><cn id=\"A2.SS1.p1.4.m4.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.4.m4.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.4.m4.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.4.m4.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. From the loss curves we can observe that even when <math alttext=\"N=24\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.5.m5.1\"><semantics id=\"A2.SS1.p1.5.m5.1a\"><mrow id=\"A2.SS1.p1.5.m5.1.1\" xref=\"A2.SS1.p1.5.m5.1.1.cmml\"><mi id=\"A2.SS1.p1.5.m5.1.1.2\" xref=\"A2.SS1.p1.5.m5.1.1.2.cmml\">N</mi><mo id=\"A2.SS1.p1.5.m5.1.1.1\" xref=\"A2.SS1.p1.5.m5.1.1.1.cmml\">=</mo><mn id=\"A2.SS1.p1.5.m5.1.1.3\" xref=\"A2.SS1.p1.5.m5.1.1.3.cmml\">24</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.5.m5.1b\"><apply id=\"A2.SS1.p1.5.m5.1.1.cmml\" xref=\"A2.SS1.p1.5.m5.1.1\"><eq id=\"A2.SS1.p1.5.m5.1.1.1.cmml\" xref=\"A2.SS1.p1.5.m5.1.1.1\"></eq><ci id=\"A2.SS1.p1.5.m5.1.1.2.cmml\" xref=\"A2.SS1.p1.5.m5.1.1.2\">𝑁</ci><cn id=\"A2.SS1.p1.5.m5.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.5.m5.1.1.3\">24</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.5.m5.1c\">N=24</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.5.m5.1d\">italic_N = 24</annotation></semantics></math> we can still observe some anticipatory recovery, suggesting that the order of the tasks between two consecutive repetitions of the <math alttext=\"\\bm{x}_{25}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.6.m6.1\"><semantics id=\"A2.SS1.p1.6.m6.1a\"><msub id=\"A2.SS1.p1.6.m6.1.1\" xref=\"A2.SS1.p1.6.m6.1.1.cmml\"><mi id=\"A2.SS1.p1.6.m6.1.1.2\" xref=\"A2.SS1.p1.6.m6.1.1.2.cmml\">𝒙</mi><mn id=\"A2.SS1.p1.6.m6.1.1.3\" xref=\"A2.SS1.p1.6.m6.1.1.3.cmml\">25</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.6.m6.1b\"><apply id=\"A2.SS1.p1.6.m6.1.1.cmml\" xref=\"A2.SS1.p1.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.6.m6.1.1.1.cmml\" xref=\"A2.SS1.p1.6.m6.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.6.m6.1.1.2.cmml\" xref=\"A2.SS1.p1.6.m6.1.1.2\">𝒙</ci><cn id=\"A2.SS1.p1.6.m6.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.6.m6.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.6.m6.1c\">\\bm{x}_{25}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.6.m6.1d\">bold_italic_x start_POSTSUBSCRIPT 25 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.7.m7.1\"><semantics id=\"A2.SS1.p1.7.m7.1a\"><msub id=\"A2.SS1.p1.7.m7.1.1\" xref=\"A2.SS1.p1.7.m7.1.1.cmml\"><mi id=\"A2.SS1.p1.7.m7.1.1.2\" xref=\"A2.SS1.p1.7.m7.1.1.2.cmml\">𝒙</mi><mn id=\"A2.SS1.p1.7.m7.1.1.3\" xref=\"A2.SS1.p1.7.m7.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.7.m7.1b\"><apply id=\"A2.SS1.p1.7.m7.1.1.cmml\" xref=\"A2.SS1.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.7.m7.1.1.1.cmml\" xref=\"A2.SS1.p1.7.m7.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.7.m7.1.1.2.cmml\" xref=\"A2.SS1.p1.7.m7.1.1.2\">𝒙</ci><cn id=\"A2.SS1.p1.7.m7.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.7.m7.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.7.m7.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.7.m7.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> can be arbitrary for us to observe recovery on <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.8.m8.1\"><semantics id=\"A2.SS1.p1.8.m8.1a\"><msub id=\"A2.SS1.p1.8.m8.1.1\" xref=\"A2.SS1.p1.8.m8.1.1.cmml\"><mi id=\"A2.SS1.p1.8.m8.1.1.2\" xref=\"A2.SS1.p1.8.m8.1.1.2.cmml\">𝒙</mi><mn id=\"A2.SS1.p1.8.m8.1.1.3\" xref=\"A2.SS1.p1.8.m8.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.8.m8.1b\"><apply id=\"A2.SS1.p1.8.m8.1.1.cmml\" xref=\"A2.SS1.p1.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.8.m8.1.1.1.cmml\" xref=\"A2.SS1.p1.8.m8.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.8.m8.1.1.2.cmml\" xref=\"A2.SS1.p1.8.m8.1.1.2\">𝒙</ci><cn id=\"A2.SS1.p1.8.m8.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS1.p1.8.m8.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.8.m8.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS1.p1.8.m8.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>One-step Gradient Descent with Larger Learning Rate</h3>\n<div class=\"ltx_para\" id=\"A2.SS2.p1\">\n<p class=\"ltx_p\" id=\"A2.SS2.p1.1\">In Figure <a class=\"ltx_ref\" href=\"#S3.F5\" title=\"Figure 5 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) we observe that there is no anticipation when we take only one gradient descent step on each document with learning rate 0.001. We experimented with one-step gradient descent using a higher learning rate, 0.01. We plot the resulting average loss curves under the same training setup in Figure <a class=\"ltx_ref\" href=\"#A2.F13\" title=\"Figure 13 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe that, with a larger learning rate, slight anticipation is still observed for <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.1.m1.1\"><semantics id=\"A2.SS2.p1.1.m1.1a\"><mn id=\"A2.SS2.p1.1.m1.1.1\" xref=\"A2.SS2.p1.1.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS2.p1.1.m1.1b\"><cn id=\"A2.SS2.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"A2.SS2.p1.1.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS2.p1.1.m1.1c\">1</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS2.p1.1.m1.1d\">1</annotation></semantics></math> gradient step.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Effect of Number of Attention Heads</h3>\n<div class=\"ltx_para\" id=\"A2.SS3.p1\">\n<p class=\"ltx_p\" id=\"A2.SS3.p1.2\">In addition to varying the model width and model depth in Figure <a class=\"ltx_ref\" href=\"#S3.F4\" title=\"Figure 4 ‣ Effects of Model Width and Depth. ‣ 3.2 Anticipatory Recovery is an Emergent Behavior ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we also experimented with varying the number of attention heads <math alttext=\"h\\in\\{2,4,8,16\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.1.m1.4\"><semantics id=\"A2.SS3.p1.1.m1.4a\"><mrow id=\"A2.SS3.p1.1.m1.4.5\" xref=\"A2.SS3.p1.1.m1.4.5.cmml\"><mi id=\"A2.SS3.p1.1.m1.4.5.2\" xref=\"A2.SS3.p1.1.m1.4.5.2.cmml\">h</mi><mo id=\"A2.SS3.p1.1.m1.4.5.1\" xref=\"A2.SS3.p1.1.m1.4.5.1.cmml\">∈</mo><mrow id=\"A2.SS3.p1.1.m1.4.5.3.2\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\"><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.1\" stretchy=\"false\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">{</mo><mn id=\"A2.SS3.p1.1.m1.1.1\" xref=\"A2.SS3.p1.1.m1.1.1.cmml\">2</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.2\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"A2.SS3.p1.1.m1.2.2\" xref=\"A2.SS3.p1.1.m1.2.2.cmml\">4</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.3\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"A2.SS3.p1.1.m1.3.3\" xref=\"A2.SS3.p1.1.m1.3.3.cmml\">8</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.4\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"A2.SS3.p1.1.m1.4.4\" xref=\"A2.SS3.p1.1.m1.4.4.cmml\">16</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.5\" stretchy=\"false\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS3.p1.1.m1.4b\"><apply id=\"A2.SS3.p1.1.m1.4.5.cmml\" xref=\"A2.SS3.p1.1.m1.4.5\"><in id=\"A2.SS3.p1.1.m1.4.5.1.cmml\" xref=\"A2.SS3.p1.1.m1.4.5.1\"></in><ci id=\"A2.SS3.p1.1.m1.4.5.2.cmml\" xref=\"A2.SS3.p1.1.m1.4.5.2\">ℎ</ci><set id=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\" xref=\"A2.SS3.p1.1.m1.4.5.3.2\"><cn id=\"A2.SS3.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"A2.SS3.p1.1.m1.1.1\">2</cn><cn id=\"A2.SS3.p1.1.m1.2.2.cmml\" type=\"integer\" xref=\"A2.SS3.p1.1.m1.2.2\">4</cn><cn id=\"A2.SS3.p1.1.m1.3.3.cmml\" type=\"integer\" xref=\"A2.SS3.p1.1.m1.3.3\">8</cn><cn id=\"A2.SS3.p1.1.m1.4.4.cmml\" type=\"integer\" xref=\"A2.SS3.p1.1.m1.4.4\">16</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS3.p1.1.m1.4c\">h\\in\\{2,4,8,16\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS3.p1.1.m1.4d\">italic_h ∈ { 2 , 4 , 8 , 16 }</annotation></semantics></math> while keeping model width to be 2048 and model depth to be 16. Loss curves on document <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p1.2.m2.1\"><semantics id=\"A2.SS3.p1.2.m2.1a\"><msub id=\"A2.SS3.p1.2.m2.1.1\" xref=\"A2.SS3.p1.2.m2.1.1.cmml\"><mi id=\"A2.SS3.p1.2.m2.1.1.2\" xref=\"A2.SS3.p1.2.m2.1.1.2.cmml\">𝒙</mi><mn id=\"A2.SS3.p1.2.m2.1.1.3\" xref=\"A2.SS3.p1.2.m2.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS3.p1.2.m2.1b\"><apply id=\"A2.SS3.p1.2.m2.1.1.cmml\" xref=\"A2.SS3.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS3.p1.2.m2.1.1.1.cmml\" xref=\"A2.SS3.p1.2.m2.1.1\">subscript</csymbol><ci id=\"A2.SS3.p1.2.m2.1.1.2.cmml\" xref=\"A2.SS3.p1.2.m2.1.1.2\">𝒙</ci><cn id=\"A2.SS3.p1.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS3.p1.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS3.p1.2.m2.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS3.p1.2.m2.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> are shown in Figure <a class=\"ltx_ref\" href=\"#A2.F14\" title=\"Figure 14 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>. The results suggest that the number of attention heads does not have a big effect on cyclic training in our setting.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>Effect of LLM Model Initialization</h3>\n<div class=\"ltx_para\" id=\"A2.SS4.p1\">\n<p class=\"ltx_p\" id=\"A2.SS4.p1.1\">Here we compare the performance of the initialization scheme used by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">11</a>]</cite> (also used for all randomly initialized models in the main text) and a simple initialization scheme that samples the weight matrices from an isotropic Gaussian distribution with <math alttext=\"\\sigma=0.02\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS4.p1.1.m1.1\"><semantics id=\"A2.SS4.p1.1.m1.1a\"><mrow id=\"A2.SS4.p1.1.m1.1.1\" xref=\"A2.SS4.p1.1.m1.1.1.cmml\"><mi id=\"A2.SS4.p1.1.m1.1.1.2\" xref=\"A2.SS4.p1.1.m1.1.1.2.cmml\">σ</mi><mo id=\"A2.SS4.p1.1.m1.1.1.1\" xref=\"A2.SS4.p1.1.m1.1.1.1.cmml\">=</mo><mn id=\"A2.SS4.p1.1.m1.1.1.3\" xref=\"A2.SS4.p1.1.m1.1.1.3.cmml\">0.02</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS4.p1.1.m1.1b\"><apply id=\"A2.SS4.p1.1.m1.1.1.cmml\" xref=\"A2.SS4.p1.1.m1.1.1\"><eq id=\"A2.SS4.p1.1.m1.1.1.1.cmml\" xref=\"A2.SS4.p1.1.m1.1.1.1\"></eq><ci id=\"A2.SS4.p1.1.m1.1.1.2.cmml\" xref=\"A2.SS4.p1.1.m1.1.1.2\">𝜎</ci><cn id=\"A2.SS4.p1.1.m1.1.1.3.cmml\" type=\"float\" xref=\"A2.SS4.p1.1.m1.1.1.3\">0.02</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS4.p1.1.m1.1c\">\\sigma=0.02</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS4.p1.1.m1.1d\">italic_σ = 0.02</annotation></semantics></math>. The loss curves for document 1 under these two initializations of the Pythia-1B model are plotted in Figure <a class=\"ltx_ref\" href=\"#A2.F15\" title=\"Figure 15 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>. We observe that Pythia’s initialization scheme achieves much better average loss and also exhibits stronger anticipatory recovery. This demonstrates the importance of LLM initializations. The result is consistent with our observations in section <a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> that the model’s ability to fit on each task is correlated with the amount of anticipatory recovery.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.5 </span>Effect of Pre-training Steps</h3>\n<div class=\"ltx_para\" id=\"A2.SS5.p1\">\n<p class=\"ltx_p\" id=\"A2.SS5.p1.1\">In addition to comparing pre-trained models and randomly initialized models in Figure <a class=\"ltx_ref\" href=\"#S3.F2\" title=\"Figure 2 ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we further study the effect of model pre-training by examining model checkpoints with different numbers of pre-training steps. We took Pythia models pre-trained for 6K, 12K, 24K, 48K, and 96K steps and plot the shift-averaged loss curves for cyclic fine-tuning in Figure <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. We found that more pre-training does give rise to higher anticipatory recovery. As we summarize at the end of section <a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Other Influential Factors ‣ 3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, we hypothesize this result fits a broader pattern in which the strength of the anticipatory recovery effect is related to how well the model can fit each successive training task. Models with more pre-training steps are more capable of fitting each successive training task, and therefore exhibit higher anticipatory recovery.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS6\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.6 </span>Effect of Cosine Learning Rate Schedule</h3>\n<div class=\"ltx_para\" id=\"A2.SS6.p1\">\n<p class=\"ltx_p\" id=\"A2.SS6.p1.1\">For experiments in the main paper we used a constant learning rate during the fine-tuning process. To study whether anticipatory recovery occurs in typical LLM optimization schemes, we experimented with cosine learning rate scheduling on the Pythia-1B model with 10 epochs (minimum learning rate = 0, maximum number of epochs = 10), and plot the results in Figure <a class=\"ltx_ref\" href=\"#A2.F17\" title=\"Figure 17 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>. We show that the model also exhibits the anticipatory recovery effect in other learning rate schedules.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS7\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.7 </span>Effect of Inserting Random Documents after the Repeating Sequence</h3>\n<div class=\"ltx_para\" id=\"A2.SS7.p1\">\n<p class=\"ltx_p\" id=\"A2.SS7.p1.1\">To examine how the anticipatory recovery effect may generalize to other forms of structured training, we experimented with a new setting where only the first 20 documents are kept fixed in each epoch, and a random number of other documents (between 20 and 100) are inserted after the first 20. These random padding documents appear only once in the entire sequence. This new setting generalizes cyclic training in that (1) rather than having the same documents in every epoch, we insert random other documents between every repetition (2) epochs can have different lengths. The resulting loss curve is plotted in Figure <a class=\"ltx_ref\" href=\"#A2.F18\" title=\"Figure 18 ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>. We still observe anticipatory recovery for documents 2 through 20 in this setting, suggesting that anticipatory recovery exists as long as there is a repeating sub-sequence in the data stream.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"A2.F19\">\n<div class=\"ltx_block\" id=\"A2.F19.2\">\n<figure class=\"ltx_figure ltx_align_center\" id=\"A2.F19.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"158\" id=\"A2.F19.sf1.g1\" src=\"./assets/x20.png\" width=\"377\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F19.sf1.2.1.1\" style=\"font-size:90%;\">(a)</span> </span><span class=\"ltx_text\" id=\"A2.F19.sf1.3.2\" style=\"font-size:90%;\">Number of Gradient Steps with Inverse LR Scaling</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_align_center\" id=\"A2.F19.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"158\" id=\"A2.F19.sf2.g1\" src=\"./assets/x21.png\" width=\"377\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F19.sf2.2.1.1\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" id=\"A2.F19.sf2.3.2\" style=\"font-size:90%;\">Number of Gradient Steps for Context Length 1024</span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F19.3.1.1\" style=\"font-size:90%;\">Figure 19</span>: </span><span class=\"ltx_text\" id=\"A2.F19.4.2\" style=\"font-size:90%;\">Effect of number of gradient steps (a) with inverse learning rate scaling and (b) for context length 1024.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS8\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.8 </span>Effect of Number of Gradient Steps with Inverse Learning Rate Scaling</h3>\n<div class=\"ltx_para\" id=\"A2.SS8.p1\">\n<p class=\"ltx_p\" id=\"A2.SS8.p1.8\">In Figure <a class=\"ltx_ref\" href=\"#A2.F19.sf1\" title=\"Figure 19(a) ‣ Figure 19 ‣ B.7 Effect of Inserting Random Documents after the Repeating Sequence ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">19(a)</span></a> we experimented with inversely scaling the learning rate with the number of gradient steps. We use a learning rate of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.1.m1.1\"><semantics id=\"A2.SS8.p1.1.m1.1a\"><mn id=\"A2.SS8.p1.1.m1.1.1\" xref=\"A2.SS8.p1.1.m1.1.1.cmml\">0.01</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.1.m1.1b\"><cn id=\"A2.SS8.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"A2.SS8.p1.1.m1.1.1\">0.01</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.1.m1.1c\">0.01</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.1.m1.1d\">0.01</annotation></semantics></math> for <math alttext=\"M=1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.2.m2.1\"><semantics id=\"A2.SS8.p1.2.m2.1a\"><mrow id=\"A2.SS8.p1.2.m2.1.1\" xref=\"A2.SS8.p1.2.m2.1.1.cmml\"><mi id=\"A2.SS8.p1.2.m2.1.1.2\" xref=\"A2.SS8.p1.2.m2.1.1.2.cmml\">M</mi><mo id=\"A2.SS8.p1.2.m2.1.1.1\" xref=\"A2.SS8.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"A2.SS8.p1.2.m2.1.1.3\" xref=\"A2.SS8.p1.2.m2.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.2.m2.1b\"><apply id=\"A2.SS8.p1.2.m2.1.1.cmml\" xref=\"A2.SS8.p1.2.m2.1.1\"><eq id=\"A2.SS8.p1.2.m2.1.1.1.cmml\" xref=\"A2.SS8.p1.2.m2.1.1.1\"></eq><ci id=\"A2.SS8.p1.2.m2.1.1.2.cmml\" xref=\"A2.SS8.p1.2.m2.1.1.2\">𝑀</ci><cn id=\"A2.SS8.p1.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS8.p1.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.2.m2.1c\">M=1</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.2.m2.1d\">italic_M = 1</annotation></semantics></math>, learning rate <math alttext=\"0.002\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.3.m3.1\"><semantics id=\"A2.SS8.p1.3.m3.1a\"><mn id=\"A2.SS8.p1.3.m3.1.1\" xref=\"A2.SS8.p1.3.m3.1.1.cmml\">0.002</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.3.m3.1b\"><cn id=\"A2.SS8.p1.3.m3.1.1.cmml\" type=\"float\" xref=\"A2.SS8.p1.3.m3.1.1\">0.002</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.3.m3.1c\">0.002</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.3.m3.1d\">0.002</annotation></semantics></math> for <math alttext=\"M=5\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.4.m4.1\"><semantics id=\"A2.SS8.p1.4.m4.1a\"><mrow id=\"A2.SS8.p1.4.m4.1.1\" xref=\"A2.SS8.p1.4.m4.1.1.cmml\"><mi id=\"A2.SS8.p1.4.m4.1.1.2\" xref=\"A2.SS8.p1.4.m4.1.1.2.cmml\">M</mi><mo id=\"A2.SS8.p1.4.m4.1.1.1\" xref=\"A2.SS8.p1.4.m4.1.1.1.cmml\">=</mo><mn id=\"A2.SS8.p1.4.m4.1.1.3\" xref=\"A2.SS8.p1.4.m4.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.4.m4.1b\"><apply id=\"A2.SS8.p1.4.m4.1.1.cmml\" xref=\"A2.SS8.p1.4.m4.1.1\"><eq id=\"A2.SS8.p1.4.m4.1.1.1.cmml\" xref=\"A2.SS8.p1.4.m4.1.1.1\"></eq><ci id=\"A2.SS8.p1.4.m4.1.1.2.cmml\" xref=\"A2.SS8.p1.4.m4.1.1.2\">𝑀</ci><cn id=\"A2.SS8.p1.4.m4.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS8.p1.4.m4.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.4.m4.1c\">M=5</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.4.m4.1d\">italic_M = 5</annotation></semantics></math>, learning rate <math alttext=\"0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.5.m5.1\"><semantics id=\"A2.SS8.p1.5.m5.1a\"><mn id=\"A2.SS8.p1.5.m5.1.1\" xref=\"A2.SS8.p1.5.m5.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.5.m5.1b\"><cn id=\"A2.SS8.p1.5.m5.1.1.cmml\" type=\"float\" xref=\"A2.SS8.p1.5.m5.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.5.m5.1c\">0.001</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.5.m5.1d\">0.001</annotation></semantics></math> for <math alttext=\"M=10\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.6.m6.1\"><semantics id=\"A2.SS8.p1.6.m6.1a\"><mrow id=\"A2.SS8.p1.6.m6.1.1\" xref=\"A2.SS8.p1.6.m6.1.1.cmml\"><mi id=\"A2.SS8.p1.6.m6.1.1.2\" xref=\"A2.SS8.p1.6.m6.1.1.2.cmml\">M</mi><mo id=\"A2.SS8.p1.6.m6.1.1.1\" xref=\"A2.SS8.p1.6.m6.1.1.1.cmml\">=</mo><mn id=\"A2.SS8.p1.6.m6.1.1.3\" xref=\"A2.SS8.p1.6.m6.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.6.m6.1b\"><apply id=\"A2.SS8.p1.6.m6.1.1.cmml\" xref=\"A2.SS8.p1.6.m6.1.1\"><eq id=\"A2.SS8.p1.6.m6.1.1.1.cmml\" xref=\"A2.SS8.p1.6.m6.1.1.1\"></eq><ci id=\"A2.SS8.p1.6.m6.1.1.2.cmml\" xref=\"A2.SS8.p1.6.m6.1.1.2\">𝑀</ci><cn id=\"A2.SS8.p1.6.m6.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS8.p1.6.m6.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.6.m6.1c\">M=10</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.6.m6.1d\">italic_M = 10</annotation></semantics></math>, and learning rate <math alttext=\"0.0005\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.7.m7.1\"><semantics id=\"A2.SS8.p1.7.m7.1a\"><mn id=\"A2.SS8.p1.7.m7.1.1\" xref=\"A2.SS8.p1.7.m7.1.1.cmml\">0.0005</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.7.m7.1b\"><cn id=\"A2.SS8.p1.7.m7.1.1.cmml\" type=\"float\" xref=\"A2.SS8.p1.7.m7.1.1\">0.0005</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.7.m7.1c\">0.0005</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.7.m7.1d\">0.0005</annotation></semantics></math> for <math alttext=\"M=20\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS8.p1.8.m8.1\"><semantics id=\"A2.SS8.p1.8.m8.1a\"><mrow id=\"A2.SS8.p1.8.m8.1.1\" xref=\"A2.SS8.p1.8.m8.1.1.cmml\"><mi id=\"A2.SS8.p1.8.m8.1.1.2\" xref=\"A2.SS8.p1.8.m8.1.1.2.cmml\">M</mi><mo id=\"A2.SS8.p1.8.m8.1.1.1\" xref=\"A2.SS8.p1.8.m8.1.1.1.cmml\">=</mo><mn id=\"A2.SS8.p1.8.m8.1.1.3\" xref=\"A2.SS8.p1.8.m8.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS8.p1.8.m8.1b\"><apply id=\"A2.SS8.p1.8.m8.1.1.cmml\" xref=\"A2.SS8.p1.8.m8.1.1\"><eq id=\"A2.SS8.p1.8.m8.1.1.1.cmml\" xref=\"A2.SS8.p1.8.m8.1.1.1\"></eq><ci id=\"A2.SS8.p1.8.m8.1.1.2.cmml\" xref=\"A2.SS8.p1.8.m8.1.1.2\">𝑀</ci><cn id=\"A2.SS8.p1.8.m8.1.1.3.cmml\" type=\"integer\" xref=\"A2.SS8.p1.8.m8.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS8.p1.8.m8.1c\">M=20</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS8.p1.8.m8.1d\">italic_M = 20</annotation></semantics></math>. The results suggest that the anticipation effect is stronger when the same total update is divided among more gradient steps.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS9\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.9 </span>Effect of Number of Gradient Steps for Long Context Length</h3>\n<div class=\"ltx_para\" id=\"A2.SS9.p1\">\n<p class=\"ltx_p\" id=\"A2.SS9.p1.1\">In Figure <a class=\"ltx_ref\" href=\"#A2.F19.sf2\" title=\"Figure 19(b) ‣ Figure 19 ‣ B.7 Effect of Inserting Random Documents after the Repeating Sequence ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">19(b)</span></a> we experimented with different number of gradient steps <math alttext=\"M\\in\\{10,20,40\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS9.p1.1.m1.3\"><semantics id=\"A2.SS9.p1.1.m1.3a\"><mrow id=\"A2.SS9.p1.1.m1.3.4\" xref=\"A2.SS9.p1.1.m1.3.4.cmml\"><mi id=\"A2.SS9.p1.1.m1.3.4.2\" xref=\"A2.SS9.p1.1.m1.3.4.2.cmml\">M</mi><mo id=\"A2.SS9.p1.1.m1.3.4.1\" xref=\"A2.SS9.p1.1.m1.3.4.1.cmml\">∈</mo><mrow id=\"A2.SS9.p1.1.m1.3.4.3.2\" xref=\"A2.SS9.p1.1.m1.3.4.3.1.cmml\"><mo id=\"A2.SS9.p1.1.m1.3.4.3.2.1\" stretchy=\"false\" xref=\"A2.SS9.p1.1.m1.3.4.3.1.cmml\">{</mo><mn id=\"A2.SS9.p1.1.m1.1.1\" xref=\"A2.SS9.p1.1.m1.1.1.cmml\">10</mn><mo id=\"A2.SS9.p1.1.m1.3.4.3.2.2\" xref=\"A2.SS9.p1.1.m1.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS9.p1.1.m1.2.2\" xref=\"A2.SS9.p1.1.m1.2.2.cmml\">20</mn><mo id=\"A2.SS9.p1.1.m1.3.4.3.2.3\" xref=\"A2.SS9.p1.1.m1.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS9.p1.1.m1.3.3\" xref=\"A2.SS9.p1.1.m1.3.3.cmml\">40</mn><mo id=\"A2.SS9.p1.1.m1.3.4.3.2.4\" stretchy=\"false\" xref=\"A2.SS9.p1.1.m1.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS9.p1.1.m1.3b\"><apply id=\"A2.SS9.p1.1.m1.3.4.cmml\" xref=\"A2.SS9.p1.1.m1.3.4\"><in id=\"A2.SS9.p1.1.m1.3.4.1.cmml\" xref=\"A2.SS9.p1.1.m1.3.4.1\"></in><ci id=\"A2.SS9.p1.1.m1.3.4.2.cmml\" xref=\"A2.SS9.p1.1.m1.3.4.2\">𝑀</ci><set id=\"A2.SS9.p1.1.m1.3.4.3.1.cmml\" xref=\"A2.SS9.p1.1.m1.3.4.3.2\"><cn id=\"A2.SS9.p1.1.m1.1.1.cmml\" type=\"integer\" xref=\"A2.SS9.p1.1.m1.1.1\">10</cn><cn id=\"A2.SS9.p1.1.m1.2.2.cmml\" type=\"integer\" xref=\"A2.SS9.p1.1.m1.2.2\">20</cn><cn id=\"A2.SS9.p1.1.m1.3.3.cmml\" type=\"integer\" xref=\"A2.SS9.p1.1.m1.3.3\">40</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS9.p1.1.m1.3c\">M\\in\\{10,20,40\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A2.SS9.p1.1.m1.3d\">italic_M ∈ { 10 , 20 , 40 }</annotation></semantics></math> for context length 1024. The results confirm that longer context length is not a fundamental limitation to anticipatory recovery, and we can achieve the same recovery score as a smaller context length with more gradient steps.</p>\n</div>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F22\" style=\"width:216.8pt;\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div class=\"ltx_block ltx_figure_panel\" id=\"A2.F22.3\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F20\" style=\"width:104.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"131\" id=\"A2.SS9.1.1.g1\" src=\"./assets/x22.png\" width=\"168\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F20.2.1.1\" style=\"font-size:90%;\">Figure 20</span>: </span><span class=\"ltx_text\" id=\"A2.F20.3.2\" style=\"font-size:90%;\">Experiments with GPT2-large.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" id=\"A2.F21\" style=\"width:104.1pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" height=\"131\" id=\"A2.SS9.2.2.g1\" src=\"./assets/x23.png\" width=\"168\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F21.2.1.1\" style=\"font-size:90%;\">Figure 21</span>: </span><span class=\"ltx_text\" id=\"A2.F21.3.2\" style=\"font-size:90%;\">Experiments with wikitext-103 dataset.</span></figcaption>\n</figure>\n</div>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div class=\"ltx_block ltx_figure_panel\" id=\"A2.F22.4\">\n<figure class=\"ltx_figure\" id=\"A2.F22.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"131\" id=\"A2.F22.sf1.g1\" src=\"./assets/x24.png\" width=\"181\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F22.sf1.2.1.1\" style=\"font-size:90%;\">(a)</span> </span><span class=\"ltx_text\" id=\"A2.F22.sf1.3.2\" style=\"font-size:90%;\">Model Activations</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_align_center\" id=\"A2.F22.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"131\" id=\"A2.F22.sf2.g1\" src=\"./assets/x25.png\" width=\"179\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F22.sf2.2.1.1\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" id=\"A2.F22.sf2.3.2\" style=\"font-size:90%;\">Model Weights </span></figcaption>\n</figure>\n</div>\n</div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A2.F22.6.1.1\" style=\"font-size:90%;\">Figure 22</span>: </span><span class=\"ltx_text\" id=\"A2.F22.7.2\" style=\"font-size:90%;\">Magnitude of (a) model activation updates and (b) model weight updates through cyclic training.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS10\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.10 </span>Experiments with GPT-2</h3>\n<div class=\"ltx_para\" id=\"A2.SS10.p1\">\n<p class=\"ltx_p\" id=\"A2.SS10.p1.1\">To evaluate how the anticipatory recovery phenomenon generalizes across different LLM architectures, we experimented with GPT-2 architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib64\" title=\"\">64</a>]</cite>, specifically the GPT2-large pre-trained model (812M parameters) on the CNN/Daily Mail dataset. The loss curve for document 1 is plotted in Figure <a class=\"ltx_ref\" href=\"#A2.F20\" title=\"Figure 20 ‣ Figure 22 ‣ B.9 Effect of Number of Gradient Steps for Long Context Length ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>. The model consistently observed anticipatory recovery. Note that GPT-2 is the predecessor of many modern LLMs and therefore the results further suggest that the anticipatory recovery phenomenon is prevalent among more recent LLM architectures.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS11\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.11 </span>Experiments with Wikitext</h3>\n<div class=\"ltx_para\" id=\"A2.SS11.p1\">\n<p class=\"ltx_p\" id=\"A2.SS11.p1.1\">To evaluate how the anticipatory recovery phenomenon generalizes across different natural language datasets, we experimented with the wikitext-103 dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib65\" title=\"\">65</a>]</cite>, which contains over 100 million tokens from articles on Wikipedia. Since Wikipedia data is part of the pre-training dataset of Pythia, we only experiment with randomly initialized models. The loss curve for document 1 is plotted in Figure <a class=\"ltx_ref\" href=\"#A2.F21\" title=\"Figure 21 ‣ Figure 22 ‣ B.9 Effect of Number of Gradient Steps for Long Context Length ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>. The result suggest that the anticipatory recovery phenomenon is generalizable to different data sources.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Visualizations</h2>\n<figure class=\"ltx_figure\" id=\"A3.F23\">\n<div class=\"ltx_block\" id=\"A3.F23.6\">\n<figure class=\"ltx_figure ltx_align_center\" id=\"A3.F23.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"174\" id=\"A3.F23.sf1.g1\" src=\"./assets/x26.png\" width=\"227\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A3.F23.sf1.2.1.1\" style=\"font-size:90%;\">(a)</span> </span><span class=\"ltx_text\" id=\"A3.F23.sf1.3.2\" style=\"font-size:90%;\">50 Documents</span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_align_center\" id=\"A3.F23.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"174\" id=\"A3.F23.sf2.g1\" src=\"./assets/x27.png\" width=\"220\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A3.F23.sf2.2.1.1\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" id=\"A3.F23.sf2.3.2\" style=\"font-size:90%;\">100 Documents </span></figcaption>\n</figure>\n<figure class=\"ltx_figure ltx_align_center\" id=\"A3.F23.sf3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"174\" id=\"A3.F23.sf3.g1\" src=\"./assets/x28.png\" width=\"225\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A3.F23.sf3.2.1.1\" style=\"font-size:90%;\">(c)</span> </span><span class=\"ltx_text\" id=\"A3.F23.sf3.3.2\" style=\"font-size:90%;\">200 Documents </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" id=\"A3.F23.7.3.1\" style=\"font-size:90%;\">Figure 23</span>: </span><span class=\"ltx_text\" id=\"A3.F23.4.2\" style=\"font-size:90%;\">Loss recoveries for training on task <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.F23.3.1.m1.1\"><semantics id=\"A3.F23.3.1.m1.1b\"><msub id=\"A3.F23.3.1.m1.1.1\" xref=\"A3.F23.3.1.m1.1.1.cmml\"><mi id=\"A3.F23.3.1.m1.1.1.2\" xref=\"A3.F23.3.1.m1.1.1.2.cmml\">𝒙</mi><mi id=\"A3.F23.3.1.m1.1.1.3\" xref=\"A3.F23.3.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.F23.3.1.m1.1c\"><apply id=\"A3.F23.3.1.m1.1.1.cmml\" xref=\"A3.F23.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.F23.3.1.m1.1.1.1.cmml\" xref=\"A3.F23.3.1.m1.1.1\">subscript</csymbol><ci id=\"A3.F23.3.1.m1.1.1.2.cmml\" xref=\"A3.F23.3.1.m1.1.1.2\">𝒙</ci><ci id=\"A3.F23.3.1.m1.1.1.3.cmml\" xref=\"A3.F23.3.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.F23.3.1.m1.1d\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.F23.3.1.m1.1e\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (y-axis) and evaluating on task <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.F23.4.2.m2.1\"><semantics id=\"A3.F23.4.2.m2.1b\"><msub id=\"A3.F23.4.2.m2.1.1\" xref=\"A3.F23.4.2.m2.1.1.cmml\"><mi id=\"A3.F23.4.2.m2.1.1.2\" xref=\"A3.F23.4.2.m2.1.1.2.cmml\">𝒙</mi><mi id=\"A3.F23.4.2.m2.1.1.3\" xref=\"A3.F23.4.2.m2.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.F23.4.2.m2.1c\"><apply id=\"A3.F23.4.2.m2.1.1.cmml\" xref=\"A3.F23.4.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.F23.4.2.m2.1.1.1.cmml\" xref=\"A3.F23.4.2.m2.1.1\">subscript</csymbol><ci id=\"A3.F23.4.2.m2.1.1.2.cmml\" xref=\"A3.F23.4.2.m2.1.1.2\">𝒙</ci><ci id=\"A3.F23.4.2.m2.1.1.3.cmml\" xref=\"A3.F23.4.2.m2.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.F23.4.2.m2.1d\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.F23.4.2.m2.1e\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> (x-axis) for longer document sequences of different lengths.</span></figcaption>\n</figure>\n<section class=\"ltx_subsection\" id=\"A3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.1 </span>Magnitude of Changes in Model Weights and Model Activations</h3>\n<div class=\"ltx_para\" id=\"A3.SS1.p1\">\n<p class=\"ltx_p\" id=\"A3.SS1.p1.1\">We plot the magnitude of the difference between the <math alttext=\"\\bm{x}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.1.m1.1\"><semantics id=\"A3.SS1.p1.1.m1.1a\"><msub id=\"A3.SS1.p1.1.m1.1.1\" xref=\"A3.SS1.p1.1.m1.1.1.cmml\"><mi id=\"A3.SS1.p1.1.m1.1.1.2\" xref=\"A3.SS1.p1.1.m1.1.1.2.cmml\">𝒙</mi><mn id=\"A3.SS1.p1.1.m1.1.1.3\" xref=\"A3.SS1.p1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS1.p1.1.m1.1b\"><apply id=\"A3.SS1.p1.1.m1.1.1.cmml\" xref=\"A3.SS1.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS1.p1.1.m1.1.1.1.cmml\" xref=\"A3.SS1.p1.1.m1.1.1\">subscript</csymbol><ci id=\"A3.SS1.p1.1.m1.1.1.2.cmml\" xref=\"A3.SS1.p1.1.m1.1.1.2\">𝒙</ci><cn id=\"A3.SS1.p1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"A3.SS1.p1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS1.p1.1.m1.1c\">\\bm{x}_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS1.p1.1.m1.1d\">bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> activations of model checkpoints saved at consecutive episodes throughout four epochs of cyclic training of a Pythia-410M model in Figure <a class=\"ltx_ref\" href=\"#A2.F22.sf1\" title=\"Figure 22(a) ‣ Figure 22 ‣ B.9 Effect of Number of Gradient Steps for Long Context Length ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">22(a)</span></a>, and observe a clear stepwise pattern. In contrast, the magnitude of model weight updates (Figure <a class=\"ltx_ref\" href=\"#A2.F22.sf2\" title=\"Figure 22(b) ‣ Figure 22 ‣ B.9 Effect of Number of Gradient Steps for Long Context Length ‣ Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">22(b)</span></a>) decreases monotonically over the training episodes and do not exhibit this stepwise pattern. This result is consistent with the pattern we observe in section <a class=\"ltx_ref\" href=\"#S4.SS3\" title=\"4.3 Temporal Structure of Activations ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.2 </span>Pairwise Recovery Matrices for Longer Document Sequences</h3>\n<div class=\"ltx_para\" id=\"A3.SS2.p1\">\n<p class=\"ltx_p\" id=\"A3.SS2.p1.6\">Similar to Figure <a class=\"ltx_ref\" href=\"#S4.F8\" title=\"Figure 8 ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(b), we plot the pairwise loss recoveries for each pair of documents <math alttext=\"(\\bm{x}_{i},\\bm{x}_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.1.m1.2\"><semantics id=\"A3.SS2.p1.1.m1.2a\"><mrow id=\"A3.SS2.p1.1.m1.2.2.2\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\"><mo id=\"A3.SS2.p1.1.m1.2.2.2.3\" stretchy=\"false\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\">(</mo><msub id=\"A3.SS2.p1.1.m1.1.1.1.1\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.cmml\"><mi id=\"A3.SS2.p1.1.m1.1.1.1.1.2\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.2.cmml\">𝒙</mi><mi id=\"A3.SS2.p1.1.m1.1.1.1.1.3\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"A3.SS2.p1.1.m1.2.2.2.4\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\">,</mo><msub id=\"A3.SS2.p1.1.m1.2.2.2.2\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.cmml\"><mi id=\"A3.SS2.p1.1.m1.2.2.2.2.2\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.2.cmml\">𝒙</mi><mi id=\"A3.SS2.p1.1.m1.2.2.2.2.3\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.3.cmml\">j</mi></msub><mo id=\"A3.SS2.p1.1.m1.2.2.2.5\" stretchy=\"false\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.1.m1.2b\"><interval closure=\"open\" id=\"A3.SS2.p1.1.m1.2.2.3.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2\"><apply id=\"A3.SS2.p1.1.m1.1.1.1.1.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.1.m1.1.1.1.1.1.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.1.m1.1.1.1.1.2.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.2\">𝒙</ci><ci id=\"A3.SS2.p1.1.m1.1.1.1.1.3.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.3\">𝑖</ci></apply><apply id=\"A3.SS2.p1.1.m1.2.2.2.2.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.1.m1.2.2.2.2.1.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2\">subscript</csymbol><ci id=\"A3.SS2.p1.1.m1.2.2.2.2.2.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.2\">𝒙</ci><ci id=\"A3.SS2.p1.1.m1.2.2.2.2.3.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.3\">𝑗</ci></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.1.m1.2c\">(\\bm{x}_{i},\\bm{x}_{j})</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS2.p1.1.m1.2d\">( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math> in longer document sequences, where <math alttext=\"T=50,100,200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.2.m2.3\"><semantics id=\"A3.SS2.p1.2.m2.3a\"><mrow id=\"A3.SS2.p1.2.m2.3.4\" xref=\"A3.SS2.p1.2.m2.3.4.cmml\"><mi id=\"A3.SS2.p1.2.m2.3.4.2\" xref=\"A3.SS2.p1.2.m2.3.4.2.cmml\">T</mi><mo id=\"A3.SS2.p1.2.m2.3.4.1\" xref=\"A3.SS2.p1.2.m2.3.4.1.cmml\">=</mo><mrow id=\"A3.SS2.p1.2.m2.3.4.3.2\" xref=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\"><mn id=\"A3.SS2.p1.2.m2.1.1\" xref=\"A3.SS2.p1.2.m2.1.1.cmml\">50</mn><mo id=\"A3.SS2.p1.2.m2.3.4.3.2.1\" xref=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\">,</mo><mn id=\"A3.SS2.p1.2.m2.2.2\" xref=\"A3.SS2.p1.2.m2.2.2.cmml\">100</mn><mo id=\"A3.SS2.p1.2.m2.3.4.3.2.2\" xref=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\">,</mo><mn id=\"A3.SS2.p1.2.m2.3.3\" xref=\"A3.SS2.p1.2.m2.3.3.cmml\">200</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.2.m2.3b\"><apply id=\"A3.SS2.p1.2.m2.3.4.cmml\" xref=\"A3.SS2.p1.2.m2.3.4\"><eq id=\"A3.SS2.p1.2.m2.3.4.1.cmml\" xref=\"A3.SS2.p1.2.m2.3.4.1\"></eq><ci id=\"A3.SS2.p1.2.m2.3.4.2.cmml\" xref=\"A3.SS2.p1.2.m2.3.4.2\">𝑇</ci><list id=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\" xref=\"A3.SS2.p1.2.m2.3.4.3.2\"><cn id=\"A3.SS2.p1.2.m2.1.1.cmml\" type=\"integer\" xref=\"A3.SS2.p1.2.m2.1.1\">50</cn><cn id=\"A3.SS2.p1.2.m2.2.2.cmml\" type=\"integer\" xref=\"A3.SS2.p1.2.m2.2.2\">100</cn><cn id=\"A3.SS2.p1.2.m2.3.3.cmml\" type=\"integer\" xref=\"A3.SS2.p1.2.m2.3.3\">200</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.2.m2.3c\">T=50,100,200</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS2.p1.2.m2.3d\">italic_T = 50 , 100 , 200</annotation></semantics></math> respectively, in Figure <a class=\"ltx_ref\" href=\"#A3.F23\" title=\"Figure 23 ‣ Appendix C Additional Visualizations ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">23</span></a>. We use the 1B model and default hyperparameters. We observe that, as we increase the length of the document sequence, the highlight area near the center of the matrix is separated into two blobs, one in the top-left corner and the other in the bottom-right corner. We also observe a \"boundary\" on the sides of the matrix where there is little or no recovery. The width of this \"boundary\" stays relatively constant across different lengths of document sequences and is around 10 to 15 documents. This confirms our observation in the main text that the recovery on document <math alttext=\"\\bm{x}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.3.m3.1\"><semantics id=\"A3.SS2.p1.3.m3.1a\"><msub id=\"A3.SS2.p1.3.m3.1.1\" xref=\"A3.SS2.p1.3.m3.1.1.cmml\"><mi id=\"A3.SS2.p1.3.m3.1.1.2\" xref=\"A3.SS2.p1.3.m3.1.1.2.cmml\">𝒙</mi><mi id=\"A3.SS2.p1.3.m3.1.1.3\" xref=\"A3.SS2.p1.3.m3.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.3.m3.1b\"><apply id=\"A3.SS2.p1.3.m3.1.1.cmml\" xref=\"A3.SS2.p1.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.3.m3.1.1.1.cmml\" xref=\"A3.SS2.p1.3.m3.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.3.m3.1.1.2.cmml\" xref=\"A3.SS2.p1.3.m3.1.1.2\">𝒙</ci><ci id=\"A3.SS2.p1.3.m3.1.1.3.cmml\" xref=\"A3.SS2.p1.3.m3.1.1.3\">𝑗</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.3.m3.1c\">\\bm{x}_{j}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS2.p1.3.m3.1d\">bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> when fine-tuning on a proximal document <math alttext=\"\\bm{x}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.4.m4.1\"><semantics id=\"A3.SS2.p1.4.m4.1a\"><msub id=\"A3.SS2.p1.4.m4.1.1\" xref=\"A3.SS2.p1.4.m4.1.1.cmml\"><mi id=\"A3.SS2.p1.4.m4.1.1.2\" xref=\"A3.SS2.p1.4.m4.1.1.2.cmml\">𝒙</mi><mi id=\"A3.SS2.p1.4.m4.1.1.3\" xref=\"A3.SS2.p1.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.4.m4.1b\"><apply id=\"A3.SS2.p1.4.m4.1.1.cmml\" xref=\"A3.SS2.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.4.m4.1.1.1.cmml\" xref=\"A3.SS2.p1.4.m4.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.4.m4.1.1.2.cmml\" xref=\"A3.SS2.p1.4.m4.1.1.2\">𝒙</ci><ci id=\"A3.SS2.p1.4.m4.1.1.3.cmml\" xref=\"A3.SS2.p1.4.m4.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.4.m4.1c\">\\bm{x}_{i}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS2.p1.4.m4.1d\">bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is highest when the model checkpoint is taken from document <math alttext=\"\\bm{x}_{j\\pm b}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.5.m5.1\"><semantics id=\"A3.SS2.p1.5.m5.1a\"><msub id=\"A3.SS2.p1.5.m5.1.1\" xref=\"A3.SS2.p1.5.m5.1.1.cmml\"><mi id=\"A3.SS2.p1.5.m5.1.1.2\" xref=\"A3.SS2.p1.5.m5.1.1.2.cmml\">𝒙</mi><mrow id=\"A3.SS2.p1.5.m5.1.1.3\" xref=\"A3.SS2.p1.5.m5.1.1.3.cmml\"><mi id=\"A3.SS2.p1.5.m5.1.1.3.2\" xref=\"A3.SS2.p1.5.m5.1.1.3.2.cmml\">j</mi><mo id=\"A3.SS2.p1.5.m5.1.1.3.1\" xref=\"A3.SS2.p1.5.m5.1.1.3.1.cmml\">±</mo><mi id=\"A3.SS2.p1.5.m5.1.1.3.3\" xref=\"A3.SS2.p1.5.m5.1.1.3.3.cmml\">b</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.5.m5.1b\"><apply id=\"A3.SS2.p1.5.m5.1.1.cmml\" xref=\"A3.SS2.p1.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.5.m5.1.1.1.cmml\" xref=\"A3.SS2.p1.5.m5.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.5.m5.1.1.2.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.2\">𝒙</ci><apply id=\"A3.SS2.p1.5.m5.1.1.3.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3\"><csymbol cd=\"latexml\" id=\"A3.SS2.p1.5.m5.1.1.3.1.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3.1\">plus-or-minus</csymbol><ci id=\"A3.SS2.p1.5.m5.1.1.3.2.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3.2\">𝑗</ci><ci id=\"A3.SS2.p1.5.m5.1.1.3.3.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3.3\">𝑏</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.5.m5.1c\">\\bm{x}_{j\\pm b}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS2.p1.5.m5.1d\">bold_italic_x start_POSTSUBSCRIPT italic_j ± italic_b end_POSTSUBSCRIPT</annotation></semantics></math> where <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.6.m6.1\"><semantics id=\"A3.SS2.p1.6.m6.1a\"><mi id=\"A3.SS2.p1.6.m6.1.1\" xref=\"A3.SS2.p1.6.m6.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.6.m6.1b\"><ci id=\"A3.SS2.p1.6.m6.1.1.cmml\" xref=\"A3.SS2.p1.6.m6.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.6.m6.1c\">b</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.SS2.p1.6.m6.1d\">italic_b</annotation></semantics></math> is a small number relative to the length of the document sequence.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Additional Related Work</h2>\n<section class=\"ltx_paragraph\" id=\"A4.SS0.SSS0.Px1\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Learning in Structured Environments.</h4>\n<div class=\"ltx_para\" id=\"A4.SS0.SSS0.Px1.p1\">\n<p class=\"ltx_p\" id=\"A4.SS0.SSS0.Px1.p1.1\">Our research also relates to the more general topic of learning in structured environments. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib62\" title=\"\">62</a>]</cite> studied regression and classification tasks with multi-scale temporal structure in the environment characterized by <math alttext=\"1/f\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS0.SSS0.Px1.p1.1.m1.1\"><semantics id=\"A4.SS0.SSS0.Px1.p1.1.m1.1a\"><mrow id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml\"><mn id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.2\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml\">1</mn><mo id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.1\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml\">/</mo><mi id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.3\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml\">f</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A4.SS0.SSS0.Px1.p1.1.m1.1b\"><apply id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1\"><divide id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.1\"></divide><cn id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.2\">1</cn><ci id=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml\" xref=\"A4.SS0.SSS0.Px1.p1.1.m1.1.1.3\">𝑓</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.SS0.SSS0.Px1.p1.1.m1.1c\">1/f</annotation><annotation encoding=\"application/x-llamapun\" id=\"A4.SS0.SSS0.Px1.p1.1.m1.1d\">1 / italic_f</annotation></semantics></math> dynamics. While the cyclic training setting that we study is a more simplified setup than that of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib62\" title=\"\">62</a>]</cite>, we aim at unveiling more insights on applying standard SGD on over-parameterized networks. A potential direction for future work would be to study anticipatory recovery in regimes with richer, hierarchical sequence structure.</p>\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"A4.SS0.SSS0.Px2\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Emergent Capabilities.</h4>\n<div class=\"ltx_para\" id=\"A4.SS0.SSS0.Px2.p1\">\n<p class=\"ltx_p\" id=\"A4.SS0.SSS0.Px2.p1.1\">Recent advancements in large-scale Transformer networks <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib66\" title=\"\">66</a>, <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">1</a>]</cite> have demonstrated exceptional ability to model long sequence language data. Beyond basic language modeling and downstream task performance, these models have shown emergent behaviors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib67\" title=\"\">67</a>]</cite> that appear to manifest only beyond a certain model scale <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"#bib.bib68\" title=\"\">68</a>, <a class=\"ltx_ref\" href=\"#bib.bib69\" title=\"\">69</a>, <a class=\"ltx_ref\" href=\"#bib.bib70\" title=\"\">70</a>]</cite>. Related to our research, recent studies reveal that LLMs possess remarkable memorization skills, enabling them to recall news sentences after just a few exposures <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"#bib.bib71\" title=\"\">71</a>, <a class=\"ltx_ref\" href=\"#bib.bib72\" title=\"\">72</a>]</cite>. However, the sequential learning dynamics behind such memorization have not been thoroughly examined. Our work comprehensively explore the sequential learning setting with cyclic task repetition and demonstrates task anticipation, a new emergent capability of large models.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A5\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Broader Impact</h2>\n<div class=\"ltx_para\" id=\"A5.p1\">\n<p class=\"ltx_p\" id=\"A5.p1.1\">This research deepens our understanding of catastrophic interference in naturalistic training setups. This understanding could lead to the design of better training algorithms of LLMs and other large neural networks that are more similar to human learning. These algorithms may give rise to more powerful and embodied AI systems with online adaptive learning capability, which may have many potential societal consequences.</p>\n</div>\n<div class=\"ltx_para\" id=\"A5.p2\">\n<p class=\"ltx_p\" id=\"A5.p2.1\">Recovery from catastrophic interference might be undesirable in scenarios where some documents are intended to be forgotten. Our research provide more understanding into the anticipatory recovery phenomenon. Future research into the unlearning problem as well as a deeper understanding of the training dynamics of different types of structured environments can help further address the privacy issue.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"Ax1\">\n<h2 class=\"ltx_title ltx_title_appendix\">NeurIPS Paper Checklist</h2>\n<div class=\"ltx_para\" id=\"Ax1.p1\">\n<ol class=\"ltx_enumerate\" id=\"Ax1.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i1.p1.1.1\">Claims</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix1.p1.1\">Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix2.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix2.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix3.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix3.p1.1.1\" style=\"color:#0000FF;\">The authors confirm that the claims made in the abstract and introduction accurately reflect the paper’s contributions and scope.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix4.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix4.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix4.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix4.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix4.I1.i1.p1.1\">The answer NA means that the abstract and introduction do not include the claims made in the paper.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix4.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix4.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix4.I1.i2.p1.1\">The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix4.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix4.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix4.I1.i3.p1.1\">The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix4.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix4.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix4.I1.i4.p1.1\">It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i2.p1.1.1\">Limitations</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix5.p1.1\">Question: Does the paper discuss the limitations of the work performed by the authors?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix6.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix6.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix7.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix7.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix7.p1.1.1\" style=\"color:#0000FF;\">The limitations of this work are discussed in Section <a class=\"ltx_ref\" href=\"#S6\" title=\"6 Discussion and Limitations ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix8.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i1.p1.1\">The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i2.p1.1\">The authors are encouraged to create a separate \"Limitations\" section in their paper.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i3.p1.1\">The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i4.p1.1\">The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i5.p1.1\">The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i6.p1.1\">The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i7.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i7.p1.1\">If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix8.I1.i8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix8.I1.i8.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix8.I1.i8.p1.1\">While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i3.p1.1.1\">Theory Assumptions and Proofs</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix9\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix9.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix9.p1.1\">Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix10\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix10.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix10.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix10.p1.1.1\" style=\"color:#808080;\">[N/A] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix11\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix11.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix11.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix11.p1.1.1\" style=\"color:#0000FF;\">The paper does not include theoretical results. While we provide a well-defined computational model in section <a class=\"ltx_ref\" href=\"#S4.SS4\" title=\"4.4 Computational Toy Model ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>, we are not trying to prove new theoretical results and the purpose of section <a class=\"ltx_ref\" href=\"#S4.SS4\" title=\"4.4 Computational Toy Model ‣ 4 Understanding Cyclic Training Dynamics ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a> is to provide more intuition on how the anticipatory recovery phenomenon might occur.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix12.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.I1.i1.p1.1\">The answer NA means that the paper does not include theoretical results.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.I1.i2.p1.1\">All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.I1.i3.p1.1\">All assumptions should be clearly stated or referenced in the statement of any theorems.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.I1.i4.p1.1\">The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.I1.i5.p1.1\">Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix12.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix12.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix12.I1.i6.p1.1\">Theorems and Lemmas that the proof relies upon should be properly referenced.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">4.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i4.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i4.p1.1.1\">Experimental Result Reproducibility</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix13\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix13.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix13.p1.1\">Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix14\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix14.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix14.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix14.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix15\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix15.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix15.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix15.p1.1.1\" style=\"color:#0000FF;\">In Sections <a class=\"ltx_ref\" href=\"#S2\" title=\"2 Data and Experiment Setup ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and Appendices <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, <a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Additional Experiment Results ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> we disclose all the information needed to reproduce the main experimental results of the paper. The pre-trained models and datasets we use are also publicly available. We also include the code for reproducing main experimental results in the supplementary material.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix16.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i1.p1.1\">The answer NA means that the paper does not include experiments.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i2.p1.1\">If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i3.p1.1\">If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i4.p1.1\">Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i5.p1.1\">While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example</p>\n<ol class=\"ltx_enumerate\" id=\"Ax1.I1.ix16.I1.i5.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i5.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(a)</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i5.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i5.I1.i1.p1.1\">If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i5.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(b)</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i5.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i5.I1.i2.p1.1\">If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i5.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(c)</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i5.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i5.I1.i3.p1.1\">If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix16.I1.i5.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">(d)</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix16.I1.i5.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix16.I1.i5.I1.i4.p1.1\">We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>\n</div>\n</li>\n</ol>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">5.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i5.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i5.p1.1.1\">Open access to data and code</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix17\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix17.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix17.p1.1\">Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix18\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix18.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix18.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix18.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix19\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix19.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix19.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix19.p1.1.1\" style=\"color:#0000FF;\">We provide the code and instructions for reproducing main experimental results in the supplementary material.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix20.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i1.p1.1\">The answer NA means that paper does not include experiments requiring code.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i2.p1.1\">Please see the NeurIPS code and data submission guidelines (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://nips.cc/public/guides/CodeSubmissionPolicy\" title=\"\">https://nips.cc/public/guides/CodeSubmissionPolicy</a>) for more details.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i3.p1.1\">While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i4.p1.1\">The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://nips.cc/public/guides/CodeSubmissionPolicy\" title=\"\">https://nips.cc/public/guides/CodeSubmissionPolicy</a>) for more details.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i5.p1.1\">The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i6.p1.1\">The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i7.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i7.p1.1\">At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix20.I1.i8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix20.I1.i8.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix20.I1.i8.p1.1\">Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">6.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i6.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i6.p1.1.1\">Experimental Setting/Details</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix21\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix21.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix21.p1.1\">Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix22\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix22.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix22.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix22.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix23\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix23.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix23.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix23.p1.1.1\" style=\"color:#0000FF;\">The experimental settings and details are provided in Section <a class=\"ltx_ref\" href=\"#S2\" title=\"2 Data and Experiment Setup ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and Appendix <a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix24\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix24.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix24.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix24.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix24.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix24.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix24.I1.i1.p1.1\">The answer NA means that the paper does not include experiments.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix24.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix24.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix24.I1.i2.p1.1\">The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix24.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix24.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix24.I1.i3.p1.1\">The full details can be provided either with the code, in appendix, or as supplemental material.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">7.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i7.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i7.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i7.p1.1.1\">Experiment Statistical Significance</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix25\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix25.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix25.p1.1\">Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix26\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix26.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix26.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix26.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix27\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix27.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix27.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix27.p1.1.1\" style=\"color:#0000FF;\">All experiment result figures and tables in Section <a class=\"ltx_ref\" href=\"#S3\" title=\"3 Emergent Anticipatory Recovery ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> of the paper are accompanied by error bars or confidence intervals. Some error bars might not be visible since they are too small.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix28.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i1.p1.1\">The answer NA means that the paper does not include experiments.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i2.p1.1\">The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i3.p1.1\">The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i4.p1.1\">The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i5.p1.1\">The assumptions made should be given (e.g., Normally distributed errors).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i6.p1.1\">It should be clear whether the error bar is the standard deviation or the standard error of the mean.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i7.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i7.p1.1\">It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i8.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i8.p1.1\">For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix28.I1.i9\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix28.I1.i9.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix28.I1.i9.p1.1\">If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">8.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i8.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i8.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i8.p1.1.1\">Experiments Compute Resources</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix29\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix29.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix29.p1.1\">Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix30\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix30.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix30.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix30.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix31\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix31.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix31.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix31.p1.1.1\" style=\"color:#0000FF;\">We provided detailed information on the compute resources needed to reproduce the experiments in Section <a class=\"ltx_ref\" href=\"#A1.SS6\" title=\"A.6 Compute Resources ‣ Appendix A Additional Experiment Details ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">A.6</span></a>.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix32\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix32.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix32.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix32.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix32.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix32.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix32.I1.i1.p1.1\">The answer NA means that the paper does not include experiments.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix32.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix32.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix32.I1.i2.p1.1\">The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix32.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix32.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix32.I1.i3.p1.1\">The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix32.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix32.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix32.I1.i4.p1.1\">The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper).</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i9\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">9.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i9.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i9.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i9.p1.1.1\">Code Of Ethics</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix33\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix33.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix33.p1.1\">Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://neurips.cc/public/EthicsGuidelines\" title=\"\">https://neurips.cc/public/EthicsGuidelines</a>?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix34\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix34.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix34.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix34.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix35\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix35.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix35.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix35.p1.1.1\" style=\"color:#0000FF;\">The authors confirm that this paper conforms with the NeurIPS Code of Ethics. We used publicly available, non-deprecated datasets and models, and conform with their licenses.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix36\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix36.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix36.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix36.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix36.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix36.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix36.I1.i1.p1.1\">The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix36.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix36.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix36.I1.i2.p1.1\">If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix36.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix36.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix36.I1.i3.p1.1\">The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i10\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">10.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i10.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i10.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i10.p1.1.1\">Broader Impacts</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix37\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix37.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix37.p1.1\">Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix38\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix38.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix38.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix38.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix39\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix39.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix39.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix39.p1.1.1\" style=\"color:#0000FF;\">The broader impacts of this work are discussed in Appendix <a class=\"ltx_ref\" href=\"#A5\" title=\"Appendix E Broader Impact ‣ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix40.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.I1.i1.p1.1\">The answer NA means that there is no societal impact of the work performed.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.I1.i2.p1.1\">If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.I1.i3.p1.1\">Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.I1.i4.p1.1\">The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.I1.i5.p1.1\">The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix40.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix40.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix40.I1.i6.p1.1\">If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i11\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">11.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i11.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i11.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i11.p1.1.1\">Safeguards</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix41\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix41.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix41.p1.1\">Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix42\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix42.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix42.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix42.p1.1.1\" style=\"color:#808080;\">[N/A] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix43\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix43.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix43.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix43.p1.1.1\" style=\"color:#0000FF;\">This paper does not release new data or models.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix44\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix44.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix44.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix44.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix44.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix44.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix44.I1.i1.p1.1\">The answer NA means that the paper poses no such risks.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix44.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix44.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix44.I1.i2.p1.1\">Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix44.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix44.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix44.I1.i3.p1.1\">Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix44.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix44.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix44.I1.i4.p1.1\">We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i12\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">12.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i12.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i12.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i12.p1.1.1\">Licenses for existing assets</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix45\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix45.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix45.p1.1\">Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix46\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix46.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix46.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix46.p1.1.1\" style=\"color:#0000FF;\">[Yes] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix47\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix47.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix47.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix47.p1.1.1\" style=\"color:#0000FF;\">The original papers for all the code, data and models used in the main experiments of the paper are cited. All these existing assets involved are publicly available, under the Apache 2.0 License (Pythia, huggingface transformers) or MIT License (cnn/dailymail version 3.0.0).</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix48.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i1.p1.1\">The answer NA means that the paper does not use existing assets.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i2.p1.1\">The authors should cite the original paper that produced the code package or dataset.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i3.p1.1\">The authors should state which version of the asset is used and, if possible, include a URL.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i4.p1.1\">The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i5.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i5.p1.1\">For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i6.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i6.p1.1\">If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"paperswithcode.com/datasets\" title=\"\">paperswithcode.com/datasets</a> has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i7.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i7.p1.1\">For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix48.I1.i8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix48.I1.i8.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix48.I1.i8.p1.1\">If this information is not available online, the authors are encouraged to reach out to the asset’s creators.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i13\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">13.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i13.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i13.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i13.p1.1.1\">New Assets</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix49\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix49.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix49.p1.1\">Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix50\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix50.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix50.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix50.p1.1.1\" style=\"color:#808080;\">[N/A] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix51\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix51.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix51.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix51.p1.1.1\" style=\"color:#0000FF;\">This paper does not release new assets.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix52\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix52.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix52.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix52.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix52.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix52.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix52.I1.i1.p1.1\">The answer NA means that the paper does not release new assets.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix52.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix52.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix52.I1.i2.p1.1\">Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix52.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix52.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix52.I1.i3.p1.1\">The paper should discuss whether and how consent was obtained from people whose asset is used.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix52.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix52.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix52.I1.i4.p1.1\">At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i14\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">14.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i14.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i14.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i14.p1.1.1\">Crowdsourcing and Research with Human Subjects</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix53\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix53.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix53.p1.1\">Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix54\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix54.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix54.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix54.p1.1.1\" style=\"color:#808080;\">[N/A] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix55\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix55.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix55.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix55.p1.1.1\" style=\"color:#0000FF;\">This paper does not involve crowdsourcing nor research with human subjects.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix56\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix56.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix56.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix56.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix56.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix56.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix56.I1.i1.p1.1\">The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix56.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix56.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix56.I1.i2.p1.1\">Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix56.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix56.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix56.I1.i3.p1.1\">According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.i15\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">15.</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.i15.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.i15.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Ax1.I1.i15.p1.1.1\">Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix57\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix57.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix57.p1.1\">Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix58\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix58.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix58.p1.1\">Answer: <span class=\"ltx_text\" id=\"Ax1.I1.ix58.p1.1.1\" style=\"color:#808080;\">[N/A] </span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix59\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix59.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix59.p1.1\">Justification: <span class=\"ltx_text\" id=\"Ax1.I1.ix59.p1.1.1\" style=\"color:#0000FF;\">This paper does not involve crowdsourcing nor research with human subjects.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix60\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\"></span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix60.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix60.p1.1\">Guidelines:</p>\n<ul class=\"ltx_itemize\" id=\"Ax1.I1.ix60.I1\">\n<li class=\"ltx_item\" id=\"Ax1.I1.ix60.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix60.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix60.I1.i1.p1.1\">The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix60.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix60.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix60.I1.i2.p1.1\">Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix60.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix60.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix60.I1.i3.p1.1\">We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Ax1.I1.ix60.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"Ax1.I1.ix60.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"Ax1.I1.ix60.I1.i4.p1.1\">For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>",
  "css": "",
  "arxiv_id": "2403.09613",
  "source": "arxiv-experimental",
  "generated": "2025-10-18T01:15:39.636Z"
}
