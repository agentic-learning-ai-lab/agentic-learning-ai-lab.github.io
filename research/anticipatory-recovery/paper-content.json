{
  "html": "<section id=\"S1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n\n<figure id=\"S1.F1\" class=\"ltx_figure\">\n<div id=\"S1.F1.2\" class=\"ltx_block\">\n<figure id=\"S1.F1.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x1.png\" id=\"S1.F1.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"153\" height=\"121\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S1.F1.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S1.F1.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Loss on Document 1 </span></figcaption>\n</figure>\n<figure id=\"S1.F1.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x2.png\" id=\"S1.F1.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"152\" height=\"121\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S1.F1.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S1.F1.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Shift-averaged loss </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S1.F1.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>: </span><span id=\"S1.F1.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Loss curves for cyclic fine-tuning on the pre-trained Pythia-1b model, with 25 documents. The small black circles indicate points just prior to training on the focal task. These bell-shaped loss curves within each epoch demonstrate the anticipatory recovery phenomenon.\n</span></figcaption>\n</figure>\n<div id=\"S1.p1\" class=\"ltx_para\">\n<p id=\"S1.p1.1\" class=\"ltx_p\">Large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin etÂ al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2019</a>; Brown etÂ al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2020</a>; Touvron etÂ al., <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2023</a>; OpenAI, <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> have demonstrated remarkable general capabilities in a wide range of natural language tasks.\nDuring the training of LLMs, documents are typically uniformly sampled at random. Due to the large scale of the training setâ€”in contrast to many other domainsâ€”LLM training typically occurs in an online fashion: each document is used only once for just one update step without further repetition <cite class=\"ltx_cite ltx_citemacro_citep\">(Hoffmann etÂ al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2022</a>; Chowdhery etÂ al., <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2023</a>; Xue etÂ al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2024</a>)</cite>.</p>\n</div>\n<div id=\"S1.p2\" class=\"ltx_para\">\n<p id=\"S1.p2.1\" class=\"ltx_p\">Such a training style is in stark contrast with how real world agents like humans acquire new knowledge. In naturalistic settings, the material weâ€™re exposed to is structured in time and often repeats. And given the cost of acquiring information in the real world, people aim to maximize their information gain from each episode. Obtaining new data is often associated with a cost, whether a mental switching costâ€”as when we go from one lecture to anotherâ€”or a time cost of waiting for the information. In such scenarios, we hypothesize that real-world agents will spend more compute to best leverage each document than to switch among many.</p>\n</div>\n<div id=\"S1.p3\" class=\"ltx_para\">\n<p id=\"S1.p3.1\" class=\"ltx_p\">Toward the goal of investigating more naturalistic training setups, we study a simplistic setting involving structured training of LLMs: documents are presented cyclically in a fixed sequence and are repeated multiple times, just as we humans go through our daily routines. Moreover, to account for the cost of switching among documents, we allow the network take multiple gradient steps for each document.</p>\n</div>\n<div id=\"S1.p4\" class=\"ltx_para\">\n<p id=\"S1.p4.1\" class=\"ltx_p\">Typically, networks exhibit <em id=\"S1.p4.1.1\" class=\"ltx_emph ltx_font_italic\">catastrophic interference</em> (also known as catastrophic forgetting) <cite class=\"ltx_cite ltx_citemacro_citep\">(McCloskey and Cohen, <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">1989</a>)</cite> when training on a sequence of tasks: the loss on a given document increases as the training advances to other documents.\nSurprisingly, we show that in a structured training environment, LLMs exhibit a curious <em id=\"S1.p4.1.2\" class=\"ltx_emph ltx_font_italic\">anticipatory recovery</em> behavior: they\nrecover from the forgetting of one document before seeing it again, multiple steps in the sequence prior to the recurrence of the document (see FigureÂ <a href=\"#S1.F1\" title=\"Figure 1 â€£ 1 Introduction â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). It is analogous to a person anticipating to eat breakfast while taking a morning shower, but leaving the thought aside for the rest of the day. It is remarkable as there is no explicit memory in LLMs that stores sequential knowledge across documents, and there is no systematic overlap of content across documentsâ€”the behavior emerges from a random document sequence after repeated exposure to that sequence.\nFurthermore, only large scale networks reawaken their old knowledge during cyclic training, while smaller ones exhibit no such behavior.</p>\n</div>\n<div id=\"S1.p5\" class=\"ltx_para\">\n<p id=\"S1.p5.1\" class=\"ltx_p\">Through extensive experiments, we study how different factors in model architecture and training contribute to the anticipatory recovery phenomenon. We offer insights on the training dynamics in sequentially and cyclically structured input data, and we propose hypotheses for the causes of the behavior.\nWe also show that this phenomenon is not unique to LLMs; some vision models with sufficient width and depth also demonstrate a similar behavior, but LLMs on language modeling tasks exhibit the strongest recovery.</p>\n</div>\n</section>\n<section id=\"S2\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Data and Experiment Setup</h2>\n\n<div id=\"S2.p1\" class=\"ltx_para\">\n<p id=\"S2.p1.1\" class=\"ltx_p\">In this section we describe the models, datasets, and training setups that we use in the subsequent LLM experiments. Additional details are presented in Appendix <a href=\"#A1\" title=\"Appendix A Additional Experiment Setups â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<section id=\"S2.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models.</h4>\n\n<div id=\"S2.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px1.p1.1\" class=\"ltx_p\">We use Pythia <cite class=\"ltx_cite ltx_citemacro_citep\">(Biderman etÂ al., <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, a suite of decoder-only autoregressive language models pre-trained on the Pile dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao etÂ al., <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2020</a>; Biderman etÂ al., <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. The Pythia suite has 8 models of different sizes. For our purposes, we use the five medium-sized models from 160M to 2.8B parameters. We use the fully pre-trained model (at 143K training steps) as well as the untrained initializations to study the effect of pre-training.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets.</h4>\n\n<div id=\"S2.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px2.p1.1\" class=\"ltx_p\">We use the CNN/Daily Mail news dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Nallapati etÂ al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>. The dataset is originally designed for text summarization; we re-purpose it for causal language modeling by discarding the summaries and only using the articles as training data. Importantly, the CNN/Daily Mail dataset is not part of the Pile dataset and hence it is a new domain for the Pythia pre-trained models.</p>\n</div>\n<div id=\"S2.SS0.SSS0.Px2.p2\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px2.p2.1\" class=\"ltx_p\">We use the same documents for both training and evaluation. Our goal here is not to determine whether a trained model generalizes to new documents, but rather to study the memory for a particular document as a function of position within the training history.</p>\n</div>\n</section>\n<section id=\"S2.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training Setup.</h4>\n\n<div id=\"S2.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S2.SS0.SSS0.Px3.p1.11\" class=\"ltx_p\">We randomly sample <math id=\"S2.SS0.SSS0.Px3.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.1.m1.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.1.m1.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.1.m1.1.1\">ğ‘‡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.1.m1.1c\">T</annotation></semantics></math> documents from the CNN/Daily Mail news dataset. In pre-processing, we truncate each document and take only the first <math id=\"S2.SS0.SSS0.Px3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.2.m2.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.2.m2.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.2.m2.1.1\">ğ¶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.2.m2.1c\">C</annotation></semantics></math> tokens (we refer to <math id=\"S2.SS0.SSS0.Px3.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.3.m3.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.3.m3.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.3.m3.1.1\">ğ¶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.3.m3.1c\">C</annotation></semantics></math> as â€œcontext lengthâ€ in subsequent text). We then fine-tune our language model on each pre-processed sample for <math id=\"S2.SS0.SSS0.Px3.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.4.m4.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.4.m4.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.4.m4.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.4.m4.1c\">M</annotation></semantics></math> gradient steps (i.e., using a batch size of 1). We refer to the multiple gradient updates of each document as an â€œepisodeâ€. After each episode we evaluate the loss on all <math id=\"S2.SS0.SSS0.Px3.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.5.m5.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.5.m5.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.5.m5.1.1\">ğ‘‡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.5.m5.1c\">T</annotation></semantics></math> documents. We repeat the training process for <math id=\"S2.SS0.SSS0.Px3.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"E\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.6.m6.1a\"><mi id=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.6.m6.1b\"><ci id=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.6.m6.1.1\">ğ¸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.6.m6.1c\">E</annotation></semantics></math> epochs, where an epoch consists of one episode of each document in a fixed sequence. We use a vanilla gradient descent optimizer with learning rate <math id=\"S2.SS0.SSS0.Px3.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.7.m7.1a\"><mn id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1b\"><cn type=\"float\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.7.m7.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.7.m7.1c\">0.001</annotation></semantics></math>. Unless otherwise stated, the default hyper-parameters in the subsequent experiments are <math id=\"S2.SS0.SSS0.Px3.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"T=25\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.8.m8.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml\">T</mi><mo id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.2\">ğ‘‡</ci><cn type=\"integer\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.8.m8.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.8.m8.1c\">T=25</annotation></semantics></math>, <math id=\"S2.SS0.SSS0.Px3.p1.9.m9.1\" class=\"ltx_Math\" alttext=\"C=256\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.9.m9.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2.cmml\">C</mi><mo id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3.cmml\">256</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.2\">ğ¶</ci><cn type=\"integer\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.9.m9.1.1.3\">256</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.9.m9.1c\">C=256</annotation></semantics></math>, <math id=\"S2.SS0.SSS0.Px3.p1.10.m10.1\" class=\"ltx_Math\" alttext=\"M=10\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.10.m10.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2.cmml\">M</mi><mo id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.10.m10.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.10.m10.1c\">M=10</annotation></semantics></math>, <math id=\"S2.SS0.SSS0.Px3.p1.11.m11.1\" class=\"ltx_Math\" alttext=\"E=5\" display=\"inline\"><semantics id=\"S2.SS0.SSS0.Px3.p1.11.m11.1a\"><mrow id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.cmml\"><mi id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.2\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.2.cmml\">E</mi><mo id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.1\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.1.cmml\">=</mo><mn id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.3\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS0.SSS0.Px3.p1.11.m11.1b\"><apply id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1\"><eq id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.1.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.1\"></eq><ci id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.2.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.2\">ğ¸</ci><cn type=\"integer\" id=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.3.cmml\" xref=\"S2.SS0.SSS0.Px3.p1.11.m11.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS0.SSS0.Px3.p1.11.m11.1c\">E=5</annotation></semantics></math>. We use the average cross entropy loss (average negative log-likelihood for each token) as our training and evaluation metric. For all experiments we run 3 to 5 trials with different random seeds. The shaded area in the figures denotes standard deviation among trials.</p>\n</div>\n</section>\n</section>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Emergent Anticipatory Recovery</h2>\n\n<figure id=\"S3.F2\" class=\"ltx_figure\">\n<div id=\"S3.F2.2\" class=\"ltx_block\">\n<figure id=\"S3.F2.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x3.png\" id=\"S3.F2.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"243\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F2.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S3.F2.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Pre-trained Models</span></figcaption>\n</figure>\n<figure id=\"S3.F2.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x4.png\" id=\"S3.F2.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"246\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F2.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S3.F2.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Random Initializations </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F2.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>: </span><span id=\"S3.F2.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of model size for (a) pre-trained models and (b) random initializations. In each subfigure, the left shows shift-averaged loss curves for five model sizes and the right shows the recovery score as a function of model size.</span></figcaption>\n</figure>\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p id=\"S3.p1.1\" class=\"ltx_p\">In this section, we present our experimental results that reveal the anticipatory recovery phenomenon in the cyclic fine-tuning of large language models. We then demonstrate that anticipatory recovery is an emergent behavior that appears only for models with sufficient capacity.</p>\n</div>\n<section id=\"S3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>The Anticipatory Recovery Phenomenon</h3>\n\n<div id=\"S3.SS1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.p1.15\" class=\"ltx_p\">In this first experiment (Figure <a href=\"#S1.F1\" title=\"Figure 1 â€£ 1 Introduction â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), we have <math id=\"S3.SS1.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"T=25\" display=\"inline\"><semantics id=\"S3.SS1.p1.1.m1.1a\"><mrow id=\"S3.SS1.p1.1.m1.1.1\" xref=\"S3.SS1.p1.1.m1.1.1.cmml\"><mi id=\"S3.SS1.p1.1.m1.1.1.2\" xref=\"S3.SS1.p1.1.m1.1.1.2.cmml\">T</mi><mo id=\"S3.SS1.p1.1.m1.1.1.1\" xref=\"S3.SS1.p1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.SS1.p1.1.m1.1.1.3\" xref=\"S3.SS1.p1.1.m1.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.1.m1.1b\"><apply id=\"S3.SS1.p1.1.m1.1.1.cmml\" xref=\"S3.SS1.p1.1.m1.1.1\"><eq id=\"S3.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S3.SS1.p1.1.m1.1.1.1\"></eq><ci id=\"S3.SS1.p1.1.m1.1.1.2.cmml\" xref=\"S3.SS1.p1.1.m1.1.1.2\">ğ‘‡</ci><cn type=\"integer\" id=\"S3.SS1.p1.1.m1.1.1.3.cmml\" xref=\"S3.SS1.p1.1.m1.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.1.m1.1c\">T=25</annotation></semantics></math> documents, and we do cyclic fine-tuning on the documents for <math id=\"S3.SS1.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"E=5\" display=\"inline\"><semantics id=\"S3.SS1.p1.2.m2.1a\"><mrow id=\"S3.SS1.p1.2.m2.1.1\" xref=\"S3.SS1.p1.2.m2.1.1.cmml\"><mi id=\"S3.SS1.p1.2.m2.1.1.2\" xref=\"S3.SS1.p1.2.m2.1.1.2.cmml\">E</mi><mo id=\"S3.SS1.p1.2.m2.1.1.1\" xref=\"S3.SS1.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"S3.SS1.p1.2.m2.1.1.3\" xref=\"S3.SS1.p1.2.m2.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.2.m2.1b\"><apply id=\"S3.SS1.p1.2.m2.1.1.cmml\" xref=\"S3.SS1.p1.2.m2.1.1\"><eq id=\"S3.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS1.p1.2.m2.1.1.1\"></eq><ci id=\"S3.SS1.p1.2.m2.1.1.2.cmml\" xref=\"S3.SS1.p1.2.m2.1.1.2\">ğ¸</ci><cn type=\"integer\" id=\"S3.SS1.p1.2.m2.1.1.3.cmml\" xref=\"S3.SS1.p1.2.m2.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.2.m2.1c\">E=5</annotation></semantics></math> epochs in the same ordering. Both the documents and the ordering are sampled at random beforehand, but kept fixed during the sequential fine-tuning process. We refer to these <math id=\"S3.SS1.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S3.SS1.p1.3.m3.1a\"><mi id=\"S3.SS1.p1.3.m3.1.1\" xref=\"S3.SS1.p1.3.m3.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.3.m3.1b\"><ci id=\"S3.SS1.p1.3.m3.1.1.cmml\" xref=\"S3.SS1.p1.3.m3.1.1\">ğ‘‡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.3.m3.1c\">T</annotation></semantics></math> documents as <math id=\"S3.SS1.p1.4.m4.3\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1},\\cdots,\\bm{x}_{T}\" display=\"inline\"><semantics id=\"S3.SS1.p1.4.m4.3a\"><mrow id=\"S3.SS1.p1.4.m4.3.3.2\" xref=\"S3.SS1.p1.4.m4.3.3.3.cmml\"><msub id=\"S3.SS1.p1.4.m4.2.2.1.1\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.cmml\"><mi id=\"S3.SS1.p1.4.m4.2.2.1.1.2\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.4.m4.2.2.1.1.3\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.3.cmml\">1</mn></msub><mo id=\"S3.SS1.p1.4.m4.3.3.2.3\" xref=\"S3.SS1.p1.4.m4.3.3.3.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S3.SS1.p1.4.m4.1.1\" xref=\"S3.SS1.p1.4.m4.1.1.cmml\">â‹¯</mi><mo id=\"S3.SS1.p1.4.m4.3.3.2.4\" xref=\"S3.SS1.p1.4.m4.3.3.3.cmml\">,</mo><msub id=\"S3.SS1.p1.4.m4.3.3.2.2\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.cmml\"><mi id=\"S3.SS1.p1.4.m4.3.3.2.2.2\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.2.cmml\">ğ’™</mi><mi id=\"S3.SS1.p1.4.m4.3.3.2.2.3\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.3.cmml\">T</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.4.m4.3b\"><list id=\"S3.SS1.p1.4.m4.3.3.3.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2\"><apply id=\"S3.SS1.p1.4.m4.2.2.1.1.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.4.m4.2.2.1.1.1.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.4.m4.2.2.1.1.2.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.4.m4.2.2.1.1.3.cmml\" xref=\"S3.SS1.p1.4.m4.2.2.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p1.4.m4.1.1.cmml\" xref=\"S3.SS1.p1.4.m4.1.1\">â‹¯</ci><apply id=\"S3.SS1.p1.4.m4.3.3.2.2.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.4.m4.3.3.2.2.1.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2\">subscript</csymbol><ci id=\"S3.SS1.p1.4.m4.3.3.2.2.2.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.2\">ğ’™</ci><ci id=\"S3.SS1.p1.4.m4.3.3.2.2.3.cmml\" xref=\"S3.SS1.p1.4.m4.3.3.2.2.3\">ğ‘‡</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.4.m4.3c\">\\bm{x}_{1},\\cdots,\\bm{x}_{T}</annotation></semantics></math>. At the start, we fine-tune on document 1 for 10 gradient steps, leading to a significant decrease in the modelâ€™s loss on <math id=\"S3.SS1.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.5.m5.1a\"><msub id=\"S3.SS1.p1.5.m5.1.1\" xref=\"S3.SS1.p1.5.m5.1.1.cmml\"><mi id=\"S3.SS1.p1.5.m5.1.1.2\" xref=\"S3.SS1.p1.5.m5.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.5.m5.1.1.3\" xref=\"S3.SS1.p1.5.m5.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.5.m5.1b\"><apply id=\"S3.SS1.p1.5.m5.1.1.cmml\" xref=\"S3.SS1.p1.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.5.m5.1.1.1.cmml\" xref=\"S3.SS1.p1.5.m5.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.5.m5.1.1.2.cmml\" xref=\"S3.SS1.p1.5.m5.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.5.m5.1.1.3.cmml\" xref=\"S3.SS1.p1.5.m5.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.5.m5.1c\">\\bm{x}_{1}</annotation></semantics></math>. As we move away from <math id=\"S3.SS1.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.6.m6.1a\"><msub id=\"S3.SS1.p1.6.m6.1.1\" xref=\"S3.SS1.p1.6.m6.1.1.cmml\"><mi id=\"S3.SS1.p1.6.m6.1.1.2\" xref=\"S3.SS1.p1.6.m6.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.6.m6.1.1.3\" xref=\"S3.SS1.p1.6.m6.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.6.m6.1b\"><apply id=\"S3.SS1.p1.6.m6.1.1.cmml\" xref=\"S3.SS1.p1.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.6.m6.1.1.1.cmml\" xref=\"S3.SS1.p1.6.m6.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.6.m6.1.1.2.cmml\" xref=\"S3.SS1.p1.6.m6.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.6.m6.1.1.3.cmml\" xref=\"S3.SS1.p1.6.m6.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.6.m6.1c\">\\bm{x}_{1}</annotation></semantics></math> and fine-tune on other documents, we naturally observe forgetting: the modelâ€™s loss on <math id=\"S3.SS1.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.7.m7.1a\"><msub id=\"S3.SS1.p1.7.m7.1.1\" xref=\"S3.SS1.p1.7.m7.1.1.cmml\"><mi id=\"S3.SS1.p1.7.m7.1.1.2\" xref=\"S3.SS1.p1.7.m7.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.7.m7.1.1.3\" xref=\"S3.SS1.p1.7.m7.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.7.m7.1b\"><apply id=\"S3.SS1.p1.7.m7.1.1.cmml\" xref=\"S3.SS1.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.7.m7.1.1.1.cmml\" xref=\"S3.SS1.p1.7.m7.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.7.m7.1.1.2.cmml\" xref=\"S3.SS1.p1.7.m7.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.7.m7.1.1.3.cmml\" xref=\"S3.SS1.p1.7.m7.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.7.m7.1c\">\\bm{x}_{1}</annotation></semantics></math> gradually increases, due to catastrophic interference, until we finish fine-tuning on all other documents and return to <math id=\"S3.SS1.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.8.m8.1a\"><msub id=\"S3.SS1.p1.8.m8.1.1\" xref=\"S3.SS1.p1.8.m8.1.1.cmml\"><mi id=\"S3.SS1.p1.8.m8.1.1.2\" xref=\"S3.SS1.p1.8.m8.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.8.m8.1.1.3\" xref=\"S3.SS1.p1.8.m8.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.8.m8.1b\"><apply id=\"S3.SS1.p1.8.m8.1.1.cmml\" xref=\"S3.SS1.p1.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.8.m8.1.1.1.cmml\" xref=\"S3.SS1.p1.8.m8.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.8.m8.1.1.2.cmml\" xref=\"S3.SS1.p1.8.m8.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.8.m8.1.1.3.cmml\" xref=\"S3.SS1.p1.8.m8.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.8.m8.1c\">\\bm{x}_{1}</annotation></semantics></math>. As we iterate through the same document sequence for a second time, we would normally expect the loss on <math id=\"S3.SS1.p1.9.m9.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.9.m9.1a\"><msub id=\"S3.SS1.p1.9.m9.1.1\" xref=\"S3.SS1.p1.9.m9.1.1.cmml\"><mi id=\"S3.SS1.p1.9.m9.1.1.2\" xref=\"S3.SS1.p1.9.m9.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.9.m9.1.1.3\" xref=\"S3.SS1.p1.9.m9.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.9.m9.1b\"><apply id=\"S3.SS1.p1.9.m9.1.1.cmml\" xref=\"S3.SS1.p1.9.m9.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.9.m9.1.1.1.cmml\" xref=\"S3.SS1.p1.9.m9.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.9.m9.1.1.2.cmml\" xref=\"S3.SS1.p1.9.m9.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.9.m9.1.1.3.cmml\" xref=\"S3.SS1.p1.9.m9.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.9.m9.1c\">\\bm{x}_{1}</annotation></semantics></math> to still keep increasing after the initial decrease. However, Figure <a href=\"#S1.F1.sf1\" title=\"Figure 1(a) â€£ Figure 1 â€£ 1 Introduction â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> shows that the loss on <math id=\"S3.SS1.p1.10.m10.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.10.m10.1a\"><msub id=\"S3.SS1.p1.10.m10.1.1\" xref=\"S3.SS1.p1.10.m10.1.1.cmml\"><mi id=\"S3.SS1.p1.10.m10.1.1.2\" xref=\"S3.SS1.p1.10.m10.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.10.m10.1.1.3\" xref=\"S3.SS1.p1.10.m10.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.10.m10.1b\"><apply id=\"S3.SS1.p1.10.m10.1.1.cmml\" xref=\"S3.SS1.p1.10.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.10.m10.1.1.1.cmml\" xref=\"S3.SS1.p1.10.m10.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.10.m10.1.1.2.cmml\" xref=\"S3.SS1.p1.10.m10.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.10.m10.1.1.3.cmml\" xref=\"S3.SS1.p1.10.m10.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.10.m10.1c\">\\bm{x}_{1}</annotation></semantics></math> peaks around <math id=\"S3.SS1.p1.11.m11.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{15}\" display=\"inline\"><semantics id=\"S3.SS1.p1.11.m11.1a\"><msub id=\"S3.SS1.p1.11.m11.1.1\" xref=\"S3.SS1.p1.11.m11.1.1.cmml\"><mi id=\"S3.SS1.p1.11.m11.1.1.2\" xref=\"S3.SS1.p1.11.m11.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.11.m11.1.1.3\" xref=\"S3.SS1.p1.11.m11.1.1.3.cmml\">15</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.11.m11.1b\"><apply id=\"S3.SS1.p1.11.m11.1.1.cmml\" xref=\"S3.SS1.p1.11.m11.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.11.m11.1.1.1.cmml\" xref=\"S3.SS1.p1.11.m11.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.11.m11.1.1.2.cmml\" xref=\"S3.SS1.p1.11.m11.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.11.m11.1.1.3.cmml\" xref=\"S3.SS1.p1.11.m11.1.1.3\">15</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.11.m11.1c\">\\bm{x}_{15}</annotation></semantics></math> (episode 40) and then starts to decrease. Before we return to <math id=\"S3.SS1.p1.12.m12.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.12.m12.1a\"><msub id=\"S3.SS1.p1.12.m12.1.1\" xref=\"S3.SS1.p1.12.m12.1.1.cmml\"><mi id=\"S3.SS1.p1.12.m12.1.1.2\" xref=\"S3.SS1.p1.12.m12.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.12.m12.1.1.3\" xref=\"S3.SS1.p1.12.m12.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.12.m12.1b\"><apply id=\"S3.SS1.p1.12.m12.1.1.cmml\" xref=\"S3.SS1.p1.12.m12.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.12.m12.1.1.1.cmml\" xref=\"S3.SS1.p1.12.m12.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.12.m12.1.1.2.cmml\" xref=\"S3.SS1.p1.12.m12.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.12.m12.1.1.3.cmml\" xref=\"S3.SS1.p1.12.m12.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.12.m12.1c\">\\bm{x}_{1}</annotation></semantics></math>, it has recovered more than half of the modelâ€™s initial forgetting during the second epoch. We refer to this counter-intuitive decrease in loss as the <em id=\"S3.SS1.p1.15.1\" class=\"ltx_emph ltx_font_italic\">anticipatory recovery</em> phenomenon. In FigureÂ <a href=\"#S1.F1.sf2\" title=\"Figure 1(b) â€£ Figure 1 â€£ 1 Introduction â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>, we plot the losses for all the documents and re-align them so that <math id=\"S3.SS1.p1.13.m13.1\" class=\"ltx_Math\" alttext=\"0\" display=\"inline\"><semantics id=\"S3.SS1.p1.13.m13.1a\"><mn id=\"S3.SS1.p1.13.m13.1.1\" xref=\"S3.SS1.p1.13.m13.1.1.cmml\">0</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.13.m13.1b\"><cn type=\"integer\" id=\"S3.SS1.p1.13.m13.1.1.cmml\" xref=\"S3.SS1.p1.13.m13.1.1\">0</cn></annotation-xml></semantics></math> on the x-axis refers to the loss on each document <math id=\"S3.SS1.p1.14.m14.1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics id=\"S3.SS1.p1.14.m14.1a\"><mi id=\"S3.SS1.p1.14.m14.1.1\" xref=\"S3.SS1.p1.14.m14.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.14.m14.1b\"><ci id=\"S3.SS1.p1.14.m14.1.1.cmml\" xref=\"S3.SS1.p1.14.m14.1.1\">ğ‘¡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.14.m14.1c\">t</annotation></semantics></math> immediately before training on it for the first time. The figure confirms that the anticipatory recovery phenomenon exists for not only <math id=\"S3.SS1.p1.15.m15.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S3.SS1.p1.15.m15.1a\"><msub id=\"S3.SS1.p1.15.m15.1.1\" xref=\"S3.SS1.p1.15.m15.1.1.cmml\"><mi id=\"S3.SS1.p1.15.m15.1.1.2\" xref=\"S3.SS1.p1.15.m15.1.1.2.cmml\">ğ’™</mi><mn id=\"S3.SS1.p1.15.m15.1.1.3\" xref=\"S3.SS1.p1.15.m15.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p1.15.m15.1b\"><apply id=\"S3.SS1.p1.15.m15.1.1.cmml\" xref=\"S3.SS1.p1.15.m15.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p1.15.m15.1.1.1.cmml\" xref=\"S3.SS1.p1.15.m15.1.1\">subscript</csymbol><ci id=\"S3.SS1.p1.15.m15.1.1.2.cmml\" xref=\"S3.SS1.p1.15.m15.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S3.SS1.p1.15.m15.1.1.3.cmml\" xref=\"S3.SS1.p1.15.m15.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p1.15.m15.1c\">\\bm{x}_{1}</annotation></semantics></math> but all documents.</p>\n</div>\n<div id=\"S3.SS1.p2\" class=\"ltx_para\">\n<p id=\"S3.SS1.p2.10\" class=\"ltx_p\">To quantify the strength of the anticipatory recovery phenomenon, we define the <em id=\"S3.SS1.p2.10.1\" class=\"ltx_emph ltx_font_italic\">recovery score</em> as the proportion of the initial forgetting during the current epoch that the model recovers before returning to the same document. Mathematically, let the mean (over <math id=\"S3.SS1.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics id=\"S3.SS1.p2.1.m1.1a\"><mi id=\"S3.SS1.p2.1.m1.1.1\" xref=\"S3.SS1.p2.1.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.1.m1.1b\"><ci id=\"S3.SS1.p2.1.m1.1.1.cmml\" xref=\"S3.SS1.p2.1.m1.1.1\">ğ‘¡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.1.m1.1c\">t</annotation></semantics></math>) of the maximum loss on each document <math id=\"S3.SS1.p2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t}\" display=\"inline\"><semantics id=\"S3.SS1.p2.2.m2.1a\"><msub id=\"S3.SS1.p2.2.m2.1.1\" xref=\"S3.SS1.p2.2.m2.1.1.cmml\"><mi id=\"S3.SS1.p2.2.m2.1.1.2\" xref=\"S3.SS1.p2.2.m2.1.1.2.cmml\">ğ’™</mi><mi id=\"S3.SS1.p2.2.m2.1.1.3\" xref=\"S3.SS1.p2.2.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.2.m2.1b\"><apply id=\"S3.SS1.p2.2.m2.1.1.cmml\" xref=\"S3.SS1.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.2.m2.1.1.1.cmml\" xref=\"S3.SS1.p2.2.m2.1.1\">subscript</csymbol><ci id=\"S3.SS1.p2.2.m2.1.1.2.cmml\" xref=\"S3.SS1.p2.2.m2.1.1.2\">ğ’™</ci><ci id=\"S3.SS1.p2.2.m2.1.1.3.cmml\" xref=\"S3.SS1.p2.2.m2.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.2.m2.1c\">\\bm{x}_{t}</annotation></semantics></math> between the <math id=\"S3.SS1.p2.3.m3.1\" class=\"ltx_Math\" alttext=\"n^{\\text{th}}\" display=\"inline\"><semantics id=\"S3.SS1.p2.3.m3.1a\"><msup id=\"S3.SS1.p2.3.m3.1.1\" xref=\"S3.SS1.p2.3.m3.1.1.cmml\"><mi id=\"S3.SS1.p2.3.m3.1.1.2\" xref=\"S3.SS1.p2.3.m3.1.1.2.cmml\">n</mi><mtext id=\"S3.SS1.p2.3.m3.1.1.3\" xref=\"S3.SS1.p2.3.m3.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.3.m3.1b\"><apply id=\"S3.SS1.p2.3.m3.1.1.cmml\" xref=\"S3.SS1.p2.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.3.m3.1.1.1.cmml\" xref=\"S3.SS1.p2.3.m3.1.1\">superscript</csymbol><ci id=\"S3.SS1.p2.3.m3.1.1.2.cmml\" xref=\"S3.SS1.p2.3.m3.1.1.2\">ğ‘›</ci><ci id=\"S3.SS1.p2.3.m3.1.1.3a.cmml\" xref=\"S3.SS1.p2.3.m3.1.1.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.3.m3.1.1.3.cmml\" xref=\"S3.SS1.p2.3.m3.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.3.m3.1c\">n^{\\text{th}}</annotation></semantics></math> and <math id=\"S3.SS1.p2.4.m4.1\" class=\"ltx_Math\" alttext=\"(n+1)^{\\text{th}}\" display=\"inline\"><semantics id=\"S3.SS1.p2.4.m4.1a\"><msup id=\"S3.SS1.p2.4.m4.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.cmml\"><mrow id=\"S3.SS1.p2.4.m4.1.1.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS1.p2.4.m4.1.1.1.1.2\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.4.m4.1.1.1.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\"><mi id=\"S3.SS1.p2.4.m4.1.1.1.1.1.2\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.4.m4.1.1.1.1.1.1\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml\">+</mo><mn id=\"S3.SS1.p2.4.m4.1.1.1.1.1.3\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml\">1</mn></mrow><mo stretchy=\"false\" id=\"S3.SS1.p2.4.m4.1.1.1.1.3\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\">)</mo></mrow><mtext id=\"S3.SS1.p2.4.m4.1.1.3\" xref=\"S3.SS1.p2.4.m4.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.4.m4.1b\"><apply id=\"S3.SS1.p2.4.m4.1.1.cmml\" xref=\"S3.SS1.p2.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.4.m4.1.1.2.cmml\" xref=\"S3.SS1.p2.4.m4.1.1\">superscript</csymbol><apply id=\"S3.SS1.p2.4.m4.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1\"><plus id=\"S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.1\"></plus><ci id=\"S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.2\">ğ‘›</ci><cn type=\"integer\" id=\"S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.1.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p2.4.m4.1.1.3a.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.4.m4.1.1.3.cmml\" xref=\"S3.SS1.p2.4.m4.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.4.m4.1c\">(n+1)^{\\text{th}}</annotation></semantics></math> time we train on that document be <math id=\"S3.SS1.p2.5.m5.1\" class=\"ltx_Math\" alttext=\"l_{\\text{max}}(n)\" display=\"inline\"><semantics id=\"S3.SS1.p2.5.m5.1a\"><mrow id=\"S3.SS1.p2.5.m5.1.2\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\"><msub id=\"S3.SS1.p2.5.m5.1.2.2\" xref=\"S3.SS1.p2.5.m5.1.2.2.cmml\"><mi id=\"S3.SS1.p2.5.m5.1.2.2.2\" xref=\"S3.SS1.p2.5.m5.1.2.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.5.m5.1.2.2.3\" xref=\"S3.SS1.p2.5.m5.1.2.2.3a.cmml\">max</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS1.p2.5.m5.1.2.1\" xref=\"S3.SS1.p2.5.m5.1.2.1.cmml\">â€‹</mo><mrow id=\"S3.SS1.p2.5.m5.1.2.3.2\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\"><mo stretchy=\"false\" id=\"S3.SS1.p2.5.m5.1.2.3.2.1\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.5.m5.1.1\" xref=\"S3.SS1.p2.5.m5.1.1.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.SS1.p2.5.m5.1.2.3.2.2\" xref=\"S3.SS1.p2.5.m5.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.5.m5.1b\"><apply id=\"S3.SS1.p2.5.m5.1.2.cmml\" xref=\"S3.SS1.p2.5.m5.1.2\"><times id=\"S3.SS1.p2.5.m5.1.2.1.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.1\"></times><apply id=\"S3.SS1.p2.5.m5.1.2.2.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.5.m5.1.2.2.1.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2\">subscript</csymbol><ci id=\"S3.SS1.p2.5.m5.1.2.2.2.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2.2\">ğ‘™</ci><ci id=\"S3.SS1.p2.5.m5.1.2.2.3a.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.5.m5.1.2.2.3.cmml\" xref=\"S3.SS1.p2.5.m5.1.2.2.3\">max</mtext></ci></apply><ci id=\"S3.SS1.p2.5.m5.1.1.cmml\" xref=\"S3.SS1.p2.5.m5.1.1\">ğ‘›</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.5.m5.1c\">l_{\\text{max}}(n)</annotation></semantics></math>, right before the <math id=\"S3.SS1.p2.6.m6.1\" class=\"ltx_Math\" alttext=\"(n+1)^{\\text{th}}\" display=\"inline\"><semantics id=\"S3.SS1.p2.6.m6.1a\"><msup id=\"S3.SS1.p2.6.m6.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.cmml\"><mrow id=\"S3.SS1.p2.6.m6.1.1.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS1.p2.6.m6.1.1.1.1.2\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.6.m6.1.1.1.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\"><mi id=\"S3.SS1.p2.6.m6.1.1.1.1.1.2\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.6.m6.1.1.1.1.1.1\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.1.cmml\">+</mo><mn id=\"S3.SS1.p2.6.m6.1.1.1.1.1.3\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml\">1</mn></mrow><mo stretchy=\"false\" id=\"S3.SS1.p2.6.m6.1.1.1.1.3\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\">)</mo></mrow><mtext id=\"S3.SS1.p2.6.m6.1.1.3\" xref=\"S3.SS1.p2.6.m6.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.6.m6.1b\"><apply id=\"S3.SS1.p2.6.m6.1.1.cmml\" xref=\"S3.SS1.p2.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.6.m6.1.1.2.cmml\" xref=\"S3.SS1.p2.6.m6.1.1\">superscript</csymbol><apply id=\"S3.SS1.p2.6.m6.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1\"><plus id=\"S3.SS1.p2.6.m6.1.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.1\"></plus><ci id=\"S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.2\">ğ‘›</ci><cn type=\"integer\" id=\"S3.SS1.p2.6.m6.1.1.1.1.1.3.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.1.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p2.6.m6.1.1.3a.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.6.m6.1.1.3.cmml\" xref=\"S3.SS1.p2.6.m6.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.6.m6.1c\">(n+1)^{\\text{th}}</annotation></semantics></math> time we train on it be <math id=\"S3.SS1.p2.7.m7.1\" class=\"ltx_Math\" alttext=\"l_{\\text{before}}(n)\" display=\"inline\"><semantics id=\"S3.SS1.p2.7.m7.1a\"><mrow id=\"S3.SS1.p2.7.m7.1.2\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\"><msub id=\"S3.SS1.p2.7.m7.1.2.2\" xref=\"S3.SS1.p2.7.m7.1.2.2.cmml\"><mi id=\"S3.SS1.p2.7.m7.1.2.2.2\" xref=\"S3.SS1.p2.7.m7.1.2.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.7.m7.1.2.2.3\" xref=\"S3.SS1.p2.7.m7.1.2.2.3a.cmml\">before</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS1.p2.7.m7.1.2.1\" xref=\"S3.SS1.p2.7.m7.1.2.1.cmml\">â€‹</mo><mrow id=\"S3.SS1.p2.7.m7.1.2.3.2\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\"><mo stretchy=\"false\" id=\"S3.SS1.p2.7.m7.1.2.3.2.1\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.7.m7.1.1\" xref=\"S3.SS1.p2.7.m7.1.1.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.SS1.p2.7.m7.1.2.3.2.2\" xref=\"S3.SS1.p2.7.m7.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.7.m7.1b\"><apply id=\"S3.SS1.p2.7.m7.1.2.cmml\" xref=\"S3.SS1.p2.7.m7.1.2\"><times id=\"S3.SS1.p2.7.m7.1.2.1.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.1\"></times><apply id=\"S3.SS1.p2.7.m7.1.2.2.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.7.m7.1.2.2.1.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2\">subscript</csymbol><ci id=\"S3.SS1.p2.7.m7.1.2.2.2.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2.2\">ğ‘™</ci><ci id=\"S3.SS1.p2.7.m7.1.2.2.3a.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.7.m7.1.2.2.3.cmml\" xref=\"S3.SS1.p2.7.m7.1.2.2.3\">before</mtext></ci></apply><ci id=\"S3.SS1.p2.7.m7.1.1.cmml\" xref=\"S3.SS1.p2.7.m7.1.1\">ğ‘›</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.7.m7.1c\">l_{\\text{before}}(n)</annotation></semantics></math>, and right after the <math id=\"S3.SS1.p2.8.m8.1\" class=\"ltx_Math\" alttext=\"(n+1)^{\\text{th}}\" display=\"inline\"><semantics id=\"S3.SS1.p2.8.m8.1a\"><msup id=\"S3.SS1.p2.8.m8.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.cmml\"><mrow id=\"S3.SS1.p2.8.m8.1.1.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS1.p2.8.m8.1.1.1.1.2\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.SS1.p2.8.m8.1.1.1.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\"><mi id=\"S3.SS1.p2.8.m8.1.1.1.1.1.2\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml\">n</mi><mo id=\"S3.SS1.p2.8.m8.1.1.1.1.1.1\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml\">+</mo><mn id=\"S3.SS1.p2.8.m8.1.1.1.1.1.3\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml\">1</mn></mrow><mo stretchy=\"false\" id=\"S3.SS1.p2.8.m8.1.1.1.1.3\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\">)</mo></mrow><mtext id=\"S3.SS1.p2.8.m8.1.1.3\" xref=\"S3.SS1.p2.8.m8.1.1.3a.cmml\">th</mtext></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.8.m8.1b\"><apply id=\"S3.SS1.p2.8.m8.1.1.cmml\" xref=\"S3.SS1.p2.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.8.m8.1.1.2.cmml\" xref=\"S3.SS1.p2.8.m8.1.1\">superscript</csymbol><apply id=\"S3.SS1.p2.8.m8.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1\"><plus id=\"S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.1\"></plus><ci id=\"S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.2\">ğ‘›</ci><cn type=\"integer\" id=\"S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.1.1.1.3\">1</cn></apply><ci id=\"S3.SS1.p2.8.m8.1.1.3a.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.8.m8.1.1.3.cmml\" xref=\"S3.SS1.p2.8.m8.1.1.3\">th</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.8.m8.1c\">(n+1)^{\\text{th}}</annotation></semantics></math> time we train on it be <math id=\"S3.SS1.p2.9.m9.1\" class=\"ltx_Math\" alttext=\"l_{\\text{after}}(n)\" display=\"inline\"><semantics id=\"S3.SS1.p2.9.m9.1a\"><mrow id=\"S3.SS1.p2.9.m9.1.2\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\"><msub id=\"S3.SS1.p2.9.m9.1.2.2\" xref=\"S3.SS1.p2.9.m9.1.2.2.cmml\"><mi id=\"S3.SS1.p2.9.m9.1.2.2.2\" xref=\"S3.SS1.p2.9.m9.1.2.2.2.cmml\">l</mi><mtext id=\"S3.SS1.p2.9.m9.1.2.2.3\" xref=\"S3.SS1.p2.9.m9.1.2.2.3a.cmml\">after</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS1.p2.9.m9.1.2.1\" xref=\"S3.SS1.p2.9.m9.1.2.1.cmml\">â€‹</mo><mrow id=\"S3.SS1.p2.9.m9.1.2.3.2\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\"><mo stretchy=\"false\" id=\"S3.SS1.p2.9.m9.1.2.3.2.1\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\">(</mo><mi id=\"S3.SS1.p2.9.m9.1.1\" xref=\"S3.SS1.p2.9.m9.1.1.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.SS1.p2.9.m9.1.2.3.2.2\" xref=\"S3.SS1.p2.9.m9.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.9.m9.1b\"><apply id=\"S3.SS1.p2.9.m9.1.2.cmml\" xref=\"S3.SS1.p2.9.m9.1.2\"><times id=\"S3.SS1.p2.9.m9.1.2.1.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.1\"></times><apply id=\"S3.SS1.p2.9.m9.1.2.2.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS1.p2.9.m9.1.2.2.1.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2\">subscript</csymbol><ci id=\"S3.SS1.p2.9.m9.1.2.2.2.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2.2\">ğ‘™</ci><ci id=\"S3.SS1.p2.9.m9.1.2.2.3a.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2.3\"><mtext mathsize=\"70%\" id=\"S3.SS1.p2.9.m9.1.2.2.3.cmml\" xref=\"S3.SS1.p2.9.m9.1.2.2.3\">after</mtext></ci></apply><ci id=\"S3.SS1.p2.9.m9.1.1.cmml\" xref=\"S3.SS1.p2.9.m9.1.1\">ğ‘›</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.9.m9.1c\">l_{\\text{after}}(n)</annotation></semantics></math>. Then we define the recovery score (RS) for epoch <math id=\"S3.SS1.p2.10.m10.1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics id=\"S3.SS1.p2.10.m10.1a\"><mi id=\"S3.SS1.p2.10.m10.1.1\" xref=\"S3.SS1.p2.10.m10.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.p2.10.m10.1b\"><ci id=\"S3.SS1.p2.10.m10.1.1.cmml\" xref=\"S3.SS1.p2.10.m10.1.1\">ğ‘›</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.p2.10.m10.1c\">n</annotation></semantics></math> to be<span id=\"footnote1\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In some cases, a randomly initialized model will produce loss curves that decrease throughout the epoch, because its knowledge is so poor that it enjoys positive generalization among all documents. This yields an infinite or misleadingly large recovery score under <a href=\"#S3.E1\" title=\"In 3.1 The Anticipatory Recovery Phenomenon â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>Â <span class=\"ltx_text ltx_ref_tag\">1</span></a>. We do not include such cases in our experiments so do not bother with more nuanced recovery scores.</span></span></span></p>\n<table id=\"A3.EGx1\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S3.E1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S3.E1.m1.6\" class=\"ltx_Math\" alttext=\"\\displaystyle RS(n)=\\frac{l_{\\text{max}}(n)-l_{\\text{before}}(n)}{l_{\\text{max}}(n)-l_{\\text{after}}(n-1)}.\" display=\"inline\"><semantics id=\"S3.E1.m1.6a\"><mrow id=\"S3.E1.m1.6.6.1\" xref=\"S3.E1.m1.6.6.1.1.cmml\"><mrow id=\"S3.E1.m1.6.6.1.1\" xref=\"S3.E1.m1.6.6.1.1.cmml\"><mrow id=\"S3.E1.m1.6.6.1.1.2\" xref=\"S3.E1.m1.6.6.1.1.2.cmml\"><mi id=\"S3.E1.m1.6.6.1.1.2.2\" xref=\"S3.E1.m1.6.6.1.1.2.2.cmml\">R</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.6.6.1.1.2.1\" xref=\"S3.E1.m1.6.6.1.1.2.1.cmml\">â€‹</mo><mi id=\"S3.E1.m1.6.6.1.1.2.3\" xref=\"S3.E1.m1.6.6.1.1.2.3.cmml\">S</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.6.6.1.1.2.1a\" xref=\"S3.E1.m1.6.6.1.1.2.1.cmml\">â€‹</mo><mrow id=\"S3.E1.m1.6.6.1.1.2.4.2\" xref=\"S3.E1.m1.6.6.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.6.6.1.1.2.4.2.1\" xref=\"S3.E1.m1.6.6.1.1.2.cmml\">(</mo><mi id=\"S3.E1.m1.5.5\" xref=\"S3.E1.m1.5.5.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.E1.m1.6.6.1.1.2.4.2.2\" xref=\"S3.E1.m1.6.6.1.1.2.cmml\">)</mo></mrow></mrow><mo id=\"S3.E1.m1.6.6.1.1.1\" xref=\"S3.E1.m1.6.6.1.1.1.cmml\">=</mo><mstyle displaystyle=\"true\" id=\"S3.E1.m1.4.4\" xref=\"S3.E1.m1.4.4.cmml\"><mfrac id=\"S3.E1.m1.4.4a\" xref=\"S3.E1.m1.4.4.cmml\"><mrow id=\"S3.E1.m1.2.2.2\" xref=\"S3.E1.m1.2.2.2.cmml\"><mrow id=\"S3.E1.m1.2.2.2.4\" xref=\"S3.E1.m1.2.2.2.4.cmml\"><msub id=\"S3.E1.m1.2.2.2.4.2\" xref=\"S3.E1.m1.2.2.2.4.2.cmml\"><mi id=\"S3.E1.m1.2.2.2.4.2.2\" xref=\"S3.E1.m1.2.2.2.4.2.2.cmml\">l</mi><mtext id=\"S3.E1.m1.2.2.2.4.2.3\" xref=\"S3.E1.m1.2.2.2.4.2.3a.cmml\">max</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.2.4.1\" xref=\"S3.E1.m1.2.2.2.4.1.cmml\">â€‹</mo><mrow id=\"S3.E1.m1.2.2.2.4.3.2\" xref=\"S3.E1.m1.2.2.2.4.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.2.4.3.2.1\" xref=\"S3.E1.m1.2.2.2.4.cmml\">(</mo><mi id=\"S3.E1.m1.1.1.1.1\" xref=\"S3.E1.m1.1.1.1.1.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.2.4.3.2.2\" xref=\"S3.E1.m1.2.2.2.4.cmml\">)</mo></mrow></mrow><mo id=\"S3.E1.m1.2.2.2.3\" xref=\"S3.E1.m1.2.2.2.3.cmml\">âˆ’</mo><mrow id=\"S3.E1.m1.2.2.2.5\" xref=\"S3.E1.m1.2.2.2.5.cmml\"><msub id=\"S3.E1.m1.2.2.2.5.2\" xref=\"S3.E1.m1.2.2.2.5.2.cmml\"><mi id=\"S3.E1.m1.2.2.2.5.2.2\" xref=\"S3.E1.m1.2.2.2.5.2.2.cmml\">l</mi><mtext id=\"S3.E1.m1.2.2.2.5.2.3\" xref=\"S3.E1.m1.2.2.2.5.2.3a.cmml\">before</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.2.5.1\" xref=\"S3.E1.m1.2.2.2.5.1.cmml\">â€‹</mo><mrow id=\"S3.E1.m1.2.2.2.5.3.2\" xref=\"S3.E1.m1.2.2.2.5.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.2.5.3.2.1\" xref=\"S3.E1.m1.2.2.2.5.cmml\">(</mo><mi id=\"S3.E1.m1.2.2.2.2\" xref=\"S3.E1.m1.2.2.2.2.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.2.5.3.2.2\" xref=\"S3.E1.m1.2.2.2.5.cmml\">)</mo></mrow></mrow></mrow><mrow id=\"S3.E1.m1.4.4.4\" xref=\"S3.E1.m1.4.4.4.cmml\"><mrow id=\"S3.E1.m1.4.4.4.4\" xref=\"S3.E1.m1.4.4.4.4.cmml\"><msub id=\"S3.E1.m1.4.4.4.4.2\" xref=\"S3.E1.m1.4.4.4.4.2.cmml\"><mi id=\"S3.E1.m1.4.4.4.4.2.2\" xref=\"S3.E1.m1.4.4.4.4.2.2.cmml\">l</mi><mtext id=\"S3.E1.m1.4.4.4.4.2.3\" xref=\"S3.E1.m1.4.4.4.4.2.3a.cmml\">max</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.4.4.4.4.1\" xref=\"S3.E1.m1.4.4.4.4.1.cmml\">â€‹</mo><mrow id=\"S3.E1.m1.4.4.4.4.3.2\" xref=\"S3.E1.m1.4.4.4.4.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.4.4.4.4.3.2.1\" xref=\"S3.E1.m1.4.4.4.4.cmml\">(</mo><mi id=\"S3.E1.m1.3.3.3.1\" xref=\"S3.E1.m1.3.3.3.1.cmml\">n</mi><mo stretchy=\"false\" id=\"S3.E1.m1.4.4.4.4.3.2.2\" xref=\"S3.E1.m1.4.4.4.4.cmml\">)</mo></mrow></mrow><mo id=\"S3.E1.m1.4.4.4.3\" xref=\"S3.E1.m1.4.4.4.3.cmml\">âˆ’</mo><mrow id=\"S3.E1.m1.4.4.4.2\" xref=\"S3.E1.m1.4.4.4.2.cmml\"><msub id=\"S3.E1.m1.4.4.4.2.3\" xref=\"S3.E1.m1.4.4.4.2.3.cmml\"><mi id=\"S3.E1.m1.4.4.4.2.3.2\" xref=\"S3.E1.m1.4.4.4.2.3.2.cmml\">l</mi><mtext id=\"S3.E1.m1.4.4.4.2.3.3\" xref=\"S3.E1.m1.4.4.4.2.3.3a.cmml\">after</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.4.4.4.2.2\" xref=\"S3.E1.m1.4.4.4.2.2.cmml\">â€‹</mo><mrow id=\"S3.E1.m1.4.4.4.2.1.1\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.4.4.4.2.1.1.2\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.cmml\">(</mo><mrow id=\"S3.E1.m1.4.4.4.2.1.1.1\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.cmml\"><mi id=\"S3.E1.m1.4.4.4.2.1.1.1.2\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.2.cmml\">n</mi><mo id=\"S3.E1.m1.4.4.4.2.1.1.1.1\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.1.cmml\">âˆ’</mo><mn id=\"S3.E1.m1.4.4.4.2.1.1.1.3\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.3.cmml\">1</mn></mrow><mo stretchy=\"false\" id=\"S3.E1.m1.4.4.4.2.1.1.3\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.cmml\">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo lspace=\"0em\" id=\"S3.E1.m1.6.6.1.2\" xref=\"S3.E1.m1.6.6.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.E1.m1.6b\"><apply id=\"S3.E1.m1.6.6.1.1.cmml\" xref=\"S3.E1.m1.6.6.1\"><eq id=\"S3.E1.m1.6.6.1.1.1.cmml\" xref=\"S3.E1.m1.6.6.1.1.1\"></eq><apply id=\"S3.E1.m1.6.6.1.1.2.cmml\" xref=\"S3.E1.m1.6.6.1.1.2\"><times id=\"S3.E1.m1.6.6.1.1.2.1.cmml\" xref=\"S3.E1.m1.6.6.1.1.2.1\"></times><ci id=\"S3.E1.m1.6.6.1.1.2.2.cmml\" xref=\"S3.E1.m1.6.6.1.1.2.2\">ğ‘…</ci><ci id=\"S3.E1.m1.6.6.1.1.2.3.cmml\" xref=\"S3.E1.m1.6.6.1.1.2.3\">ğ‘†</ci><ci id=\"S3.E1.m1.5.5.cmml\" xref=\"S3.E1.m1.5.5\">ğ‘›</ci></apply><apply id=\"S3.E1.m1.4.4.cmml\" xref=\"S3.E1.m1.4.4\"><divide id=\"S3.E1.m1.4.4.5.cmml\" xref=\"S3.E1.m1.4.4\"></divide><apply id=\"S3.E1.m1.2.2.2.cmml\" xref=\"S3.E1.m1.2.2.2\"><minus id=\"S3.E1.m1.2.2.2.3.cmml\" xref=\"S3.E1.m1.2.2.2.3\"></minus><apply id=\"S3.E1.m1.2.2.2.4.cmml\" xref=\"S3.E1.m1.2.2.2.4\"><times id=\"S3.E1.m1.2.2.2.4.1.cmml\" xref=\"S3.E1.m1.2.2.2.4.1\"></times><apply id=\"S3.E1.m1.2.2.2.4.2.cmml\" xref=\"S3.E1.m1.2.2.2.4.2\"><csymbol cd=\"ambiguous\" id=\"S3.E1.m1.2.2.2.4.2.1.cmml\" xref=\"S3.E1.m1.2.2.2.4.2\">subscript</csymbol><ci id=\"S3.E1.m1.2.2.2.4.2.2.cmml\" xref=\"S3.E1.m1.2.2.2.4.2.2\">ğ‘™</ci><ci id=\"S3.E1.m1.2.2.2.4.2.3a.cmml\" xref=\"S3.E1.m1.2.2.2.4.2.3\"><mtext mathsize=\"70%\" id=\"S3.E1.m1.2.2.2.4.2.3.cmml\" xref=\"S3.E1.m1.2.2.2.4.2.3\">max</mtext></ci></apply><ci id=\"S3.E1.m1.1.1.1.1.cmml\" xref=\"S3.E1.m1.1.1.1.1\">ğ‘›</ci></apply><apply id=\"S3.E1.m1.2.2.2.5.cmml\" xref=\"S3.E1.m1.2.2.2.5\"><times id=\"S3.E1.m1.2.2.2.5.1.cmml\" xref=\"S3.E1.m1.2.2.2.5.1\"></times><apply id=\"S3.E1.m1.2.2.2.5.2.cmml\" xref=\"S3.E1.m1.2.2.2.5.2\"><csymbol cd=\"ambiguous\" id=\"S3.E1.m1.2.2.2.5.2.1.cmml\" xref=\"S3.E1.m1.2.2.2.5.2\">subscript</csymbol><ci id=\"S3.E1.m1.2.2.2.5.2.2.cmml\" xref=\"S3.E1.m1.2.2.2.5.2.2\">ğ‘™</ci><ci id=\"S3.E1.m1.2.2.2.5.2.3a.cmml\" xref=\"S3.E1.m1.2.2.2.5.2.3\"><mtext mathsize=\"70%\" id=\"S3.E1.m1.2.2.2.5.2.3.cmml\" xref=\"S3.E1.m1.2.2.2.5.2.3\">before</mtext></ci></apply><ci id=\"S3.E1.m1.2.2.2.2.cmml\" xref=\"S3.E1.m1.2.2.2.2\">ğ‘›</ci></apply></apply><apply id=\"S3.E1.m1.4.4.4.cmml\" xref=\"S3.E1.m1.4.4.4\"><minus id=\"S3.E1.m1.4.4.4.3.cmml\" xref=\"S3.E1.m1.4.4.4.3\"></minus><apply id=\"S3.E1.m1.4.4.4.4.cmml\" xref=\"S3.E1.m1.4.4.4.4\"><times id=\"S3.E1.m1.4.4.4.4.1.cmml\" xref=\"S3.E1.m1.4.4.4.4.1\"></times><apply id=\"S3.E1.m1.4.4.4.4.2.cmml\" xref=\"S3.E1.m1.4.4.4.4.2\"><csymbol cd=\"ambiguous\" id=\"S3.E1.m1.4.4.4.4.2.1.cmml\" xref=\"S3.E1.m1.4.4.4.4.2\">subscript</csymbol><ci id=\"S3.E1.m1.4.4.4.4.2.2.cmml\" xref=\"S3.E1.m1.4.4.4.4.2.2\">ğ‘™</ci><ci id=\"S3.E1.m1.4.4.4.4.2.3a.cmml\" xref=\"S3.E1.m1.4.4.4.4.2.3\"><mtext mathsize=\"70%\" id=\"S3.E1.m1.4.4.4.4.2.3.cmml\" xref=\"S3.E1.m1.4.4.4.4.2.3\">max</mtext></ci></apply><ci id=\"S3.E1.m1.3.3.3.1.cmml\" xref=\"S3.E1.m1.3.3.3.1\">ğ‘›</ci></apply><apply id=\"S3.E1.m1.4.4.4.2.cmml\" xref=\"S3.E1.m1.4.4.4.2\"><times id=\"S3.E1.m1.4.4.4.2.2.cmml\" xref=\"S3.E1.m1.4.4.4.2.2\"></times><apply id=\"S3.E1.m1.4.4.4.2.3.cmml\" xref=\"S3.E1.m1.4.4.4.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.E1.m1.4.4.4.2.3.1.cmml\" xref=\"S3.E1.m1.4.4.4.2.3\">subscript</csymbol><ci id=\"S3.E1.m1.4.4.4.2.3.2.cmml\" xref=\"S3.E1.m1.4.4.4.2.3.2\">ğ‘™</ci><ci id=\"S3.E1.m1.4.4.4.2.3.3a.cmml\" xref=\"S3.E1.m1.4.4.4.2.3.3\"><mtext mathsize=\"70%\" id=\"S3.E1.m1.4.4.4.2.3.3.cmml\" xref=\"S3.E1.m1.4.4.4.2.3.3\">after</mtext></ci></apply><apply id=\"S3.E1.m1.4.4.4.2.1.1.1.cmml\" xref=\"S3.E1.m1.4.4.4.2.1.1\"><minus id=\"S3.E1.m1.4.4.4.2.1.1.1.1.cmml\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.1\"></minus><ci id=\"S3.E1.m1.4.4.4.2.1.1.1.2.cmml\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.2\">ğ‘›</ci><cn type=\"integer\" id=\"S3.E1.m1.4.4.4.2.1.1.1.3.cmml\" xref=\"S3.E1.m1.4.4.4.2.1.1.1.3\">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.E1.m1.6c\">\\displaystyle RS(n)=\\frac{l_{\\text{max}}(n)-l_{\\text{before}}(n)}{l_{\\text{max}}(n)-l_{\\text{after}}(n-1)}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S3.SS1.p2.11\" class=\"ltx_p\">In the following subsections we compute the recovery scores for different model sizes and training hyper-parameters (Figures <a href=\"#S3.F2\" title=\"Figure 2 â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a href=\"#S3.F5\" title=\"Figure 5 â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) to investigate their effects on the anticipatory recovery phenomenon.</p>\n</div>\n</section>\n<section id=\"S3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Anticipatory Recovery is an Emergent Behavior</h3>\n\n<div id=\"S3.SS2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.p1.1\" class=\"ltx_p\">To study how the model size affects the amount of anticipatory recovery, we repeat this experiment with pre-trained Pythia models <cite class=\"ltx_cite ltx_citemacro_citep\">(Biderman etÂ al., <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> of different sizes, specifically 160M, 410M, 1B, and 2.8B. We plot the average loss curves as well as the recovery score for epoch 4 in Figure <a href=\"#S3.F2.sf1\" title=\"Figure 2(a) â€£ Figure 2 â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>. We observe that larger models clearly demonstrate stronger anticipatory recovery. The sharp increase of average recovery score from the 160M model to the 410M model indicates that anticipatory recovery is an emergent behavior.</p>\n</div>\n<section id=\"S3.SS2.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Anticipatory Recovery in Randomly Initialized Models.</h4>\n\n<div id=\"S3.SS2.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.SSS0.Px1.p1.1\" class=\"ltx_p\">All our previous experiments are conducted with pre-trained language models. To study whether anticipatory recovery is a result of pre-training, we repeat the experiments on randomly initialized models of different sizes, and plot the loss curves and average recovery scores in Figure <a href=\"#S3.F2.sf2\" title=\"Figure 2(b) â€£ Figure 2 â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>. We follow the model initialization recipe of <cite class=\"ltx_cite ltx_citemacro_citet\">Biderman etÂ al. (<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. From the loss curves for the 410M and 1B model, especially in the last epoch, we see that the anticipation phenomenon also exists in randomly initialized LLMs. We observe that the anticipation effect is not as strong as in the pre-trained models. The effect of model size still holds: larger models clearly demonstrate stronger recovery.</p>\n</div>\n</section>\n<section id=\"S3.SS2.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Effects of Model Width and Depth.</h4>\n\n<div id=\"S3.SS2.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.SSS0.Px2.p1.1\" class=\"ltx_p\">To further study the effect of model width and depth on the anticipatory recovery phenomenon beyond the model hyperparameters in the Pythia suite, we take a Pythia-1B model and vary the width (size of token embedding) and depth (number of transformer blocks) of the model and plot their average loss curves for cyclic training from random initializations in Figure <a href=\"#S3.F4\" title=\"Figure 4 â€£ Effects of Model Width and Depth. â€£ 3.2 Anticipatory Recovery is an Emergent Behavior â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The original Pythia-1B model has token embedding of size 2048 and 16 transformer blocks. From the plots we observe that the model needs sufficient width (at least 512) and depth (at least 8 transformer blocks) to exhibit noticeable recovery, confirming that it is an emergent behavior contingent on model size.</p>\n</div>\n<figure id=\"S3.F4\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F4.fig1\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:203.8pt;\">\n<div id=\"S3.F4.fig1.1\" class=\"ltx_block\">\n<figure id=\"S3.F3.sf1\" class=\"ltx_figure\"><img src=\"./assets/x5.png\" id=\"S3.F3.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"226\" height=\"174\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F3.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S3.F3.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Size of Token Embedding </span></figcaption>\n</figure>\n<figure id=\"S3.F3.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x6.png\" id=\"S3.F3.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"226\" height=\"174\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F3.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S3.F3.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Number of Model Blocks </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F4.fig1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>: </span><span id=\"S3.F4.fig1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Models trained from scratch with (a) different width (token embedding size) and (b) different depth (number of transformer blocks). </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F4.4\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:203.8pt;\">\n<div id=\"S3.F4.4.5\" class=\"ltx_block\">\n<figure id=\"S3.F3.sf3\" class=\"ltx_figure\"><img src=\"./assets/x7.png\" id=\"S3.F3.sf3.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"226\" height=\"178\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F3.sf3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span><span id=\"S3.F3.sf3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Random Masking </span></figcaption>\n</figure>\n<figure id=\"S3.F3.sf4\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x8.png\" id=\"S3.F3.sf4.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"226\" height=\"179\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F3.sf4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(d)</span> </span><span id=\"S3.F3.sf4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Random Window Shift</span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F4.4.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>: </span><span id=\"S3.F4.4.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of different levels of data randomization, with (a) random masking with probability up to <math id=\"S3.F4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"0.3\" display=\"inline\"><semantics id=\"S3.F4.3.3.1.m1.1b\"><mn id=\"S3.F4.3.3.1.m1.1.1\" xref=\"S3.F4.3.3.1.m1.1.1.cmml\">0.3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F4.3.3.1.m1.1c\"><cn type=\"float\" id=\"S3.F4.3.3.1.m1.1.1.cmml\" xref=\"S3.F4.3.3.1.m1.1.1\">0.3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F4.3.3.1.m1.1d\">0.3</annotation></semantics></math>; (b) random shift of the context window up to <math id=\"S3.F4.4.4.2.m2.1\" class=\"ltx_Math\" alttext=\"128\" display=\"inline\"><semantics id=\"S3.F4.4.4.2.m2.1b\"><mn id=\"S3.F4.4.4.2.m2.1.1\" xref=\"S3.F4.4.4.2.m2.1.1.cmml\">128</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F4.4.4.2.m2.1c\"><cn type=\"integer\" id=\"S3.F4.4.4.2.m2.1.1.cmml\" xref=\"S3.F4.4.4.2.m2.1.1\">128</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F4.4.4.2.m2.1d\">128</annotation></semantics></math> tokens.</span></figcaption>\n</figure>\n</div>\n</div>\n</figure>\n</section>\n</section>\n<section id=\"S3.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>Other Influential Factors</h3>\n\n<div id=\"S3.SS3.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.p1.1\" class=\"ltx_p\">In this section we discuss the effect of other training hyperparameters on the anticipatory recovery phenomenon. In each of the following paragraphs we vary one factor in the training setup and plot the different average loss curves for cyclic fine-tuning of a pre-trained Pythia-1B model. We also include some additional results in Appendix <a href=\"#A2\" title=\"Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n</div>\n<figure id=\"S3.F5\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div id=\"S3.F5.2\" class=\"ltx_block ltx_figure_panel\">\n<figure id=\"S3.F5.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x9.png\" id=\"S3.F5.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"237\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F5.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S3.F5.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Number of Documents \n</span></figcaption>\n</figure>\n<figure id=\"S3.F5.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x10.png\" id=\"S3.F5.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"241\" height=\"102\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F5.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S3.F5.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Number of Gradient Steps </span></figcaption>\n</figure>\n</div>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div id=\"S3.F5.3\" class=\"ltx_block ltx_figure_panel\">\n<figure id=\"S3.F5.sf3\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x11.png\" id=\"S3.F5.sf3.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"243\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F5.sf3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span><span id=\"S3.F5.sf3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Context Length\n</span></figcaption>\n</figure>\n<figure id=\"S3.F5.sf4\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x12.png\" id=\"S3.F5.sf4.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"243\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F5.sf4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(d)</span> </span><span id=\"S3.F5.sf4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Number of Frozen Blocks </span></figcaption>\n</figure>\n</div>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>: </span><span id=\"S3.F5.5.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effects of (a) number of documents (b) number of gradient steps (c) context length and (d) number of frozen blocks.</span></figcaption>\n</figure>\n<section id=\"S3.SS3.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Number of Tasks.</h4>\n\n<div id=\"S3.SS3.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px1.p1.2\" class=\"ltx_p\">We first try to increase the number of documents/tasks (<math id=\"S3.SS3.SSS0.Px1.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px1.p1.1.m1.1a\"><mi id=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px1.p1.1.m1.1b\"><ci id=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.1.m1.1.1\">ğ‘‡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px1.p1.1.m1.1c\">T</annotation></semantics></math>) the model is fine-tuned on in each epoch, with <math id=\"S3.SS3.SSS0.Px1.p1.2.m2.5\" class=\"ltx_Math\" alttext=\"T\\in\\{10,25,50,100,200\\}\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px1.p1.2.m2.5a\"><mrow id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.cmml\"><mi id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.2\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.2.cmml\">T</mi><mo id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.1\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.1.cmml\">âˆˆ</mo><mrow id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2.1\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px1.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml\">10</mn><mo id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2.2\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.2.m2.2.2\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.2.2.cmml\">25</mn><mo id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2.3\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.2.m2.3.3\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.3.3.cmml\">50</mn><mo id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2.4\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.2.m2.4.4\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.4.4.cmml\">100</mn><mo id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2.5\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.5\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.5.cmml\">200</mn><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2.6\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.5b\"><apply id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6\"><in id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.1.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.1\"></in><ci id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.2.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.2\">ğ‘‡</ci><set id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.1.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.6.3.2\"><cn type=\"integer\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.1.1\">10</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.2.2.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.2.2\">25</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.3.3.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.3.3\">50</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.4.4.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.4.4\">100</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.5.5.cmml\" xref=\"S3.SS3.SSS0.Px1.p1.2.m2.5.5\">200</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px1.p1.2.m2.5c\">T\\in\\{10,25,50,100,200\\}</annotation></semantics></math>, and plot all the loss curves in Figure <a href=\"#S3.F5.sf1\" title=\"Figure 5(a) â€£ Figure 5 â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5(a)</span></a>. From the figure we can observe clear recovery for all the curves, suggesting that the model can â€œmemorizeâ€ a certain task transition even after training on 200 other tasks.</p>\n</div>\n</section>\n<section id=\"S3.SS3.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Number of Gradient Steps.</h4>\n\n<div id=\"S3.SS3.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px2.p1.4\" class=\"ltx_p\">Figure <a href=\"#S3.F5.sf2\" title=\"Figure 5(b) â€£ Figure 5 â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a> plots training curves with different numbers of gradient steps (<math id=\"S3.SS3.SSS0.Px2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px2.p1.1.m1.1a\"><mi id=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.1b\"><ci id=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.1.m1.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.1.m1.1c\">M</annotation></semantics></math>) taken on each task (<math id=\"S3.SS3.SSS0.Px2.p1.2.m2.4\" class=\"ltx_Math\" alttext=\"M\\in\\{1,5,10,20\\}\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px2.p1.2.m2.4a\"><mrow id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.cmml\"><mi id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.2\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.2.cmml\">M</mi><mo id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.1\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.1.cmml\">âˆˆ</mo><mrow id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2.1\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml\">1</mn><mo id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2.2\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px2.p1.2.m2.2.2\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.2.2.cmml\">5</mn><mo id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2.3\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px2.p1.2.m2.3.3\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.3.3.cmml\">10</mn><mo id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2.4\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.4\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.4.cmml\">20</mn><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2.5\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.4b\"><apply id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5\"><in id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.1\"></in><ci id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.2.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.2\">ğ‘€</ci><set id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.5.3.2\"><cn type=\"integer\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.1.1\">1</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.2.2.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.2.2\">5</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.3.3.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.3.3\">10</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.4.4.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.2.m2.4.4\">20</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.2.m2.4c\">M\\in\\{1,5,10,20\\}</annotation></semantics></math>). More gradient steps in general leads to higher recovery score, although in Appendix <a href=\"#A2.SS2\" title=\"B.2 One-step Gradient Descent with Larger Learning Rate â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a> we show that slight anticipation is still observed for <math id=\"S3.SS3.SSS0.Px2.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px2.p1.3.m3.1a\"><mn id=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1b\"><cn type=\"integer\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.3.m3.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.3.m3.1c\">1</annotation></semantics></math> gradient step if we use a larger learning rate. We also demonstrate in the appendix that the anticipation effect is stronger when the same total update is divided among more gradient steps by scaling the learning rate inversely with <math id=\"S3.SS3.SSS0.Px2.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px2.p1.4.m4.1a\"><mi id=\"S3.SS3.SSS0.Px2.p1.4.m4.1.1\" xref=\"S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px2.p1.4.m4.1b\"><ci id=\"S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml\" xref=\"S3.SS3.SSS0.Px2.p1.4.m4.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px2.p1.4.m4.1c\">M</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"S3.SS3.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Context Length.</h4>\n\n<div id=\"S3.SS3.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px3.p1.3\" class=\"ltx_p\">We experiment with different context lengths (<math id=\"S3.SS3.SSS0.Px3.p1.1.m1.4\" class=\"ltx_Math\" alttext=\"C\\in\\{128,256,512,1024\\}\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px3.p1.1.m1.4a\"><mrow id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.cmml\"><mi id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2.cmml\">C</mi><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1.cmml\">âˆˆ</mo><mrow id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.1\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml\">128</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2.cmml\">256</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.3\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3.cmml\">512</mn><mo id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.4\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4.cmml\">1024</mn><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2.5\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4b\"><apply id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5\"><in id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.1\"></in><ci id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.2\">ğ¶</ci><set id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.5.3.2\"><cn type=\"integer\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.1.1\">128</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.2.2\">256</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.3.3\">512</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.1.m1.4.4\">1024</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.1.m1.4c\">C\\in\\{128,256,512,1024\\}</annotation></semantics></math>). Documents are padded to the same length with padding tokens. The loss curves are plotted in Figure <a href=\"#S3.F5.sf3\" title=\"Figure 5(c) â€£ Figure 5 â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5(c)</span></a>. With the same number of gradient steps, larger context length is correlated with lower recovery score. This suggests that sufficient training on each task is necessary, and for longer input context it takes more gradient descent steps to memorize the task. For example, we show in <a href=\"#A2.SS6\" title=\"B.6 Effect of Number of Gradient Steps for Long Context Length â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Section</span>Â <span class=\"ltx_text ltx_ref_tag\">B.6</span></a> that we can achieve the recovery score of context length 256 with <math id=\"S3.SS3.SSS0.Px3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"M=10\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px3.p1.2.m2.1a\"><mrow id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml\"><mi id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.2\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml\">M</mi><mo id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.3\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1b\"><apply id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1\"><eq id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.1\"></eq><ci id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.2.m2.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.2.m2.1c\">M=10</annotation></semantics></math> for context length 1024 with <math id=\"S3.SS3.SSS0.Px3.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"M=40\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px3.p1.3.m3.1a\"><mrow id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml\"><mi id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.2\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.2.cmml\">M</mi><mo id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.1\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.1.cmml\">=</mo><mn id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.3\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.3.cmml\">40</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1b\"><apply id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1\"><eq id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.1\"></eq><ci id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.2.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.3.cmml\" xref=\"S3.SS3.SSS0.Px3.p1.3.m3.1.1.3\">40</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px3.p1.3.m3.1c\">M=40</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"S3.SS3.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Number of Frozen Blocks.</h4>\n\n<div id=\"S3.SS3.SSS0.Px4.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px4.p1.2\" class=\"ltx_p\">We experiment with keeping only a subset of transformer blocks in the pre-trained model tunable and freezing the other layers during fine-tuning. Specifically, we freeze the first <math id=\"S3.SS3.SSS0.Px4.p1.1.m1.5\" class=\"ltx_Math\" alttext=\"B\\in\\{4,6,8,10,12\\}\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px4.p1.1.m1.5a\"><mrow id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.cmml\"><mi id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2.cmml\">B</mi><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1.cmml\">âˆˆ</mo><mrow id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.1\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">{</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml\">4</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2.cmml\">6</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.3\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3.cmml\">8</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.4\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4.cmml\">10</mn><mo id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.5\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">,</mo><mn id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5.cmml\">12</mn><mo stretchy=\"false\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2.6\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5b\"><apply id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6\"><in id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.1\"></in><ci id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.2\">ğµ</ci><set id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.6.3.2\"><cn type=\"integer\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.1.1\">4</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.2.2\">6</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.3.3\">8</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.4.4\">10</cn><cn type=\"integer\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.1.m1.5.5\">12</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px4.p1.1.m1.5c\">B\\in\\{4,6,8,10,12\\}</annotation></semantics></math> transformer blocks (out of 16 total blocks in the Pythia-1B model) and tune only the last <math id=\"S3.SS3.SSS0.Px4.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"16-B\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px4.p1.2.m2.1a\"><mrow id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml\"><mn id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml\">16</mn><mo id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1.cmml\">âˆ’</mo><mi id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml\">B</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1b\"><apply id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1\"><minus id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.1\"></minus><cn type=\"integer\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.2\">16</cn><ci id=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml\" xref=\"S3.SS3.SSS0.Px4.p1.2.m2.1.1.3\">ğµ</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px4.p1.2.m2.1c\">16-B</annotation></semantics></math> blocks. Loss curves are plotted in Figure <a href=\"#S3.F5.sf4\" title=\"Figure 5(d) â€£ Figure 5 â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5(d)</span></a>. Results show that we need at least 8 tunable transformer blocks to observe a strong anticipation phenomenon in the last epoch. The results are consistent with our observations in section <a href=\"#S3.SS2\" title=\"3.2 Anticipatory Recovery is an Emergent Behavior â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and confirm that the model needs sufficient depth to exhibit anticipatory recovery even with a frozen pre-trained deep representation.</p>\n</div>\n</section>\n<section id=\"S3.SS3.SSS0.Px5\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Optimizer.</h4>\n\n<div id=\"S3.SS3.SSS0.Px5.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px5.p1.2\" class=\"ltx_p\">In addition to the gradient descent optimizer, we also experiment with the Adam optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma and Ba, <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2015</a>)</cite>. For each document, we reset the optimizer state. Loss curves are plotted in Figure <a href=\"#S3.F6\" title=\"Figure 6 â€£ Figure 7 â€£ Summary. â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. Results show that Adam, which is a stronger optimizer than standard gradient descent, further facilitates anticipatory recovery, at least in part by producing greater initial forgetting. The average recovery score for Adam is <math id=\"S3.SS3.SSS0.Px5.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.689\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px5.p1.1.m1.1a\"><mn id=\"S3.SS3.SSS0.Px5.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px5.p1.1.m1.1.1.cmml\">0.689</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px5.p1.1.m1.1b\"><cn type=\"float\" id=\"S3.SS3.SSS0.Px5.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px5.p1.1.m1.1.1\">0.689</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px5.p1.1.m1.1c\">0.689</annotation></semantics></math> while for vanilla gradient descent is <math id=\"S3.SS3.SSS0.Px5.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"0.211\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px5.p1.2.m2.1a\"><mn id=\"S3.SS3.SSS0.Px5.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px5.p1.2.m2.1.1.cmml\">0.211</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px5.p1.2.m2.1b\"><cn type=\"float\" id=\"S3.SS3.SSS0.Px5.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px5.p1.2.m2.1.1\">0.211</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px5.p1.2.m2.1c\">0.211</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"S3.SS3.SSS0.Px6\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Data Randomness.</h4>\n\n<div id=\"S3.SS3.SSS0.Px6.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px6.p1.2\" class=\"ltx_p\">So far we have experimented with the exact same training documents across different epochs. However, in a more realistic sequential learning setting the data points might be slightly different for each repetition (e.g.Â different descriptions of the same concept, different perspectives of the same object), leading to stochasticity in the optimization process. To explore sequential cyclic training with data randomness, we design the following two training settings: for each gradient step on a document, we experimented with (1) randomly masking a subset (up to 30%) of the tokens in the input, and (2) randomly shifting the â€œwindowâ€ of <math id=\"S3.SS3.SSS0.Px6.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px6.p1.1.m1.1a\"><mi id=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1\" xref=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px6.p1.1.m1.1b\"><ci id=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px6.p1.1.m1.1.1\">ğ¶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px6.p1.1.m1.1c\">C</annotation></semantics></math> tokens used for training by up to <math id=\"S3.SS3.SSS0.Px6.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"C/2\" display=\"inline\"><semantics id=\"S3.SS3.SSS0.Px6.p1.2.m2.1a\"><mrow id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.cmml\"><mi id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.2\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.2.cmml\">C</mi><mo id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.1\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.1.cmml\">/</mo><mn id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.3\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.SSS0.Px6.p1.2.m2.1b\"><apply id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.cmml\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1\"><divide id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.1.cmml\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.1\"></divide><ci id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.2.cmml\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.2\">ğ¶</ci><cn type=\"integer\" id=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.3.cmml\" xref=\"S3.SS3.SSS0.Px6.p1.2.m2.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.SSS0.Px6.p1.2.m2.1c\">C/2</annotation></semantics></math> tokens. The resulting loss curves are plotted in Figure <a href=\"#S3.F4\" title=\"Figure 4 â€£ Effects of Model Width and Depth. â€£ 3.2 Anticipatory Recovery is an Emergent Behavior â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that, while the anticipatory recovery phenomenon is generally weaker when there is more variation in each data point, the recovery still clearly exists.</p>\n</div>\n</section>\n<section id=\"S3.SS3.SSS0.Px7\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Summary.</h4>\n\n<div id=\"S3.SS3.SSS0.Px7.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.SSS0.Px7.p1.1\" class=\"ltx_p\">The experiment results in this subsection suggest that the modelâ€™s ability to fit on each task is crucial for the appearance of the anticipatory recovery phenomenon. With a larger number of gradient steps, shorter context length, more learnable layers, and a better optimizer, the model is more capable of fitting to the focal task, and those factors also correlate with larger recovery score. We also confirmed that anticipatory recovery exists for long task sequences and slightly augmented data points within each episode, and again these factors that make learning harder also reduce the anticipatory recovery.</p>\n</div>\n<figure id=\"S3.F7\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F6\" class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" style=\"width:138.8pt;\"><img src=\"./assets/x13.png\" id=\"S3.F7.1.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" width=\"129\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F6.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>: </span><span id=\"S3.F6.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Comparison between Adam and vanilla gradient descent optimizer for randomly initialized Pythia-1B models on cyclic training.</span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F7.fig1\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:277.5pt;\">\n<div id=\"S3.F7.fig1.1\" class=\"ltx_block\">\n<figure id=\"S3.F7.sf1\" class=\"ltx_figure\"><img src=\"./assets/x14.png\" id=\"S3.F7.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"126\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F7.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S3.F7.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Causal Image Modeling </span></figcaption>\n</figure>\n<figure id=\"S3.F7.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x15.png\" id=\"S3.F7.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"126\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F7.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S3.F7.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Image Classification </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S3.F7.fig1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>: </span><span id=\"S3.F7.fig1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Results for cyclic training on vision tasks: (a) Causal image modeling with Image GPT. (b) image classification with vision transformers and VGG convolutional networks.</span></figcaption>\n</figure>\n</div>\n</div>\n</figure>\n</section>\n</section>\n<section id=\"S3.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>Anticipatory Recovery in Vision Models</h3>\n\n<div id=\"S3.SS4.p1\" class=\"ltx_para\">\n<p id=\"S3.SS4.p1.1\" class=\"ltx_p\">To examine the generality of the anticipatory recovery phenomenon, in this subsection we explore the sequential cyclic training setting on two tasks in computer vision: causal image modeling and image classification. More detailed experiment setup is available in Appendix <a href=\"#A1\" title=\"Appendix A Additional Experiment Setups â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n</div>\n<section id=\"S3.SS4.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Causal Image Modeling.</h4>\n\n<div id=\"S3.SS4.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS4.SSS0.Px1.p1.2\" class=\"ltx_p\">For the causal image modeling task, we use pre-trained Image GPT models and the corresponding pixel tokenizers from <cite class=\"ltx_cite ltx_citemacro_citet\">Chen etÂ al. (<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, and we use CIFAR-10 <cite class=\"ltx_cite ltx_citemacro_citep\">(Krizhevsky etÂ al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2009</a>)</cite> as the fine-tuning dataset. Similar to the LLM experiments, we fine-tune our model on each sampled image for <math id=\"S3.SS4.SSS0.Px1.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S3.SS4.SSS0.Px1.p1.1.m1.1a\"><mi id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1b\"><ci id=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml\" xref=\"S3.SS4.SSS0.Px1.p1.1.m1.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px1.p1.1.m1.1c\">M</annotation></semantics></math> gradient steps, and repeat <math id=\"S3.SS4.SSS0.Px1.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"E\" display=\"inline\"><semantics id=\"S3.SS4.SSS0.Px1.p1.2.m2.1a\"><mi id=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1\" xref=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px1.p1.2.m2.1b\"><ci id=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1.cmml\" xref=\"S3.SS4.SSS0.Px1.p1.2.m2.1.1\">ğ¸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px1.p1.2.m2.1c\">E</annotation></semantics></math> epochs with a fixed order of the images. The resulting loss curves are shown in Figure <a href=\"#S3.F7.sf1\" title=\"Figure 7(a) â€£ Figure 7 â€£ Summary. â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7(a)</span></a>. The results show that the anticipatory recovery phenomenon also exists for sequential cyclic training of image modeling in addition to language modeling.</p>\n</div>\n</section>\n<section id=\"S3.SS4.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Image Classification.</h4>\n\n<div id=\"S3.SS4.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS4.SSS0.Px2.p1.2\" class=\"ltx_p\">For the image classification task, we use pre-trained vision transformer (ViT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy etÂ al., <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> and VGG-19 <cite class=\"ltx_cite ltx_citemacro_citep\">(Simonyan and Zisserman, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2015</a>)</cite> models, and we use ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2009</a>)</cite> as the training dataset. For each experiment, we randomly sample 800 images from Imagenet and divide them into 25 batches of 32 images each. We train our model on each batch for <math id=\"S3.SS4.SSS0.Px2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S3.SS4.SSS0.Px2.p1.1.m1.1a\"><mi id=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1\" xref=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px2.p1.1.m1.1b\"><ci id=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml\" xref=\"S3.SS4.SSS0.Px2.p1.1.m1.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px2.p1.1.m1.1c\">M</annotation></semantics></math> gradient steps and repeat <math id=\"S3.SS4.SSS0.Px2.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"E\" display=\"inline\"><semantics id=\"S3.SS4.SSS0.Px2.p1.2.m2.1a\"><mi id=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1\" xref=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.SSS0.Px2.p1.2.m2.1b\"><ci id=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1.cmml\" xref=\"S3.SS4.SSS0.Px2.p1.2.m2.1.1\">ğ¸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.SSS0.Px2.p1.2.m2.1c\">E</annotation></semantics></math> epochs with a fixed order of the batches. The resulting loss curves are plotted in Figure <a href=\"#S3.F7.sf2\" title=\"Figure 7(b) â€£ Figure 7 â€£ Summary. â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7(b)</span></a>. Results show that both the transformer ViT and the convolutional VGG exhibit anticipatory recovery in cyclic training.</p>\n</div>\n<div id=\"S3.SS4.SSS0.Px2.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.SSS0.Px2.p2.1\" class=\"ltx_p\">By these experiments we confirm that anticipatory recovery occurs not only for LLMs but also for at least some of the widespread image classification models and non-transformer architectures.</p>\n</div>\n</section>\n</section>\n</section>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Understanding Cyclic Training Dynamics</h2>\n\n<figure id=\"S4.F8\" class=\"ltx_figure\">\n<div id=\"S4.F8.8\" class=\"ltx_block\">\n<figure id=\"S4.F8.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x16.png\" id=\"S4.F8.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"127\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F8.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"S4.F8.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Gradients </span></figcaption>\n</figure>\n<figure id=\"S4.F8.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x17.png\" id=\"S4.F8.sf2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"122\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F8.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"S4.F8.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Amount of Recovery </span></figcaption>\n</figure>\n<figure id=\"S4.F8.sf3\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x18.png\" id=\"S4.F8.sf3.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"125\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F8.sf3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span><span id=\"S4.F8.sf3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Weights </span></figcaption>\n</figure>\n<figure id=\"S4.F8.sf4\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x19.png\" id=\"S4.F8.sf4.g1\" class=\"ltx_graphics ltx_img_square\" width=\"120\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F8.sf4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(d)</span> </span><span id=\"S4.F8.sf4.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Activations</span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F8.9.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>: </span><span id=\"S4.F8.6.3\" class=\"ltx_text\" style=\"font-size:90%;\">Heat map visualizations for (a) cosine similarities between the gradient vectors of the attention layer in transformer block 12 of the model for each task; (b) loss recoveries for training on task <math id=\"S4.F8.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.F8.4.1.m1.1b\"><msub id=\"S4.F8.4.1.m1.1.1\" xref=\"S4.F8.4.1.m1.1.1.cmml\"><mi id=\"S4.F8.4.1.m1.1.1.2\" xref=\"S4.F8.4.1.m1.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.F8.4.1.m1.1.1.3\" xref=\"S4.F8.4.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F8.4.1.m1.1c\"><apply id=\"S4.F8.4.1.m1.1.1.cmml\" xref=\"S4.F8.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F8.4.1.m1.1.1.1.cmml\" xref=\"S4.F8.4.1.m1.1.1\">subscript</csymbol><ci id=\"S4.F8.4.1.m1.1.1.2.cmml\" xref=\"S4.F8.4.1.m1.1.1.2\">ğ’™</ci><ci id=\"S4.F8.4.1.m1.1.1.3.cmml\" xref=\"S4.F8.4.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F8.4.1.m1.1d\">\\bm{x}_{i}</annotation></semantics></math> (y-axis) and evaluating on task <math id=\"S4.F8.5.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"S4.F8.5.2.m2.1b\"><msub id=\"S4.F8.5.2.m2.1.1\" xref=\"S4.F8.5.2.m2.1.1.cmml\"><mi id=\"S4.F8.5.2.m2.1.1.2\" xref=\"S4.F8.5.2.m2.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.F8.5.2.m2.1.1.3\" xref=\"S4.F8.5.2.m2.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F8.5.2.m2.1c\"><apply id=\"S4.F8.5.2.m2.1.1.cmml\" xref=\"S4.F8.5.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F8.5.2.m2.1.1.1.cmml\" xref=\"S4.F8.5.2.m2.1.1\">subscript</csymbol><ci id=\"S4.F8.5.2.m2.1.1.2.cmml\" xref=\"S4.F8.5.2.m2.1.1.2\">ğ’™</ci><ci id=\"S4.F8.5.2.m2.1.1.3.cmml\" xref=\"S4.F8.5.2.m2.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F8.5.2.m2.1d\">\\bm{x}_{j}</annotation></semantics></math> (x-axis); (c) cosine similarities between the flattened model weight vectors at each point in training; (d) cosine similarities between the last layer activations for document <math id=\"S4.F8.6.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S4.F8.6.3.m3.1b\"><msub id=\"S4.F8.6.3.m3.1.1\" xref=\"S4.F8.6.3.m3.1.1.cmml\"><mi id=\"S4.F8.6.3.m3.1.1.2\" xref=\"S4.F8.6.3.m3.1.1.2.cmml\">ğ’™</mi><mn id=\"S4.F8.6.3.m3.1.1.3\" xref=\"S4.F8.6.3.m3.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F8.6.3.m3.1c\"><apply id=\"S4.F8.6.3.m3.1.1.cmml\" xref=\"S4.F8.6.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F8.6.3.m3.1.1.1.cmml\" xref=\"S4.F8.6.3.m3.1.1\">subscript</csymbol><ci id=\"S4.F8.6.3.m3.1.1.2.cmml\" xref=\"S4.F8.6.3.m3.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S4.F8.6.3.m3.1.1.3.cmml\" xref=\"S4.F8.6.3.m3.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F8.6.3.m3.1d\">\\bm{x}_{1}</annotation></semantics></math> at each point in training.</span></figcaption>\n</figure>\n<div id=\"S4.p1\" class=\"ltx_para\">\n<p id=\"S4.p1.2\" class=\"ltx_p\">An important general question about the anticipatory recovery phenomenon is whether it is due to some causal mechanism relating the dynamics of model parameters to the training sequence, or whether it is more correlational in that adjacent tasks come to be represented more similarly by the model. We found initial evidence for the latter hypothesis in experiments locally reversing the task sequence (e.g., showing that <math id=\"S4.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"x_{t+1}\" display=\"inline\"><semantics id=\"S4.p1.1.m1.1a\"><msub id=\"S4.p1.1.m1.1.1\" xref=\"S4.p1.1.m1.1.1.cmml\"><mi id=\"S4.p1.1.m1.1.1.2\" xref=\"S4.p1.1.m1.1.1.2.cmml\">x</mi><mrow id=\"S4.p1.1.m1.1.1.3\" xref=\"S4.p1.1.m1.1.1.3.cmml\"><mi id=\"S4.p1.1.m1.1.1.3.2\" xref=\"S4.p1.1.m1.1.1.3.2.cmml\">t</mi><mo id=\"S4.p1.1.m1.1.1.3.1\" xref=\"S4.p1.1.m1.1.1.3.1.cmml\">+</mo><mn id=\"S4.p1.1.m1.1.1.3.3\" xref=\"S4.p1.1.m1.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.p1.1.m1.1b\"><apply id=\"S4.p1.1.m1.1.1.cmml\" xref=\"S4.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.p1.1.m1.1.1.1.cmml\" xref=\"S4.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.p1.1.m1.1.1.2.cmml\" xref=\"S4.p1.1.m1.1.1.2\">ğ‘¥</ci><apply id=\"S4.p1.1.m1.1.1.3.cmml\" xref=\"S4.p1.1.m1.1.1.3\"><plus id=\"S4.p1.1.m1.1.1.3.1.cmml\" xref=\"S4.p1.1.m1.1.1.3.1\"></plus><ci id=\"S4.p1.1.m1.1.1.3.2.cmml\" xref=\"S4.p1.1.m1.1.1.3.2\">ğ‘¡</ci><cn type=\"integer\" id=\"S4.p1.1.m1.1.1.3.3.cmml\" xref=\"S4.p1.1.m1.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.p1.1.m1.1c\">x_{t+1}</annotation></semantics></math> primes <math id=\"S4.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"x_{t}\" display=\"inline\"><semantics id=\"S4.p1.2.m2.1a\"><msub id=\"S4.p1.2.m2.1.1\" xref=\"S4.p1.2.m2.1.1.cmml\"><mi id=\"S4.p1.2.m2.1.1.2\" xref=\"S4.p1.2.m2.1.1.2.cmml\">x</mi><mi id=\"S4.p1.2.m2.1.1.3\" xref=\"S4.p1.2.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.p1.2.m2.1b\"><apply id=\"S4.p1.2.m2.1.1.cmml\" xref=\"S4.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.p1.2.m2.1.1.1.cmml\" xref=\"S4.p1.2.m2.1.1\">subscript</csymbol><ci id=\"S4.p1.2.m2.1.1.2.cmml\" xref=\"S4.p1.2.m2.1.1.2\">ğ‘¥</ci><ci id=\"S4.p1.2.m2.1.1.3.cmml\" xref=\"S4.p1.2.m2.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.p1.2.m2.1c\">x_{t}</annotation></semantics></math> nearly as much as vice versa). To further test this learned similarity hypothesis, we explore the relationships between tasks and across training history of the modelâ€™s loss gradients, weights, and activations. The results enable us to better understand the dynamics of cyclic training. Unless otherwise stated, all visualizations in this section use the 410M model and default hyperparameters.</p>\n</div>\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Temporal Structure of Gradients</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.p1.1\" class=\"ltx_p\">We first explore the similarities of gradient information between documents during the training process. Our goal is to test the hypothesis that anticipatory recovery is mediated by increased similarity between gradients of proximal documents in our training sequence.</p>\n</div>\n<div id=\"S4.SS1.p2\" class=\"ltx_para\">\n<p id=\"S4.SS1.p2.20\" class=\"ltx_p\">We do cyclic training for 4 epochs and compute the gradient of each document at the attention layer of transformer block 12 at the conclusion of training. In Figure <a href=\"#S4.F8.sf1\" title=\"Figure 8(a) â€£ Figure 8 â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8(a)</span></a>, we plot the cosine similarities between these gradient vectors of each document. Results show that the gradients have mostly positive cosine similarities (except for the last document, on which the model has just been trained). We predicted the gradient similarities to depend only on <math id=\"S4.SS1.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"|i-j|\" display=\"inline\"><semantics id=\"S4.SS1.p2.1.m1.1a\"><mrow id=\"S4.SS1.p2.1.m1.1.1.1\" xref=\"S4.SS1.p2.1.m1.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.p2.1.m1.1.1.1.2\" xref=\"S4.SS1.p2.1.m1.1.1.2.1.cmml\">|</mo><mrow id=\"S4.SS1.p2.1.m1.1.1.1.1\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.cmml\"><mi id=\"S4.SS1.p2.1.m1.1.1.1.1.2\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.2.cmml\">i</mi><mo id=\"S4.SS1.p2.1.m1.1.1.1.1.1\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.1.cmml\">âˆ’</mo><mi id=\"S4.SS1.p2.1.m1.1.1.1.1.3\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.3.cmml\">j</mi></mrow><mo stretchy=\"false\" id=\"S4.SS1.p2.1.m1.1.1.1.3\" xref=\"S4.SS1.p2.1.m1.1.1.2.1.cmml\">|</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.1.m1.1b\"><apply id=\"S4.SS1.p2.1.m1.1.1.2.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.1\"><abs id=\"S4.SS1.p2.1.m1.1.1.2.1.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.1.2\"></abs><apply id=\"S4.SS1.p2.1.m1.1.1.1.1.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.1.1\"><minus id=\"S4.SS1.p2.1.m1.1.1.1.1.1.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.1\"></minus><ci id=\"S4.SS1.p2.1.m1.1.1.1.1.2.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.2\">ğ‘–</ci><ci id=\"S4.SS1.p2.1.m1.1.1.1.1.3.cmml\" xref=\"S4.SS1.p2.1.m1.1.1.1.1.3\">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.1.m1.1c\">|i-j|</annotation></semantics></math>, peaking along the diagonal. Instead, to our surprise, the gradient similarities are highest near the center of the heat map. That is, the gradient similarity between documents <math id=\"S4.SS1.p2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t-1}\" display=\"inline\"><semantics id=\"S4.SS1.p2.2.m2.1a\"><msub id=\"S4.SS1.p2.2.m2.1.1\" xref=\"S4.SS1.p2.2.m2.1.1.cmml\"><mi id=\"S4.SS1.p2.2.m2.1.1.2\" xref=\"S4.SS1.p2.2.m2.1.1.2.cmml\">ğ’™</mi><mrow id=\"S4.SS1.p2.2.m2.1.1.3\" xref=\"S4.SS1.p2.2.m2.1.1.3.cmml\"><mi id=\"S4.SS1.p2.2.m2.1.1.3.2\" xref=\"S4.SS1.p2.2.m2.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.2.m2.1.1.3.1\" xref=\"S4.SS1.p2.2.m2.1.1.3.1.cmml\">âˆ’</mo><mn id=\"S4.SS1.p2.2.m2.1.1.3.3\" xref=\"S4.SS1.p2.2.m2.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.2.m2.1b\"><apply id=\"S4.SS1.p2.2.m2.1.1.cmml\" xref=\"S4.SS1.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.2.m2.1.1.1.cmml\" xref=\"S4.SS1.p2.2.m2.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.2.m2.1.1.2.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.2\">ğ’™</ci><apply id=\"S4.SS1.p2.2.m2.1.1.3.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.3\"><minus id=\"S4.SS1.p2.2.m2.1.1.3.1.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.3.1\"></minus><ci id=\"S4.SS1.p2.2.m2.1.1.3.2.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.3.2\">ğ‘¡</ci><cn type=\"integer\" id=\"S4.SS1.p2.2.m2.1.1.3.3.cmml\" xref=\"S4.SS1.p2.2.m2.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.2.m2.1c\">\\bm{x}_{t-1}</annotation></semantics></math> and <math id=\"S4.SS1.p2.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t}\" display=\"inline\"><semantics id=\"S4.SS1.p2.3.m3.1a\"><msub id=\"S4.SS1.p2.3.m3.1.1\" xref=\"S4.SS1.p2.3.m3.1.1.cmml\"><mi id=\"S4.SS1.p2.3.m3.1.1.2\" xref=\"S4.SS1.p2.3.m3.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.3.m3.1.1.3\" xref=\"S4.SS1.p2.3.m3.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.3.m3.1b\"><apply id=\"S4.SS1.p2.3.m3.1.1.cmml\" xref=\"S4.SS1.p2.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.3.m3.1.1.1.cmml\" xref=\"S4.SS1.p2.3.m3.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.3.m3.1.1.2.cmml\" xref=\"S4.SS1.p2.3.m3.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.3.m3.1.1.3.cmml\" xref=\"S4.SS1.p2.3.m3.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.3.m3.1c\">\\bm{x}_{t}</annotation></semantics></math> depends on where we are in the cycle. This result suggests an additional layer to the anticipatory recovery phenomenon: Recovery for document <math id=\"S4.SS1.p2.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t}\" display=\"inline\"><semantics id=\"S4.SS1.p2.4.m4.1a\"><msub id=\"S4.SS1.p2.4.m4.1.1\" xref=\"S4.SS1.p2.4.m4.1.1.cmml\"><mi id=\"S4.SS1.p2.4.m4.1.1.2\" xref=\"S4.SS1.p2.4.m4.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.4.m4.1.1.3\" xref=\"S4.SS1.p2.4.m4.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.4.m4.1b\"><apply id=\"S4.SS1.p2.4.m4.1.1.cmml\" xref=\"S4.SS1.p2.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.4.m4.1.1.1.cmml\" xref=\"S4.SS1.p2.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.4.m4.1.1.2.cmml\" xref=\"S4.SS1.p2.4.m4.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.4.m4.1.1.3.cmml\" xref=\"S4.SS1.p2.4.m4.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.4.m4.1c\">\\bm{x}_{t}</annotation></semantics></math> is greatest from training on document <math id=\"S4.SS1.p2.5.m5.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t-1}\" display=\"inline\"><semantics id=\"S4.SS1.p2.5.m5.1a\"><msub id=\"S4.SS1.p2.5.m5.1.1\" xref=\"S4.SS1.p2.5.m5.1.1.cmml\"><mi id=\"S4.SS1.p2.5.m5.1.1.2\" xref=\"S4.SS1.p2.5.m5.1.1.2.cmml\">ğ’™</mi><mrow id=\"S4.SS1.p2.5.m5.1.1.3\" xref=\"S4.SS1.p2.5.m5.1.1.3.cmml\"><mi id=\"S4.SS1.p2.5.m5.1.1.3.2\" xref=\"S4.SS1.p2.5.m5.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.5.m5.1.1.3.1\" xref=\"S4.SS1.p2.5.m5.1.1.3.1.cmml\">âˆ’</mo><mn id=\"S4.SS1.p2.5.m5.1.1.3.3\" xref=\"S4.SS1.p2.5.m5.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.5.m5.1b\"><apply id=\"S4.SS1.p2.5.m5.1.1.cmml\" xref=\"S4.SS1.p2.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.5.m5.1.1.1.cmml\" xref=\"S4.SS1.p2.5.m5.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.5.m5.1.1.2.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.2\">ğ’™</ci><apply id=\"S4.SS1.p2.5.m5.1.1.3.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3\"><minus id=\"S4.SS1.p2.5.m5.1.1.3.1.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3.1\"></minus><ci id=\"S4.SS1.p2.5.m5.1.1.3.2.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3.2\">ğ‘¡</ci><cn type=\"integer\" id=\"S4.SS1.p2.5.m5.1.1.3.3.cmml\" xref=\"S4.SS1.p2.5.m5.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.5.m5.1c\">\\bm{x}_{t-1}</annotation></semantics></math>, but the strength of the potential facilitation between <math id=\"S4.SS1.p2.6.m6.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t-1}\" display=\"inline\"><semantics id=\"S4.SS1.p2.6.m6.1a\"><msub id=\"S4.SS1.p2.6.m6.1.1\" xref=\"S4.SS1.p2.6.m6.1.1.cmml\"><mi id=\"S4.SS1.p2.6.m6.1.1.2\" xref=\"S4.SS1.p2.6.m6.1.1.2.cmml\">ğ’™</mi><mrow id=\"S4.SS1.p2.6.m6.1.1.3\" xref=\"S4.SS1.p2.6.m6.1.1.3.cmml\"><mi id=\"S4.SS1.p2.6.m6.1.1.3.2\" xref=\"S4.SS1.p2.6.m6.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.6.m6.1.1.3.1\" xref=\"S4.SS1.p2.6.m6.1.1.3.1.cmml\">âˆ’</mo><mn id=\"S4.SS1.p2.6.m6.1.1.3.3\" xref=\"S4.SS1.p2.6.m6.1.1.3.3.cmml\">1</mn></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.6.m6.1b\"><apply id=\"S4.SS1.p2.6.m6.1.1.cmml\" xref=\"S4.SS1.p2.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.6.m6.1.1.1.cmml\" xref=\"S4.SS1.p2.6.m6.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.6.m6.1.1.2.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.2\">ğ’™</ci><apply id=\"S4.SS1.p2.6.m6.1.1.3.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.3\"><minus id=\"S4.SS1.p2.6.m6.1.1.3.1.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.3.1\"></minus><ci id=\"S4.SS1.p2.6.m6.1.1.3.2.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.3.2\">ğ‘¡</ci><cn type=\"integer\" id=\"S4.SS1.p2.6.m6.1.1.3.3.cmml\" xref=\"S4.SS1.p2.6.m6.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.6.m6.1c\">\\bm{x}_{t-1}</annotation></semantics></math> and <math id=\"S4.SS1.p2.7.m7.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t}\" display=\"inline\"><semantics id=\"S4.SS1.p2.7.m7.1a\"><msub id=\"S4.SS1.p2.7.m7.1.1\" xref=\"S4.SS1.p2.7.m7.1.1.cmml\"><mi id=\"S4.SS1.p2.7.m7.1.1.2\" xref=\"S4.SS1.p2.7.m7.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.7.m7.1.1.3\" xref=\"S4.SS1.p2.7.m7.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.7.m7.1b\"><apply id=\"S4.SS1.p2.7.m7.1.1.cmml\" xref=\"S4.SS1.p2.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.7.m7.1.1.1.cmml\" xref=\"S4.SS1.p2.7.m7.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.7.m7.1.1.2.cmml\" xref=\"S4.SS1.p2.7.m7.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.7.m7.1.1.3.cmml\" xref=\"S4.SS1.p2.7.m7.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.7.m7.1c\">\\bm{x}_{t}</annotation></semantics></math> is actually greatest after we train for another <math id=\"S4.SS1.p2.8.m8.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"S4.SS1.p2.8.m8.1a\"><mi id=\"S4.SS1.p2.8.m8.1.1\" xref=\"S4.SS1.p2.8.m8.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.8.m8.1b\"><ci id=\"S4.SS1.p2.8.m8.1.1.cmml\" xref=\"S4.SS1.p2.8.m8.1.1\">ğ‘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.8.m8.1c\">b</annotation></semantics></math> documents (for some small number <math id=\"S4.SS1.p2.9.m9.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"S4.SS1.p2.9.m9.1a\"><mi id=\"S4.SS1.p2.9.m9.1.1\" xref=\"S4.SS1.p2.9.m9.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.9.m9.1b\"><ci id=\"S4.SS1.p2.9.m9.1.1.cmml\" xref=\"S4.SS1.p2.9.m9.1.1\">ğ‘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.9.m9.1c\">b</annotation></semantics></math>, which ranges from roughly 10 to 15 in this particular case). We verify this by computing the pairwise recovery: we take the model checkpoint after 4 epochs of cyclic training, do <math id=\"S4.SS1.p2.10.m10.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S4.SS1.p2.10.m10.1a\"><mi id=\"S4.SS1.p2.10.m10.1.1\" xref=\"S4.SS1.p2.10.m10.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.10.m10.1b\"><ci id=\"S4.SS1.p2.10.m10.1.1.cmml\" xref=\"S4.SS1.p2.10.m10.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.10.m10.1c\">M</annotation></semantics></math> gradient updates on document <math id=\"S4.SS1.p2.11.m11.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.SS1.p2.11.m11.1a\"><msub id=\"S4.SS1.p2.11.m11.1.1\" xref=\"S4.SS1.p2.11.m11.1.1.cmml\"><mi id=\"S4.SS1.p2.11.m11.1.1.2\" xref=\"S4.SS1.p2.11.m11.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.11.m11.1.1.3\" xref=\"S4.SS1.p2.11.m11.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.11.m11.1b\"><apply id=\"S4.SS1.p2.11.m11.1.1.cmml\" xref=\"S4.SS1.p2.11.m11.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.11.m11.1.1.1.cmml\" xref=\"S4.SS1.p2.11.m11.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.11.m11.1.1.2.cmml\" xref=\"S4.SS1.p2.11.m11.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.11.m11.1.1.3.cmml\" xref=\"S4.SS1.p2.11.m11.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.11.m11.1c\">\\bm{x}_{i}</annotation></semantics></math> and compute the difference in the loss of document <math id=\"S4.SS1.p2.12.m12.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"S4.SS1.p2.12.m12.1a\"><msub id=\"S4.SS1.p2.12.m12.1.1\" xref=\"S4.SS1.p2.12.m12.1.1.cmml\"><mi id=\"S4.SS1.p2.12.m12.1.1.2\" xref=\"S4.SS1.p2.12.m12.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.12.m12.1.1.3\" xref=\"S4.SS1.p2.12.m12.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.12.m12.1b\"><apply id=\"S4.SS1.p2.12.m12.1.1.cmml\" xref=\"S4.SS1.p2.12.m12.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.12.m12.1.1.1.cmml\" xref=\"S4.SS1.p2.12.m12.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.12.m12.1.1.2.cmml\" xref=\"S4.SS1.p2.12.m12.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.12.m12.1.1.3.cmml\" xref=\"S4.SS1.p2.12.m12.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.12.m12.1c\">\\bm{x}_{j}</annotation></semantics></math> before and after these gradient updates, for each pair of documents <math id=\"S4.SS1.p2.13.m13.2\" class=\"ltx_Math\" alttext=\"(\\bm{x}_{i},\\bm{x}_{j})\" display=\"inline\"><semantics id=\"S4.SS1.p2.13.m13.2a\"><mrow id=\"S4.SS1.p2.13.m13.2.2.2\" xref=\"S4.SS1.p2.13.m13.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.p2.13.m13.2.2.2.3\" xref=\"S4.SS1.p2.13.m13.2.2.3.cmml\">(</mo><msub id=\"S4.SS1.p2.13.m13.1.1.1.1\" xref=\"S4.SS1.p2.13.m13.1.1.1.1.cmml\"><mi id=\"S4.SS1.p2.13.m13.1.1.1.1.2\" xref=\"S4.SS1.p2.13.m13.1.1.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.13.m13.1.1.1.1.3\" xref=\"S4.SS1.p2.13.m13.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S4.SS1.p2.13.m13.2.2.2.4\" xref=\"S4.SS1.p2.13.m13.2.2.3.cmml\">,</mo><msub id=\"S4.SS1.p2.13.m13.2.2.2.2\" xref=\"S4.SS1.p2.13.m13.2.2.2.2.cmml\"><mi id=\"S4.SS1.p2.13.m13.2.2.2.2.2\" xref=\"S4.SS1.p2.13.m13.2.2.2.2.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.13.m13.2.2.2.2.3\" xref=\"S4.SS1.p2.13.m13.2.2.2.2.3.cmml\">j</mi></msub><mo stretchy=\"false\" id=\"S4.SS1.p2.13.m13.2.2.2.5\" xref=\"S4.SS1.p2.13.m13.2.2.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.13.m13.2b\"><interval closure=\"open\" id=\"S4.SS1.p2.13.m13.2.2.3.cmml\" xref=\"S4.SS1.p2.13.m13.2.2.2\"><apply id=\"S4.SS1.p2.13.m13.1.1.1.1.cmml\" xref=\"S4.SS1.p2.13.m13.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.13.m13.1.1.1.1.1.cmml\" xref=\"S4.SS1.p2.13.m13.1.1.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.13.m13.1.1.1.1.2.cmml\" xref=\"S4.SS1.p2.13.m13.1.1.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.13.m13.1.1.1.1.3.cmml\" xref=\"S4.SS1.p2.13.m13.1.1.1.1.3\">ğ‘–</ci></apply><apply id=\"S4.SS1.p2.13.m13.2.2.2.2.cmml\" xref=\"S4.SS1.p2.13.m13.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.13.m13.2.2.2.2.1.cmml\" xref=\"S4.SS1.p2.13.m13.2.2.2.2\">subscript</csymbol><ci id=\"S4.SS1.p2.13.m13.2.2.2.2.2.cmml\" xref=\"S4.SS1.p2.13.m13.2.2.2.2.2\">ğ’™</ci><ci id=\"S4.SS1.p2.13.m13.2.2.2.2.3.cmml\" xref=\"S4.SS1.p2.13.m13.2.2.2.2.3\">ğ‘—</ci></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.13.m13.2c\">(\\bm{x}_{i},\\bm{x}_{j})</annotation></semantics></math>. We plot these pairwise loss recoveries in Figure <a href=\"#S4.F8.sf2\" title=\"Figure 8(b) â€£ Figure 8 â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8(b)</span></a>. Results confirm that the amount of recovery on document <math id=\"S4.SS1.p2.14.m14.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"S4.SS1.p2.14.m14.1a\"><msub id=\"S4.SS1.p2.14.m14.1.1\" xref=\"S4.SS1.p2.14.m14.1.1.cmml\"><mi id=\"S4.SS1.p2.14.m14.1.1.2\" xref=\"S4.SS1.p2.14.m14.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.14.m14.1.1.3\" xref=\"S4.SS1.p2.14.m14.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.14.m14.1b\"><apply id=\"S4.SS1.p2.14.m14.1.1.cmml\" xref=\"S4.SS1.p2.14.m14.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.14.m14.1.1.1.cmml\" xref=\"S4.SS1.p2.14.m14.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.14.m14.1.1.2.cmml\" xref=\"S4.SS1.p2.14.m14.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.14.m14.1.1.3.cmml\" xref=\"S4.SS1.p2.14.m14.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.14.m14.1c\">\\bm{x}_{j}</annotation></semantics></math> is highest when the model checkpoint is taken from roughly 10 to 15 documents before or after document <math id=\"S4.SS1.p2.15.m15.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"S4.SS1.p2.15.m15.1a\"><msub id=\"S4.SS1.p2.15.m15.1.1\" xref=\"S4.SS1.p2.15.m15.1.1.cmml\"><mi id=\"S4.SS1.p2.15.m15.1.1.2\" xref=\"S4.SS1.p2.15.m15.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.15.m15.1.1.3\" xref=\"S4.SS1.p2.15.m15.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.15.m15.1b\"><apply id=\"S4.SS1.p2.15.m15.1.1.cmml\" xref=\"S4.SS1.p2.15.m15.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.15.m15.1.1.1.cmml\" xref=\"S4.SS1.p2.15.m15.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.15.m15.1.1.2.cmml\" xref=\"S4.SS1.p2.15.m15.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.15.m15.1.1.3.cmml\" xref=\"S4.SS1.p2.15.m15.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.15.m15.1c\">\\bm{x}_{j}</annotation></semantics></math> in cyclic training and then fine-tuned on a proximal document <math id=\"S4.SS1.p2.16.m16.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.SS1.p2.16.m16.1a\"><msub id=\"S4.SS1.p2.16.m16.1.1\" xref=\"S4.SS1.p2.16.m16.1.1.cmml\"><mi id=\"S4.SS1.p2.16.m16.1.1.2\" xref=\"S4.SS1.p2.16.m16.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.16.m16.1.1.3\" xref=\"S4.SS1.p2.16.m16.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.16.m16.1b\"><apply id=\"S4.SS1.p2.16.m16.1.1.cmml\" xref=\"S4.SS1.p2.16.m16.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.16.m16.1.1.1.cmml\" xref=\"S4.SS1.p2.16.m16.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.16.m16.1.1.2.cmml\" xref=\"S4.SS1.p2.16.m16.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.16.m16.1.1.3.cmml\" xref=\"S4.SS1.p2.16.m16.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.16.m16.1c\">\\bm{x}_{i}</annotation></semantics></math> in the sequence. The fact that this pairwise loss recovery matrix is roughly symmetric also suggests that the anticipatory recovery phenomenon approximately exhibits task symmetry: gradient updates on document <math id=\"S4.SS1.p2.17.m17.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t}\" display=\"inline\"><semantics id=\"S4.SS1.p2.17.m17.1a\"><msub id=\"S4.SS1.p2.17.m17.1.1\" xref=\"S4.SS1.p2.17.m17.1.1.cmml\"><mi id=\"S4.SS1.p2.17.m17.1.1.2\" xref=\"S4.SS1.p2.17.m17.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS1.p2.17.m17.1.1.3\" xref=\"S4.SS1.p2.17.m17.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.17.m17.1b\"><apply id=\"S4.SS1.p2.17.m17.1.1.cmml\" xref=\"S4.SS1.p2.17.m17.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.17.m17.1.1.1.cmml\" xref=\"S4.SS1.p2.17.m17.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.17.m17.1.1.2.cmml\" xref=\"S4.SS1.p2.17.m17.1.1.2\">ğ’™</ci><ci id=\"S4.SS1.p2.17.m17.1.1.3.cmml\" xref=\"S4.SS1.p2.17.m17.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.17.m17.1c\">\\bm{x}_{t}</annotation></semantics></math> decrease the loss for document <math id=\"S4.SS1.p2.18.m18.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{t+k}\" display=\"inline\"><semantics id=\"S4.SS1.p2.18.m18.1a\"><msub id=\"S4.SS1.p2.18.m18.1.1\" xref=\"S4.SS1.p2.18.m18.1.1.cmml\"><mi id=\"S4.SS1.p2.18.m18.1.1.2\" xref=\"S4.SS1.p2.18.m18.1.1.2.cmml\">ğ’™</mi><mrow id=\"S4.SS1.p2.18.m18.1.1.3\" xref=\"S4.SS1.p2.18.m18.1.1.3.cmml\"><mi id=\"S4.SS1.p2.18.m18.1.1.3.2\" xref=\"S4.SS1.p2.18.m18.1.1.3.2.cmml\">t</mi><mo id=\"S4.SS1.p2.18.m18.1.1.3.1\" xref=\"S4.SS1.p2.18.m18.1.1.3.1.cmml\">+</mo><mi id=\"S4.SS1.p2.18.m18.1.1.3.3\" xref=\"S4.SS1.p2.18.m18.1.1.3.3.cmml\">k</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.18.m18.1b\"><apply id=\"S4.SS1.p2.18.m18.1.1.cmml\" xref=\"S4.SS1.p2.18.m18.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p2.18.m18.1.1.1.cmml\" xref=\"S4.SS1.p2.18.m18.1.1\">subscript</csymbol><ci id=\"S4.SS1.p2.18.m18.1.1.2.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.2\">ğ’™</ci><apply id=\"S4.SS1.p2.18.m18.1.1.3.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3\"><plus id=\"S4.SS1.p2.18.m18.1.1.3.1.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3.1\"></plus><ci id=\"S4.SS1.p2.18.m18.1.1.3.2.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3.2\">ğ‘¡</ci><ci id=\"S4.SS1.p2.18.m18.1.1.3.3.cmml\" xref=\"S4.SS1.p2.18.m18.1.1.3.3\">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.18.m18.1c\">\\bm{x}_{t+k}</annotation></semantics></math> for small integers <math id=\"S4.SS1.p2.19.m19.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"S4.SS1.p2.19.m19.1a\"><mi id=\"S4.SS1.p2.19.m19.1.1\" xref=\"S4.SS1.p2.19.m19.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.19.m19.1b\"><ci id=\"S4.SS1.p2.19.m19.1.1.cmml\" xref=\"S4.SS1.p2.19.m19.1.1\">ğ‘˜</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.19.m19.1c\">k</annotation></semantics></math>, and vice versa. We provide additional visualizations for <math id=\"S4.SS1.p2.20.m20.3\" class=\"ltx_Math\" alttext=\"T=50,100,200\" display=\"inline\"><semantics id=\"S4.SS1.p2.20.m20.3a\"><mrow id=\"S4.SS1.p2.20.m20.3.4\" xref=\"S4.SS1.p2.20.m20.3.4.cmml\"><mi id=\"S4.SS1.p2.20.m20.3.4.2\" xref=\"S4.SS1.p2.20.m20.3.4.2.cmml\">T</mi><mo id=\"S4.SS1.p2.20.m20.3.4.1\" xref=\"S4.SS1.p2.20.m20.3.4.1.cmml\">=</mo><mrow id=\"S4.SS1.p2.20.m20.3.4.3.2\" xref=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\"><mn id=\"S4.SS1.p2.20.m20.1.1\" xref=\"S4.SS1.p2.20.m20.1.1.cmml\">50</mn><mo id=\"S4.SS1.p2.20.m20.3.4.3.2.1\" xref=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\">,</mo><mn id=\"S4.SS1.p2.20.m20.2.2\" xref=\"S4.SS1.p2.20.m20.2.2.cmml\">100</mn><mo id=\"S4.SS1.p2.20.m20.3.4.3.2.2\" xref=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\">,</mo><mn id=\"S4.SS1.p2.20.m20.3.3\" xref=\"S4.SS1.p2.20.m20.3.3.cmml\">200</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p2.20.m20.3b\"><apply id=\"S4.SS1.p2.20.m20.3.4.cmml\" xref=\"S4.SS1.p2.20.m20.3.4\"><eq id=\"S4.SS1.p2.20.m20.3.4.1.cmml\" xref=\"S4.SS1.p2.20.m20.3.4.1\"></eq><ci id=\"S4.SS1.p2.20.m20.3.4.2.cmml\" xref=\"S4.SS1.p2.20.m20.3.4.2\">ğ‘‡</ci><list id=\"S4.SS1.p2.20.m20.3.4.3.1.cmml\" xref=\"S4.SS1.p2.20.m20.3.4.3.2\"><cn type=\"integer\" id=\"S4.SS1.p2.20.m20.1.1.cmml\" xref=\"S4.SS1.p2.20.m20.1.1\">50</cn><cn type=\"integer\" id=\"S4.SS1.p2.20.m20.2.2.cmml\" xref=\"S4.SS1.p2.20.m20.2.2\">100</cn><cn type=\"integer\" id=\"S4.SS1.p2.20.m20.3.3.cmml\" xref=\"S4.SS1.p2.20.m20.3.3\">200</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p2.20.m20.3c\">T=50,100,200</annotation></semantics></math> in <a href=\"#A3\" title=\"Appendix C Additional Visualizations â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>Â <span class=\"ltx_text ltx_ref_tag\">C</span></a> and a more detailed description for this phenomenon.</p>\n</div>\n<figure id=\"S4.F10\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S4.F9\" class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" style=\"width:138.8pt;\"><img src=\"./assets/x20.png\" id=\"S4.F10.1.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_square\" width=\"88\" height=\"91\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F9.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>: </span><span id=\"S4.F9.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Top three PCA components of last layer weights in the first three epochs.</span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S4.F10.3\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:277.5pt;\">\n<div id=\"S4.F10.3.3\" class=\"ltx_block\">\n<figure id=\"S4.F10.sf1\" class=\"ltx_figure\"><img src=\"./assets/x21.png\" id=\"S4.F10.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"129\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F10.sf1.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><math id=\"S4.F10.sf1.2.m1.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{w}\" display=\"inline\"><semantics id=\"S4.F10.sf1.2.m1.1b\"><mrow id=\"S4.F10.sf1.2.m1.1.2\" xref=\"S4.F10.sf1.2.m1.1.2.cmml\"><mrow id=\"S4.F10.sf1.2.m1.1.2.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\"><msub id=\"S4.F10.sf1.2.m1.1.2.2.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.cmml\"><mi mathsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.2.2.2.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.2.cmml\">f</mi><mi mathsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.2.2.2.3\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.F10.sf1.2.m1.1.2.2.1\" xref=\"S4.F10.sf1.2.m1.1.2.2.1.cmml\">â€‹</mo><mrow id=\"S4.F10.sf1.2.m1.1.2.2.3.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.2.2.3.2.1\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\">(</mo><mi mathsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.1\" xref=\"S4.F10.sf1.2.m1.1.1.cmml\">ğ’˜</mi><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.2.2.3.2.2\" xref=\"S4.F10.sf1.2.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo mathsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.2.1\" xref=\"S4.F10.sf1.2.m1.1.2.1.cmml\">=</mo><mi mathsize=\"90%\" id=\"S4.F10.sf1.2.m1.1.2.3\" xref=\"S4.F10.sf1.2.m1.1.2.3.cmml\">ğ’˜</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F10.sf1.2.m1.1c\"><apply id=\"S4.F10.sf1.2.m1.1.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2\"><eq id=\"S4.F10.sf1.2.m1.1.2.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.1\"></eq><apply id=\"S4.F10.sf1.2.m1.1.2.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2\"><times id=\"S4.F10.sf1.2.m1.1.2.2.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.1\"></times><apply id=\"S4.F10.sf1.2.m1.1.2.2.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.F10.sf1.2.m1.1.2.2.2.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2\">subscript</csymbol><ci id=\"S4.F10.sf1.2.m1.1.2.2.2.2.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.2\">ğ‘“</ci><ci id=\"S4.F10.sf1.2.m1.1.2.2.2.3.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"S4.F10.sf1.2.m1.1.1.cmml\" xref=\"S4.F10.sf1.2.m1.1.1\">ğ’˜</ci></apply><ci id=\"S4.F10.sf1.2.m1.1.2.3.cmml\" xref=\"S4.F10.sf1.2.m1.1.2.3\">ğ’˜</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F10.sf1.2.m1.1d\">f_{i}(\\bm{w})=\\bm{w}</annotation></semantics></math><span id=\"S4.F10.sf1.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> </span></figcaption>\n</figure>\n<figure id=\"S4.F10.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x22.png\" id=\"S4.F10.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"129\" height=\"100\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F10.sf2.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><math id=\"S4.F10.sf2.2.m1.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" display=\"inline\"><semantics id=\"S4.F10.sf2.2.m1.1b\"><mrow id=\"S4.F10.sf2.2.m1.1.2\" xref=\"S4.F10.sf2.2.m1.1.2.cmml\"><mrow id=\"S4.F10.sf2.2.m1.1.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\"><msub id=\"S4.F10.sf2.2.m1.1.2.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.cmml\"><mi mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.2.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.2.cmml\">f</mi><mi mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.2.2.3\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.F10.sf2.2.m1.1.2.2.1\" xref=\"S4.F10.sf2.2.m1.1.2.2.1.cmml\">â€‹</mo><mrow id=\"S4.F10.sf2.2.m1.1.2.2.3.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.2.3.2.1\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\">(</mo><mi mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.1\" xref=\"S4.F10.sf2.2.m1.1.1.cmml\">ğ’˜</mi><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.2.3.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.1\" xref=\"S4.F10.sf2.2.m1.1.2.1.cmml\">=</mo><mrow id=\"S4.F10.sf2.2.m1.1.2.3\" xref=\"S4.F10.sf2.2.m1.1.2.3.cmml\"><msub id=\"S4.F10.sf2.2.m1.1.2.3.2\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.cmml\"><mi mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.3.2.2\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.2.cmml\">ğ’š</mi><mi mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.3.2.3\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.3.cmml\">i</mi></msub><mo mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.3.1\" xref=\"S4.F10.sf2.2.m1.1.2.3.1.cmml\">âˆ’</mo><mi mathsize=\"90%\" id=\"S4.F10.sf2.2.m1.1.2.3.3\" xref=\"S4.F10.sf2.2.m1.1.2.3.3.cmml\">ğ’˜</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F10.sf2.2.m1.1c\"><apply id=\"S4.F10.sf2.2.m1.1.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2\"><eq id=\"S4.F10.sf2.2.m1.1.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.1\"></eq><apply id=\"S4.F10.sf2.2.m1.1.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2\"><times id=\"S4.F10.sf2.2.m1.1.2.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.1\"></times><apply id=\"S4.F10.sf2.2.m1.1.2.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.F10.sf2.2.m1.1.2.2.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2\">subscript</csymbol><ci id=\"S4.F10.sf2.2.m1.1.2.2.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.2\">ğ‘“</ci><ci id=\"S4.F10.sf2.2.m1.1.2.2.2.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"S4.F10.sf2.2.m1.1.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.1\">ğ’˜</ci></apply><apply id=\"S4.F10.sf2.2.m1.1.2.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3\"><minus id=\"S4.F10.sf2.2.m1.1.2.3.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.1\"></minus><apply id=\"S4.F10.sf2.2.m1.1.2.3.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.F10.sf2.2.m1.1.2.3.2.1.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2\">subscript</csymbol><ci id=\"S4.F10.sf2.2.m1.1.2.3.2.2.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.2\">ğ’š</ci><ci id=\"S4.F10.sf2.2.m1.1.2.3.2.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.2.3\">ğ‘–</ci></apply><ci id=\"S4.F10.sf2.2.m1.1.2.3.3.cmml\" xref=\"S4.F10.sf2.2.m1.1.2.3.3\">ğ’˜</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F10.sf2.2.m1.1d\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation></semantics></math><span id=\"S4.F10.sf2.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F10.3.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>: </span><span id=\"S4.F10.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Loss curve for task 1 in computational toy model, with different <math id=\"S4.F10.3.2.1.m1.1\" class=\"ltx_Math\" alttext=\"f_{i}\" display=\"inline\"><semantics id=\"S4.F10.3.2.1.m1.1b\"><msub id=\"S4.F10.3.2.1.m1.1.1\" xref=\"S4.F10.3.2.1.m1.1.1.cmml\"><mi id=\"S4.F10.3.2.1.m1.1.1.2\" xref=\"S4.F10.3.2.1.m1.1.1.2.cmml\">f</mi><mi id=\"S4.F10.3.2.1.m1.1.1.3\" xref=\"S4.F10.3.2.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.F10.3.2.1.m1.1c\"><apply id=\"S4.F10.3.2.1.m1.1.1.cmml\" xref=\"S4.F10.3.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.F10.3.2.1.m1.1.1.1.cmml\" xref=\"S4.F10.3.2.1.m1.1.1\">subscript</csymbol><ci id=\"S4.F10.3.2.1.m1.1.1.2.cmml\" xref=\"S4.F10.3.2.1.m1.1.1.2\">ğ‘“</ci><ci id=\"S4.F10.3.2.1.m1.1.1.3.cmml\" xref=\"S4.F10.3.2.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F10.3.2.1.m1.1d\">f_{i}</annotation></semantics></math>. More experiment details in Appendix <a href=\"#A1.SS3\" title=\"A.3 Computational Toy Model â€£ Appendix A Additional Experiment Setups â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.</span></figcaption>\n</figure>\n</div>\n</div>\n</figure>\n<figure id=\"S4.F11\" class=\"ltx_figure ltx_figure_panel ltx_align_center\"><img src=\"./assets/x23.png\" id=\"S4.SS1.1.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" width=\"415\" height=\"81\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"S4.F11.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>: </span><span id=\"S4.F11.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Visualization of PCA embeddings of the projected data points (<math id=\"S4.F11.3.1.m1.1\" class=\"ltx_Math\" alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" display=\"inline\"><semantics id=\"S4.F11.3.1.m1.1b\"><mrow id=\"S4.F11.3.1.m1.1.1\" xref=\"S4.F11.3.1.m1.1.1.cmml\"><msubsup id=\"S4.F11.3.1.m1.1.1.3\" xref=\"S4.F11.3.1.m1.1.1.3.cmml\"><mi id=\"S4.F11.3.1.m1.1.1.3.2.2\" xref=\"S4.F11.3.1.m1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.F11.3.1.m1.1.1.3.2.3\" xref=\"S4.F11.3.1.m1.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.F11.3.1.m1.1.1.3.3\" xref=\"S4.F11.3.1.m1.1.1.3.3.cmml\"><mo id=\"S4.F11.3.1.m1.1.1.3.3b\" xref=\"S4.F11.3.1.m1.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.F11.3.1.m1.1.1.3.3.2\" xref=\"S4.F11.3.1.m1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.F11.3.1.m1.1.1.2\" xref=\"S4.F11.3.1.m1.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.F11.3.1.m1.1.1.1.1\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.F11.3.1.m1.1.1.1.1.2\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.F11.3.1.m1.1.1.1.1.1\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\"><mi id=\"S4.F11.3.1.m1.1.1.1.1.1.2\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.F11.3.1.m1.1.1.1.1.1.1\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.F11.3.1.m1.1.1.1.1.1.3\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.cmml\"><mi id=\"S4.F11.3.1.m1.1.1.1.1.1.3.2\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.F11.3.1.m1.1.1.1.1.1.3.3\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S4.F11.3.1.m1.1.1.1.1.3\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F11.3.1.m1.1c\"><apply id=\"S4.F11.3.1.m1.1.1.cmml\" xref=\"S4.F11.3.1.m1.1.1\"><times id=\"S4.F11.3.1.m1.1.1.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.2\"></times><apply id=\"S4.F11.3.1.m1.1.1.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.F11.3.1.m1.1.1.3.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\">superscript</csymbol><apply id=\"S4.F11.3.1.m1.1.1.3.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.F11.3.1.m1.1.1.3.2.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.F11.3.1.m1.1.1.3.2.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.F11.3.1.m1.1.1.3.2.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.2.3\">ğ‘–</ci></apply><apply id=\"S4.F11.3.1.m1.1.1.3.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.3\"><minus id=\"S4.F11.3.1.m1.1.1.3.3.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.F11.3.1.m1.1.1.3.3.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.F11.3.1.m1.1.1.1.1.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1\"><times id=\"S4.F11.3.1.m1.1.1.1.1.1.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.1\"></times><ci id=\"S4.F11.3.1.m1.1.1.1.1.1.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.F11.3.1.m1.1.1.1.1.1.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.F11.3.1.m1.1.1.1.1.1.3.1.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.F11.3.1.m1.1.1.1.1.1.3.2.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.2\">ğ’™</ci><ci id=\"S4.F11.3.1.m1.1.1.1.1.1.3.3.cmml\" xref=\"S4.F11.3.1.m1.1.1.1.1.1.3.3\">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F11.3.1.m1.1d\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation></semantics></math>, where <math id=\"S4.F11.4.2.m2.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" display=\"inline\"><semantics id=\"S4.F11.4.2.m2.1b\"><mrow id=\"S4.F11.4.2.m2.1.2\" xref=\"S4.F11.4.2.m2.1.2.cmml\"><mrow id=\"S4.F11.4.2.m2.1.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\"><msub id=\"S4.F11.4.2.m2.1.2.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.2.cmml\"><mi id=\"S4.F11.4.2.m2.1.2.2.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.F11.4.2.m2.1.2.2.2.3\" xref=\"S4.F11.4.2.m2.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.F11.4.2.m2.1.2.2.1\" xref=\"S4.F11.4.2.m2.1.2.2.1.cmml\">â€‹</mo><mrow id=\"S4.F11.4.2.m2.1.2.2.3.2\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\"><mo stretchy=\"false\" id=\"S4.F11.4.2.m2.1.2.2.3.2.1\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\">(</mo><mi id=\"S4.F11.4.2.m2.1.1\" xref=\"S4.F11.4.2.m2.1.1.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.F11.4.2.m2.1.2.2.3.2.2\" xref=\"S4.F11.4.2.m2.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.F11.4.2.m2.1.2.1\" xref=\"S4.F11.4.2.m2.1.2.1.cmml\">=</mo><mrow id=\"S4.F11.4.2.m2.1.2.3\" xref=\"S4.F11.4.2.m2.1.2.3.cmml\"><msub id=\"S4.F11.4.2.m2.1.2.3.2\" xref=\"S4.F11.4.2.m2.1.2.3.2.cmml\"><mi id=\"S4.F11.4.2.m2.1.2.3.2.2\" xref=\"S4.F11.4.2.m2.1.2.3.2.2.cmml\">ğ’š</mi><mi id=\"S4.F11.4.2.m2.1.2.3.2.3\" xref=\"S4.F11.4.2.m2.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"S4.F11.4.2.m2.1.2.3.1\" xref=\"S4.F11.4.2.m2.1.2.3.1.cmml\">âˆ’</mo><mi id=\"S4.F11.4.2.m2.1.2.3.3\" xref=\"S4.F11.4.2.m2.1.2.3.3.cmml\">ğ’˜</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.F11.4.2.m2.1c\"><apply id=\"S4.F11.4.2.m2.1.2.cmml\" xref=\"S4.F11.4.2.m2.1.2\"><eq id=\"S4.F11.4.2.m2.1.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.1\"></eq><apply id=\"S4.F11.4.2.m2.1.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.2\"><times id=\"S4.F11.4.2.m2.1.2.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.1\"></times><apply id=\"S4.F11.4.2.m2.1.2.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.F11.4.2.m2.1.2.2.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2\">subscript</csymbol><ci id=\"S4.F11.4.2.m2.1.2.2.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2.2\">ğ‘“</ci><ci id=\"S4.F11.4.2.m2.1.2.2.2.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"S4.F11.4.2.m2.1.1.cmml\" xref=\"S4.F11.4.2.m2.1.1\">ğ’˜</ci></apply><apply id=\"S4.F11.4.2.m2.1.2.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.3\"><minus id=\"S4.F11.4.2.m2.1.2.3.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.1\"></minus><apply id=\"S4.F11.4.2.m2.1.2.3.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.F11.4.2.m2.1.2.3.2.1.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2\">subscript</csymbol><ci id=\"S4.F11.4.2.m2.1.2.3.2.2.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2.2\">ğ’š</ci><ci id=\"S4.F11.4.2.m2.1.2.3.2.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.2.3\">ğ‘–</ci></apply><ci id=\"S4.F11.4.2.m2.1.2.3.3.cmml\" xref=\"S4.F11.4.2.m2.1.2.3.3\">ğ’˜</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F11.4.2.m2.1d\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation></semantics></math>) in the toy model throughout training. Epoch 0 refers to the model before any training.</span></figcaption>\n</figure>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Temporal Structure of Model Weights</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.p1.1\" class=\"ltx_p\">We explore the similarities of model weights along the optimization trajectory of cyclic training. We flatten and concatenate the weight vectors of the model checkpoint after we fine-tune on each document. However, the cosine similarities between these raw model weight vectors are all very close to 1 without obvious structure. This is mainly because of the proximity of model weights along the same optimization trajectory. Another reason is that the size of the weight vector is huge and dominated by a few entries, leading to numerical instability. To resolve these issues, we instead explore the structure of â€œweight residuals.â€ We compute the weight residuals by subtracting the average of weights in a window of length <math id=\"S4.SS2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S4.SS2.p1.1.m1.1a\"><mi id=\"S4.SS2.p1.1.m1.1.1\" xref=\"S4.SS2.p1.1.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.1.m1.1b\"><ci id=\"S4.SS2.p1.1.m1.1.1.cmml\" xref=\"S4.SS2.p1.1.m1.1.1\">ğ‘‡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.1.m1.1c\">T</annotation></semantics></math> centered at the current document from the current weight, i.e.</p>\n<table id=\"A3.EGx2\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S4.E2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S4.E2.m1.4\" class=\"ltx_Math\" alttext=\"\\displaystyle w_{\\text{res}}(t)=w(t)-\\frac{1}{T}\\sum_{n=t-T/2}^{t+T/2}w(n).\" display=\"inline\"><semantics id=\"S4.E2.m1.4a\"><mrow id=\"S4.E2.m1.4.4.1\" xref=\"S4.E2.m1.4.4.1.1.cmml\"><mrow id=\"S4.E2.m1.4.4.1.1\" xref=\"S4.E2.m1.4.4.1.1.cmml\"><mrow id=\"S4.E2.m1.4.4.1.1.2\" xref=\"S4.E2.m1.4.4.1.1.2.cmml\"><msub id=\"S4.E2.m1.4.4.1.1.2.2\" xref=\"S4.E2.m1.4.4.1.1.2.2.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.2.2.2\" xref=\"S4.E2.m1.4.4.1.1.2.2.2.cmml\">w</mi><mtext id=\"S4.E2.m1.4.4.1.1.2.2.3\" xref=\"S4.E2.m1.4.4.1.1.2.2.3a.cmml\">res</mtext></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E2.m1.4.4.1.1.2.1\" xref=\"S4.E2.m1.4.4.1.1.2.1.cmml\">â€‹</mo><mrow id=\"S4.E2.m1.4.4.1.1.2.3.2\" xref=\"S4.E2.m1.4.4.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.E2.m1.4.4.1.1.2.3.2.1\" xref=\"S4.E2.m1.4.4.1.1.2.cmml\">(</mo><mi id=\"S4.E2.m1.1.1\" xref=\"S4.E2.m1.1.1.cmml\">t</mi><mo stretchy=\"false\" id=\"S4.E2.m1.4.4.1.1.2.3.2.2\" xref=\"S4.E2.m1.4.4.1.1.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.E2.m1.4.4.1.1.1\" xref=\"S4.E2.m1.4.4.1.1.1.cmml\">=</mo><mrow id=\"S4.E2.m1.4.4.1.1.3\" xref=\"S4.E2.m1.4.4.1.1.3.cmml\"><mrow id=\"S4.E2.m1.4.4.1.1.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.2.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.2.2\" xref=\"S4.E2.m1.4.4.1.1.3.2.2.cmml\">w</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E2.m1.4.4.1.1.3.2.1\" xref=\"S4.E2.m1.4.4.1.1.3.2.1.cmml\">â€‹</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.2.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.2.cmml\"><mo stretchy=\"false\" id=\"S4.E2.m1.4.4.1.1.3.2.3.2.1\" xref=\"S4.E2.m1.4.4.1.1.3.2.cmml\">(</mo><mi id=\"S4.E2.m1.2.2\" xref=\"S4.E2.m1.2.2.cmml\">t</mi><mo stretchy=\"false\" id=\"S4.E2.m1.4.4.1.1.3.2.3.2.2\" xref=\"S4.E2.m1.4.4.1.1.3.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.E2.m1.4.4.1.1.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.1.cmml\">âˆ’</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.cmml\"><mstyle displaystyle=\"true\" id=\"S4.E2.m1.4.4.1.1.3.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.2.cmml\"><mfrac id=\"S4.E2.m1.4.4.1.1.3.3.2a\" xref=\"S4.E2.m1.4.4.1.1.3.3.2.cmml\"><mn id=\"S4.E2.m1.4.4.1.1.3.3.2.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.2.2.cmml\">1</mn><mi id=\"S4.E2.m1.4.4.1.1.3.3.2.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.2.3.cmml\">T</mi></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E2.m1.4.4.1.1.3.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.1.cmml\">â€‹</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.cmml\"><mstyle displaystyle=\"true\" id=\"S4.E2.m1.4.4.1.1.3.3.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.cmml\"><munderover id=\"S4.E2.m1.4.4.1.1.3.3.3.1a\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.cmml\"><mo movablelimits=\"false\" id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.2.cmml\">âˆ‘</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.2.cmml\">n</mi><mo id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.1.cmml\">=</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.2.cmml\">t</mi><mo id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.1.cmml\">âˆ’</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.2.cmml\">T</mi><mo id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.1.cmml\">/</mo><mn id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.3.cmml\">2</mn></mrow></mrow></mrow><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.2.cmml\">t</mi><mo id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.1.cmml\">+</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.2.cmml\">T</mi><mo id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.1.cmml\">/</mo><mn id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.3\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.3.cmml\">2</mn></mrow></mrow></munderover></mstyle><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.cmml\"><mi id=\"S4.E2.m1.4.4.1.1.3.3.3.2.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.2.cmml\">w</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E2.m1.4.4.1.1.3.3.3.2.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.1.cmml\">â€‹</mo><mrow id=\"S4.E2.m1.4.4.1.1.3.3.3.2.3.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.cmml\"><mo stretchy=\"false\" id=\"S4.E2.m1.4.4.1.1.3.3.3.2.3.2.1\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.cmml\">(</mo><mi id=\"S4.E2.m1.3.3\" xref=\"S4.E2.m1.3.3.cmml\">n</mi><mo stretchy=\"false\" id=\"S4.E2.m1.4.4.1.1.3.3.3.2.3.2.2\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.cmml\">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace=\"0em\" id=\"S4.E2.m1.4.4.1.2\" xref=\"S4.E2.m1.4.4.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E2.m1.4b\"><apply id=\"S4.E2.m1.4.4.1.1.cmml\" xref=\"S4.E2.m1.4.4.1\"><eq id=\"S4.E2.m1.4.4.1.1.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.1\"></eq><apply id=\"S4.E2.m1.4.4.1.1.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.2\"><times id=\"S4.E2.m1.4.4.1.1.2.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.2.1\"></times><apply id=\"S4.E2.m1.4.4.1.1.2.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.E2.m1.4.4.1.1.2.2.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.2.2\">subscript</csymbol><ci id=\"S4.E2.m1.4.4.1.1.2.2.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.2.2.2\">ğ‘¤</ci><ci id=\"S4.E2.m1.4.4.1.1.2.2.3a.cmml\" xref=\"S4.E2.m1.4.4.1.1.2.2.3\"><mtext mathsize=\"70%\" id=\"S4.E2.m1.4.4.1.1.2.2.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.2.2.3\">res</mtext></ci></apply><ci id=\"S4.E2.m1.1.1.cmml\" xref=\"S4.E2.m1.1.1\">ğ‘¡</ci></apply><apply id=\"S4.E2.m1.4.4.1.1.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3\"><minus id=\"S4.E2.m1.4.4.1.1.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.1\"></minus><apply id=\"S4.E2.m1.4.4.1.1.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.2\"><times id=\"S4.E2.m1.4.4.1.1.3.2.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.2.1\"></times><ci id=\"S4.E2.m1.4.4.1.1.3.2.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.2.2\">ğ‘¤</ci><ci id=\"S4.E2.m1.2.2.cmml\" xref=\"S4.E2.m1.2.2\">ğ‘¡</ci></apply><apply id=\"S4.E2.m1.4.4.1.1.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3\"><times id=\"S4.E2.m1.4.4.1.1.3.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.1\"></times><apply id=\"S4.E2.m1.4.4.1.1.3.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.2\"><divide id=\"S4.E2.m1.4.4.1.1.3.3.2.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.2\"></divide><cn type=\"integer\" id=\"S4.E2.m1.4.4.1.1.3.3.2.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.2.2\">1</cn><ci id=\"S4.E2.m1.4.4.1.1.3.3.2.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.2.3\">ğ‘‡</ci></apply><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3\"><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1\"><csymbol cd=\"ambiguous\" id=\"S4.E2.m1.4.4.1.1.3.3.3.1.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1\">superscript</csymbol><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1\"><csymbol cd=\"ambiguous\" id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1\">subscript</csymbol><sum id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.2\"></sum><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3\"><eq id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.1\"></eq><ci id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.2\">ğ‘›</ci><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3\"><minus id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.1\"></minus><ci id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.2\">ğ‘¡</ci><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3\"><divide id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.1\"></divide><ci id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.2\">ğ‘‡</ci><cn type=\"integer\" id=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.2.3.3.3.3\">2</cn></apply></apply></apply></apply><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3\"><plus id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.1\"></plus><ci id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.2\">ğ‘¡</ci><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3\"><divide id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.1\"></divide><ci id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.2\">ğ‘‡</ci><cn type=\"integer\" id=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.3.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.1.3.3.3\">2</cn></apply></apply></apply><apply id=\"S4.E2.m1.4.4.1.1.3.3.3.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2\"><times id=\"S4.E2.m1.4.4.1.1.3.3.3.2.1.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.1\"></times><ci id=\"S4.E2.m1.4.4.1.1.3.3.3.2.2.cmml\" xref=\"S4.E2.m1.4.4.1.1.3.3.3.2.2\">ğ‘¤</ci><ci id=\"S4.E2.m1.3.3.cmml\" xref=\"S4.E2.m1.3.3\">ğ‘›</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E2.m1.4c\">\\displaystyle w_{\\text{res}}(t)=w(t)-\\frac{1}{T}\\sum_{n=t-T/2}^{t+T/2}w(n).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S4.SS2.p1.2\" class=\"ltx_p\">This removes the shared components along the optimization trajectory and allows us to focus on the model weight updates for each document. Figure <a href=\"#S4.F8.sf3\" title=\"Figure 8(c) â€£ Figure 8 â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8(c)</span></a> visualizes a heat map of the cosine similarity between each pair of weight residuals from the second epoch to the fourth epoch. The visualization shows a cyclic structure in the weight residuals, as equidistant bright stripes that align with the training epochs. Furthermore, each stripe spans several documents, suggesting the similarity of weight residuals for proximal documents.</p>\n</div>\n<div id=\"S4.SS2.p2\" class=\"ltx_para\">\n<p id=\"S4.SS2.p2.1\" class=\"ltx_p\">In addition to computing the cosine similarities between model weights, we explore using Principle Component Analysis (PCA) to reduce the dimensionality of the weights. We compute the top three PCs of the flattened last-layer weight vector (the output word embedding layer) for the Pythia-1B model, and plot its trajectory in Figure <a href=\"#S4.F9\" title=\"Figure 9 â€£ Figure 10 â€£ 4.1 Temporal Structure of Gradients â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>. The plot exhibits a clear helical structure that gradually converges. We believe this is highly relevant to anticipatory recovery: right before revisiting a task, the projected model weights in the helix move closer to the point corresponding to the previous appearance of that task, leading to anticipatory recovery on the loss of that task. As we go on with cyclic training, the model also exhibits less forgetting and gradually converges to a solution that achieves low loss on all tasks.</p>\n</div>\n<div id=\"S4.SS2.p3\" class=\"ltx_para\">\n<p id=\"S4.SS2.p3.1\" class=\"ltx_p\">It is important to note that the helical structure of the weight trajectory is not an obviously necessary consequence of the cyclical training. Cyclical training could be expected to yield a repeating pattern, but the facts that the tasks come to be organized in a circle that respects their ordering and that the trajectory goes through one full revolution per epoch (rather than some other arc length) are nontrivial and seem to be essential for anticipatory recovery.</p>\n</div>\n</section>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Temporal Structure of Activations</h3>\n\n<div id=\"S4.SS3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.p1.4\" class=\"ltx_p\">In addition to gradients and weights, we visualize the trajectory of activations on a single document during the course of cyclic training. We do cyclic training for three epochs and save model checkpoints after fine-tuning on each document. We then compute the model activations before the output word embedding layer for document <math id=\"S4.SS3.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"S4.SS3.p1.1.m1.1a\"><msub id=\"S4.SS3.p1.1.m1.1.1\" xref=\"S4.SS3.p1.1.m1.1.1.cmml\"><mi id=\"S4.SS3.p1.1.m1.1.1.2\" xref=\"S4.SS3.p1.1.m1.1.1.2.cmml\">ğ’™</mi><mn id=\"S4.SS3.p1.1.m1.1.1.3\" xref=\"S4.SS3.p1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.1.m1.1b\"><apply id=\"S4.SS3.p1.1.m1.1.1.cmml\" xref=\"S4.SS3.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.1.m1.1.1.1.cmml\" xref=\"S4.SS3.p1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.1.m1.1.1.2.cmml\" xref=\"S4.SS3.p1.1.m1.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S4.SS3.p1.1.m1.1.1.3.cmml\" xref=\"S4.SS3.p1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.1.m1.1c\">\\bm{x}_{1}</annotation></semantics></math> on each model checkpoint, and plot the cosine similarities between the flattened activation vectors in Figure <a href=\"#S4.F8.sf4\" title=\"Figure 8(d) â€£ Figure 8 â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a>. From the plot we can clearly observe the blocked pattern wherein the similarity between the activations become progressively higher across each epoch of cyclic training. This pattern suggests that every time we train on document <math id=\"S4.SS3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.SS3.p1.2.m2.1a\"><msub id=\"S4.SS3.p1.2.m2.1.1\" xref=\"S4.SS3.p1.2.m2.1.1.cmml\"><mi id=\"S4.SS3.p1.2.m2.1.1.2\" xref=\"S4.SS3.p1.2.m2.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS3.p1.2.m2.1.1.3\" xref=\"S4.SS3.p1.2.m2.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.2.m2.1b\"><apply id=\"S4.SS3.p1.2.m2.1.1.cmml\" xref=\"S4.SS3.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.2.m2.1.1.1.cmml\" xref=\"S4.SS3.p1.2.m2.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.2.m2.1.1.2.cmml\" xref=\"S4.SS3.p1.2.m2.1.1.2\">ğ’™</ci><ci id=\"S4.SS3.p1.2.m2.1.1.3.cmml\" xref=\"S4.SS3.p1.2.m2.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.2.m2.1c\">\\bm{x}_{i}</annotation></semantics></math>, the internal representation of <math id=\"S4.SS3.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.SS3.p1.3.m3.1a\"><msub id=\"S4.SS3.p1.3.m3.1.1\" xref=\"S4.SS3.p1.3.m3.1.1.cmml\"><mi id=\"S4.SS3.p1.3.m3.1.1.2\" xref=\"S4.SS3.p1.3.m3.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS3.p1.3.m3.1.1.3\" xref=\"S4.SS3.p1.3.m3.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.3.m3.1b\"><apply id=\"S4.SS3.p1.3.m3.1.1.cmml\" xref=\"S4.SS3.p1.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS3.p1.3.m3.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS3.p1.3.m3.1.1.2\">ğ’™</ci><ci id=\"S4.SS3.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS3.p1.3.m3.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.3.m3.1c\">\\bm{x}_{i}</annotation></semantics></math> in the model is more resistant to gradient updates on other documents <math id=\"S4.SS3.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"S4.SS3.p1.4.m4.1a\"><msub id=\"S4.SS3.p1.4.m4.1.1\" xref=\"S4.SS3.p1.4.m4.1.1.cmml\"><mi id=\"S4.SS3.p1.4.m4.1.1.2\" xref=\"S4.SS3.p1.4.m4.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS3.p1.4.m4.1.1.3\" xref=\"S4.SS3.p1.4.m4.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS3.p1.4.m4.1b\"><apply id=\"S4.SS3.p1.4.m4.1.1.cmml\" xref=\"S4.SS3.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS3.p1.4.m4.1.1.1.cmml\" xref=\"S4.SS3.p1.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS3.p1.4.m4.1.1.2.cmml\" xref=\"S4.SS3.p1.4.m4.1.1.2\">ğ’™</ci><ci id=\"S4.SS3.p1.4.m4.1.1.3.cmml\" xref=\"S4.SS3.p1.4.m4.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS3.p1.4.m4.1c\">\\bm{x}_{j}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"S4.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Computational Toy Model</h3>\n\n<div id=\"S4.SS4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS4.p1.10\" class=\"ltx_p\">To further understand the essential mechanism that yields anticipatory recovery, we design a minimalist â€œtoyâ€ simulation experiment.\nIn this toy simulation, each task (formerly, document) <math id=\"S4.SS4.p1.1.m1.3\" class=\"ltx_Math\" alttext=\"i\\in\\{1,\\cdots,T\\}\" display=\"inline\"><semantics id=\"S4.SS4.p1.1.m1.3a\"><mrow id=\"S4.SS4.p1.1.m1.3.4\" xref=\"S4.SS4.p1.1.m1.3.4.cmml\"><mi id=\"S4.SS4.p1.1.m1.3.4.2\" xref=\"S4.SS4.p1.1.m1.3.4.2.cmml\">i</mi><mo id=\"S4.SS4.p1.1.m1.3.4.1\" xref=\"S4.SS4.p1.1.m1.3.4.1.cmml\">âˆˆ</mo><mrow id=\"S4.SS4.p1.1.m1.3.4.3.2\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p1.1.m1.3.4.3.2.1\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">{</mo><mn id=\"S4.SS4.p1.1.m1.1.1\" xref=\"S4.SS4.p1.1.m1.1.1.cmml\">1</mn><mo id=\"S4.SS4.p1.1.m1.3.4.3.2.2\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S4.SS4.p1.1.m1.2.2\" xref=\"S4.SS4.p1.1.m1.2.2.cmml\">â‹¯</mi><mo id=\"S4.SS4.p1.1.m1.3.4.3.2.3\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">,</mo><mi id=\"S4.SS4.p1.1.m1.3.3\" xref=\"S4.SS4.p1.1.m1.3.3.cmml\">T</mi><mo stretchy=\"false\" id=\"S4.SS4.p1.1.m1.3.4.3.2.4\" xref=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.1.m1.3b\"><apply id=\"S4.SS4.p1.1.m1.3.4.cmml\" xref=\"S4.SS4.p1.1.m1.3.4\"><in id=\"S4.SS4.p1.1.m1.3.4.1.cmml\" xref=\"S4.SS4.p1.1.m1.3.4.1\"></in><ci id=\"S4.SS4.p1.1.m1.3.4.2.cmml\" xref=\"S4.SS4.p1.1.m1.3.4.2\">ğ‘–</ci><set id=\"S4.SS4.p1.1.m1.3.4.3.1.cmml\" xref=\"S4.SS4.p1.1.m1.3.4.3.2\"><cn type=\"integer\" id=\"S4.SS4.p1.1.m1.1.1.cmml\" xref=\"S4.SS4.p1.1.m1.1.1\">1</cn><ci id=\"S4.SS4.p1.1.m1.2.2.cmml\" xref=\"S4.SS4.p1.1.m1.2.2\">â‹¯</ci><ci id=\"S4.SS4.p1.1.m1.3.3.cmml\" xref=\"S4.SS4.p1.1.m1.3.3\">ğ‘‡</ci></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.1.m1.3c\">i\\in\\{1,\\cdots,T\\}</annotation></semantics></math> is described by a single data point, <math id=\"S4.SS4.p1.2.m2.3\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1},\\cdots,\\bm{x}_{T}\\in\\mathbb{R}^{N}\" display=\"inline\"><semantics id=\"S4.SS4.p1.2.m2.3a\"><mrow id=\"S4.SS4.p1.2.m2.3.3\" xref=\"S4.SS4.p1.2.m2.3.3.cmml\"><mrow id=\"S4.SS4.p1.2.m2.3.3.2.2\" xref=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\"><msub id=\"S4.SS4.p1.2.m2.2.2.1.1.1\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.cmml\"><mi id=\"S4.SS4.p1.2.m2.2.2.1.1.1.2\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.2.cmml\">ğ’™</mi><mn id=\"S4.SS4.p1.2.m2.2.2.1.1.1.3\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.3.cmml\">1</mn></msub><mo id=\"S4.SS4.p1.2.m2.3.3.2.2.3\" xref=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S4.SS4.p1.2.m2.1.1\" xref=\"S4.SS4.p1.2.m2.1.1.cmml\">â‹¯</mi><mo id=\"S4.SS4.p1.2.m2.3.3.2.2.4\" xref=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\">,</mo><msub id=\"S4.SS4.p1.2.m2.3.3.2.2.2\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.cmml\"><mi id=\"S4.SS4.p1.2.m2.3.3.2.2.2.2\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p1.2.m2.3.3.2.2.2.3\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.3.cmml\">T</mi></msub></mrow><mo id=\"S4.SS4.p1.2.m2.3.3.3\" xref=\"S4.SS4.p1.2.m2.3.3.3.cmml\">âˆˆ</mo><msup id=\"S4.SS4.p1.2.m2.3.3.4\" xref=\"S4.SS4.p1.2.m2.3.3.4.cmml\"><mi id=\"S4.SS4.p1.2.m2.3.3.4.2\" xref=\"S4.SS4.p1.2.m2.3.3.4.2.cmml\">â„</mi><mi id=\"S4.SS4.p1.2.m2.3.3.4.3\" xref=\"S4.SS4.p1.2.m2.3.3.4.3.cmml\">N</mi></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.2.m2.3b\"><apply id=\"S4.SS4.p1.2.m2.3.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3\"><in id=\"S4.SS4.p1.2.m2.3.3.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.3\"></in><list id=\"S4.SS4.p1.2.m2.3.3.2.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2\"><apply id=\"S4.SS4.p1.2.m2.2.2.1.1.1.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.2.m2.2.2.1.1.1.1.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.2.m2.2.2.1.1.1.2.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"S4.SS4.p1.2.m2.2.2.1.1.1.3.cmml\" xref=\"S4.SS4.p1.2.m2.2.2.1.1.1.3\">1</cn></apply><ci id=\"S4.SS4.p1.2.m2.1.1.cmml\" xref=\"S4.SS4.p1.2.m2.1.1\">â‹¯</ci><apply id=\"S4.SS4.p1.2.m2.3.3.2.2.2.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.2.m2.3.3.2.2.2.1.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2\">subscript</csymbol><ci id=\"S4.SS4.p1.2.m2.3.3.2.2.2.2.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.2\">ğ’™</ci><ci id=\"S4.SS4.p1.2.m2.3.3.2.2.2.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.2.2.2.3\">ğ‘‡</ci></apply></list><apply id=\"S4.SS4.p1.2.m2.3.3.4.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.2.m2.3.3.4.1.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4\">superscript</csymbol><ci id=\"S4.SS4.p1.2.m2.3.3.4.2.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4.2\">â„</ci><ci id=\"S4.SS4.p1.2.m2.3.3.4.3.cmml\" xref=\"S4.SS4.p1.2.m2.3.3.4.3\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.2.m2.3c\">\\bm{x}_{1},\\cdots,\\bm{x}_{T}\\in\\mathbb{R}^{N}</annotation></semantics></math>. We assume a learnable linear embedding <math id=\"S4.SS4.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\\in\\mathbb{R}^{M\\times N}\" display=\"inline\"><semantics id=\"S4.SS4.p1.3.m3.1a\"><mrow id=\"S4.SS4.p1.3.m3.1.1\" xref=\"S4.SS4.p1.3.m3.1.1.cmml\"><mi id=\"S4.SS4.p1.3.m3.1.1.2\" xref=\"S4.SS4.p1.3.m3.1.1.2.cmml\">ğ‘·</mi><mo id=\"S4.SS4.p1.3.m3.1.1.1\" xref=\"S4.SS4.p1.3.m3.1.1.1.cmml\">âˆˆ</mo><msup id=\"S4.SS4.p1.3.m3.1.1.3\" xref=\"S4.SS4.p1.3.m3.1.1.3.cmml\"><mi id=\"S4.SS4.p1.3.m3.1.1.3.2\" xref=\"S4.SS4.p1.3.m3.1.1.3.2.cmml\">â„</mi><mrow id=\"S4.SS4.p1.3.m3.1.1.3.3\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.cmml\"><mi id=\"S4.SS4.p1.3.m3.1.1.3.3.2\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.2.cmml\">M</mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.SS4.p1.3.m3.1.1.3.3.1\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.1.cmml\">Ã—</mo><mi id=\"S4.SS4.p1.3.m3.1.1.3.3.3\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.3.cmml\">N</mi></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.3.m3.1b\"><apply id=\"S4.SS4.p1.3.m3.1.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1\"><in id=\"S4.SS4.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.1\"></in><ci id=\"S4.SS4.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.3.m3.1.1.3.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3\">superscript</csymbol><ci id=\"S4.SS4.p1.3.m3.1.1.3.2.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.2\">â„</ci><apply id=\"S4.SS4.p1.3.m3.1.1.3.3.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3\"><times id=\"S4.SS4.p1.3.m3.1.1.3.3.1.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.1\"></times><ci id=\"S4.SS4.p1.3.m3.1.1.3.3.2.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.2\">ğ‘€</ci><ci id=\"S4.SS4.p1.3.m3.1.1.3.3.3.cmml\" xref=\"S4.SS4.p1.3.m3.1.1.3.3.3\">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.3.m3.1c\">\\bm{P}\\in\\mathbb{R}^{M\\times N}</annotation></semantics></math> that projects each <math id=\"S4.SS4.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p1.4.m4.1a\"><msub id=\"S4.SS4.p1.4.m4.1.1\" xref=\"S4.SS4.p1.4.m4.1.1.cmml\"><mi id=\"S4.SS4.p1.4.m4.1.1.2\" xref=\"S4.SS4.p1.4.m4.1.1.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p1.4.m4.1.1.3\" xref=\"S4.SS4.p1.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.4.m4.1b\"><apply id=\"S4.SS4.p1.4.m4.1.1.cmml\" xref=\"S4.SS4.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.4.m4.1.1.1.cmml\" xref=\"S4.SS4.p1.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.4.m4.1.1.2.cmml\" xref=\"S4.SS4.p1.4.m4.1.1.2\">ğ’™</ci><ci id=\"S4.SS4.p1.4.m4.1.1.3.cmml\" xref=\"S4.SS4.p1.4.m4.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.4.m4.1c\">\\bm{x}_{i}</annotation></semantics></math> into an <math id=\"S4.SS4.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S4.SS4.p1.5.m5.1a\"><mi id=\"S4.SS4.p1.5.m5.1.1\" xref=\"S4.SS4.p1.5.m5.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.5.m5.1b\"><ci id=\"S4.SS4.p1.5.m5.1.1.cmml\" xref=\"S4.SS4.p1.5.m5.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.5.m5.1c\">M</annotation></semantics></math>-dimensional embedding space. We also assume a learnable vector <math id=\"S4.SS4.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"w\" display=\"inline\"><semantics id=\"S4.SS4.p1.6.m6.1a\"><mi id=\"S4.SS4.p1.6.m6.1.1\" xref=\"S4.SS4.p1.6.m6.1.1.cmml\">w</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.6.m6.1b\"><ci id=\"S4.SS4.p1.6.m6.1.1.cmml\" xref=\"S4.SS4.p1.6.m6.1.1\">ğ‘¤</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.6.m6.1c\">w</annotation></semantics></math> and task specific mappings <math id=\"S4.SS4.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"f_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p1.7.m7.1a\"><msub id=\"S4.SS4.p1.7.m7.1.1\" xref=\"S4.SS4.p1.7.m7.1.1.cmml\"><mi id=\"S4.SS4.p1.7.m7.1.1.2\" xref=\"S4.SS4.p1.7.m7.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p1.7.m7.1.1.3\" xref=\"S4.SS4.p1.7.m7.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.7.m7.1b\"><apply id=\"S4.SS4.p1.7.m7.1.1.cmml\" xref=\"S4.SS4.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.7.m7.1.1.1.cmml\" xref=\"S4.SS4.p1.7.m7.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.7.m7.1.1.2.cmml\" xref=\"S4.SS4.p1.7.m7.1.1.2\">ğ‘“</ci><ci id=\"S4.SS4.p1.7.m7.1.1.3.cmml\" xref=\"S4.SS4.p1.7.m7.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.7.m7.1c\">f_{i}</annotation></semantics></math>, where <math id=\"S4.SS4.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"f_{i}(w)\" display=\"inline\"><semantics id=\"S4.SS4.p1.8.m8.1a\"><mrow id=\"S4.SS4.p1.8.m8.1.2\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\"><msub id=\"S4.SS4.p1.8.m8.1.2.2\" xref=\"S4.SS4.p1.8.m8.1.2.2.cmml\"><mi id=\"S4.SS4.p1.8.m8.1.2.2.2\" xref=\"S4.SS4.p1.8.m8.1.2.2.2.cmml\">f</mi><mi id=\"S4.SS4.p1.8.m8.1.2.2.3\" xref=\"S4.SS4.p1.8.m8.1.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p1.8.m8.1.2.1\" xref=\"S4.SS4.p1.8.m8.1.2.1.cmml\">â€‹</mo><mrow id=\"S4.SS4.p1.8.m8.1.2.3.2\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p1.8.m8.1.2.3.2.1\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\">(</mo><mi id=\"S4.SS4.p1.8.m8.1.1\" xref=\"S4.SS4.p1.8.m8.1.1.cmml\">w</mi><mo stretchy=\"false\" id=\"S4.SS4.p1.8.m8.1.2.3.2.2\" xref=\"S4.SS4.p1.8.m8.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.8.m8.1b\"><apply id=\"S4.SS4.p1.8.m8.1.2.cmml\" xref=\"S4.SS4.p1.8.m8.1.2\"><times id=\"S4.SS4.p1.8.m8.1.2.1.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.1\"></times><apply id=\"S4.SS4.p1.8.m8.1.2.2.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.8.m8.1.2.2.1.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2\">subscript</csymbol><ci id=\"S4.SS4.p1.8.m8.1.2.2.2.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p1.8.m8.1.2.2.3.cmml\" xref=\"S4.SS4.p1.8.m8.1.2.2.3\">ğ‘–</ci></apply><ci id=\"S4.SS4.p1.8.m8.1.1.cmml\" xref=\"S4.SS4.p1.8.m8.1.1\">ğ‘¤</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.8.m8.1c\">f_{i}(w)</annotation></semantics></math> is the target for task <math id=\"S4.SS4.p1.9.m9.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S4.SS4.p1.9.m9.1a\"><mi id=\"S4.SS4.p1.9.m9.1.1\" xref=\"S4.SS4.p1.9.m9.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.9.m9.1b\"><ci id=\"S4.SS4.p1.9.m9.1.1.cmml\" xref=\"S4.SS4.p1.9.m9.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.9.m9.1c\">i</annotation></semantics></math> in the same embedding space. We require each <math id=\"S4.SS4.p1.10.m10.1\" class=\"ltx_Math\" alttext=\"f_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p1.10.m10.1a\"><msub id=\"S4.SS4.p1.10.m10.1.1\" xref=\"S4.SS4.p1.10.m10.1.1.cmml\"><mi id=\"S4.SS4.p1.10.m10.1.1.2\" xref=\"S4.SS4.p1.10.m10.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p1.10.m10.1.1.3\" xref=\"S4.SS4.p1.10.m10.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p1.10.m10.1b\"><apply id=\"S4.SS4.p1.10.m10.1.1.cmml\" xref=\"S4.SS4.p1.10.m10.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p1.10.m10.1.1.1.cmml\" xref=\"S4.SS4.p1.10.m10.1.1\">subscript</csymbol><ci id=\"S4.SS4.p1.10.m10.1.1.2.cmml\" xref=\"S4.SS4.p1.10.m10.1.1.2\">ğ‘“</ci><ci id=\"S4.SS4.p1.10.m10.1.1.3.cmml\" xref=\"S4.SS4.p1.10.m10.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p1.10.m10.1c\">f_{i}</annotation></semantics></math> to be invertible as a simplifying assumption.</p>\n</div>\n<div id=\"S4.SS4.p2\" class=\"ltx_para\">\n<p id=\"S4.SS4.p2.1\" class=\"ltx_p\">We define the loss for task <math id=\"S4.SS4.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S4.SS4.p2.1.m1.1a\"><mi id=\"S4.SS4.p2.1.m1.1.1\" xref=\"S4.SS4.p2.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.1.m1.1b\"><ci id=\"S4.SS4.p2.1.m1.1.1.cmml\" xref=\"S4.SS4.p2.1.m1.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.1.m1.1c\">i</annotation></semantics></math> as</p>\n<table id=\"A3.EGx3\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S4.E3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S4.E3.m1.4\" class=\"ltx_Math\" alttext=\"\\displaystyle\\ell_{i}(\\bm{P},\\bm{w})=\\frac{1}{2}\\lVert\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w})\\rVert^{2}_{2}.\" display=\"inline\"><semantics id=\"S4.E3.m1.4a\"><mrow id=\"S4.E3.m1.4.4.1\" xref=\"S4.E3.m1.4.4.1.1.cmml\"><mrow id=\"S4.E3.m1.4.4.1.1\" xref=\"S4.E3.m1.4.4.1.1.cmml\"><mrow id=\"S4.E3.m1.4.4.1.1.3\" xref=\"S4.E3.m1.4.4.1.1.3.cmml\"><msub id=\"S4.E3.m1.4.4.1.1.3.2\" xref=\"S4.E3.m1.4.4.1.1.3.2.cmml\"><mi mathvariant=\"normal\" id=\"S4.E3.m1.4.4.1.1.3.2.2\" xref=\"S4.E3.m1.4.4.1.1.3.2.2.cmml\">â„“</mi><mi id=\"S4.E3.m1.4.4.1.1.3.2.3\" xref=\"S4.E3.m1.4.4.1.1.3.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E3.m1.4.4.1.1.3.1\" xref=\"S4.E3.m1.4.4.1.1.3.1.cmml\">â€‹</mo><mrow id=\"S4.E3.m1.4.4.1.1.3.3.2\" xref=\"S4.E3.m1.4.4.1.1.3.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.E3.m1.4.4.1.1.3.3.2.1\" xref=\"S4.E3.m1.4.4.1.1.3.3.1.cmml\">(</mo><mi id=\"S4.E3.m1.1.1\" xref=\"S4.E3.m1.1.1.cmml\">ğ‘·</mi><mo id=\"S4.E3.m1.4.4.1.1.3.3.2.2\" xref=\"S4.E3.m1.4.4.1.1.3.3.1.cmml\">,</mo><mi id=\"S4.E3.m1.2.2\" xref=\"S4.E3.m1.2.2.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.E3.m1.4.4.1.1.3.3.2.3\" xref=\"S4.E3.m1.4.4.1.1.3.3.1.cmml\">)</mo></mrow></mrow><mo id=\"S4.E3.m1.4.4.1.1.2\" xref=\"S4.E3.m1.4.4.1.1.2.cmml\">=</mo><mrow id=\"S4.E3.m1.4.4.1.1.1\" xref=\"S4.E3.m1.4.4.1.1.1.cmml\"><mstyle displaystyle=\"true\" id=\"S4.E3.m1.4.4.1.1.1.3\" xref=\"S4.E3.m1.4.4.1.1.1.3.cmml\"><mfrac id=\"S4.E3.m1.4.4.1.1.1.3a\" xref=\"S4.E3.m1.4.4.1.1.1.3.cmml\"><mn id=\"S4.E3.m1.4.4.1.1.1.3.2\" xref=\"S4.E3.m1.4.4.1.1.1.3.2.cmml\">1</mn><mn id=\"S4.E3.m1.4.4.1.1.1.3.3\" xref=\"S4.E3.m1.4.4.1.1.1.3.3.cmml\">2</mn></mfrac></mstyle><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E3.m1.4.4.1.1.1.2\" xref=\"S4.E3.m1.4.4.1.1.1.2.cmml\">â€‹</mo><msubsup id=\"S4.E3.m1.4.4.1.1.1.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.cmml\"><mrow id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.2.cmml\"><mo fence=\"true\" rspace=\"0em\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.2.1.cmml\">âˆ¥</mo><mrow id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml\"><mrow id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml\"><mi id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml\">â€‹</mo><msub id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml\"><mi id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.2.cmml\">ğ’™</mi><mi id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml\">âˆ’</mo><mrow id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml\"><msub id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml\"><mi id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml\">â€‹</mo><mrow id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.3.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml\"><mo stretchy=\"false\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.3.2.1\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml\">(</mo><mi id=\"S4.E3.m1.3.3\" xref=\"S4.E3.m1.3.3.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.3.2.2\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo fence=\"true\" lspace=\"0em\" rspace=\"0em\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.2.1.cmml\">âˆ¥</mo></mrow><mn id=\"S4.E3.m1.4.4.1.1.1.1.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.3.cmml\">2</mn><mn id=\"S4.E3.m1.4.4.1.1.1.1.1.3\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.3.cmml\">2</mn></msubsup></mrow></mrow><mo lspace=\"0em\" id=\"S4.E3.m1.4.4.1.2\" xref=\"S4.E3.m1.4.4.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E3.m1.4b\"><apply id=\"S4.E3.m1.4.4.1.1.cmml\" xref=\"S4.E3.m1.4.4.1\"><eq id=\"S4.E3.m1.4.4.1.1.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.2\"></eq><apply id=\"S4.E3.m1.4.4.1.1.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.3\"><times id=\"S4.E3.m1.4.4.1.1.3.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.3.1\"></times><apply id=\"S4.E3.m1.4.4.1.1.3.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.E3.m1.4.4.1.1.3.2.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.3.2\">subscript</csymbol><ci id=\"S4.E3.m1.4.4.1.1.3.2.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.3.2.2\">â„“</ci><ci id=\"S4.E3.m1.4.4.1.1.3.2.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.3.2.3\">ğ‘–</ci></apply><interval closure=\"open\" id=\"S4.E3.m1.4.4.1.1.3.3.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.3.3.2\"><ci id=\"S4.E3.m1.1.1.cmml\" xref=\"S4.E3.m1.1.1\">ğ‘·</ci><ci id=\"S4.E3.m1.2.2.cmml\" xref=\"S4.E3.m1.2.2\">ğ’˜</ci></interval></apply><apply id=\"S4.E3.m1.4.4.1.1.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1\"><times id=\"S4.E3.m1.4.4.1.1.1.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.2\"></times><apply id=\"S4.E3.m1.4.4.1.1.1.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.3\"><divide id=\"S4.E3.m1.4.4.1.1.1.3.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.3\"></divide><cn type=\"integer\" id=\"S4.E3.m1.4.4.1.1.1.3.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.3.2\">1</cn><cn type=\"integer\" id=\"S4.E3.m1.4.4.1.1.1.3.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.3.3\">2</cn></apply><apply id=\"S4.E3.m1.4.4.1.1.1.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.E3.m1.4.4.1.1.1.1.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1\">subscript</csymbol><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.E3.m1.4.4.1.1.1.1.1.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1\">superscript</csymbol><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.1.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.2.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.2\">delimited-âˆ¥âˆ¥</csymbol><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1\"><minus id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.1\"></minus><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2\"><times id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.1\"></times><ci id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.2\">ğ‘·</ci><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3\">subscript</csymbol><ci id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.2\">ğ’™</ci><ci id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.2.3.3\">ğ‘–</ci></apply></apply><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3\"><times id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.1\"></times><apply id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2\">subscript</csymbol><ci id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.1.1.1.3.2.3\">ğ‘–</ci></apply><ci id=\"S4.E3.m1.3.3.cmml\" xref=\"S4.E3.m1.3.3\">ğ’˜</ci></apply></apply></apply><cn type=\"integer\" id=\"S4.E3.m1.4.4.1.1.1.1.1.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.1.3\">2</cn></apply><cn type=\"integer\" id=\"S4.E3.m1.4.4.1.1.1.1.3.cmml\" xref=\"S4.E3.m1.4.4.1.1.1.1.3\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E3.m1.4c\">\\displaystyle\\ell_{i}(\\bm{P},\\bm{w})=\\frac{1}{2}\\lVert\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w})\\rVert^{2}_{2}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S4.SS4.p2.4\" class=\"ltx_p\">Just as when training a deep net, we assume here that representation learning occurs slowly, and that one training step for task <math id=\"S4.SS4.p2.2.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S4.SS4.p2.2.m1.1a\"><mi id=\"S4.SS4.p2.2.m1.1.1\" xref=\"S4.SS4.p2.2.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.2.m1.1b\"><ci id=\"S4.SS4.p2.2.m1.1.1.cmml\" xref=\"S4.SS4.p2.2.m1.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.2.m1.1c\">i</annotation></semantics></math> involves a single gradient update of <math id=\"S4.SS4.p2.3.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\" display=\"inline\"><semantics id=\"S4.SS4.p2.3.m2.1a\"><mi id=\"S4.SS4.p2.3.m2.1.1\" xref=\"S4.SS4.p2.3.m2.1.1.cmml\">ğ‘·</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.3.m2.1b\"><ci id=\"S4.SS4.p2.3.m2.1.1.cmml\" xref=\"S4.SS4.p2.3.m2.1.1\">ğ‘·</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.3.m2.1c\">\\bm{P}</annotation></semantics></math> with step size <math id=\"S4.SS4.p2.4.m3.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.SS4.p2.4.m3.1a\"><mi id=\"S4.SS4.p2.4.m3.1.1\" xref=\"S4.SS4.p2.4.m3.1.1.cmml\">Î±</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.4.m3.1b\"><ci id=\"S4.SS4.p2.4.m3.1.1.cmml\" xref=\"S4.SS4.p2.4.m3.1.1\">ğ›¼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.4.m3.1c\">\\alpha</annotation></semantics></math>:</p>\n<table id=\"A3.EGx4\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S4.E4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S4.E4.m1.2\" class=\"ltx_Math\" alttext=\"\\displaystyle\\bm{P}\\leftarrow\\bm{P}-\\alpha(\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w}))\\bm{x}_{i}^{\\top}.\" display=\"inline\"><semantics id=\"S4.E4.m1.2a\"><mrow id=\"S4.E4.m1.2.2.1\" xref=\"S4.E4.m1.2.2.1.1.cmml\"><mrow id=\"S4.E4.m1.2.2.1.1\" xref=\"S4.E4.m1.2.2.1.1.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.3\" xref=\"S4.E4.m1.2.2.1.1.3.cmml\">ğ‘·</mi><mo stretchy=\"false\" id=\"S4.E4.m1.2.2.1.1.2\" xref=\"S4.E4.m1.2.2.1.1.2.cmml\">â†</mo><mrow id=\"S4.E4.m1.2.2.1.1.1\" xref=\"S4.E4.m1.2.2.1.1.1.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.1.3\" xref=\"S4.E4.m1.2.2.1.1.1.3.cmml\">ğ‘·</mi><mo id=\"S4.E4.m1.2.2.1.1.1.2\" xref=\"S4.E4.m1.2.2.1.1.1.2.cmml\">âˆ’</mo><mrow id=\"S4.E4.m1.2.2.1.1.1.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.1.1.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.3.cmml\">Î±</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E4.m1.2.2.1.1.1.1.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.E4.m1.2.2.1.1.1.1.1.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.cmml\"><mrow id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml\">â€‹</mo><msub id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.2.cmml\">ğ’™</mi><mi id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.3.cmml\">i</mi></msub></mrow><mo id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml\">âˆ’</mo><mrow id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml\"><msub id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.1.cmml\">â€‹</mo><mrow id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.3.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml\"><mo stretchy=\"false\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.3.2.1\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml\">(</mo><mi id=\"S4.E4.m1.1.1\" xref=\"S4.E4.m1.1.1.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.3.2.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.cmml\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E4.m1.2.2.1.1.1.1.2a\" xref=\"S4.E4.m1.2.2.1.1.1.1.2.cmml\">â€‹</mo><msubsup id=\"S4.E4.m1.2.2.1.1.1.1.4\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.cmml\"><mi id=\"S4.E4.m1.2.2.1.1.1.1.4.2.2\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.2.2.cmml\">ğ’™</mi><mi id=\"S4.E4.m1.2.2.1.1.1.1.4.2.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.2.3.cmml\">i</mi><mo id=\"S4.E4.m1.2.2.1.1.1.1.4.3\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.3.cmml\">âŠ¤</mo></msubsup></mrow></mrow></mrow><mo lspace=\"0em\" id=\"S4.E4.m1.2.2.1.2\" xref=\"S4.E4.m1.2.2.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E4.m1.2b\"><apply id=\"S4.E4.m1.2.2.1.1.cmml\" xref=\"S4.E4.m1.2.2.1\"><ci id=\"S4.E4.m1.2.2.1.1.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.2\">â†</ci><ci id=\"S4.E4.m1.2.2.1.1.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.3\">ğ‘·</ci><apply id=\"S4.E4.m1.2.2.1.1.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1\"><minus id=\"S4.E4.m1.2.2.1.1.1.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.2\"></minus><ci id=\"S4.E4.m1.2.2.1.1.1.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.3\">ğ‘·</ci><apply id=\"S4.E4.m1.2.2.1.1.1.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1\"><times id=\"S4.E4.m1.2.2.1.1.1.1.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.2\"></times><ci id=\"S4.E4.m1.2.2.1.1.1.1.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.3\">ğ›¼</ci><apply id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1\"><minus id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.1\"></minus><apply id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2\"><times id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.1\"></times><ci id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.2\">ğ‘·</ci><apply id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3\">subscript</csymbol><ci id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.2\">ğ’™</ci><ci id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.2.3.3\">ğ‘–</ci></apply></apply><apply id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3\"><times id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.1\"></times><apply id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2\">subscript</csymbol><ci id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.1.1.1.3.2.3\">ğ‘–</ci></apply><ci id=\"S4.E4.m1.1.1.cmml\" xref=\"S4.E4.m1.1.1\">ğ’˜</ci></apply></apply><apply id=\"S4.E4.m1.2.2.1.1.1.1.4.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S4.E4.m1.2.2.1.1.1.1.4.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4\">superscript</csymbol><apply id=\"S4.E4.m1.2.2.1.1.1.1.4.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S4.E4.m1.2.2.1.1.1.1.4.2.1.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4\">subscript</csymbol><ci id=\"S4.E4.m1.2.2.1.1.1.1.4.2.2.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.2.2\">ğ’™</ci><ci id=\"S4.E4.m1.2.2.1.1.1.1.4.2.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.2.3\">ğ‘–</ci></apply><csymbol cd=\"latexml\" id=\"S4.E4.m1.2.2.1.1.1.1.4.3.cmml\" xref=\"S4.E4.m1.2.2.1.1.1.1.4.3\">top</csymbol></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E4.m1.2c\">\\displaystyle\\bm{P}\\leftarrow\\bm{P}-\\alpha(\\bm{P}\\bm{x}_{i}-f_{i}(\\bm{w}))\\bm{x}_{i}^{\\top}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S4.SS4.p2.7\" class=\"ltx_p\">In contrast, at each training step, <math id=\"S4.SS4.p2.5.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{w}\" display=\"inline\"><semantics id=\"S4.SS4.p2.5.m1.1a\"><mi id=\"S4.SS4.p2.5.m1.1.1\" xref=\"S4.SS4.p2.5.m1.1.1.cmml\">ğ’˜</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.5.m1.1b\"><ci id=\"S4.SS4.p2.5.m1.1.1.cmml\" xref=\"S4.SS4.p2.5.m1.1.1\">ğ’˜</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.5.m1.1c\">\\bm{w}</annotation></semantics></math>, analogous to the fast-adapting weights in a neural network, can be rapidly tuned to solve for task <math id=\"S4.SS4.p2.6.m2.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S4.SS4.p2.6.m2.1a\"><mi id=\"S4.SS4.p2.6.m2.1.1\" xref=\"S4.SS4.p2.6.m2.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.6.m2.1b\"><ci id=\"S4.SS4.p2.6.m2.1.1.cmml\" xref=\"S4.SS4.p2.6.m2.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.6.m2.1c\">i</annotation></semantics></math>, yielding the loss minimizer conditional on <math id=\"S4.SS4.p2.7.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\" display=\"inline\"><semantics id=\"S4.SS4.p2.7.m3.1a\"><mi id=\"S4.SS4.p2.7.m3.1.1\" xref=\"S4.SS4.p2.7.m3.1.1.cmml\">ğ‘·</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p2.7.m3.1b\"><ci id=\"S4.SS4.p2.7.m3.1.1.cmml\" xref=\"S4.SS4.p2.7.m3.1.1\">ğ‘·</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p2.7.m3.1c\">\\bm{P}</annotation></semantics></math>:</p>\n<table id=\"A3.EGx5\" class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\">\n\n<tbody id=\"S4.E5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math id=\"S4.E5.m1.1\" class=\"ltx_Math\" alttext=\"\\displaystyle\\bm{w}\\leftarrow f_{i}^{-1}(\\bm{P}\\bm{x}_{i}).\" display=\"inline\"><semantics id=\"S4.E5.m1.1a\"><mrow id=\"S4.E5.m1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.cmml\"><mrow id=\"S4.E5.m1.1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.cmml\"><mi id=\"S4.E5.m1.1.1.1.1.3\" xref=\"S4.E5.m1.1.1.1.1.3.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.E5.m1.1.1.1.1.2\" xref=\"S4.E5.m1.1.1.1.1.2.cmml\">â†</mo><mrow id=\"S4.E5.m1.1.1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.1.cmml\"><msubsup id=\"S4.E5.m1.1.1.1.1.1.3\" xref=\"S4.E5.m1.1.1.1.1.1.3.cmml\"><mi id=\"S4.E5.m1.1.1.1.1.1.3.2.2\" xref=\"S4.E5.m1.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.E5.m1.1.1.1.1.1.3.2.3\" xref=\"S4.E5.m1.1.1.1.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.E5.m1.1.1.1.1.1.3.3\" xref=\"S4.E5.m1.1.1.1.1.1.3.3.cmml\"><mo id=\"S4.E5.m1.1.1.1.1.1.3.3a\" xref=\"S4.E5.m1.1.1.1.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.E5.m1.1.1.1.1.1.3.3.2\" xref=\"S4.E5.m1.1.1.1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E5.m1.1.1.1.1.1.2\" xref=\"S4.E5.m1.1.1.1.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.E5.m1.1.1.1.1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.1.1.1.1.1.1.1.2\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.E5.m1.1.1.1.1.1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S4.E5.m1.1.1.1.1.1.1.1.1.2\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E5.m1.1.1.1.1.1.1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.2\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.3\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S4.E5.m1.1.1.1.1.1.1.1.3\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo lspace=\"0em\" id=\"S4.E5.m1.1.1.1.2\" xref=\"S4.E5.m1.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E5.m1.1b\"><apply id=\"S4.E5.m1.1.1.1.1.cmml\" xref=\"S4.E5.m1.1.1.1\"><ci id=\"S4.E5.m1.1.1.1.1.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.2\">â†</ci><ci id=\"S4.E5.m1.1.1.1.1.3.cmml\" xref=\"S4.E5.m1.1.1.1.1.3\">ğ’˜</ci><apply id=\"S4.E5.m1.1.1.1.1.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1\"><times id=\"S4.E5.m1.1.1.1.1.1.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.2\"></times><apply id=\"S4.E5.m1.1.1.1.1.1.3.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3\">superscript</csymbol><apply id=\"S4.E5.m1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.E5.m1.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.E5.m1.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3.2.3\">ğ‘–</ci></apply><apply id=\"S4.E5.m1.1.1.1.1.1.3.3.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3.3\"><minus id=\"S4.E5.m1.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.E5.m1.1.1.1.1.1.3.3.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.E5.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1\"><times id=\"S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.1\"></times><ci id=\"S4.E5.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.2\">ğ’™</ci><ci id=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S4.E5.m1.1.1.1.1.1.1.1.1.3.3\">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E5.m1.1c\">\\displaystyle\\bm{w}\\leftarrow f_{i}^{-1}(\\bm{P}\\bm{x}_{i}).</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS4.p3\" class=\"ltx_para\">\n<p id=\"S4.SS4.p3.15\" class=\"ltx_p\">As in our main experiments, we sequentially optimize each <math id=\"S4.SS4.p3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p3.1.m1.1a\"><msub id=\"S4.SS4.p3.1.m1.1.1\" xref=\"S4.SS4.p3.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S4.SS4.p3.1.m1.1.1.2\" xref=\"S4.SS4.p3.1.m1.1.1.2.cmml\">â„“</mi><mi id=\"S4.SS4.p3.1.m1.1.1.3\" xref=\"S4.SS4.p3.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.1.m1.1b\"><apply id=\"S4.SS4.p3.1.m1.1.1.cmml\" xref=\"S4.SS4.p3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.1.m1.1.1.1.cmml\" xref=\"S4.SS4.p3.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS4.p3.1.m1.1.1.2.cmml\" xref=\"S4.SS4.p3.1.m1.1.1.2\">â„“</ci><ci id=\"S4.SS4.p3.1.m1.1.1.3.cmml\" xref=\"S4.SS4.p3.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.1.m1.1c\">\\ell_{i}</annotation></semantics></math> as we iterate through the sequence of tasks.\nIn each training step, we first update <math id=\"S4.SS4.p3.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\" display=\"inline\"><semantics id=\"S4.SS4.p3.2.m2.1a\"><mi id=\"S4.SS4.p3.2.m2.1.1\" xref=\"S4.SS4.p3.2.m2.1.1.cmml\">ğ‘·</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.2.m2.1b\"><ci id=\"S4.SS4.p3.2.m2.1.1.cmml\" xref=\"S4.SS4.p3.2.m2.1.1\">ğ‘·</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.2.m2.1c\">\\bm{P}</annotation></semantics></math> and then solve for <math id=\"S4.SS4.p3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{w}\" display=\"inline\"><semantics id=\"S4.SS4.p3.3.m3.1a\"><mi id=\"S4.SS4.p3.3.m3.1.1\" xref=\"S4.SS4.p3.3.m3.1.1.cmml\">ğ’˜</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.3.m3.1b\"><ci id=\"S4.SS4.p3.3.m3.1.1.cmml\" xref=\"S4.SS4.p3.3.m3.1.1\">ğ’˜</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.3.m3.1c\">\\bm{w}</annotation></semantics></math> given the update to <math id=\"S4.SS4.p3.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\" display=\"inline\"><semantics id=\"S4.SS4.p3.4.m4.1a\"><mi id=\"S4.SS4.p3.4.m4.1.1\" xref=\"S4.SS4.p3.4.m4.1.1.cmml\">ğ‘·</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.4.m4.1b\"><ci id=\"S4.SS4.p3.4.m4.1.1.cmml\" xref=\"S4.SS4.p3.4.m4.1.1\">ğ‘·</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.4.m4.1c\">\\bm{P}</annotation></semantics></math>.\nIn essence, updating <math id=\"S4.SS4.p3.5.m5.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\" display=\"inline\"><semantics id=\"S4.SS4.p3.5.m5.1a\"><mi id=\"S4.SS4.p3.5.m5.1.1\" xref=\"S4.SS4.p3.5.m5.1.1.cmml\">ğ‘·</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.5.m5.1b\"><ci id=\"S4.SS4.p3.5.m5.1.1.cmml\" xref=\"S4.SS4.p3.5.m5.1.1\">ğ‘·</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.5.m5.1c\">\\bm{P}</annotation></semantics></math> approximately reduces the distance between\n<math id=\"S4.SS4.p3.6.m6.1\" class=\"ltx_Math\" alttext=\"\\bm{P}\\bm{x}_{i+1}\" display=\"inline\"><semantics id=\"S4.SS4.p3.6.m6.1a\"><mrow id=\"S4.SS4.p3.6.m6.1.1\" xref=\"S4.SS4.p3.6.m6.1.1.cmml\"><mi id=\"S4.SS4.p3.6.m6.1.1.2\" xref=\"S4.SS4.p3.6.m6.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.6.m6.1.1.1\" xref=\"S4.SS4.p3.6.m6.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p3.6.m6.1.1.3\" xref=\"S4.SS4.p3.6.m6.1.1.3.cmml\"><mi id=\"S4.SS4.p3.6.m6.1.1.3.2\" xref=\"S4.SS4.p3.6.m6.1.1.3.2.cmml\">ğ’™</mi><mrow id=\"S4.SS4.p3.6.m6.1.1.3.3\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.6.m6.1.1.3.3.2\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.6.m6.1.1.3.3.1\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.6.m6.1.1.3.3.3\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.6.m6.1b\"><apply id=\"S4.SS4.p3.6.m6.1.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1\"><times id=\"S4.SS4.p3.6.m6.1.1.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.1\"></times><ci id=\"S4.SS4.p3.6.m6.1.1.2.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p3.6.m6.1.1.3.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.6.m6.1.1.3.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.6.m6.1.1.3.2.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.2\">ğ’™</ci><apply id=\"S4.SS4.p3.6.m6.1.1.3.3.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3\"><plus id=\"S4.SS4.p3.6.m6.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.6.m6.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.6.m6.1.1.3.3.3.cmml\" xref=\"S4.SS4.p3.6.m6.1.1.3.3.3\">1</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.6.m6.1c\">\\bm{P}\\bm{x}_{i+1}</annotation></semantics></math> and <math id=\"S4.SS4.p3.7.m7.1\" class=\"ltx_Math\" alttext=\"f_{i+1}(f_{i}^{-1}(\\bm{P}\\bm{x}_{i}))\" display=\"inline\"><semantics id=\"S4.SS4.p3.7.m7.1a\"><mrow id=\"S4.SS4.p3.7.m7.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.cmml\"><msub id=\"S4.SS4.p3.7.m7.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.3.2.cmml\">f</mi><mrow id=\"S4.SS4.p3.7.m7.1.1.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.3.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.7.m7.1.1.3.3.1\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.7.m7.1.1.3.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.3.cmml\">1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.7.m7.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p3.7.m7.1.1.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\"><msubsup id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3a\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo stretchy=\"false\" id=\"S4.SS4.p3.7.m7.1.1.1.1.3\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.7.m7.1b\"><apply id=\"S4.SS4.p3.7.m7.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1\"><times id=\"S4.SS4.p3.7.m7.1.1.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.2\"></times><apply id=\"S4.SS4.p3.7.m7.1.1.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.7.m7.1.1.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.2\">ğ‘“</ci><apply id=\"S4.SS4.p3.7.m7.1.1.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3\"><plus id=\"S4.SS4.p3.7.m7.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.7.m7.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.7.m7.1.1.3.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.3.3.3\">1</cn></apply></apply><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1\"><times id=\"S4.SS4.p3.7.m7.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.2\"></times><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.2.3\">ğ‘–</ci></apply><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3\"><minus id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1\"><times id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.2\">ğ’™</ci><ci id=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.7.m7.1.1.1.1.1.1.1.1.3.3\">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.7.m7.1c\">f_{i+1}(f_{i}^{-1}(\\bm{P}\\bm{x}_{i}))</annotation></semantics></math>. Assume each <math id=\"S4.SS4.p3.8.m8.1\" class=\"ltx_Math\" alttext=\"f_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p3.8.m8.1a\"><msub id=\"S4.SS4.p3.8.m8.1.1\" xref=\"S4.SS4.p3.8.m8.1.1.cmml\"><mi id=\"S4.SS4.p3.8.m8.1.1.2\" xref=\"S4.SS4.p3.8.m8.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p3.8.m8.1.1.3\" xref=\"S4.SS4.p3.8.m8.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.8.m8.1b\"><apply id=\"S4.SS4.p3.8.m8.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.8.m8.1.1.1.cmml\" xref=\"S4.SS4.p3.8.m8.1.1\">subscript</csymbol><ci id=\"S4.SS4.p3.8.m8.1.1.2.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.2\">ğ‘“</ci><ci id=\"S4.SS4.p3.8.m8.1.1.3.cmml\" xref=\"S4.SS4.p3.8.m8.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.8.m8.1c\">f_{i}</annotation></semantics></math> is Lipschitz, then this will entail bringing <math id=\"S4.SS4.p3.9.m9.1\" class=\"ltx_Math\" alttext=\"f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})\" display=\"inline\"><semantics id=\"S4.SS4.p3.9.m9.1a\"><mrow id=\"S4.SS4.p3.9.m9.1.1\" xref=\"S4.SS4.p3.9.m9.1.1.cmml\"><msubsup id=\"S4.SS4.p3.9.m9.1.1.3\" xref=\"S4.SS4.p3.9.m9.1.1.3.cmml\"><mi id=\"S4.SS4.p3.9.m9.1.1.3.2.2\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.2.cmml\">f</mi><mrow id=\"S4.SS4.p3.9.m9.1.1.3.2.3\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.cmml\"><mi id=\"S4.SS4.p3.9.m9.1.1.3.2.3.2\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.9.m9.1.1.3.2.3.1\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.9.m9.1.1.3.2.3.3\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.3.cmml\">1</mn></mrow><mrow id=\"S4.SS4.p3.9.m9.1.1.3.3\" xref=\"S4.SS4.p3.9.m9.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.9.m9.1.1.3.3a\" xref=\"S4.SS4.p3.9.m9.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.SS4.p3.9.m9.1.1.3.3.2\" xref=\"S4.SS4.p3.9.m9.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.9.m9.1.1.2\" xref=\"S4.SS4.p3.9.m9.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p3.9.m9.1.1.1.1\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p3.9.m9.1.1.1.1.2\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.9.m9.1.1.1.1.1\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.9.m9.1.1.1.1.1.2\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.9.m9.1.1.1.1.1.1\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mrow id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.1\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.3\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><mo stretchy=\"false\" id=\"S4.SS4.p3.9.m9.1.1.1.1.3\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.9.m9.1b\"><apply id=\"S4.SS4.p3.9.m9.1.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1\"><times id=\"S4.SS4.p3.9.m9.1.1.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.2\"></times><apply id=\"S4.SS4.p3.9.m9.1.1.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.9.m9.1.1.3.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.9.m9.1.1.3.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.9.m9.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.9.m9.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.2\">ğ‘“</ci><apply id=\"S4.SS4.p3.9.m9.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3\"><plus id=\"S4.SS4.p3.9.m9.1.1.3.2.3.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.1\"></plus><ci id=\"S4.SS4.p3.9.m9.1.1.3.2.3.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.9.m9.1.1.3.2.3.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.2.3.3\">1</cn></apply></apply><apply id=\"S4.SS4.p3.9.m9.1.1.3.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.3\"><minus id=\"S4.SS4.p3.9.m9.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.SS4.p3.9.m9.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.9.m9.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1\"><times id=\"S4.SS4.p3.9.m9.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.9.m9.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.2\">ğ’™</ci><apply id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3\"><plus id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.3.cmml\" xref=\"S4.SS4.p3.9.m9.1.1.1.1.1.3.3.3\">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.9.m9.1c\">f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})</annotation></semantics></math> toward <math id=\"S4.SS4.p3.10.m10.1\" class=\"ltx_Math\" alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" display=\"inline\"><semantics id=\"S4.SS4.p3.10.m10.1a\"><mrow id=\"S4.SS4.p3.10.m10.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.cmml\"><msubsup id=\"S4.SS4.p3.10.m10.1.1.3\" xref=\"S4.SS4.p3.10.m10.1.1.3.cmml\"><mi id=\"S4.SS4.p3.10.m10.1.1.3.2.2\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.10.m10.1.1.3.2.3\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.10.m10.1.1.3.3\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.10.m10.1.1.3.3a\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.SS4.p3.10.m10.1.1.3.3.2\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.10.m10.1.1.2\" xref=\"S4.SS4.p3.10.m10.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p3.10.m10.1.1.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p3.10.m10.1.1.1.1.2\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.10.m10.1.1.1.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.10.m10.1.1.1.1.1.2\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.10.m10.1.1.1.1.1.1\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S4.SS4.p3.10.m10.1.1.1.1.3\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.10.m10.1b\"><apply id=\"S4.SS4.p3.10.m10.1.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1\"><times id=\"S4.SS4.p3.10.m10.1.1.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.2\"></times><apply id=\"S4.SS4.p3.10.m10.1.1.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.10.m10.1.1.3.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.10.m10.1.1.3.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.10.m10.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.10.m10.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p3.10.m10.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.2.3\">ğ‘–</ci></apply><apply id=\"S4.SS4.p3.10.m10.1.1.3.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.3\"><minus id=\"S4.SS4.p3.10.m10.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.SS4.p3.10.m10.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.10.m10.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1\"><times id=\"S4.SS4.p3.10.m10.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.10.m10.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.2\">ğ’™</ci><ci id=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.10.m10.1.1.1.1.1.3.3\">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.10.m10.1c\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation></semantics></math>. As a result of this optimization objective, the model will evolve along the optimization trajectory such that the <math id=\"S4.SS4.p3.11.m11.1\" class=\"ltx_Math\" alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" display=\"inline\"><semantics id=\"S4.SS4.p3.11.m11.1a\"><mrow id=\"S4.SS4.p3.11.m11.1.1\" xref=\"S4.SS4.p3.11.m11.1.1.cmml\"><msubsup id=\"S4.SS4.p3.11.m11.1.1.3\" xref=\"S4.SS4.p3.11.m11.1.1.3.cmml\"><mi id=\"S4.SS4.p3.11.m11.1.1.3.2.2\" xref=\"S4.SS4.p3.11.m11.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p3.11.m11.1.1.3.2.3\" xref=\"S4.SS4.p3.11.m11.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p3.11.m11.1.1.3.3\" xref=\"S4.SS4.p3.11.m11.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.11.m11.1.1.3.3a\" xref=\"S4.SS4.p3.11.m11.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.SS4.p3.11.m11.1.1.3.3.2\" xref=\"S4.SS4.p3.11.m11.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.11.m11.1.1.2\" xref=\"S4.SS4.p3.11.m11.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p3.11.m11.1.1.1.1\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p3.11.m11.1.1.1.1.2\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.11.m11.1.1.1.1.1\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.11.m11.1.1.1.1.1.2\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.11.m11.1.1.1.1.1.1\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S4.SS4.p3.11.m11.1.1.1.1.3\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.11.m11.1b\"><apply id=\"S4.SS4.p3.11.m11.1.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1\"><times id=\"S4.SS4.p3.11.m11.1.1.2.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.2\"></times><apply id=\"S4.SS4.p3.11.m11.1.1.3.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.11.m11.1.1.3.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.11.m11.1.1.3.2.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.11.m11.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.11.m11.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p3.11.m11.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3.2.3\">ğ‘–</ci></apply><apply id=\"S4.SS4.p3.11.m11.1.1.3.3.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3.3\"><minus id=\"S4.SS4.p3.11.m11.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.SS4.p3.11.m11.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.11.m11.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1\"><times id=\"S4.SS4.p3.11.m11.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.11.m11.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.2\">ğ’™</ci><ci id=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.11.m11.1.1.1.1.1.3.3\">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.11.m11.1c\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation></semantics></math> for all tasks <math id=\"S4.SS4.p3.12.m12.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S4.SS4.p3.12.m12.1a\"><mi id=\"S4.SS4.p3.12.m12.1.1\" xref=\"S4.SS4.p3.12.m12.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.12.m12.1b\"><ci id=\"S4.SS4.p3.12.m12.1.1.cmml\" xref=\"S4.SS4.p3.12.m12.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.12.m12.1c\">i</annotation></semantics></math> gradually form a circular pattern. This gives an intuitive explanation on the anticipatory recovery phenomenon, since updaing <math id=\"S4.SS4.p3.13.m13.1\" class=\"ltx_Math\" alttext=\"\\bm{w}\" display=\"inline\"><semantics id=\"S4.SS4.p3.13.m13.1a\"><mi id=\"S4.SS4.p3.13.m13.1.1\" xref=\"S4.SS4.p3.13.m13.1.1.cmml\">ğ’˜</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.13.m13.1b\"><ci id=\"S4.SS4.p3.13.m13.1.1.cmml\" xref=\"S4.SS4.p3.13.m13.1.1\">ğ’˜</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.13.m13.1c\">\\bm{w}</annotation></semantics></math> according to equation <a href=\"#S4.E5\" title=\"Equation 5 â€£ 4.4 Computational Toy Model â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> will also bring it closer to <math id=\"S4.SS4.p3.14.m14.1\" class=\"ltx_Math\" alttext=\"f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})\" display=\"inline\"><semantics id=\"S4.SS4.p3.14.m14.1a\"><mrow id=\"S4.SS4.p3.14.m14.1.1\" xref=\"S4.SS4.p3.14.m14.1.1.cmml\"><msubsup id=\"S4.SS4.p3.14.m14.1.1.3\" xref=\"S4.SS4.p3.14.m14.1.1.3.cmml\"><mi id=\"S4.SS4.p3.14.m14.1.1.3.2.2\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.2.cmml\">f</mi><mrow id=\"S4.SS4.p3.14.m14.1.1.3.2.3\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.cmml\"><mi id=\"S4.SS4.p3.14.m14.1.1.3.2.3.2\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.14.m14.1.1.3.2.3.1\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.14.m14.1.1.3.2.3.3\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.3.cmml\">1</mn></mrow><mrow id=\"S4.SS4.p3.14.m14.1.1.3.3\" xref=\"S4.SS4.p3.14.m14.1.1.3.3.cmml\"><mo id=\"S4.SS4.p3.14.m14.1.1.3.3a\" xref=\"S4.SS4.p3.14.m14.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.SS4.p3.14.m14.1.1.3.3.2\" xref=\"S4.SS4.p3.14.m14.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.14.m14.1.1.2\" xref=\"S4.SS4.p3.14.m14.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p3.14.m14.1.1.1.1\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p3.14.m14.1.1.1.1.2\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p3.14.m14.1.1.1.1.1\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p3.14.m14.1.1.1.1.1.2\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p3.14.m14.1.1.1.1.1.1\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.2\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mrow id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.cmml\"><mi id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.2\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.1\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.3\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><mo stretchy=\"false\" id=\"S4.SS4.p3.14.m14.1.1.1.1.3\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.14.m14.1b\"><apply id=\"S4.SS4.p3.14.m14.1.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1\"><times id=\"S4.SS4.p3.14.m14.1.1.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.2\"></times><apply id=\"S4.SS4.p3.14.m14.1.1.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.14.m14.1.1.3.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p3.14.m14.1.1.3.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.14.m14.1.1.3.2.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.14.m14.1.1.3.2.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.2\">ğ‘“</ci><apply id=\"S4.SS4.p3.14.m14.1.1.3.2.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3\"><plus id=\"S4.SS4.p3.14.m14.1.1.3.2.3.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.1\"></plus><ci id=\"S4.SS4.p3.14.m14.1.1.3.2.3.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.14.m14.1.1.3.2.3.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.2.3.3\">1</cn></apply></apply><apply id=\"S4.SS4.p3.14.m14.1.1.3.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.3\"><minus id=\"S4.SS4.p3.14.m14.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.SS4.p3.14.m14.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p3.14.m14.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1\"><times id=\"S4.SS4.p3.14.m14.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p3.14.m14.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.2\">ğ’™</ci><apply id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3\"><plus id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.1.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.2.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.3.cmml\" xref=\"S4.SS4.p3.14.m14.1.1.1.1.1.3.3.3\">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.14.m14.1c\">f_{i+1}^{-1}(\\bm{P}\\bm{x}_{i+1})</annotation></semantics></math>, thus reducing the loss on task <math id=\"S4.SS4.p3.15.m15.1\" class=\"ltx_Math\" alttext=\"i+1\" display=\"inline\"><semantics id=\"S4.SS4.p3.15.m15.1a\"><mrow id=\"S4.SS4.p3.15.m15.1.1\" xref=\"S4.SS4.p3.15.m15.1.1.cmml\"><mi id=\"S4.SS4.p3.15.m15.1.1.2\" xref=\"S4.SS4.p3.15.m15.1.1.2.cmml\">i</mi><mo id=\"S4.SS4.p3.15.m15.1.1.1\" xref=\"S4.SS4.p3.15.m15.1.1.1.cmml\">+</mo><mn id=\"S4.SS4.p3.15.m15.1.1.3\" xref=\"S4.SS4.p3.15.m15.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p3.15.m15.1b\"><apply id=\"S4.SS4.p3.15.m15.1.1.cmml\" xref=\"S4.SS4.p3.15.m15.1.1\"><plus id=\"S4.SS4.p3.15.m15.1.1.1.cmml\" xref=\"S4.SS4.p3.15.m15.1.1.1\"></plus><ci id=\"S4.SS4.p3.15.m15.1.1.2.cmml\" xref=\"S4.SS4.p3.15.m15.1.1.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p3.15.m15.1.1.3.cmml\" xref=\"S4.SS4.p3.15.m15.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p3.15.m15.1c\">i+1</annotation></semantics></math> and exhibits anticipatory recovery.</p>\n</div>\n<div id=\"S4.SS4.p4\" class=\"ltx_para\">\n<p id=\"S4.SS4.p4.6\" class=\"ltx_p\">We experimented with two very simple choices of <math id=\"S4.SS4.p4.1.m1.1\" class=\"ltx_Math\" alttext=\"f_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p4.1.m1.1a\"><msub id=\"S4.SS4.p4.1.m1.1.1\" xref=\"S4.SS4.p4.1.m1.1.1.cmml\"><mi id=\"S4.SS4.p4.1.m1.1.1.2\" xref=\"S4.SS4.p4.1.m1.1.1.2.cmml\">f</mi><mi id=\"S4.SS4.p4.1.m1.1.1.3\" xref=\"S4.SS4.p4.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.1.m1.1b\"><apply id=\"S4.SS4.p4.1.m1.1.1.cmml\" xref=\"S4.SS4.p4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.1.m1.1.1.1.cmml\" xref=\"S4.SS4.p4.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS4.p4.1.m1.1.1.2.cmml\" xref=\"S4.SS4.p4.1.m1.1.1.2\">ğ‘“</ci><ci id=\"S4.SS4.p4.1.m1.1.1.3.cmml\" xref=\"S4.SS4.p4.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.1.m1.1c\">f_{i}</annotation></semantics></math>: <math id=\"S4.SS4.p4.2.m2.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{w}\" display=\"inline\"><semantics id=\"S4.SS4.p4.2.m2.1a\"><mrow id=\"S4.SS4.p4.2.m2.1.2\" xref=\"S4.SS4.p4.2.m2.1.2.cmml\"><mrow id=\"S4.SS4.p4.2.m2.1.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\"><msub id=\"S4.SS4.p4.2.m2.1.2.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.cmml\"><mi id=\"S4.SS4.p4.2.m2.1.2.2.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.SS4.p4.2.m2.1.2.2.2.3\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p4.2.m2.1.2.2.1\" xref=\"S4.SS4.p4.2.m2.1.2.2.1.cmml\">â€‹</mo><mrow id=\"S4.SS4.p4.2.m2.1.2.2.3.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p4.2.m2.1.2.2.3.2.1\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\">(</mo><mi id=\"S4.SS4.p4.2.m2.1.1\" xref=\"S4.SS4.p4.2.m2.1.1.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.SS4.p4.2.m2.1.2.2.3.2.2\" xref=\"S4.SS4.p4.2.m2.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p4.2.m2.1.2.1\" xref=\"S4.SS4.p4.2.m2.1.2.1.cmml\">=</mo><mi id=\"S4.SS4.p4.2.m2.1.2.3\" xref=\"S4.SS4.p4.2.m2.1.2.3.cmml\">ğ’˜</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.2.m2.1b\"><apply id=\"S4.SS4.p4.2.m2.1.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2\"><eq id=\"S4.SS4.p4.2.m2.1.2.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.1\"></eq><apply id=\"S4.SS4.p4.2.m2.1.2.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2\"><times id=\"S4.SS4.p4.2.m2.1.2.2.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.1\"></times><apply id=\"S4.SS4.p4.2.m2.1.2.2.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.2.m2.1.2.2.2.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2\">subscript</csymbol><ci id=\"S4.SS4.p4.2.m2.1.2.2.2.2.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p4.2.m2.1.2.2.2.3.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"S4.SS4.p4.2.m2.1.1.cmml\" xref=\"S4.SS4.p4.2.m2.1.1\">ğ’˜</ci></apply><ci id=\"S4.SS4.p4.2.m2.1.2.3.cmml\" xref=\"S4.SS4.p4.2.m2.1.2.3\">ğ’˜</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.2.m2.1c\">f_{i}(\\bm{w})=\\bm{w}</annotation></semantics></math> and <math id=\"S4.SS4.p4.3.m3.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" display=\"inline\"><semantics id=\"S4.SS4.p4.3.m3.1a\"><mrow id=\"S4.SS4.p4.3.m3.1.2\" xref=\"S4.SS4.p4.3.m3.1.2.cmml\"><mrow id=\"S4.SS4.p4.3.m3.1.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\"><msub id=\"S4.SS4.p4.3.m3.1.2.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.cmml\"><mi id=\"S4.SS4.p4.3.m3.1.2.2.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.2.cmml\">f</mi><mi id=\"S4.SS4.p4.3.m3.1.2.2.2.3\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p4.3.m3.1.2.2.1\" xref=\"S4.SS4.p4.3.m3.1.2.2.1.cmml\">â€‹</mo><mrow id=\"S4.SS4.p4.3.m3.1.2.2.3.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p4.3.m3.1.2.2.3.2.1\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\">(</mo><mi id=\"S4.SS4.p4.3.m3.1.1\" xref=\"S4.SS4.p4.3.m3.1.1.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"S4.SS4.p4.3.m3.1.2.2.3.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.SS4.p4.3.m3.1.2.1\" xref=\"S4.SS4.p4.3.m3.1.2.1.cmml\">=</mo><mrow id=\"S4.SS4.p4.3.m3.1.2.3\" xref=\"S4.SS4.p4.3.m3.1.2.3.cmml\"><msub id=\"S4.SS4.p4.3.m3.1.2.3.2\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.cmml\"><mi id=\"S4.SS4.p4.3.m3.1.2.3.2.2\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.2.cmml\">ğ’š</mi><mi id=\"S4.SS4.p4.3.m3.1.2.3.2.3\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"S4.SS4.p4.3.m3.1.2.3.1\" xref=\"S4.SS4.p4.3.m3.1.2.3.1.cmml\">âˆ’</mo><mi id=\"S4.SS4.p4.3.m3.1.2.3.3\" xref=\"S4.SS4.p4.3.m3.1.2.3.3.cmml\">ğ’˜</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.3.m3.1b\"><apply id=\"S4.SS4.p4.3.m3.1.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2\"><eq id=\"S4.SS4.p4.3.m3.1.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.1\"></eq><apply id=\"S4.SS4.p4.3.m3.1.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2\"><times id=\"S4.SS4.p4.3.m3.1.2.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.1\"></times><apply id=\"S4.SS4.p4.3.m3.1.2.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.3.m3.1.2.2.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2\">subscript</csymbol><ci id=\"S4.SS4.p4.3.m3.1.2.2.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p4.3.m3.1.2.2.2.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"S4.SS4.p4.3.m3.1.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.1\">ğ’˜</ci></apply><apply id=\"S4.SS4.p4.3.m3.1.2.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3\"><minus id=\"S4.SS4.p4.3.m3.1.2.3.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.1\"></minus><apply id=\"S4.SS4.p4.3.m3.1.2.3.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.3.m3.1.2.3.2.1.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2\">subscript</csymbol><ci id=\"S4.SS4.p4.3.m3.1.2.3.2.2.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.2\">ğ’š</ci><ci id=\"S4.SS4.p4.3.m3.1.2.3.2.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.2.3\">ğ‘–</ci></apply><ci id=\"S4.SS4.p4.3.m3.1.2.3.3.cmml\" xref=\"S4.SS4.p4.3.m3.1.2.3.3\">ğ’˜</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.3.m3.1c\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation></semantics></math> for some task-dependent targets <math id=\"S4.SS4.p4.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{y}_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p4.4.m4.1a\"><msub id=\"S4.SS4.p4.4.m4.1.1\" xref=\"S4.SS4.p4.4.m4.1.1.cmml\"><mi id=\"S4.SS4.p4.4.m4.1.1.2\" xref=\"S4.SS4.p4.4.m4.1.1.2.cmml\">ğ’š</mi><mi id=\"S4.SS4.p4.4.m4.1.1.3\" xref=\"S4.SS4.p4.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.4.m4.1b\"><apply id=\"S4.SS4.p4.4.m4.1.1.cmml\" xref=\"S4.SS4.p4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.4.m4.1.1.1.cmml\" xref=\"S4.SS4.p4.4.m4.1.1\">subscript</csymbol><ci id=\"S4.SS4.p4.4.m4.1.1.2.cmml\" xref=\"S4.SS4.p4.4.m4.1.1.2\">ğ’š</ci><ci id=\"S4.SS4.p4.4.m4.1.1.3.cmml\" xref=\"S4.SS4.p4.4.m4.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.4.m4.1c\">\\bm{y}_{i}</annotation></semantics></math>. We follow the same order over tasksâ€”<math id=\"S4.SS4.p4.5.m5.3\" class=\"ltx_Math\" alttext=\"1,\\cdots,T\" display=\"inline\"><semantics id=\"S4.SS4.p4.5.m5.3a\"><mrow id=\"S4.SS4.p4.5.m5.3.4.2\" xref=\"S4.SS4.p4.5.m5.3.4.1.cmml\"><mn id=\"S4.SS4.p4.5.m5.1.1\" xref=\"S4.SS4.p4.5.m5.1.1.cmml\">1</mn><mo id=\"S4.SS4.p4.5.m5.3.4.2.1\" xref=\"S4.SS4.p4.5.m5.3.4.1.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S4.SS4.p4.5.m5.2.2\" xref=\"S4.SS4.p4.5.m5.2.2.cmml\">â‹¯</mi><mo id=\"S4.SS4.p4.5.m5.3.4.2.2\" xref=\"S4.SS4.p4.5.m5.3.4.1.cmml\">,</mo><mi id=\"S4.SS4.p4.5.m5.3.3\" xref=\"S4.SS4.p4.5.m5.3.3.cmml\">T</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.5.m5.3b\"><list id=\"S4.SS4.p4.5.m5.3.4.1.cmml\" xref=\"S4.SS4.p4.5.m5.3.4.2\"><cn type=\"integer\" id=\"S4.SS4.p4.5.m5.1.1.cmml\" xref=\"S4.SS4.p4.5.m5.1.1\">1</cn><ci id=\"S4.SS4.p4.5.m5.2.2.cmml\" xref=\"S4.SS4.p4.5.m5.2.2\">â‹¯</ci><ci id=\"S4.SS4.p4.5.m5.3.3.cmml\" xref=\"S4.SS4.p4.5.m5.3.3\">ğ‘‡</ci></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.5.m5.3c\">1,\\cdots,T</annotation></semantics></math>â€”for multiple epochs of training. The resulting loss curves are shown in FigureÂ <a href=\"#S4.F10\" title=\"Figure 10 â€£ 4.1 Temporal Structure of Gradients â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, which exhibits very similar anticipatory recovery trajectory as the full-blown LLM experiment. Visualizations of the 2-dimensional PCA embeddings for <math id=\"S4.SS4.p4.6.m6.1\" class=\"ltx_Math\" alttext=\"f_{i}^{-1}(\\bm{P}\\bm{x}_{i})\" display=\"inline\"><semantics id=\"S4.SS4.p4.6.m6.1a\"><mrow id=\"S4.SS4.p4.6.m6.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.cmml\"><msubsup id=\"S4.SS4.p4.6.m6.1.1.3\" xref=\"S4.SS4.p4.6.m6.1.1.3.cmml\"><mi id=\"S4.SS4.p4.6.m6.1.1.3.2.2\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.2.cmml\">f</mi><mi id=\"S4.SS4.p4.6.m6.1.1.3.2.3\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.3.cmml\">i</mi><mrow id=\"S4.SS4.p4.6.m6.1.1.3.3\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.cmml\"><mo id=\"S4.SS4.p4.6.m6.1.1.3.3a\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S4.SS4.p4.6.m6.1.1.3.3.2\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.2.cmml\">1</mn></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p4.6.m6.1.1.2\" xref=\"S4.SS4.p4.6.m6.1.1.2.cmml\">â€‹</mo><mrow id=\"S4.SS4.p4.6.m6.1.1.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS4.p4.6.m6.1.1.1.1.2\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.SS4.p4.6.m6.1.1.1.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\"><mi id=\"S4.SS4.p4.6.m6.1.1.1.1.1.2\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.2.cmml\">ğ‘·</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p4.6.m6.1.1.1.1.1.1\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.cmml\"><mi id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S4.SS4.p4.6.m6.1.1.1.1.3\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p4.6.m6.1b\"><apply id=\"S4.SS4.p4.6.m6.1.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1\"><times id=\"S4.SS4.p4.6.m6.1.1.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.2\"></times><apply id=\"S4.SS4.p4.6.m6.1.1.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.6.m6.1.1.3.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\">superscript</csymbol><apply id=\"S4.SS4.p4.6.m6.1.1.3.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.6.m6.1.1.3.2.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p4.6.m6.1.1.3.2.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.2\">ğ‘“</ci><ci id=\"S4.SS4.p4.6.m6.1.1.3.2.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.2.3\">ğ‘–</ci></apply><apply id=\"S4.SS4.p4.6.m6.1.1.3.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.3\"><minus id=\"S4.SS4.p4.6.m6.1.1.3.3.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.SS4.p4.6.m6.1.1.3.3.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.3.3.2\">1</cn></apply></apply><apply id=\"S4.SS4.p4.6.m6.1.1.1.1.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1\"><times id=\"S4.SS4.p4.6.m6.1.1.1.1.1.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.1\"></times><ci id=\"S4.SS4.p4.6.m6.1.1.1.1.1.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.2\">ğ‘·</ci><apply id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.1.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.2\">ğ’™</ci><ci id=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3.cmml\" xref=\"S4.SS4.p4.6.m6.1.1.1.1.1.3.3\">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p4.6.m6.1c\">f_{i}^{-1}(\\bm{P}\\bm{x}_{i})</annotation></semantics></math> in the second experiment are shown in FigureÂ <a href=\"#S4.F11\" title=\"Figure 11 â€£ 4.1 Temporal Structure of Gradients â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, which confirms our analysis that they gradually self-organize into a cyclic structure.</p>\n</div>\n<div id=\"S4.SS4.p5\" class=\"ltx_para\">\n<p id=\"S4.SS4.p5.3\" class=\"ltx_p\">There are two potential reasons large overparameterized networks might produce the anticipatory recovery in a way analogous to the toy simulation. First, as the scale of the network grows, it is likely that the network can develop task-specific parameters that quickly adapt to and memorize the new input data, corresponding to EquationÂ <a href=\"#S4.E5\" title=\"Equation 5 â€£ 4.4 Computational Toy Model â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. And when the fast memorization is achieved, the gradient descent dynamics of the slow weights push the representations of the two adjacent tasks (<math id=\"S4.SS4.p5.1.m1.1\" class=\"ltx_Math\" alttext=\"P\\bm{x}_{i}\" display=\"inline\"><semantics id=\"S4.SS4.p5.1.m1.1a\"><mrow id=\"S4.SS4.p5.1.m1.1.1\" xref=\"S4.SS4.p5.1.m1.1.1.cmml\"><mi id=\"S4.SS4.p5.1.m1.1.1.2\" xref=\"S4.SS4.p5.1.m1.1.1.2.cmml\">P</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p5.1.m1.1.1.1\" xref=\"S4.SS4.p5.1.m1.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p5.1.m1.1.1.3\" xref=\"S4.SS4.p5.1.m1.1.1.3.cmml\"><mi id=\"S4.SS4.p5.1.m1.1.1.3.2\" xref=\"S4.SS4.p5.1.m1.1.1.3.2.cmml\">ğ’™</mi><mi id=\"S4.SS4.p5.1.m1.1.1.3.3\" xref=\"S4.SS4.p5.1.m1.1.1.3.3.cmml\">i</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p5.1.m1.1b\"><apply id=\"S4.SS4.p5.1.m1.1.1.cmml\" xref=\"S4.SS4.p5.1.m1.1.1\"><times id=\"S4.SS4.p5.1.m1.1.1.1.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.1\"></times><ci id=\"S4.SS4.p5.1.m1.1.1.2.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.2\">ğ‘ƒ</ci><apply id=\"S4.SS4.p5.1.m1.1.1.3.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p5.1.m1.1.1.3.1.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p5.1.m1.1.1.3.2.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3.2\">ğ’™</ci><ci id=\"S4.SS4.p5.1.m1.1.1.3.3.cmml\" xref=\"S4.SS4.p5.1.m1.1.1.3.3\">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p5.1.m1.1c\">P\\bm{x}_{i}</annotation></semantics></math> and <math id=\"S4.SS4.p5.2.m2.1\" class=\"ltx_Math\" alttext=\"P\\bm{x}_{i+1}\" display=\"inline\"><semantics id=\"S4.SS4.p5.2.m2.1a\"><mrow id=\"S4.SS4.p5.2.m2.1.1\" xref=\"S4.SS4.p5.2.m2.1.1.cmml\"><mi id=\"S4.SS4.p5.2.m2.1.1.2\" xref=\"S4.SS4.p5.2.m2.1.1.2.cmml\">P</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS4.p5.2.m2.1.1.1\" xref=\"S4.SS4.p5.2.m2.1.1.1.cmml\">â€‹</mo><msub id=\"S4.SS4.p5.2.m2.1.1.3\" xref=\"S4.SS4.p5.2.m2.1.1.3.cmml\"><mi id=\"S4.SS4.p5.2.m2.1.1.3.2\" xref=\"S4.SS4.p5.2.m2.1.1.3.2.cmml\">ğ’™</mi><mrow id=\"S4.SS4.p5.2.m2.1.1.3.3\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.cmml\"><mi id=\"S4.SS4.p5.2.m2.1.1.3.3.2\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.2.cmml\">i</mi><mo id=\"S4.SS4.p5.2.m2.1.1.3.3.1\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.1.cmml\">+</mo><mn id=\"S4.SS4.p5.2.m2.1.1.3.3.3\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.3.cmml\">1</mn></mrow></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p5.2.m2.1b\"><apply id=\"S4.SS4.p5.2.m2.1.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1\"><times id=\"S4.SS4.p5.2.m2.1.1.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.1\"></times><ci id=\"S4.SS4.p5.2.m2.1.1.2.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.2\">ğ‘ƒ</ci><apply id=\"S4.SS4.p5.2.m2.1.1.3.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS4.p5.2.m2.1.1.3.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3\">subscript</csymbol><ci id=\"S4.SS4.p5.2.m2.1.1.3.2.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.2\">ğ’™</ci><apply id=\"S4.SS4.p5.2.m2.1.1.3.3.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3\"><plus id=\"S4.SS4.p5.2.m2.1.1.3.3.1.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.1\"></plus><ci id=\"S4.SS4.p5.2.m2.1.1.3.3.2.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.2\">ğ‘–</ci><cn type=\"integer\" id=\"S4.SS4.p5.2.m2.1.1.3.3.3.cmml\" xref=\"S4.SS4.p5.2.m2.1.1.3.3.3\">1</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p5.2.m2.1c\">P\\bm{x}_{i+1}</annotation></semantics></math>) closer when <math id=\"S4.SS4.p5.3.m3.1\" class=\"ltx_Math\" alttext=\"f\" display=\"inline\"><semantics id=\"S4.SS4.p5.3.m3.1a\"><mi id=\"S4.SS4.p5.3.m3.1.1\" xref=\"S4.SS4.p5.3.m3.1.1.cmml\">f</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.p5.3.m3.1b\"><ci id=\"S4.SS4.p5.3.m3.1.1.cmml\" xref=\"S4.SS4.p5.3.m3.1.1\">ğ‘“</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.p5.3.m3.1c\">f</annotation></semantics></math> is an identity function, according to EquationÂ <a href=\"#S4.E4\" title=\"Equation 4 â€£ 4.4 Computational Toy Model â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. In earlier LLM experiments, this effect can be seen in FigureÂ <a href=\"#S3.F2\" title=\"Figure 2 â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where larger models achieve significantly lower losses within a few gradient update steps during sequential learning. Second, larger networks have more learning capacity to map the features of two adjacent tasks closer. In our linear projection model, anticipatory recovery keeps growing over many epochs, whereas in LLM experiments, the anticipatory effect is already at the strongest within two or three epochs. Moreover, in the toy model all data points are randomly generated, which makes it easier to separate and map their representations according to a temporal structure than real-world data. In contrast, real-world data could require even more representation capacity since data points are noisy and correlated.</p>\n</div>\n</section>\n<section id=\"S4.SS5\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.5 </span>Summary</h3>\n\n<div id=\"S4.SS5.p1\" class=\"ltx_para\">\n<p id=\"S4.SS5.p1.2\" class=\"ltx_p\">In this section, we visualized model weight dynamics with heatmaps and we showed model activations and gradients during cyclic training. We discussed the special temporal structure that is exhibited in these heat maps. We also plotted the pairwise degree of recovery for fine-tuning on document <math id=\"S4.SS5.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S4.SS5.p1.1.m1.1a\"><mi id=\"S4.SS5.p1.1.m1.1.1\" xref=\"S4.SS5.p1.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS5.p1.1.m1.1b\"><ci id=\"S4.SS5.p1.1.m1.1.1.cmml\" xref=\"S4.SS5.p1.1.m1.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS5.p1.1.m1.1c\">i</annotation></semantics></math> and evaluating on document <math id=\"S4.SS5.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"j\" display=\"inline\"><semantics id=\"S4.SS5.p1.2.m2.1a\"><mi id=\"S4.SS5.p1.2.m2.1.1\" xref=\"S4.SS5.p1.2.m2.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS5.p1.2.m2.1b\"><ci id=\"S4.SS5.p1.2.m2.1.1.cmml\" xref=\"S4.SS5.p1.2.m2.1.1\">ğ‘—</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS5.p1.2.m2.1c\">j</annotation></semantics></math>, as well as the change of distance between fine-tuned model weights on different tasks. The results suggest that after we train on a document, the modelâ€™s representation of that document becomes less sensitive to gradient updates on other documents. Finally, we showed a simple toy experiment that demonstrates a similar anticipatory recovery phenomenon in its loss curve, and discuss its connections to neural network training dynamics through the lens of task-specific and task-general parameters. Overall, these results shed some light on the dynamics of cyclic training.</p>\n</div>\n</section>\n</section>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Related Work</h2>\n\n<section id=\"S5.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Cyclic and Structured Training.</h4>\n\n<div id=\"S5.SS0.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px1.p1.1\" class=\"ltx_p\">Prior theoretical works have studied convergence rates, under various assumptions, for the training setup where the data points are shuffled only once and that order is reused for all epochs <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahn etÂ al., <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2020</a>; Gurbuzbalaban etÂ al., <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2019</a>; Mishchenko etÂ al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2020</a>; Safran and Shamir, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. On the empirical side, <cite class=\"ltx_cite ltx_citemacro_citet\">Xu etÂ al. (<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> found that shuffling the data only once in the beginning can achieve a convergence rate comparable to shuffling every epoch.\nThe training setup is equivalent to our cyclic training setup, but our research examines the loss on each task throughout the training cycle and discovers the anticipatory recovery effect. We also extend it to multiple gradient update steps on each data point.</p>\n</div>\n<div id=\"S5.SS0.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px1.p2.1\" class=\"ltx_p\">Our research also relates to the more general topic of learning in structured environments. <cite class=\"ltx_cite ltx_citemacro_citet\">Jones etÂ al. (<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> studied regression and classification tasks with multi-scale temporal structure in the environment characterized by <math id=\"S5.SS0.SSS0.Px1.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"1/f\" display=\"inline\"><semantics id=\"S5.SS0.SSS0.Px1.p2.1.m1.1a\"><mrow id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml\"><mn id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.2\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml\">1</mn><mo id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.1\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml\">/</mo><mi id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.3\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.3.cmml\">f</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS0.SSS0.Px1.p2.1.m1.1b\"><apply id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1\"><divide id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.1\"></divide><cn type=\"integer\" id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.2\">1</cn><ci id=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.3.cmml\" xref=\"S5.SS0.SSS0.Px1.p2.1.m1.1.1.3\">ğ‘“</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS0.SSS0.Px1.p2.1.m1.1c\">1/f</annotation></semantics></math> dynamics. While the cyclic training setting that we study is a more simplified setup than that of <cite class=\"ltx_cite ltx_citemacro_citet\">Jones etÂ al. (<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, we aim at unveiling more insights on applying standard SGD on over-parameterized networks. A potential direction for future work would be to study anticipatory recovery in regimes with richer, hierarchical sequence structure.</p>\n</div>\n</section>\n<section id=\"S5.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Online Learning.</h4>\n\n<div id=\"S5.SS0.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px2.p1.1\" class=\"ltx_p\">Online learning deals with the setting where the tasks come from an online sequential stream. One of the simplest algorithms in online learning is follow-the-leader <cite class=\"ltx_cite ltx_citemacro_citep\">(Hannan, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">1957</a>)</cite>, which stores all previous data from the stream and minimizes the total loss. It has strong performance guarantees but is computationally very expensive, and it also might not be feasible to store all the past data. Many subsequent works have developed cheaper algorithms under different assumptions <cite class=\"ltx_cite ltx_citemacro_citep\">(Zinkevich, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2003</a>; Cesa-Bianchi and Lugosi, <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2006</a>; Shalev-Shwartz etÂ al., <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2012</a>)</cite>. Many recent works also explore the connection between online learning and meta-learning or continual learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Denevi etÂ al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2019a</a>; Finn etÂ al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2019</a>; Denevi etÂ al., <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">2019b</a>; Javed and White, <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2019</a>; Fini etÂ al., <a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">2020</a>; Ren etÂ al., <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2021</a>; Wang etÂ al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</p>\n</div>\n<div id=\"S5.SS0.SSS0.Px2.p2\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px2.p2.1\" class=\"ltx_p\">The cyclic training setting that we explore in this research can be considered as a special case of the online learning setting where the data stream has a cyclic repetition structure. We employ multiple steps of online gradient descent <cite class=\"ltx_cite ltx_citemacro_citep\">(Biehl and Schwarze, <a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">1995</a>)</cite> on each document from the stream and study the training dynamics of over-parameterized neural networks.</p>\n</div>\n</section>\n<section id=\"S5.SS0.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">LLM Emergent Capabilities.</h4>\n\n<div id=\"S5.SS0.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px3.p1.1\" class=\"ltx_p\">Recent advancements in large-scale Transformer networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani etÂ al., <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2017</a>; Devlin etÂ al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> have demonstrated exceptional ability to model long sequence language data. Beyond basic language modeling and downstream task performance, these models have shown emergent behaviors <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei etÂ al., <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2022a</a>)</cite> that appear to manifest only beyond a certain model scaleÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown etÂ al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2020</a>; Wei etÂ al., <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2022b</a>; Ganguli etÂ al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2022</a>; Srivastava etÂ al., <a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</p>\n</div>\n<div id=\"S5.SS0.SSS0.Px3.p2\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px3.p2.1\" class=\"ltx_p\">Related to our research, recent studies reveal that LLMs possess remarkable memorization skills, enabling them to recall news sentences after just a few exposures <cite class=\"ltx_cite ltx_citemacro_citep\">(Biderman etÂ al., <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a>; Carlini etÂ al., <a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2023</a>; Orhan, <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>. However, the sequential learning dynamics behind such memorization have not been thoroughly examined. Our work comprehensively explore the sequential learning setting with cyclic task repetition and demonstrates task anticipation, a new emergent capability of large models.</p>\n</div>\n</section>\n<section id=\"S5.SS0.SSS0.Px4\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Catastrophic Interference.</h4>\n\n<div id=\"S5.SS0.SSS0.Px4.p1\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px4.p1.1\" class=\"ltx_p\">Traditionally, neural networks are trained for a single objective or task. However, when transitioning between tasks sequentially, they often experience â€œcatastrophic interferenceâ€ <cite class=\"ltx_cite ltx_citemacro_citep\">(McCloskey and Cohen, <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">1989</a>)</cite>, marked by a significant drop in performance on previously learned tasks. Continual learningÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(VanÂ de Ven etÂ al., <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, a subset of machine learning, addresses a simplified setup where a model sequentially learns a set of tasks without revision. Numerous algorithms have been proposed to mitigate catastrophic forgetting, focusing on methods like parameter regularization <cite class=\"ltx_cite ltx_citemacro_citep\">(Kirkpatrick etÂ al., <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2017</a>; Zenke etÂ al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2017</a>; Aljundi etÂ al., <a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>, data replay <cite class=\"ltx_cite ltx_citemacro_citep\">(Rebuffi etÂ al., <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2017</a>; Rolnick etÂ al., <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2019</a>; Chaudhry etÂ al., <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>, knowledge distillation <cite class=\"ltx_cite ltx_citemacro_citep\">(Hinton etÂ al., <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2015</a>; Li and Hoiem, <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2017</a>; Buzzega etÂ al., <a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">2020</a>; Madaan etÂ al., <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>, and architectural isolation and expansion <cite class=\"ltx_cite ltx_citemacro_citep\">(Yoon etÂ al., <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2018</a>; Serra etÂ al., <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2018</a>; Gurbuz and Dovrolis, <a href=\"#bib.bib57\" title=\"\" class=\"ltx_ref\">2022</a>; Kang etÂ al., <a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</p>\n</div>\n<div id=\"S5.SS0.SSS0.Px4.p2\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px4.p2.1\" class=\"ltx_p\">More recently, there have been debates over the practicality of continual learning setups. Studies like <cite class=\"ltx_cite ltx_citemacro_citet\">Davidson and Mozer (<a href=\"#bib.bib59\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> have shown that as networks learn more tasks, they improve in learning speed and reduce forgetting. In large models, studies suggest that pre-trained vision classifiers can undertake continual learning with ease, by either freezing or fine-tuning representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Janson etÂ al., <a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2022</a>; Lee etÂ al., <a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">2023</a>; Fini etÂ al., <a href=\"#bib.bib62\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>. In the language domain, research also suggests that LLMs exhibit emerging continual learning capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Scialom etÂ al., <a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">2022</a>; Ke etÂ al., <a href=\"#bib.bib64\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</p>\n</div>\n<div id=\"S5.SS0.SSS0.Px4.p3\" class=\"ltx_para\">\n<p id=\"S5.SS0.SSS0.Px4.p3.1\" class=\"ltx_p\">Unlike prior literature on continual learning, our research uniquely focuses on sequential learning environments with cyclic repetition. Our work extends interleaved training <cite class=\"ltx_cite ltx_citemacro_citep\">(Mayo etÂ al., <a href=\"#bib.bib65\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> to a larger number of tasks, specifically investigating the emergent anticipatory recovery phenomenon in cyclic training.\nThis finding adds to the above literature by demonstrating a new mechanism by which large networks can avoid or recover from catastrophic interference.</p>\n</div>\n</section>\n</section>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Discussion</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p id=\"S6.p1.1\" class=\"ltx_p\">In this work, we explored the training dynamics of overparametrized neural networks, especially LLMs, in sequential cyclic fine-tuning, where a finite set of documents are presented in the same order within each epoch. We demonstrated the remarkable phenomenon of anticipatory recoveryâ€”networks recover from the initial forgetting before seeing the same document again. The effect holds across many different network instances and training hyper-parameters. This phenomenon is a sharp contrast with the well known phenomenon of catastrophic interference, where forgetting increases monotonically as a network is trained on a sequence of different documents.</p>\n</div>\n<div id=\"S6.p2\" class=\"ltx_para\">\n<p id=\"S6.p2.1\" class=\"ltx_p\">We showed that anticipatory recovery occurs only when the network has sufficient width and depth and when it is well fitted to each document before moving to the next. We also discussed the effect of other important factors that influence the degree of recovery, such as the optimizer. Visualizations of model weights, model activations, and gradients exhibit clear temporal structure, which provide insights on the underlying mechanisms of anticipatory recovery.\nOur computational toy model decouples the shared representations from task-specific parameters and reproduces the phenomenon; however, the mathematical foundation of anticipatory recovery requires further investigation. Additionally, the toy model does not explain why the effect is stronger in LLMs in autoregressive tasks than in other types of architecture or learning objectives.</p>\n</div>\n<div id=\"S6.p3\" class=\"ltx_para\">\n<p id=\"S6.p3.1\" class=\"ltx_p\">The cyclic training setup investigated in this work is distinct from the IID training setting assumed in the vast majority of the machine learning literature. It accounts for task repetition and task switching costs, which are critical components of the learning experience of humans and other real world agents. However, our current setup is still highly simplified from real world experience. We have yet to perform experiments on more naturalistic environments with sequential structures.\nFuture research could investigate the emerging training dynamics of neural networks in different types of structured environments,\nsuch as multiscale temporal dynamicsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Jones etÂ al., <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>,\nfrom both theoretical and empirical perspectives.</p>\n</div>\n<div id=\"S6.p4\" class=\"ltx_para\">\n<p id=\"S6.p4.1\" class=\"ltx_p\">Designing good learning curricula that achieve both learning efficiency and a low task-switching cost is a promising future direction that can be applied to more naturalistic training setups. On the modeling side, it will be worthwhile to rethink classic continual learning algorithms through the lens of LLMs and exploit and improve their emergent sequential memorization capability.</p>\n</div>\n</section>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">We thank members of the NYU Agentic Learning AI Lab for helpful discussions. The compute was supported by the NYU High Performance Computing resources, services, and staff expertise.</p>\n</div>\n</section>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Devlin etÂ al. (2019)</span>\n<span class=\"ltx_bibblock\">\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n</span>\n<span class=\"ltx_bibblock\">BERT: Pre-training of deep bidirectional transformers for language understanding.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Brown etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Language models are few-shot learners.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 33:1877â€“1901, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Touvron etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">LLaMA: Open and efficient foundation language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2302.13971</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenAI (2023)</span>\n<span class=\"ltx_bibblock\">\nOpenAI.\n\n</span>\n<span class=\"ltx_bibblock\">Gpt-4 technical report, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hoffmann etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego deÂ Las Casas, LisaÂ Anne Hendricks, Johannes Welbl, Aidan Clark, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Training compute-optimal large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2203.15556</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chowdhery etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian Gehrmann, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">PaLM: Scaling language modeling with pathways.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">Journal of Machine Learning Research</em>, 24(240):1â€“113, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xue etÂ al. (2024)</span>\n<span class=\"ltx_bibblock\">\nFuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.\n\n</span>\n<span class=\"ltx_bibblock\">To repeat or not to repeat: Insights from scaling llm under token-crisis.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib7.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 36, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McCloskey and Cohen (1989)</span>\n<span class=\"ltx_bibblock\">\nMichael McCloskey and NealÂ J Cohen.\n\n</span>\n<span class=\"ltx_bibblock\">Catastrophic interference in connectionist networks: The sequential learning problem.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib8.1.1\" class=\"ltx_emph ltx_font_italic\">Psychology of Learning and Motivation</em>, volumeÂ 24, pages 109â€“165. Elsevier, 1989.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Biderman etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nStella Biderman, Hailey Schoelkopf, QuentinÂ Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan, MohammadÂ Aflah Khan, Shivanshu Purohit, USVSNÂ Sai Prashanth, Edward Raff, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Pythia: A suite for analyzing large language models across training and scaling.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib9.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 2397â€“2430. PMLR, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gao etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">The Pile: An 800gb dataset of diverse text for language modeling.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib10.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2101.00027</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Biderman etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nStella Biderman, Kieran Bicheno, and Leo Gao.\n\n</span>\n<span class=\"ltx_bibblock\">Datasheet for the Pile.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2201.07311</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Nallapati etÂ al. (2016)</span>\n<span class=\"ltx_bibblock\">\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Ã‡aÄŸlar GulÃ§ehre, and Bing Xiang.\n\n</span>\n<span class=\"ltx_bibblock\">Abstractive text summarization using sequence-to-sequence rnns and beyond.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</em>, pages 280â€“290, 2016.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kingma and Ba (2015)</span>\n<span class=\"ltx_bibblock\">\nDiederikÂ P. Kingma and Jimmy Ba.\n\n</span>\n<span class=\"ltx_bibblock\">Adam: A method for stochastic optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\n\n</span>\n<span class=\"ltx_bibblock\">Generative pretraining from pixels.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 1691â€“1703. PMLR, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Krizhevsky etÂ al. (2009)</span>\n<span class=\"ltx_bibblock\">\nAlex Krizhevsky, Geoffrey Hinton, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Learning multiple layers of features from tiny images.\n\n</span>\n<span class=\"ltx_bibblock\">2009.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dosovitskiy etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">An image is worth 16x16 words: Transformers for image recognition at scale.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Simonyan and Zisserman (2015)</span>\n<span class=\"ltx_bibblock\">\nKaren Simonyan and Andrew Zisserman.\n\n</span>\n<span class=\"ltx_bibblock\">Very deep convolutional networks for large-scale image recognition.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Deng etÂ al. (2009)</span>\n<span class=\"ltx_bibblock\">\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiÂ Fei-Fei.\n\n</span>\n<span class=\"ltx_bibblock\">Imagenet: A large-scale hierarchical image database.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">2009 IEEE conference on Computer Vision and Pattern Recognition</em>, pages 248â€“255. IEEE, 2009.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ahn etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nKwangjun Ahn, Chulhee Yun, and Suvrit Sra.\n\n</span>\n<span class=\"ltx_bibblock\">SGD with shuffling: Optimal rates without component convexity and large epoch requirements.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 33:17526â€“17535, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gurbuzbalaban etÂ al. (2019)</span>\n<span class=\"ltx_bibblock\">\nMÂ Gurbuzbalaban, Asu Ozdaglar, and PabloÂ A Parrilo.\n\n</span>\n<span class=\"ltx_bibblock\">Convergence rate of incremental gradient and incremental newton methods.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">SIAM Journal on Optimization</em>, 29(4):2542â€“2565, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mishchenko etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nKonstantin Mishchenko, Ahmed Khaled, and Peter RichtÃ¡rik.\n\n</span>\n<span class=\"ltx_bibblock\">Random reshuffling: Simple analysis with vast improvements.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 33:17309â€“17320, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Safran and Shamir (2020)</span>\n<span class=\"ltx_bibblock\">\nItay Safran and Ohad Shamir.\n\n</span>\n<span class=\"ltx_bibblock\">How good is SGD with random shuffling?\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Conference on Learning Theory</em>, pages 3250â€“3284. PMLR, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xu etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nLijie Xu, Shuang Qiu, Binhang Yuan, Jiawei Jiang, Cedric Renggli, Shaoduo Gan, Kaan Kara, Guoliang Li, JiÂ Liu, Wentao Wu, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Stochastic gradient descent without full data shuffle.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2206.05830</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jones etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nMatt Jones, TylerÂ R Scott, Mengye Ren, Gamaleldin ElSayed, Katherine Hermann, David Mayo, and Michael Mozer.\n\n</span>\n<span class=\"ltx_bibblock\">Learning in temporally structured environments.\n\n</span>\n<span class=\"ltx_bibblock\">2023.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hannan (1957)</span>\n<span class=\"ltx_bibblock\">\nJames Hannan.\n\n</span>\n<span class=\"ltx_bibblock\">Approximation to Bayes risk in repeated play.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Contributions to the Theory of Games</em>, 3:97â€“139, 1957.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zinkevich (2003)</span>\n<span class=\"ltx_bibblock\">\nMartin Zinkevich.\n\n</span>\n<span class=\"ltx_bibblock\">Online convex programming and generalized infinitesimal gradient ascent.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib26.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 928â€“936, 2003.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cesa-Bianchi and Lugosi (2006)</span>\n<span class=\"ltx_bibblock\">\nNicolo Cesa-Bianchi and GÃ¡bor Lugosi.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">Prediction, learning, and games</em>.\n\n</span>\n<span class=\"ltx_bibblock\">Cambridge University Press, 2006.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shalev-Shwartz etÂ al. (2012)</span>\n<span class=\"ltx_bibblock\">\nShai Shalev-Shwartz etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Online learning and online convex optimization.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">Foundations and TrendsÂ® in Machine Learning</em>, 4(2):107â€“194, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Denevi etÂ al. (2019a)</span>\n<span class=\"ltx_bibblock\">\nGiulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil.\n\n</span>\n<span class=\"ltx_bibblock\">Learning-to-learn stochastic gradient descent with biased regularization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 1566â€“1575. PMLR, 2019a.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Finn etÂ al. (2019)</span>\n<span class=\"ltx_bibblock\">\nChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.\n\n</span>\n<span class=\"ltx_bibblock\">Online meta-learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 1920â€“1930. PMLR, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Denevi etÂ al. (2019b)</span>\n<span class=\"ltx_bibblock\">\nGiulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil.\n\n</span>\n<span class=\"ltx_bibblock\">Online-within-online meta-learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 32, 2019b.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Javed and White (2019)</span>\n<span class=\"ltx_bibblock\">\nKhurram Javed and Martha White.\n\n</span>\n<span class=\"ltx_bibblock\">Meta-learning representations for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 32, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fini etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nEnrico Fini, StÃ©phane Lathuiliere, Enver Sangineto, Moin Nabi, and Elisa Ricci.\n\n</span>\n<span class=\"ltx_bibblock\">Online continual learning under extreme memory constraints.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib33.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the European Conference on Computer Vision</em>, pages 720â€“735. Springer, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ren etÂ al. (2021)</span>\n<span class=\"ltx_bibblock\">\nMengye Ren, MichaelÂ Louis Iuzzolino, MichaelÂ Curtis Mozer, and RichardÂ S. Zemel.\n\n</span>\n<span class=\"ltx_bibblock\">Wandering within a world: Online contextualized few-shot learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang etÂ al. (2021)</span>\n<span class=\"ltx_bibblock\">\nJianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav Gupta.\n\n</span>\n<span class=\"ltx_bibblock\">Wanderlust: Online continual object detection in the real world.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib35.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 10829â€“10838, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Biehl and Schwarze (1995)</span>\n<span class=\"ltx_bibblock\">\nMichael Biehl and Holm Schwarze.\n\n</span>\n<span class=\"ltx_bibblock\">Learning by on-line gradient descent.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">Journal of Physics A: Mathematical and general</em>, 28(3):643, 1995.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vaswani etÂ al. (2017)</span>\n<span class=\"ltx_bibblock\">\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.\n\n</span>\n<span class=\"ltx_bibblock\">Attention is all you need.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 30, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei etÂ al. (2022a)</span>\n<span class=\"ltx_bibblock\">\nJason Wei, YiÂ Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Emergent abilities of large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">Transactions on Machine Learning Research</em>, 2022a.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wei etÂ al. (2022b)</span>\n<span class=\"ltx_bibblock\">\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V Le, Denny Zhou, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Chain-of-thought prompting elicits reasoning in large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 35:24824â€“24837, 2022b.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ganguli etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Predictability and surprise in large generative models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 1747â€“1764, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Srivastava etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu AwalÂ Md Shoeb, Abubakar Abid, Adam Fisch, AdamÂ R Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2206.04615</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Carlini etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian TramÃ¨r, and Chiyuan Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Quantifying memorization across neural language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib42.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Orhan (2023)</span>\n<span class=\"ltx_bibblock\">\nAÂ Emin Orhan.\n\n</span>\n<span class=\"ltx_bibblock\">Recognition, recall, and retention of few-shot memories in large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib43.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2303.17557</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">VanÂ de Ven etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nGidoÂ M VanÂ de Ven, HavaÂ T Siegelmann, and AndreasÂ S Tolias.\n\n</span>\n<span class=\"ltx_bibblock\">Brain-inspired replay for continual learning with artificial neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib44.1.1\" class=\"ltx_emph ltx_font_italic\">Nature Communications</em>, 11(1):4069, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kirkpatrick etÂ al. (2017)</span>\n<span class=\"ltx_bibblock\">\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, AndreiÂ A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Overcoming catastrophic forgetting in neural networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib45.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the National Academy of Sciences</em>, 114(13):3521â€“3526, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zenke etÂ al. (2017)</span>\n<span class=\"ltx_bibblock\">\nFriedemann Zenke, Ben Poole, and Surya Ganguli.\n\n</span>\n<span class=\"ltx_bibblock\">Continual learning through synaptic intelligence.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib46.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 3987â€“3995. PMLR, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Aljundi etÂ al. (2018)</span>\n<span class=\"ltx_bibblock\">\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.\n\n</span>\n<span class=\"ltx_bibblock\">Memory aware synapses: Learning what (not) to forget.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib47.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the European Conference on Computer Vision</em>, pages 139â€“154, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rebuffi etÂ al. (2017)</span>\n<span class=\"ltx_bibblock\">\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and ChristophÂ H Lampert.\n\n</span>\n<span class=\"ltx_bibblock\">iCaRL: Incremental classifier and representation learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib48.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</em>, pages 2001â€“2010, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rolnick etÂ al. (2019)</span>\n<span class=\"ltx_bibblock\">\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne.\n\n</span>\n<span class=\"ltx_bibblock\">Experience replay for continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib49.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 32, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chaudhry etÂ al. (2019)</span>\n<span class=\"ltx_bibblock\">\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, PuneetÂ K Dokania, PhilipÂ HS Torr, and Marcâ€™Aurelio Ranzato.\n\n</span>\n<span class=\"ltx_bibblock\">On tiny episodic memories in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib50.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1902.10486</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hinton etÂ al. (2015)</span>\n<span class=\"ltx_bibblock\">\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean.\n\n</span>\n<span class=\"ltx_bibblock\">Distilling the knowledge in a neural network.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib51.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1503.02531</em>, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib52\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li and Hoiem (2017)</span>\n<span class=\"ltx_bibblock\">\nZhizhong Li and Derek Hoiem.\n\n</span>\n<span class=\"ltx_bibblock\">Learning without forgetting.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib52.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 40(12):2935â€“2947, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib53\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Buzzega etÂ al. (2020)</span>\n<span class=\"ltx_bibblock\">\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.\n\n</span>\n<span class=\"ltx_bibblock\">Dark experience for general continual learning: a strong, simple baseline.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib53.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, 33:15920â€“15930, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib54\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Madaan etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nDivyam Madaan, Hongxu Yin, Wonmin Byeon, Jan Kautz, and Pavlo Molchanov.\n\n</span>\n<span class=\"ltx_bibblock\">Heterogeneous continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib54.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 15985â€“15995, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib55\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yoon etÂ al. (2018)</span>\n<span class=\"ltx_bibblock\">\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and SungÂ Ju Hwang.\n\n</span>\n<span class=\"ltx_bibblock\">Lifelong learning with dynamically expandable networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib55.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib56\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Serra etÂ al. (2018)</span>\n<span class=\"ltx_bibblock\">\nJoan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.\n\n</span>\n<span class=\"ltx_bibblock\">Overcoming catastrophic forgetting with hard attention to the task.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib56.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 4548â€“4557. PMLR, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib57\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Gurbuz and Dovrolis (2022)</span>\n<span class=\"ltx_bibblock\">\nMustafaÂ B Gurbuz and Constantine Dovrolis.\n\n</span>\n<span class=\"ltx_bibblock\">NISPA: Neuro-inspired stability-plasticity adaptation for continual learning in sparse networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib57.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 8157â€“8174. PMLR, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib58\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kang etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nHaeyong Kang, Rusty JohnÂ Lloyd Mina, Sultan RizkyÂ Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, SungÂ Ju Hwang, and ChangÂ D Yoo.\n\n</span>\n<span class=\"ltx_bibblock\">Forget-free continual learning with winning subnetworks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib58.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages 10734â€“10750. PMLR, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib59\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Davidson and Mozer (2020)</span>\n<span class=\"ltx_bibblock\">\nGuy Davidson and MichaelÂ C Mozer.\n\n</span>\n<span class=\"ltx_bibblock\">Sequential mastery of multiple visual tasks: Networks naturally learn to learn and forget to forget.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib59.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</em>, pages 9282â€“9293, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib60\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Janson etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nPaul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.\n\n</span>\n<span class=\"ltx_bibblock\">A simple baseline that questions the use of pretrained-models in continual learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib60.1.1\" class=\"ltx_emph ltx_font_italic\">NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib61\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lee etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nKuan-Ying Lee, Yuanyi Zhong, and Yu-Xiong Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Do pre-trained models benefit equally in continual learning?\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib61.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pages 6485â€“6493, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib62\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Fini etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nEnrico Fini, Victor GÂ Turrisi DaÂ Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised models are continual learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib62.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 9621â€“9630, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib63\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Scialom etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nThomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan.\n\n</span>\n<span class=\"ltx_bibblock\">Fine-tuned language models are continual learners.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib63.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 6107â€“6122, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib64\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ke etÂ al. (2022)</span>\n<span class=\"ltx_bibblock\">\nZixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.\n\n</span>\n<span class=\"ltx_bibblock\">Continual pre-training of language models.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib64.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib65\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mayo etÂ al. (2023)</span>\n<span class=\"ltx_bibblock\">\nDavid Mayo, Tyler Scott, Mengye Ren, Gamaleldin Elsayed, Katherine Hermann, Matt Jones, and Michael Mozer.\n\n</span>\n<span class=\"ltx_bibblock\">Multitask learning via interleaving: A neural network investigation.\n\n</span>\n<span class=\"ltx_bibblock\">In M.Â Goldwater, F.Â K. Anggoro, B.Â K. Hayes, and D.Â C. Ong, editors, <em id=\"bib.bib65.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 45th Annual Conference of the Cognitive Science Society</em>, volumeÂ 45, 2023.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Additional Experiment Setups</h2>\n\n<section id=\"A1.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Causal Image Modeling Experiments</h3>\n\n<section id=\"A1.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Models</h4>\n\n<div id=\"A1.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"A1.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">Image GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen etÂ al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> is a GPT-2-like model trained to predict the next pixel value in an image. It is pre-trained on the Imagenet dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2009</a>)</cite> resized to 32x32. The Image GPT authors provide three pre-trained models of different sizes. In our experiments, we use the Image GPT-small and Image GPT-medium models.</p>\n</div>\n</section>\n<section id=\"A1.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Datasets</h4>\n\n<div id=\"A1.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"A1.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">We use the CIFAR-10 <cite class=\"ltx_cite ltx_citemacro_citep\">(Krizhevsky etÂ al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2009</a>)</cite> dataset for fine-tuning. For tokenization, the pixel RGB values are categorized into 512 pre-determined clusters with the nearest-neighbor classifier, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Chen etÂ al. (<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. After pre-processing, each image is transformed to a sequence of length 1024, with code book of size 512.</p>\n</div>\n</section>\n<section id=\"A1.SS1.SSS0.Px3\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Training Setup</h4>\n\n<div id=\"A1.SS1.SSS0.Px3.p1\" class=\"ltx_para\">\n<p id=\"A1.SS1.SSS0.Px3.p1.4\" class=\"ltx_p\">We did not manage to sequentially fine-tune the model stably with the dropout layers, so the dropout layers are turned off during the Image GPT fine-tuning experiments. We use the Adam optimizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma and Ba, <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2015</a>)</cite> with learning rate <math id=\"A1.SS1.SSS0.Px3.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics id=\"A1.SS1.SSS0.Px3.p1.1.m1.1a\"><mn id=\"A1.SS1.SSS0.Px3.p1.1.m1.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.1.m1.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS1.SSS0.Px3.p1.1.m1.1b\"><cn type=\"float\" id=\"A1.SS1.SSS0.Px3.p1.1.m1.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.1.m1.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS1.SSS0.Px3.p1.1.m1.1c\">0.001</annotation></semantics></math>. The default hyper-parameters in the experiments are <math id=\"A1.SS1.SSS0.Px3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"T=25\" display=\"inline\"><semantics id=\"A1.SS1.SSS0.Px3.p1.2.m2.1a\"><mrow id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.cmml\"><mi id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.2\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml\">T</mi><mo id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.3\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS1.SSS0.Px3.p1.2.m2.1b\"><apply id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1\"><eq id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.1\"></eq><ci id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.2\">ğ‘‡</ci><cn type=\"integer\" id=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.2.m2.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS1.SSS0.Px3.p1.2.m2.1c\">T=25</annotation></semantics></math> images, <math id=\"A1.SS1.SSS0.Px3.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"M=10\" display=\"inline\"><semantics id=\"A1.SS1.SSS0.Px3.p1.3.m3.1a\"><mrow id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.cmml\"><mi id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.2\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml\">M</mi><mo id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml\">=</mo><mn id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.3\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS1.SSS0.Px3.p1.3.m3.1b\"><apply id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1\"><eq id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.1\"></eq><ci id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.3.m3.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS1.SSS0.Px3.p1.3.m3.1c\">M=10</annotation></semantics></math> gradient update steps, <math id=\"A1.SS1.SSS0.Px3.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"E=5\" display=\"inline\"><semantics id=\"A1.SS1.SSS0.Px3.p1.4.m4.1a\"><mrow id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.cmml\"><mi id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.2\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.2.cmml\">E</mi><mo id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.1\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.1.cmml\">=</mo><mn id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.3\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS1.SSS0.Px3.p1.4.m4.1b\"><apply id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1\"><eq id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.1.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.1\"></eq><ci id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.2.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.2\">ğ¸</ci><cn type=\"integer\" id=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml\" xref=\"A1.SS1.SSS0.Px3.p1.4.m4.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS1.SSS0.Px3.p1.4.m4.1c\">E=5</annotation></semantics></math> epochs. Same as the LLM experiments, we use the average cross-entropy loss as our evaluation metric.</p>\n</div>\n</section>\n</section>\n<section id=\"A1.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Image Classification Experiments</h3>\n\n<div id=\"A1.SS2.p1\" class=\"ltx_para\">\n<p id=\"A1.SS2.p1.2\" class=\"ltx_p\">The images are resized to 256x256 followed by a center crop of 224x224. We use the Adam optimizer with learning rate <math id=\"A1.SS2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.0001\" display=\"inline\"><semantics id=\"A1.SS2.p1.1.m1.1a\"><mn id=\"A1.SS2.p1.1.m1.1.1\" xref=\"A1.SS2.p1.1.m1.1.1.cmml\">0.0001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS2.p1.1.m1.1b\"><cn type=\"float\" id=\"A1.SS2.p1.1.m1.1.1.cmml\" xref=\"A1.SS2.p1.1.m1.1.1\">0.0001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS2.p1.1.m1.1c\">0.0001</annotation></semantics></math> for <math id=\"A1.SS2.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"M=10\" display=\"inline\"><semantics id=\"A1.SS2.p1.2.m2.1a\"><mrow id=\"A1.SS2.p1.2.m2.1.1\" xref=\"A1.SS2.p1.2.m2.1.1.cmml\"><mi id=\"A1.SS2.p1.2.m2.1.1.2\" xref=\"A1.SS2.p1.2.m2.1.1.2.cmml\">M</mi><mo id=\"A1.SS2.p1.2.m2.1.1.1\" xref=\"A1.SS2.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"A1.SS2.p1.2.m2.1.1.3\" xref=\"A1.SS2.p1.2.m2.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS2.p1.2.m2.1b\"><apply id=\"A1.SS2.p1.2.m2.1.1.cmml\" xref=\"A1.SS2.p1.2.m2.1.1\"><eq id=\"A1.SS2.p1.2.m2.1.1.1.cmml\" xref=\"A1.SS2.p1.2.m2.1.1.1\"></eq><ci id=\"A1.SS2.p1.2.m2.1.1.2.cmml\" xref=\"A1.SS2.p1.2.m2.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"A1.SS2.p1.2.m2.1.1.3.cmml\" xref=\"A1.SS2.p1.2.m2.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS2.p1.2.m2.1c\">M=10</annotation></semantics></math> gradient steps on each batch of images.</p>\n</div>\n</section>\n<section id=\"A1.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>Computational Toy Model</h3>\n\n<div id=\"A1.SS3.p1\" class=\"ltx_para\">\n<p id=\"A1.SS3.p1.14\" class=\"ltx_p\">For Figure <a href=\"#S4.F10.sf1\" title=\"Figure 10(a) â€£ Figure 10 â€£ 4.1 Temporal Structure of Gradients â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10(a)</span></a>, we pick <math id=\"A1.SS3.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{w}\" display=\"inline\"><semantics id=\"A1.SS3.p1.1.m1.1a\"><mrow id=\"A1.SS3.p1.1.m1.1.2\" xref=\"A1.SS3.p1.1.m1.1.2.cmml\"><mrow id=\"A1.SS3.p1.1.m1.1.2.2\" xref=\"A1.SS3.p1.1.m1.1.2.2.cmml\"><msub id=\"A1.SS3.p1.1.m1.1.2.2.2\" xref=\"A1.SS3.p1.1.m1.1.2.2.2.cmml\"><mi id=\"A1.SS3.p1.1.m1.1.2.2.2.2\" xref=\"A1.SS3.p1.1.m1.1.2.2.2.2.cmml\">f</mi><mi id=\"A1.SS3.p1.1.m1.1.2.2.2.3\" xref=\"A1.SS3.p1.1.m1.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.SS3.p1.1.m1.1.2.2.1\" xref=\"A1.SS3.p1.1.m1.1.2.2.1.cmml\">â€‹</mo><mrow id=\"A1.SS3.p1.1.m1.1.2.2.3.2\" xref=\"A1.SS3.p1.1.m1.1.2.2.cmml\"><mo stretchy=\"false\" id=\"A1.SS3.p1.1.m1.1.2.2.3.2.1\" xref=\"A1.SS3.p1.1.m1.1.2.2.cmml\">(</mo><mi id=\"A1.SS3.p1.1.m1.1.1\" xref=\"A1.SS3.p1.1.m1.1.1.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"A1.SS3.p1.1.m1.1.2.2.3.2.2\" xref=\"A1.SS3.p1.1.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"A1.SS3.p1.1.m1.1.2.1\" xref=\"A1.SS3.p1.1.m1.1.2.1.cmml\">=</mo><mi id=\"A1.SS3.p1.1.m1.1.2.3\" xref=\"A1.SS3.p1.1.m1.1.2.3.cmml\">ğ’˜</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.1.m1.1b\"><apply id=\"A1.SS3.p1.1.m1.1.2.cmml\" xref=\"A1.SS3.p1.1.m1.1.2\"><eq id=\"A1.SS3.p1.1.m1.1.2.1.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.1\"></eq><apply id=\"A1.SS3.p1.1.m1.1.2.2.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.2\"><times id=\"A1.SS3.p1.1.m1.1.2.2.1.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.2.1\"></times><apply id=\"A1.SS3.p1.1.m1.1.2.2.2.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.1.m1.1.2.2.2.1.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.2.2\">subscript</csymbol><ci id=\"A1.SS3.p1.1.m1.1.2.2.2.2.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.2.2.2\">ğ‘“</ci><ci id=\"A1.SS3.p1.1.m1.1.2.2.2.3.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"A1.SS3.p1.1.m1.1.1.cmml\" xref=\"A1.SS3.p1.1.m1.1.1\">ğ’˜</ci></apply><ci id=\"A1.SS3.p1.1.m1.1.2.3.cmml\" xref=\"A1.SS3.p1.1.m1.1.2.3\">ğ’˜</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.1.m1.1c\">f_{i}(\\bm{w})=\\bm{w}</annotation></semantics></math>, and each data point <math id=\"A1.SS3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"A1.SS3.p1.2.m2.1a\"><msub id=\"A1.SS3.p1.2.m2.1.1\" xref=\"A1.SS3.p1.2.m2.1.1.cmml\"><mi id=\"A1.SS3.p1.2.m2.1.1.2\" xref=\"A1.SS3.p1.2.m2.1.1.2.cmml\">ğ’™</mi><mi id=\"A1.SS3.p1.2.m2.1.1.3\" xref=\"A1.SS3.p1.2.m2.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.2.m2.1b\"><apply id=\"A1.SS3.p1.2.m2.1.1.cmml\" xref=\"A1.SS3.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.2.m2.1.1.1.cmml\" xref=\"A1.SS3.p1.2.m2.1.1\">subscript</csymbol><ci id=\"A1.SS3.p1.2.m2.1.1.2.cmml\" xref=\"A1.SS3.p1.2.m2.1.1.2\">ğ’™</ci><ci id=\"A1.SS3.p1.2.m2.1.1.3.cmml\" xref=\"A1.SS3.p1.2.m2.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.2.m2.1c\">\\bm{x}_{i}</annotation></semantics></math> and <math id=\"A1.SS3.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{w}\" display=\"inline\"><semantics id=\"A1.SS3.p1.3.m3.1a\"><mi id=\"A1.SS3.p1.3.m3.1.1\" xref=\"A1.SS3.p1.3.m3.1.1.cmml\">ğ’˜</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.3.m3.1b\"><ci id=\"A1.SS3.p1.3.m3.1.1.cmml\" xref=\"A1.SS3.p1.3.m3.1.1\">ğ’˜</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.3.m3.1c\">\\bm{w}</annotation></semantics></math> is a vector of length <math id=\"A1.SS3.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"N=M=1000\" display=\"inline\"><semantics id=\"A1.SS3.p1.4.m4.1a\"><mrow id=\"A1.SS3.p1.4.m4.1.1\" xref=\"A1.SS3.p1.4.m4.1.1.cmml\"><mi id=\"A1.SS3.p1.4.m4.1.1.2\" xref=\"A1.SS3.p1.4.m4.1.1.2.cmml\">N</mi><mo id=\"A1.SS3.p1.4.m4.1.1.3\" xref=\"A1.SS3.p1.4.m4.1.1.3.cmml\">=</mo><mi id=\"A1.SS3.p1.4.m4.1.1.4\" xref=\"A1.SS3.p1.4.m4.1.1.4.cmml\">M</mi><mo id=\"A1.SS3.p1.4.m4.1.1.5\" xref=\"A1.SS3.p1.4.m4.1.1.5.cmml\">=</mo><mn id=\"A1.SS3.p1.4.m4.1.1.6\" xref=\"A1.SS3.p1.4.m4.1.1.6.cmml\">1000</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.4.m4.1b\"><apply id=\"A1.SS3.p1.4.m4.1.1.cmml\" xref=\"A1.SS3.p1.4.m4.1.1\"><and id=\"A1.SS3.p1.4.m4.1.1a.cmml\" xref=\"A1.SS3.p1.4.m4.1.1\"></and><apply id=\"A1.SS3.p1.4.m4.1.1b.cmml\" xref=\"A1.SS3.p1.4.m4.1.1\"><eq id=\"A1.SS3.p1.4.m4.1.1.3.cmml\" xref=\"A1.SS3.p1.4.m4.1.1.3\"></eq><ci id=\"A1.SS3.p1.4.m4.1.1.2.cmml\" xref=\"A1.SS3.p1.4.m4.1.1.2\">ğ‘</ci><ci id=\"A1.SS3.p1.4.m4.1.1.4.cmml\" xref=\"A1.SS3.p1.4.m4.1.1.4\">ğ‘€</ci></apply><apply id=\"A1.SS3.p1.4.m4.1.1c.cmml\" xref=\"A1.SS3.p1.4.m4.1.1\"><eq id=\"A1.SS3.p1.4.m4.1.1.5.cmml\" xref=\"A1.SS3.p1.4.m4.1.1.5\"></eq><share href=\"#A1.SS3.p1.4.m4.1.1.4.cmml\" id=\"A1.SS3.p1.4.m4.1.1d.cmml\" xref=\"A1.SS3.p1.4.m4.1.1\"></share><cn type=\"integer\" id=\"A1.SS3.p1.4.m4.1.1.6.cmml\" xref=\"A1.SS3.p1.4.m4.1.1.6\">1000</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.4.m4.1c\">N=M=1000</annotation></semantics></math>. We have <math id=\"A1.SS3.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"T=25\" display=\"inline\"><semantics id=\"A1.SS3.p1.5.m5.1a\"><mrow id=\"A1.SS3.p1.5.m5.1.1\" xref=\"A1.SS3.p1.5.m5.1.1.cmml\"><mi id=\"A1.SS3.p1.5.m5.1.1.2\" xref=\"A1.SS3.p1.5.m5.1.1.2.cmml\">T</mi><mo id=\"A1.SS3.p1.5.m5.1.1.1\" xref=\"A1.SS3.p1.5.m5.1.1.1.cmml\">=</mo><mn id=\"A1.SS3.p1.5.m5.1.1.3\" xref=\"A1.SS3.p1.5.m5.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.5.m5.1b\"><apply id=\"A1.SS3.p1.5.m5.1.1.cmml\" xref=\"A1.SS3.p1.5.m5.1.1\"><eq id=\"A1.SS3.p1.5.m5.1.1.1.cmml\" xref=\"A1.SS3.p1.5.m5.1.1.1\"></eq><ci id=\"A1.SS3.p1.5.m5.1.1.2.cmml\" xref=\"A1.SS3.p1.5.m5.1.1.2\">ğ‘‡</ci><cn type=\"integer\" id=\"A1.SS3.p1.5.m5.1.1.3.cmml\" xref=\"A1.SS3.p1.5.m5.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.5.m5.1c\">T=25</annotation></semantics></math> data points and use the vanilla gradient descent optimizer with learning rate 0.01. The projection matrix is initialized with every entry sampled independently from <math id=\"A1.SS3.p1.6.m6.2\" class=\"ltx_Math\" alttext=\"\\mathcal{N}(0,1/N^{2})\" display=\"inline\"><semantics id=\"A1.SS3.p1.6.m6.2a\"><mrow id=\"A1.SS3.p1.6.m6.2.2\" xref=\"A1.SS3.p1.6.m6.2.2.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"A1.SS3.p1.6.m6.2.2.3\" xref=\"A1.SS3.p1.6.m6.2.2.3.cmml\">ğ’©</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.SS3.p1.6.m6.2.2.2\" xref=\"A1.SS3.p1.6.m6.2.2.2.cmml\">â€‹</mo><mrow id=\"A1.SS3.p1.6.m6.2.2.1.1\" xref=\"A1.SS3.p1.6.m6.2.2.1.2.cmml\"><mo stretchy=\"false\" id=\"A1.SS3.p1.6.m6.2.2.1.1.2\" xref=\"A1.SS3.p1.6.m6.2.2.1.2.cmml\">(</mo><mn id=\"A1.SS3.p1.6.m6.1.1\" xref=\"A1.SS3.p1.6.m6.1.1.cmml\">0</mn><mo id=\"A1.SS3.p1.6.m6.2.2.1.1.3\" xref=\"A1.SS3.p1.6.m6.2.2.1.2.cmml\">,</mo><mrow id=\"A1.SS3.p1.6.m6.2.2.1.1.1\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.cmml\"><mn id=\"A1.SS3.p1.6.m6.2.2.1.1.1.2\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.2.cmml\">1</mn><mo id=\"A1.SS3.p1.6.m6.2.2.1.1.1.1\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.1.cmml\">/</mo><msup id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.cmml\"><mi id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.2\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.2.cmml\">N</mi><mn id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.3\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.3.cmml\">2</mn></msup></mrow><mo stretchy=\"false\" id=\"A1.SS3.p1.6.m6.2.2.1.1.4\" xref=\"A1.SS3.p1.6.m6.2.2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.6.m6.2b\"><apply id=\"A1.SS3.p1.6.m6.2.2.cmml\" xref=\"A1.SS3.p1.6.m6.2.2\"><times id=\"A1.SS3.p1.6.m6.2.2.2.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.2\"></times><ci id=\"A1.SS3.p1.6.m6.2.2.3.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.3\">ğ’©</ci><interval closure=\"open\" id=\"A1.SS3.p1.6.m6.2.2.1.2.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1\"><cn type=\"integer\" id=\"A1.SS3.p1.6.m6.1.1.cmml\" xref=\"A1.SS3.p1.6.m6.1.1\">0</cn><apply id=\"A1.SS3.p1.6.m6.2.2.1.1.1.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1\"><divide id=\"A1.SS3.p1.6.m6.2.2.1.1.1.1.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.1\"></divide><cn type=\"integer\" id=\"A1.SS3.p1.6.m6.2.2.1.1.1.2.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.2\">1</cn><apply id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.1.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3\">superscript</csymbol><ci id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.2.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.2\">ğ‘</ci><cn type=\"integer\" id=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.3.cmml\" xref=\"A1.SS3.p1.6.m6.2.2.1.1.1.3.3\">2</cn></apply></apply></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.6.m6.2c\">\\mathcal{N}(0,1/N^{2})</annotation></semantics></math>. Each entry of the data points <math id=\"A1.SS3.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"A1.SS3.p1.7.m7.1a\"><msub id=\"A1.SS3.p1.7.m7.1.1\" xref=\"A1.SS3.p1.7.m7.1.1.cmml\"><mi id=\"A1.SS3.p1.7.m7.1.1.2\" xref=\"A1.SS3.p1.7.m7.1.1.2.cmml\">ğ’™</mi><mi id=\"A1.SS3.p1.7.m7.1.1.3\" xref=\"A1.SS3.p1.7.m7.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.7.m7.1b\"><apply id=\"A1.SS3.p1.7.m7.1.1.cmml\" xref=\"A1.SS3.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.7.m7.1.1.1.cmml\" xref=\"A1.SS3.p1.7.m7.1.1\">subscript</csymbol><ci id=\"A1.SS3.p1.7.m7.1.1.2.cmml\" xref=\"A1.SS3.p1.7.m7.1.1.2\">ğ’™</ci><ci id=\"A1.SS3.p1.7.m7.1.1.3.cmml\" xref=\"A1.SS3.p1.7.m7.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.7.m7.1c\">\\bm{x}_{i}</annotation></semantics></math> and <math id=\"A1.SS3.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"\\bm{w}\" display=\"inline\"><semantics id=\"A1.SS3.p1.8.m8.1a\"><mi id=\"A1.SS3.p1.8.m8.1.1\" xref=\"A1.SS3.p1.8.m8.1.1.cmml\">ğ’˜</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.8.m8.1b\"><ci id=\"A1.SS3.p1.8.m8.1.1.cmml\" xref=\"A1.SS3.p1.8.m8.1.1\">ğ’˜</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.8.m8.1c\">\\bm{w}</annotation></semantics></math> is sampled independently from <math id=\"A1.SS3.p1.9.m9.2\" class=\"ltx_Math\" alttext=\"{\\rm Unif}(-1,1)\" display=\"inline\"><semantics id=\"A1.SS3.p1.9.m9.2a\"><mrow id=\"A1.SS3.p1.9.m9.2.2\" xref=\"A1.SS3.p1.9.m9.2.2.cmml\"><mi id=\"A1.SS3.p1.9.m9.2.2.3\" xref=\"A1.SS3.p1.9.m9.2.2.3.cmml\">Unif</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.SS3.p1.9.m9.2.2.2\" xref=\"A1.SS3.p1.9.m9.2.2.2.cmml\">â€‹</mo><mrow id=\"A1.SS3.p1.9.m9.2.2.1.1\" xref=\"A1.SS3.p1.9.m9.2.2.1.2.cmml\"><mo stretchy=\"false\" id=\"A1.SS3.p1.9.m9.2.2.1.1.2\" xref=\"A1.SS3.p1.9.m9.2.2.1.2.cmml\">(</mo><mrow id=\"A1.SS3.p1.9.m9.2.2.1.1.1\" xref=\"A1.SS3.p1.9.m9.2.2.1.1.1.cmml\"><mo id=\"A1.SS3.p1.9.m9.2.2.1.1.1a\" xref=\"A1.SS3.p1.9.m9.2.2.1.1.1.cmml\">âˆ’</mo><mn id=\"A1.SS3.p1.9.m9.2.2.1.1.1.2\" xref=\"A1.SS3.p1.9.m9.2.2.1.1.1.2.cmml\">1</mn></mrow><mo id=\"A1.SS3.p1.9.m9.2.2.1.1.3\" xref=\"A1.SS3.p1.9.m9.2.2.1.2.cmml\">,</mo><mn id=\"A1.SS3.p1.9.m9.1.1\" xref=\"A1.SS3.p1.9.m9.1.1.cmml\">1</mn><mo stretchy=\"false\" id=\"A1.SS3.p1.9.m9.2.2.1.1.4\" xref=\"A1.SS3.p1.9.m9.2.2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.9.m9.2b\"><apply id=\"A1.SS3.p1.9.m9.2.2.cmml\" xref=\"A1.SS3.p1.9.m9.2.2\"><times id=\"A1.SS3.p1.9.m9.2.2.2.cmml\" xref=\"A1.SS3.p1.9.m9.2.2.2\"></times><ci id=\"A1.SS3.p1.9.m9.2.2.3.cmml\" xref=\"A1.SS3.p1.9.m9.2.2.3\">Unif</ci><interval closure=\"open\" id=\"A1.SS3.p1.9.m9.2.2.1.2.cmml\" xref=\"A1.SS3.p1.9.m9.2.2.1.1\"><apply id=\"A1.SS3.p1.9.m9.2.2.1.1.1.cmml\" xref=\"A1.SS3.p1.9.m9.2.2.1.1.1\"><minus id=\"A1.SS3.p1.9.m9.2.2.1.1.1.1.cmml\" xref=\"A1.SS3.p1.9.m9.2.2.1.1.1\"></minus><cn type=\"integer\" id=\"A1.SS3.p1.9.m9.2.2.1.1.1.2.cmml\" xref=\"A1.SS3.p1.9.m9.2.2.1.1.1.2\">1</cn></apply><cn type=\"integer\" id=\"A1.SS3.p1.9.m9.1.1.cmml\" xref=\"A1.SS3.p1.9.m9.1.1\">1</cn></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.9.m9.2c\">{\\rm Unif}(-1,1)</annotation></semantics></math>. For Figure <a href=\"#S4.F10.sf2\" title=\"Figure 10(b) â€£ Figure 10 â€£ 4.1 Temporal Structure of Gradients â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10(b)</span></a>, we pick <math id=\"A1.SS3.p1.10.m10.1\" class=\"ltx_Math\" alttext=\"f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}\" display=\"inline\"><semantics id=\"A1.SS3.p1.10.m10.1a\"><mrow id=\"A1.SS3.p1.10.m10.1.2\" xref=\"A1.SS3.p1.10.m10.1.2.cmml\"><mrow id=\"A1.SS3.p1.10.m10.1.2.2\" xref=\"A1.SS3.p1.10.m10.1.2.2.cmml\"><msub id=\"A1.SS3.p1.10.m10.1.2.2.2\" xref=\"A1.SS3.p1.10.m10.1.2.2.2.cmml\"><mi id=\"A1.SS3.p1.10.m10.1.2.2.2.2\" xref=\"A1.SS3.p1.10.m10.1.2.2.2.2.cmml\">f</mi><mi id=\"A1.SS3.p1.10.m10.1.2.2.2.3\" xref=\"A1.SS3.p1.10.m10.1.2.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.SS3.p1.10.m10.1.2.2.1\" xref=\"A1.SS3.p1.10.m10.1.2.2.1.cmml\">â€‹</mo><mrow id=\"A1.SS3.p1.10.m10.1.2.2.3.2\" xref=\"A1.SS3.p1.10.m10.1.2.2.cmml\"><mo stretchy=\"false\" id=\"A1.SS3.p1.10.m10.1.2.2.3.2.1\" xref=\"A1.SS3.p1.10.m10.1.2.2.cmml\">(</mo><mi id=\"A1.SS3.p1.10.m10.1.1\" xref=\"A1.SS3.p1.10.m10.1.1.cmml\">ğ’˜</mi><mo stretchy=\"false\" id=\"A1.SS3.p1.10.m10.1.2.2.3.2.2\" xref=\"A1.SS3.p1.10.m10.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"A1.SS3.p1.10.m10.1.2.1\" xref=\"A1.SS3.p1.10.m10.1.2.1.cmml\">=</mo><mrow id=\"A1.SS3.p1.10.m10.1.2.3\" xref=\"A1.SS3.p1.10.m10.1.2.3.cmml\"><msub id=\"A1.SS3.p1.10.m10.1.2.3.2\" xref=\"A1.SS3.p1.10.m10.1.2.3.2.cmml\"><mi id=\"A1.SS3.p1.10.m10.1.2.3.2.2\" xref=\"A1.SS3.p1.10.m10.1.2.3.2.2.cmml\">ğ’š</mi><mi id=\"A1.SS3.p1.10.m10.1.2.3.2.3\" xref=\"A1.SS3.p1.10.m10.1.2.3.2.3.cmml\">i</mi></msub><mo id=\"A1.SS3.p1.10.m10.1.2.3.1\" xref=\"A1.SS3.p1.10.m10.1.2.3.1.cmml\">âˆ’</mo><mi id=\"A1.SS3.p1.10.m10.1.2.3.3\" xref=\"A1.SS3.p1.10.m10.1.2.3.3.cmml\">ğ’˜</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.10.m10.1b\"><apply id=\"A1.SS3.p1.10.m10.1.2.cmml\" xref=\"A1.SS3.p1.10.m10.1.2\"><eq id=\"A1.SS3.p1.10.m10.1.2.1.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.1\"></eq><apply id=\"A1.SS3.p1.10.m10.1.2.2.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.2\"><times id=\"A1.SS3.p1.10.m10.1.2.2.1.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.2.1\"></times><apply id=\"A1.SS3.p1.10.m10.1.2.2.2.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.10.m10.1.2.2.2.1.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.2.2\">subscript</csymbol><ci id=\"A1.SS3.p1.10.m10.1.2.2.2.2.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.2.2.2\">ğ‘“</ci><ci id=\"A1.SS3.p1.10.m10.1.2.2.2.3.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.2.2.3\">ğ‘–</ci></apply><ci id=\"A1.SS3.p1.10.m10.1.1.cmml\" xref=\"A1.SS3.p1.10.m10.1.1\">ğ’˜</ci></apply><apply id=\"A1.SS3.p1.10.m10.1.2.3.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3\"><minus id=\"A1.SS3.p1.10.m10.1.2.3.1.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3.1\"></minus><apply id=\"A1.SS3.p1.10.m10.1.2.3.2.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3.2\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.10.m10.1.2.3.2.1.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3.2\">subscript</csymbol><ci id=\"A1.SS3.p1.10.m10.1.2.3.2.2.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3.2.2\">ğ’š</ci><ci id=\"A1.SS3.p1.10.m10.1.2.3.2.3.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3.2.3\">ğ‘–</ci></apply><ci id=\"A1.SS3.p1.10.m10.1.2.3.3.cmml\" xref=\"A1.SS3.p1.10.m10.1.2.3.3\">ğ’˜</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.10.m10.1c\">f_{i}(\\bm{w})=\\bm{y}_{i}-\\bm{w}</annotation></semantics></math>, <math id=\"A1.SS3.p1.11.m11.1\" class=\"ltx_Math\" alttext=\"N=M=100\" display=\"inline\"><semantics id=\"A1.SS3.p1.11.m11.1a\"><mrow id=\"A1.SS3.p1.11.m11.1.1\" xref=\"A1.SS3.p1.11.m11.1.1.cmml\"><mi id=\"A1.SS3.p1.11.m11.1.1.2\" xref=\"A1.SS3.p1.11.m11.1.1.2.cmml\">N</mi><mo id=\"A1.SS3.p1.11.m11.1.1.3\" xref=\"A1.SS3.p1.11.m11.1.1.3.cmml\">=</mo><mi id=\"A1.SS3.p1.11.m11.1.1.4\" xref=\"A1.SS3.p1.11.m11.1.1.4.cmml\">M</mi><mo id=\"A1.SS3.p1.11.m11.1.1.5\" xref=\"A1.SS3.p1.11.m11.1.1.5.cmml\">=</mo><mn id=\"A1.SS3.p1.11.m11.1.1.6\" xref=\"A1.SS3.p1.11.m11.1.1.6.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.11.m11.1b\"><apply id=\"A1.SS3.p1.11.m11.1.1.cmml\" xref=\"A1.SS3.p1.11.m11.1.1\"><and id=\"A1.SS3.p1.11.m11.1.1a.cmml\" xref=\"A1.SS3.p1.11.m11.1.1\"></and><apply id=\"A1.SS3.p1.11.m11.1.1b.cmml\" xref=\"A1.SS3.p1.11.m11.1.1\"><eq id=\"A1.SS3.p1.11.m11.1.1.3.cmml\" xref=\"A1.SS3.p1.11.m11.1.1.3\"></eq><ci id=\"A1.SS3.p1.11.m11.1.1.2.cmml\" xref=\"A1.SS3.p1.11.m11.1.1.2\">ğ‘</ci><ci id=\"A1.SS3.p1.11.m11.1.1.4.cmml\" xref=\"A1.SS3.p1.11.m11.1.1.4\">ğ‘€</ci></apply><apply id=\"A1.SS3.p1.11.m11.1.1c.cmml\" xref=\"A1.SS3.p1.11.m11.1.1\"><eq id=\"A1.SS3.p1.11.m11.1.1.5.cmml\" xref=\"A1.SS3.p1.11.m11.1.1.5\"></eq><share href=\"#A1.SS3.p1.11.m11.1.1.4.cmml\" id=\"A1.SS3.p1.11.m11.1.1d.cmml\" xref=\"A1.SS3.p1.11.m11.1.1\"></share><cn type=\"integer\" id=\"A1.SS3.p1.11.m11.1.1.6.cmml\" xref=\"A1.SS3.p1.11.m11.1.1.6\">100</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.11.m11.1c\">N=M=100</annotation></semantics></math>, <math id=\"A1.SS3.p1.12.m12.1\" class=\"ltx_Math\" alttext=\"T=25\" display=\"inline\"><semantics id=\"A1.SS3.p1.12.m12.1a\"><mrow id=\"A1.SS3.p1.12.m12.1.1\" xref=\"A1.SS3.p1.12.m12.1.1.cmml\"><mi id=\"A1.SS3.p1.12.m12.1.1.2\" xref=\"A1.SS3.p1.12.m12.1.1.2.cmml\">T</mi><mo id=\"A1.SS3.p1.12.m12.1.1.1\" xref=\"A1.SS3.p1.12.m12.1.1.1.cmml\">=</mo><mn id=\"A1.SS3.p1.12.m12.1.1.3\" xref=\"A1.SS3.p1.12.m12.1.1.3.cmml\">25</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.12.m12.1b\"><apply id=\"A1.SS3.p1.12.m12.1.1.cmml\" xref=\"A1.SS3.p1.12.m12.1.1\"><eq id=\"A1.SS3.p1.12.m12.1.1.1.cmml\" xref=\"A1.SS3.p1.12.m12.1.1.1\"></eq><ci id=\"A1.SS3.p1.12.m12.1.1.2.cmml\" xref=\"A1.SS3.p1.12.m12.1.1.2\">ğ‘‡</ci><cn type=\"integer\" id=\"A1.SS3.p1.12.m12.1.1.3.cmml\" xref=\"A1.SS3.p1.12.m12.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.12.m12.1c\">T=25</annotation></semantics></math>, and learning rate 0.01. Each entry of <math id=\"A1.SS3.p1.13.m13.1\" class=\"ltx_Math\" alttext=\"\\bm{y}_{i}\" display=\"inline\"><semantics id=\"A1.SS3.p1.13.m13.1a\"><msub id=\"A1.SS3.p1.13.m13.1.1\" xref=\"A1.SS3.p1.13.m13.1.1.cmml\"><mi id=\"A1.SS3.p1.13.m13.1.1.2\" xref=\"A1.SS3.p1.13.m13.1.1.2.cmml\">ğ’š</mi><mi id=\"A1.SS3.p1.13.m13.1.1.3\" xref=\"A1.SS3.p1.13.m13.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.13.m13.1b\"><apply id=\"A1.SS3.p1.13.m13.1.1.cmml\" xref=\"A1.SS3.p1.13.m13.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.SS3.p1.13.m13.1.1.1.cmml\" xref=\"A1.SS3.p1.13.m13.1.1\">subscript</csymbol><ci id=\"A1.SS3.p1.13.m13.1.1.2.cmml\" xref=\"A1.SS3.p1.13.m13.1.1.2\">ğ’š</ci><ci id=\"A1.SS3.p1.13.m13.1.1.3.cmml\" xref=\"A1.SS3.p1.13.m13.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.13.m13.1c\">\\bm{y}_{i}</annotation></semantics></math> is also sampled independently from <math id=\"A1.SS3.p1.14.m14.2\" class=\"ltx_Math\" alttext=\"{\\rm Unif}(-1,1)\" display=\"inline\"><semantics id=\"A1.SS3.p1.14.m14.2a\"><mrow id=\"A1.SS3.p1.14.m14.2.2\" xref=\"A1.SS3.p1.14.m14.2.2.cmml\"><mi id=\"A1.SS3.p1.14.m14.2.2.3\" xref=\"A1.SS3.p1.14.m14.2.2.3.cmml\">Unif</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.SS3.p1.14.m14.2.2.2\" xref=\"A1.SS3.p1.14.m14.2.2.2.cmml\">â€‹</mo><mrow id=\"A1.SS3.p1.14.m14.2.2.1.1\" xref=\"A1.SS3.p1.14.m14.2.2.1.2.cmml\"><mo stretchy=\"false\" id=\"A1.SS3.p1.14.m14.2.2.1.1.2\" xref=\"A1.SS3.p1.14.m14.2.2.1.2.cmml\">(</mo><mrow id=\"A1.SS3.p1.14.m14.2.2.1.1.1\" xref=\"A1.SS3.p1.14.m14.2.2.1.1.1.cmml\"><mo id=\"A1.SS3.p1.14.m14.2.2.1.1.1a\" xref=\"A1.SS3.p1.14.m14.2.2.1.1.1.cmml\">âˆ’</mo><mn id=\"A1.SS3.p1.14.m14.2.2.1.1.1.2\" xref=\"A1.SS3.p1.14.m14.2.2.1.1.1.2.cmml\">1</mn></mrow><mo id=\"A1.SS3.p1.14.m14.2.2.1.1.3\" xref=\"A1.SS3.p1.14.m14.2.2.1.2.cmml\">,</mo><mn id=\"A1.SS3.p1.14.m14.1.1\" xref=\"A1.SS3.p1.14.m14.1.1.cmml\">1</mn><mo stretchy=\"false\" id=\"A1.SS3.p1.14.m14.2.2.1.1.4\" xref=\"A1.SS3.p1.14.m14.2.2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.SS3.p1.14.m14.2b\"><apply id=\"A1.SS3.p1.14.m14.2.2.cmml\" xref=\"A1.SS3.p1.14.m14.2.2\"><times id=\"A1.SS3.p1.14.m14.2.2.2.cmml\" xref=\"A1.SS3.p1.14.m14.2.2.2\"></times><ci id=\"A1.SS3.p1.14.m14.2.2.3.cmml\" xref=\"A1.SS3.p1.14.m14.2.2.3\">Unif</ci><interval closure=\"open\" id=\"A1.SS3.p1.14.m14.2.2.1.2.cmml\" xref=\"A1.SS3.p1.14.m14.2.2.1.1\"><apply id=\"A1.SS3.p1.14.m14.2.2.1.1.1.cmml\" xref=\"A1.SS3.p1.14.m14.2.2.1.1.1\"><minus id=\"A1.SS3.p1.14.m14.2.2.1.1.1.1.cmml\" xref=\"A1.SS3.p1.14.m14.2.2.1.1.1\"></minus><cn type=\"integer\" id=\"A1.SS3.p1.14.m14.2.2.1.1.1.2.cmml\" xref=\"A1.SS3.p1.14.m14.2.2.1.1.1.2\">1</cn></apply><cn type=\"integer\" id=\"A1.SS3.p1.14.m14.1.1.cmml\" xref=\"A1.SS3.p1.14.m14.1.1\">1</cn></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.SS3.p1.14.m14.2c\">{\\rm Unif}(-1,1)</annotation></semantics></math>.</p>\n</div>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Experiment Results</h2>\n\n<figure id=\"A2.4\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div id=\"A2.4.5\" class=\"ltx_block ltx_figure_panel\">\n<figure id=\"A2.F12\" class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" style=\"width:104.1pt;\"><img src=\"./assets/x24.png\" id=\"A2.1.1.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" width=\"456\" height=\"360\" alt=\"Refer to caption\">\n<br class=\"ltx_break ltx_break\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F12.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 12</span>: </span><span id=\"A2.F12.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of partial document shuffling.</span></figcaption>\n</figure>\n<figure id=\"A2.F13\" class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" style=\"width:104.1pt;\"><img src=\"./assets/x25.png\" id=\"A2.2.2.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" width=\"456\" height=\"361\" alt=\"Refer to caption\">\n<br class=\"ltx_break ltx_break\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F13.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 13</span>: </span><span id=\"A2.F13.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of learning rate in 1-step GD.</span></figcaption>\n</figure>\n</div>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<div id=\"A2.4.6\" class=\"ltx_block ltx_figure_panel\">\n<figure id=\"A2.F14\" class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" style=\"width:104.1pt;\"><img src=\"./assets/x26.png\" id=\"A2.3.3.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" width=\"457\" height=\"352\" alt=\"Refer to caption\">\n<br class=\"ltx_break ltx_break\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F14.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 14</span>: </span><span id=\"A2.F14.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of number of attention heads.</span></figcaption>\n</figure>\n<figure id=\"A2.F15\" class=\"ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle\" style=\"width:104.1pt;\"><img src=\"./assets/x27.png\" id=\"A2.4.4.g1\" class=\"ltx_graphics ltx_figure_panel ltx_img_landscape\" width=\"457\" height=\"352\" alt=\"Refer to caption\">\n<br class=\"ltx_break ltx_break\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F15.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 15</span>: </span><span id=\"A2.F15.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of model initialization.</span></figcaption>\n</figure>\n</div>\n</div>\n</div>\n</figure>\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Partial Random Shuffling</h3>\n\n<div id=\"A2.SS1.p1\" class=\"ltx_para\">\n<p id=\"A2.SS1.p1.8\" class=\"ltx_p\">Throughout the paper we have been focusing on the setting where the document ordering is sampled once and stay fixed for all epochs. What if we only fix the first and the last document, and shuffle the documents in between? We experimented with shuffling the documents from document <math id=\"A2.SS1.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{2}\" display=\"inline\"><semantics id=\"A2.SS1.p1.1.m1.1a\"><msub id=\"A2.SS1.p1.1.m1.1.1\" xref=\"A2.SS1.p1.1.m1.1.1.cmml\"><mi id=\"A2.SS1.p1.1.m1.1.1.2\" xref=\"A2.SS1.p1.1.m1.1.1.2.cmml\">ğ’™</mi><mn id=\"A2.SS1.p1.1.m1.1.1.3\" xref=\"A2.SS1.p1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.1.m1.1b\"><apply id=\"A2.SS1.p1.1.m1.1.1.cmml\" xref=\"A2.SS1.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.1.m1.1.1.1.cmml\" xref=\"A2.SS1.p1.1.m1.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.1.m1.1.1.2.cmml\" xref=\"A2.SS1.p1.1.m1.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A2.SS1.p1.1.m1.1.1.3.cmml\" xref=\"A2.SS1.p1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.1.m1.1c\">\\bm{x}_{2}</annotation></semantics></math> through <math id=\"A2.SS1.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{N}\" display=\"inline\"><semantics id=\"A2.SS1.p1.2.m2.1a\"><msub id=\"A2.SS1.p1.2.m2.1.1\" xref=\"A2.SS1.p1.2.m2.1.1.cmml\"><mi id=\"A2.SS1.p1.2.m2.1.1.2\" xref=\"A2.SS1.p1.2.m2.1.1.2.cmml\">ğ’™</mi><mi id=\"A2.SS1.p1.2.m2.1.1.3\" xref=\"A2.SS1.p1.2.m2.1.1.3.cmml\">N</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.2.m2.1b\"><apply id=\"A2.SS1.p1.2.m2.1.1.cmml\" xref=\"A2.SS1.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.2.m2.1.1.1.cmml\" xref=\"A2.SS1.p1.2.m2.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.2.m2.1.1.2.cmml\" xref=\"A2.SS1.p1.2.m2.1.1.2\">ğ’™</ci><ci id=\"A2.SS1.p1.2.m2.1.1.3.cmml\" xref=\"A2.SS1.p1.2.m2.1.1.3\">ğ‘</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.2.m2.1c\">\\bm{x}_{N}</annotation></semantics></math> for <math id=\"A2.SS1.p1.3.m3.3\" class=\"ltx_Math\" alttext=\"N\\in\\{8,16,24\\}\" display=\"inline\"><semantics id=\"A2.SS1.p1.3.m3.3a\"><mrow id=\"A2.SS1.p1.3.m3.3.4\" xref=\"A2.SS1.p1.3.m3.3.4.cmml\"><mi id=\"A2.SS1.p1.3.m3.3.4.2\" xref=\"A2.SS1.p1.3.m3.3.4.2.cmml\">N</mi><mo id=\"A2.SS1.p1.3.m3.3.4.1\" xref=\"A2.SS1.p1.3.m3.3.4.1.cmml\">âˆˆ</mo><mrow id=\"A2.SS1.p1.3.m3.3.4.3.2\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\"><mo stretchy=\"false\" id=\"A2.SS1.p1.3.m3.3.4.3.2.1\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">{</mo><mn id=\"A2.SS1.p1.3.m3.1.1\" xref=\"A2.SS1.p1.3.m3.1.1.cmml\">8</mn><mo id=\"A2.SS1.p1.3.m3.3.4.3.2.2\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS1.p1.3.m3.2.2\" xref=\"A2.SS1.p1.3.m3.2.2.cmml\">16</mn><mo id=\"A2.SS1.p1.3.m3.3.4.3.2.3\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS1.p1.3.m3.3.3\" xref=\"A2.SS1.p1.3.m3.3.3.cmml\">24</mn><mo stretchy=\"false\" id=\"A2.SS1.p1.3.m3.3.4.3.2.4\" xref=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.3.m3.3b\"><apply id=\"A2.SS1.p1.3.m3.3.4.cmml\" xref=\"A2.SS1.p1.3.m3.3.4\"><in id=\"A2.SS1.p1.3.m3.3.4.1.cmml\" xref=\"A2.SS1.p1.3.m3.3.4.1\"></in><ci id=\"A2.SS1.p1.3.m3.3.4.2.cmml\" xref=\"A2.SS1.p1.3.m3.3.4.2\">ğ‘</ci><set id=\"A2.SS1.p1.3.m3.3.4.3.1.cmml\" xref=\"A2.SS1.p1.3.m3.3.4.3.2\"><cn type=\"integer\" id=\"A2.SS1.p1.3.m3.1.1.cmml\" xref=\"A2.SS1.p1.3.m3.1.1\">8</cn><cn type=\"integer\" id=\"A2.SS1.p1.3.m3.2.2.cmml\" xref=\"A2.SS1.p1.3.m3.2.2\">16</cn><cn type=\"integer\" id=\"A2.SS1.p1.3.m3.3.3.cmml\" xref=\"A2.SS1.p1.3.m3.3.3\">24</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.3.m3.3c\">N\\in\\{8,16,24\\}</annotation></semantics></math> every epoch. In Figure <a href=\"#A2.F12\" title=\"Figure 12 â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> we plot the loss curves for document <math id=\"A2.SS1.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"A2.SS1.p1.4.m4.1a\"><msub id=\"A2.SS1.p1.4.m4.1.1\" xref=\"A2.SS1.p1.4.m4.1.1.cmml\"><mi id=\"A2.SS1.p1.4.m4.1.1.2\" xref=\"A2.SS1.p1.4.m4.1.1.2.cmml\">ğ’™</mi><mn id=\"A2.SS1.p1.4.m4.1.1.3\" xref=\"A2.SS1.p1.4.m4.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.4.m4.1b\"><apply id=\"A2.SS1.p1.4.m4.1.1.cmml\" xref=\"A2.SS1.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.4.m4.1.1.1.cmml\" xref=\"A2.SS1.p1.4.m4.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.4.m4.1.1.2.cmml\" xref=\"A2.SS1.p1.4.m4.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A2.SS1.p1.4.m4.1.1.3.cmml\" xref=\"A2.SS1.p1.4.m4.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.4.m4.1c\">\\bm{x}_{1}</annotation></semantics></math>. From the loss curves we can observe that even when <math id=\"A2.SS1.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"N=24\" display=\"inline\"><semantics id=\"A2.SS1.p1.5.m5.1a\"><mrow id=\"A2.SS1.p1.5.m5.1.1\" xref=\"A2.SS1.p1.5.m5.1.1.cmml\"><mi id=\"A2.SS1.p1.5.m5.1.1.2\" xref=\"A2.SS1.p1.5.m5.1.1.2.cmml\">N</mi><mo id=\"A2.SS1.p1.5.m5.1.1.1\" xref=\"A2.SS1.p1.5.m5.1.1.1.cmml\">=</mo><mn id=\"A2.SS1.p1.5.m5.1.1.3\" xref=\"A2.SS1.p1.5.m5.1.1.3.cmml\">24</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.5.m5.1b\"><apply id=\"A2.SS1.p1.5.m5.1.1.cmml\" xref=\"A2.SS1.p1.5.m5.1.1\"><eq id=\"A2.SS1.p1.5.m5.1.1.1.cmml\" xref=\"A2.SS1.p1.5.m5.1.1.1\"></eq><ci id=\"A2.SS1.p1.5.m5.1.1.2.cmml\" xref=\"A2.SS1.p1.5.m5.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"A2.SS1.p1.5.m5.1.1.3.cmml\" xref=\"A2.SS1.p1.5.m5.1.1.3\">24</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.5.m5.1c\">N=24</annotation></semantics></math> we can still observe some anticipatory recovery, suggesting that the order of the tasks between two consecutive repetitions of the <math id=\"A2.SS1.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{25}\" display=\"inline\"><semantics id=\"A2.SS1.p1.6.m6.1a\"><msub id=\"A2.SS1.p1.6.m6.1.1\" xref=\"A2.SS1.p1.6.m6.1.1.cmml\"><mi id=\"A2.SS1.p1.6.m6.1.1.2\" xref=\"A2.SS1.p1.6.m6.1.1.2.cmml\">ğ’™</mi><mn id=\"A2.SS1.p1.6.m6.1.1.3\" xref=\"A2.SS1.p1.6.m6.1.1.3.cmml\">25</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.6.m6.1b\"><apply id=\"A2.SS1.p1.6.m6.1.1.cmml\" xref=\"A2.SS1.p1.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.6.m6.1.1.1.cmml\" xref=\"A2.SS1.p1.6.m6.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.6.m6.1.1.2.cmml\" xref=\"A2.SS1.p1.6.m6.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A2.SS1.p1.6.m6.1.1.3.cmml\" xref=\"A2.SS1.p1.6.m6.1.1.3\">25</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.6.m6.1c\">\\bm{x}_{25}</annotation></semantics></math> and <math id=\"A2.SS1.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"A2.SS1.p1.7.m7.1a\"><msub id=\"A2.SS1.p1.7.m7.1.1\" xref=\"A2.SS1.p1.7.m7.1.1.cmml\"><mi id=\"A2.SS1.p1.7.m7.1.1.2\" xref=\"A2.SS1.p1.7.m7.1.1.2.cmml\">ğ’™</mi><mn id=\"A2.SS1.p1.7.m7.1.1.3\" xref=\"A2.SS1.p1.7.m7.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.7.m7.1b\"><apply id=\"A2.SS1.p1.7.m7.1.1.cmml\" xref=\"A2.SS1.p1.7.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.7.m7.1.1.1.cmml\" xref=\"A2.SS1.p1.7.m7.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.7.m7.1.1.2.cmml\" xref=\"A2.SS1.p1.7.m7.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A2.SS1.p1.7.m7.1.1.3.cmml\" xref=\"A2.SS1.p1.7.m7.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.7.m7.1c\">\\bm{x}_{1}</annotation></semantics></math> can be arbitrary for us to observe recovery on <math id=\"A2.SS1.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"A2.SS1.p1.8.m8.1a\"><msub id=\"A2.SS1.p1.8.m8.1.1\" xref=\"A2.SS1.p1.8.m8.1.1.cmml\"><mi id=\"A2.SS1.p1.8.m8.1.1.2\" xref=\"A2.SS1.p1.8.m8.1.1.2.cmml\">ğ’™</mi><mn id=\"A2.SS1.p1.8.m8.1.1.3\" xref=\"A2.SS1.p1.8.m8.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS1.p1.8.m8.1b\"><apply id=\"A2.SS1.p1.8.m8.1.1.cmml\" xref=\"A2.SS1.p1.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS1.p1.8.m8.1.1.1.cmml\" xref=\"A2.SS1.p1.8.m8.1.1\">subscript</csymbol><ci id=\"A2.SS1.p1.8.m8.1.1.2.cmml\" xref=\"A2.SS1.p1.8.m8.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A2.SS1.p1.8.m8.1.1.3.cmml\" xref=\"A2.SS1.p1.8.m8.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS1.p1.8.m8.1c\">\\bm{x}_{1}</annotation></semantics></math>.</p>\n</div>\n</section>\n<section id=\"A2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>One-step Gradient Descent with Larger Learning Rate</h3>\n\n<div id=\"A2.SS2.p1\" class=\"ltx_para\">\n<p id=\"A2.SS2.p1.1\" class=\"ltx_p\">In Figure <a href=\"#S3.F5.sf2\" title=\"Figure 5(b) â€£ Figure 5 â€£ 3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a> we observe that there is no anticipation when we take only one gradient descent step on each document with learning rate 0.001. Here we experiment with one-step gradient descent using a higher learning rate, 0.01. We plot the resulting average loss curves under the same training setup in Figure <a href=\"#A2.F13\" title=\"Figure 13 â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. We observe that, with a larger learning rate, slight anticipation is still observed for <math id=\"A2.SS2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"A2.SS2.p1.1.m1.1a\"><mn id=\"A2.SS2.p1.1.m1.1.1\" xref=\"A2.SS2.p1.1.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS2.p1.1.m1.1b\"><cn type=\"integer\" id=\"A2.SS2.p1.1.m1.1.1.cmml\" xref=\"A2.SS2.p1.1.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS2.p1.1.m1.1c\">1</annotation></semantics></math> gradient step.</p>\n</div>\n</section>\n<section id=\"A2.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Effect of Number of Attention Heads</h3>\n\n<div id=\"A2.SS3.p1\" class=\"ltx_para\">\n<p id=\"A2.SS3.p1.2\" class=\"ltx_p\">In addition to varying the model width and model depth in Figure <a href=\"#S3.F4\" title=\"Figure 4 â€£ Effects of Model Width and Depth. â€£ 3.2 Anticipatory Recovery is an Emergent Behavior â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we also experimented with varying the number of attention heads <math id=\"A2.SS3.p1.1.m1.4\" class=\"ltx_Math\" alttext=\"h\\in\\{2,4,8,16\\}\" display=\"inline\"><semantics id=\"A2.SS3.p1.1.m1.4a\"><mrow id=\"A2.SS3.p1.1.m1.4.5\" xref=\"A2.SS3.p1.1.m1.4.5.cmml\"><mi id=\"A2.SS3.p1.1.m1.4.5.2\" xref=\"A2.SS3.p1.1.m1.4.5.2.cmml\">h</mi><mo id=\"A2.SS3.p1.1.m1.4.5.1\" xref=\"A2.SS3.p1.1.m1.4.5.1.cmml\">âˆˆ</mo><mrow id=\"A2.SS3.p1.1.m1.4.5.3.2\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\"><mo stretchy=\"false\" id=\"A2.SS3.p1.1.m1.4.5.3.2.1\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">{</mo><mn id=\"A2.SS3.p1.1.m1.1.1\" xref=\"A2.SS3.p1.1.m1.1.1.cmml\">2</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.2\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"A2.SS3.p1.1.m1.2.2\" xref=\"A2.SS3.p1.1.m1.2.2.cmml\">4</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.3\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"A2.SS3.p1.1.m1.3.3\" xref=\"A2.SS3.p1.1.m1.3.3.cmml\">8</mn><mo id=\"A2.SS3.p1.1.m1.4.5.3.2.4\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">,</mo><mn id=\"A2.SS3.p1.1.m1.4.4\" xref=\"A2.SS3.p1.1.m1.4.4.cmml\">16</mn><mo stretchy=\"false\" id=\"A2.SS3.p1.1.m1.4.5.3.2.5\" xref=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS3.p1.1.m1.4b\"><apply id=\"A2.SS3.p1.1.m1.4.5.cmml\" xref=\"A2.SS3.p1.1.m1.4.5\"><in id=\"A2.SS3.p1.1.m1.4.5.1.cmml\" xref=\"A2.SS3.p1.1.m1.4.5.1\"></in><ci id=\"A2.SS3.p1.1.m1.4.5.2.cmml\" xref=\"A2.SS3.p1.1.m1.4.5.2\">â„</ci><set id=\"A2.SS3.p1.1.m1.4.5.3.1.cmml\" xref=\"A2.SS3.p1.1.m1.4.5.3.2\"><cn type=\"integer\" id=\"A2.SS3.p1.1.m1.1.1.cmml\" xref=\"A2.SS3.p1.1.m1.1.1\">2</cn><cn type=\"integer\" id=\"A2.SS3.p1.1.m1.2.2.cmml\" xref=\"A2.SS3.p1.1.m1.2.2\">4</cn><cn type=\"integer\" id=\"A2.SS3.p1.1.m1.3.3.cmml\" xref=\"A2.SS3.p1.1.m1.3.3\">8</cn><cn type=\"integer\" id=\"A2.SS3.p1.1.m1.4.4.cmml\" xref=\"A2.SS3.p1.1.m1.4.4\">16</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS3.p1.1.m1.4c\">h\\in\\{2,4,8,16\\}</annotation></semantics></math> while keeping model width to be 2048 and model depth to be 16. Loss curves on document <math id=\"A2.SS3.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"A2.SS3.p1.2.m2.1a\"><msub id=\"A2.SS3.p1.2.m2.1.1\" xref=\"A2.SS3.p1.2.m2.1.1.cmml\"><mi id=\"A2.SS3.p1.2.m2.1.1.2\" xref=\"A2.SS3.p1.2.m2.1.1.2.cmml\">ğ’™</mi><mn id=\"A2.SS3.p1.2.m2.1.1.3\" xref=\"A2.SS3.p1.2.m2.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS3.p1.2.m2.1b\"><apply id=\"A2.SS3.p1.2.m2.1.1.cmml\" xref=\"A2.SS3.p1.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.SS3.p1.2.m2.1.1.1.cmml\" xref=\"A2.SS3.p1.2.m2.1.1\">subscript</csymbol><ci id=\"A2.SS3.p1.2.m2.1.1.2.cmml\" xref=\"A2.SS3.p1.2.m2.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A2.SS3.p1.2.m2.1.1.3.cmml\" xref=\"A2.SS3.p1.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS3.p1.2.m2.1c\">\\bm{x}_{1}</annotation></semantics></math> are shown in figure <a href=\"#A2.F14\" title=\"Figure 14 â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>. The results suggest that the number of attention heads does not have a big effect on cyclic training in our setting.</p>\n</div>\n</section>\n<section id=\"A2.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>Effect of LLM Model Initialization</h3>\n\n<div id=\"A2.SS4.p1\" class=\"ltx_para\">\n<p id=\"A2.SS4.p1.1\" class=\"ltx_p\">Here we compare the performance of the initialization scheme used by <cite class=\"ltx_cite ltx_citemacro_citet\">Biderman etÂ al. (<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a>)</cite> (also used for all randomly initialized models in the main text) and a simple initialization scheme that samples the weight matrices from an isotropic Gaussian distribution with <math id=\"A2.SS4.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=0.02\" display=\"inline\"><semantics id=\"A2.SS4.p1.1.m1.1a\"><mrow id=\"A2.SS4.p1.1.m1.1.1\" xref=\"A2.SS4.p1.1.m1.1.1.cmml\"><mi id=\"A2.SS4.p1.1.m1.1.1.2\" xref=\"A2.SS4.p1.1.m1.1.1.2.cmml\">Ïƒ</mi><mo id=\"A2.SS4.p1.1.m1.1.1.1\" xref=\"A2.SS4.p1.1.m1.1.1.1.cmml\">=</mo><mn id=\"A2.SS4.p1.1.m1.1.1.3\" xref=\"A2.SS4.p1.1.m1.1.1.3.cmml\">0.02</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS4.p1.1.m1.1b\"><apply id=\"A2.SS4.p1.1.m1.1.1.cmml\" xref=\"A2.SS4.p1.1.m1.1.1\"><eq id=\"A2.SS4.p1.1.m1.1.1.1.cmml\" xref=\"A2.SS4.p1.1.m1.1.1.1\"></eq><ci id=\"A2.SS4.p1.1.m1.1.1.2.cmml\" xref=\"A2.SS4.p1.1.m1.1.1.2\">ğœ</ci><cn type=\"float\" id=\"A2.SS4.p1.1.m1.1.1.3.cmml\" xref=\"A2.SS4.p1.1.m1.1.1.3\">0.02</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS4.p1.1.m1.1c\">\\sigma=0.02</annotation></semantics></math>. The loss curves for document 1 under these two initializations of the Pythia-1B model are plotted in Figure <a href=\"#A2.F15\" title=\"Figure 15 â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>. We observe that Pythiaâ€™s initialization scheme achieves much better average loss and also exhibits stronger anticipatory recovery. This demonstrates the importance of LLM initializations. The result is consistent with our observations in section <a href=\"#S3.SS3\" title=\"3.3 Other Influential Factors â€£ 3 Emergent Anticipatory Recovery â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> that the modelâ€™s ability to fit on each task is correlated with the amount of anticipatory recovery.</p>\n</div>\n<figure id=\"A2.F16\" class=\"ltx_figure\">\n<div id=\"A2.F16.2\" class=\"ltx_block\">\n<figure id=\"A2.F16.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x28.png\" id=\"A2.F16.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"237\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F16.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"A2.F16.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Number of Gradient Steps with Inverse LR Scaling</span></figcaption>\n</figure>\n<figure id=\"A2.F16.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x29.png\" id=\"A2.F16.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"244\" height=\"103\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F16.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"A2.F16.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Number of Gradient Steps for Context Length 1024</span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A2.F16.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 16</span>: </span><span id=\"A2.F16.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Effect of number of gradient steps (a) with inverse learning rate scaling and (b) for context length 1024.</span></figcaption>\n</figure>\n</section>\n<section id=\"A2.SS5\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.5 </span>Effect of Number of Gradient Steps with Inverse Learning Rate Scaling</h3>\n\n<div id=\"A2.SS5.p1\" class=\"ltx_para\">\n<p id=\"A2.SS5.p1.8\" class=\"ltx_p\">In Figure <a href=\"#A2.F16.sf1\" title=\"Figure 16(a) â€£ Figure 16 â€£ B.4 Effect of LLM Model Initialization â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16(a)</span></a> we experiment with inversely scaling the learning rate with the number of gradient steps. We use a learning rate of <math id=\"A2.SS5.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.01\" display=\"inline\"><semantics id=\"A2.SS5.p1.1.m1.1a\"><mn id=\"A2.SS5.p1.1.m1.1.1\" xref=\"A2.SS5.p1.1.m1.1.1.cmml\">0.01</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.1.m1.1b\"><cn type=\"float\" id=\"A2.SS5.p1.1.m1.1.1.cmml\" xref=\"A2.SS5.p1.1.m1.1.1\">0.01</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.1.m1.1c\">0.01</annotation></semantics></math> for <math id=\"A2.SS5.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"M=1\" display=\"inline\"><semantics id=\"A2.SS5.p1.2.m2.1a\"><mrow id=\"A2.SS5.p1.2.m2.1.1\" xref=\"A2.SS5.p1.2.m2.1.1.cmml\"><mi id=\"A2.SS5.p1.2.m2.1.1.2\" xref=\"A2.SS5.p1.2.m2.1.1.2.cmml\">M</mi><mo id=\"A2.SS5.p1.2.m2.1.1.1\" xref=\"A2.SS5.p1.2.m2.1.1.1.cmml\">=</mo><mn id=\"A2.SS5.p1.2.m2.1.1.3\" xref=\"A2.SS5.p1.2.m2.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.2.m2.1b\"><apply id=\"A2.SS5.p1.2.m2.1.1.cmml\" xref=\"A2.SS5.p1.2.m2.1.1\"><eq id=\"A2.SS5.p1.2.m2.1.1.1.cmml\" xref=\"A2.SS5.p1.2.m2.1.1.1\"></eq><ci id=\"A2.SS5.p1.2.m2.1.1.2.cmml\" xref=\"A2.SS5.p1.2.m2.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"A2.SS5.p1.2.m2.1.1.3.cmml\" xref=\"A2.SS5.p1.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.2.m2.1c\">M=1</annotation></semantics></math>, learning rate <math id=\"A2.SS5.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"0.002\" display=\"inline\"><semantics id=\"A2.SS5.p1.3.m3.1a\"><mn id=\"A2.SS5.p1.3.m3.1.1\" xref=\"A2.SS5.p1.3.m3.1.1.cmml\">0.002</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.3.m3.1b\"><cn type=\"float\" id=\"A2.SS5.p1.3.m3.1.1.cmml\" xref=\"A2.SS5.p1.3.m3.1.1\">0.002</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.3.m3.1c\">0.002</annotation></semantics></math> for <math id=\"A2.SS5.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"M=5\" display=\"inline\"><semantics id=\"A2.SS5.p1.4.m4.1a\"><mrow id=\"A2.SS5.p1.4.m4.1.1\" xref=\"A2.SS5.p1.4.m4.1.1.cmml\"><mi id=\"A2.SS5.p1.4.m4.1.1.2\" xref=\"A2.SS5.p1.4.m4.1.1.2.cmml\">M</mi><mo id=\"A2.SS5.p1.4.m4.1.1.1\" xref=\"A2.SS5.p1.4.m4.1.1.1.cmml\">=</mo><mn id=\"A2.SS5.p1.4.m4.1.1.3\" xref=\"A2.SS5.p1.4.m4.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.4.m4.1b\"><apply id=\"A2.SS5.p1.4.m4.1.1.cmml\" xref=\"A2.SS5.p1.4.m4.1.1\"><eq id=\"A2.SS5.p1.4.m4.1.1.1.cmml\" xref=\"A2.SS5.p1.4.m4.1.1.1\"></eq><ci id=\"A2.SS5.p1.4.m4.1.1.2.cmml\" xref=\"A2.SS5.p1.4.m4.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"A2.SS5.p1.4.m4.1.1.3.cmml\" xref=\"A2.SS5.p1.4.m4.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.4.m4.1c\">M=5</annotation></semantics></math>, learning rate <math id=\"A2.SS5.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics id=\"A2.SS5.p1.5.m5.1a\"><mn id=\"A2.SS5.p1.5.m5.1.1\" xref=\"A2.SS5.p1.5.m5.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.5.m5.1b\"><cn type=\"float\" id=\"A2.SS5.p1.5.m5.1.1.cmml\" xref=\"A2.SS5.p1.5.m5.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.5.m5.1c\">0.001</annotation></semantics></math> for <math id=\"A2.SS5.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"M=10\" display=\"inline\"><semantics id=\"A2.SS5.p1.6.m6.1a\"><mrow id=\"A2.SS5.p1.6.m6.1.1\" xref=\"A2.SS5.p1.6.m6.1.1.cmml\"><mi id=\"A2.SS5.p1.6.m6.1.1.2\" xref=\"A2.SS5.p1.6.m6.1.1.2.cmml\">M</mi><mo id=\"A2.SS5.p1.6.m6.1.1.1\" xref=\"A2.SS5.p1.6.m6.1.1.1.cmml\">=</mo><mn id=\"A2.SS5.p1.6.m6.1.1.3\" xref=\"A2.SS5.p1.6.m6.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.6.m6.1b\"><apply id=\"A2.SS5.p1.6.m6.1.1.cmml\" xref=\"A2.SS5.p1.6.m6.1.1\"><eq id=\"A2.SS5.p1.6.m6.1.1.1.cmml\" xref=\"A2.SS5.p1.6.m6.1.1.1\"></eq><ci id=\"A2.SS5.p1.6.m6.1.1.2.cmml\" xref=\"A2.SS5.p1.6.m6.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"A2.SS5.p1.6.m6.1.1.3.cmml\" xref=\"A2.SS5.p1.6.m6.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.6.m6.1c\">M=10</annotation></semantics></math>, and learning rate <math id=\"A2.SS5.p1.7.m7.1\" class=\"ltx_Math\" alttext=\"0.0005\" display=\"inline\"><semantics id=\"A2.SS5.p1.7.m7.1a\"><mn id=\"A2.SS5.p1.7.m7.1.1\" xref=\"A2.SS5.p1.7.m7.1.1.cmml\">0.0005</mn><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.7.m7.1b\"><cn type=\"float\" id=\"A2.SS5.p1.7.m7.1.1.cmml\" xref=\"A2.SS5.p1.7.m7.1.1\">0.0005</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.7.m7.1c\">0.0005</annotation></semantics></math> for <math id=\"A2.SS5.p1.8.m8.1\" class=\"ltx_Math\" alttext=\"M=20\" display=\"inline\"><semantics id=\"A2.SS5.p1.8.m8.1a\"><mrow id=\"A2.SS5.p1.8.m8.1.1\" xref=\"A2.SS5.p1.8.m8.1.1.cmml\"><mi id=\"A2.SS5.p1.8.m8.1.1.2\" xref=\"A2.SS5.p1.8.m8.1.1.2.cmml\">M</mi><mo id=\"A2.SS5.p1.8.m8.1.1.1\" xref=\"A2.SS5.p1.8.m8.1.1.1.cmml\">=</mo><mn id=\"A2.SS5.p1.8.m8.1.1.3\" xref=\"A2.SS5.p1.8.m8.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS5.p1.8.m8.1b\"><apply id=\"A2.SS5.p1.8.m8.1.1.cmml\" xref=\"A2.SS5.p1.8.m8.1.1\"><eq id=\"A2.SS5.p1.8.m8.1.1.1.cmml\" xref=\"A2.SS5.p1.8.m8.1.1.1\"></eq><ci id=\"A2.SS5.p1.8.m8.1.1.2.cmml\" xref=\"A2.SS5.p1.8.m8.1.1.2\">ğ‘€</ci><cn type=\"integer\" id=\"A2.SS5.p1.8.m8.1.1.3.cmml\" xref=\"A2.SS5.p1.8.m8.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS5.p1.8.m8.1c\">M=20</annotation></semantics></math>. The results suggest that the anticipation effect is stronger when the same total update is divided among more gradient steps.</p>\n</div>\n</section>\n<section id=\"A2.SS6\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.6 </span>Effect of Number of Gradient Steps for Long Context Length</h3>\n\n<div id=\"A2.SS6.p1\" class=\"ltx_para\">\n<p id=\"A2.SS6.p1.1\" class=\"ltx_p\">In Figure <a href=\"#A2.F16.sf2\" title=\"Figure 16(b) â€£ Figure 16 â€£ B.4 Effect of LLM Model Initialization â€£ Appendix B Additional Experiment Results â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16(b)</span></a> we experiment with different number of gradient steps <math id=\"A2.SS6.p1.1.m1.3\" class=\"ltx_Math\" alttext=\"M\\in\\{10,20,40\\}\" display=\"inline\"><semantics id=\"A2.SS6.p1.1.m1.3a\"><mrow id=\"A2.SS6.p1.1.m1.3.4\" xref=\"A2.SS6.p1.1.m1.3.4.cmml\"><mi id=\"A2.SS6.p1.1.m1.3.4.2\" xref=\"A2.SS6.p1.1.m1.3.4.2.cmml\">M</mi><mo id=\"A2.SS6.p1.1.m1.3.4.1\" xref=\"A2.SS6.p1.1.m1.3.4.1.cmml\">âˆˆ</mo><mrow id=\"A2.SS6.p1.1.m1.3.4.3.2\" xref=\"A2.SS6.p1.1.m1.3.4.3.1.cmml\"><mo stretchy=\"false\" id=\"A2.SS6.p1.1.m1.3.4.3.2.1\" xref=\"A2.SS6.p1.1.m1.3.4.3.1.cmml\">{</mo><mn id=\"A2.SS6.p1.1.m1.1.1\" xref=\"A2.SS6.p1.1.m1.1.1.cmml\">10</mn><mo id=\"A2.SS6.p1.1.m1.3.4.3.2.2\" xref=\"A2.SS6.p1.1.m1.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS6.p1.1.m1.2.2\" xref=\"A2.SS6.p1.1.m1.2.2.cmml\">20</mn><mo id=\"A2.SS6.p1.1.m1.3.4.3.2.3\" xref=\"A2.SS6.p1.1.m1.3.4.3.1.cmml\">,</mo><mn id=\"A2.SS6.p1.1.m1.3.3\" xref=\"A2.SS6.p1.1.m1.3.3.cmml\">40</mn><mo stretchy=\"false\" id=\"A2.SS6.p1.1.m1.3.4.3.2.4\" xref=\"A2.SS6.p1.1.m1.3.4.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.SS6.p1.1.m1.3b\"><apply id=\"A2.SS6.p1.1.m1.3.4.cmml\" xref=\"A2.SS6.p1.1.m1.3.4\"><in id=\"A2.SS6.p1.1.m1.3.4.1.cmml\" xref=\"A2.SS6.p1.1.m1.3.4.1\"></in><ci id=\"A2.SS6.p1.1.m1.3.4.2.cmml\" xref=\"A2.SS6.p1.1.m1.3.4.2\">ğ‘€</ci><set id=\"A2.SS6.p1.1.m1.3.4.3.1.cmml\" xref=\"A2.SS6.p1.1.m1.3.4.3.2\"><cn type=\"integer\" id=\"A2.SS6.p1.1.m1.1.1.cmml\" xref=\"A2.SS6.p1.1.m1.1.1\">10</cn><cn type=\"integer\" id=\"A2.SS6.p1.1.m1.2.2.cmml\" xref=\"A2.SS6.p1.1.m1.2.2\">20</cn><cn type=\"integer\" id=\"A2.SS6.p1.1.m1.3.3.cmml\" xref=\"A2.SS6.p1.1.m1.3.3\">40</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.SS6.p1.1.m1.3c\">M\\in\\{10,20,40\\}</annotation></semantics></math> for context length 1024. The results confirm that longer context length is not a fundamental limitation to anticipatory recovery, and we can achieve the same recovery score as a smaller context length with more gradient steps.</p>\n</div>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Additional Visualizations</h2>\n\n<figure id=\"A3.F17\" class=\"ltx_figure\">\n<div id=\"A3.F17.2\" class=\"ltx_block\">\n<figure id=\"A3.F17.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x30.png\" id=\"A3.F17.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"126\" height=\"91\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F17.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"A3.F17.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Model Activations</span></figcaption>\n</figure>\n<figure id=\"A3.F17.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x31.png\" id=\"A3.F17.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"124\" height=\"91\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F17.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"A3.F17.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">Model Weights </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F17.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 17</span>: </span><span id=\"A3.F17.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Magnitude of (a) model activation updates and (b) model weight updates through four epochs of cyclic training.</span></figcaption>\n</figure>\n<figure id=\"A3.F18\" class=\"ltx_figure\">\n<div id=\"A3.F18.6\" class=\"ltx_block\">\n<figure id=\"A3.F18.sf1\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x32.png\" id=\"A3.F18.sf1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"126\" height=\"97\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F18.sf1.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span id=\"A3.F18.sf1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">50 Documents</span></figcaption>\n</figure>\n<figure id=\"A3.F18.sf2\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x33.png\" id=\"A3.F18.sf2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"122\" height=\"97\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F18.sf2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span id=\"A3.F18.sf2.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">100 Documents </span></figcaption>\n</figure>\n<figure id=\"A3.F18.sf3\" class=\"ltx_figure ltx_align_center\"><img src=\"./assets/x34.png\" id=\"A3.F18.sf3.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"125\" height=\"97\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F18.sf3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span><span id=\"A3.F18.sf3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">200 Documents </span></figcaption>\n</figure>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span id=\"A3.F18.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Figure 18</span>: </span><span id=\"A3.F18.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">Loss recoveries for training on task <math id=\"A3.F18.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"A3.F18.3.1.m1.1b\"><msub id=\"A3.F18.3.1.m1.1.1\" xref=\"A3.F18.3.1.m1.1.1.cmml\"><mi id=\"A3.F18.3.1.m1.1.1.2\" xref=\"A3.F18.3.1.m1.1.1.2.cmml\">ğ’™</mi><mi id=\"A3.F18.3.1.m1.1.1.3\" xref=\"A3.F18.3.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.F18.3.1.m1.1c\"><apply id=\"A3.F18.3.1.m1.1.1.cmml\" xref=\"A3.F18.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.F18.3.1.m1.1.1.1.cmml\" xref=\"A3.F18.3.1.m1.1.1\">subscript</csymbol><ci id=\"A3.F18.3.1.m1.1.1.2.cmml\" xref=\"A3.F18.3.1.m1.1.1.2\">ğ’™</ci><ci id=\"A3.F18.3.1.m1.1.1.3.cmml\" xref=\"A3.F18.3.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.F18.3.1.m1.1d\">\\bm{x}_{i}</annotation></semantics></math> (y-axis) and evaluating on task <math id=\"A3.F18.4.2.m2.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"A3.F18.4.2.m2.1b\"><msub id=\"A3.F18.4.2.m2.1.1\" xref=\"A3.F18.4.2.m2.1.1.cmml\"><mi id=\"A3.F18.4.2.m2.1.1.2\" xref=\"A3.F18.4.2.m2.1.1.2.cmml\">ğ’™</mi><mi id=\"A3.F18.4.2.m2.1.1.3\" xref=\"A3.F18.4.2.m2.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.F18.4.2.m2.1c\"><apply id=\"A3.F18.4.2.m2.1.1.cmml\" xref=\"A3.F18.4.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.F18.4.2.m2.1.1.1.cmml\" xref=\"A3.F18.4.2.m2.1.1\">subscript</csymbol><ci id=\"A3.F18.4.2.m2.1.1.2.cmml\" xref=\"A3.F18.4.2.m2.1.1.2\">ğ’™</ci><ci id=\"A3.F18.4.2.m2.1.1.3.cmml\" xref=\"A3.F18.4.2.m2.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.F18.4.2.m2.1d\">\\bm{x}_{j}</annotation></semantics></math> (x-axis) for longer document sequences of different lengths.</span></figcaption>\n</figure>\n<section id=\"A3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.1 </span>Magnitude of Changes in Model Weights and Model Activations</h3>\n\n<div id=\"A3.SS1.p1\" class=\"ltx_para\">\n<p id=\"A3.SS1.p1.1\" class=\"ltx_p\">We plot the magnitude of the difference between the <math id=\"A3.SS1.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{1}\" display=\"inline\"><semantics id=\"A3.SS1.p1.1.m1.1a\"><msub id=\"A3.SS1.p1.1.m1.1.1\" xref=\"A3.SS1.p1.1.m1.1.1.cmml\"><mi id=\"A3.SS1.p1.1.m1.1.1.2\" xref=\"A3.SS1.p1.1.m1.1.1.2.cmml\">ğ’™</mi><mn id=\"A3.SS1.p1.1.m1.1.1.3\" xref=\"A3.SS1.p1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS1.p1.1.m1.1b\"><apply id=\"A3.SS1.p1.1.m1.1.1.cmml\" xref=\"A3.SS1.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS1.p1.1.m1.1.1.1.cmml\" xref=\"A3.SS1.p1.1.m1.1.1\">subscript</csymbol><ci id=\"A3.SS1.p1.1.m1.1.1.2.cmml\" xref=\"A3.SS1.p1.1.m1.1.1.2\">ğ’™</ci><cn type=\"integer\" id=\"A3.SS1.p1.1.m1.1.1.3.cmml\" xref=\"A3.SS1.p1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS1.p1.1.m1.1c\">\\bm{x}_{1}</annotation></semantics></math> activations of model checkpoints saved at consecutive episodes throughout four epochs of cyclic training of a Pythia-410M model in Figure <a href=\"#A3.F17.sf1\" title=\"Figure 17(a) â€£ Figure 17 â€£ Appendix C Additional Visualizations â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">17(a)</span></a>, and observe a clear stepwise pattern. In contrast, the magnitude of model weight updates (Figure <a href=\"#A3.F17.sf2\" title=\"Figure 17(b) â€£ Figure 17 â€£ Appendix C Additional Visualizations â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">17(b)</span></a>) decreases monotonically over the training episodes and do not exhibit this stepwise pattern. This result is consistent with the pattern we observe in section <a href=\"#S4.SS3\" title=\"4.3 Temporal Structure of Activations â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>.</p>\n</div>\n</section>\n<section id=\"A3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">C.2 </span>Pairwise Recovery Matrices for Longer Document Sequences</h3>\n\n<div id=\"A3.SS2.p1\" class=\"ltx_para\">\n<p id=\"A3.SS2.p1.6\" class=\"ltx_p\">Similar to Figure <a href=\"#S4.F8.sf2\" title=\"Figure 8(b) â€£ Figure 8 â€£ 4 Understanding Cyclic Training Dynamics â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8(b)</span></a>, we plot the pairwise loss recoveries for each pair of documents <math id=\"A3.SS2.p1.1.m1.2\" class=\"ltx_Math\" alttext=\"(\\bm{x}_{i},\\bm{x}_{j})\" display=\"inline\"><semantics id=\"A3.SS2.p1.1.m1.2a\"><mrow id=\"A3.SS2.p1.1.m1.2.2.2\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\"><mo stretchy=\"false\" id=\"A3.SS2.p1.1.m1.2.2.2.3\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\">(</mo><msub id=\"A3.SS2.p1.1.m1.1.1.1.1\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.cmml\"><mi id=\"A3.SS2.p1.1.m1.1.1.1.1.2\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.2.cmml\">ğ’™</mi><mi id=\"A3.SS2.p1.1.m1.1.1.1.1.3\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"A3.SS2.p1.1.m1.2.2.2.4\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\">,</mo><msub id=\"A3.SS2.p1.1.m1.2.2.2.2\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.cmml\"><mi id=\"A3.SS2.p1.1.m1.2.2.2.2.2\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.2.cmml\">ğ’™</mi><mi id=\"A3.SS2.p1.1.m1.2.2.2.2.3\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.3.cmml\">j</mi></msub><mo stretchy=\"false\" id=\"A3.SS2.p1.1.m1.2.2.2.5\" xref=\"A3.SS2.p1.1.m1.2.2.3.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.1.m1.2b\"><interval closure=\"open\" id=\"A3.SS2.p1.1.m1.2.2.3.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2\"><apply id=\"A3.SS2.p1.1.m1.1.1.1.1.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.1.m1.1.1.1.1.1.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.1.m1.1.1.1.1.2.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.2\">ğ’™</ci><ci id=\"A3.SS2.p1.1.m1.1.1.1.1.3.cmml\" xref=\"A3.SS2.p1.1.m1.1.1.1.1.3\">ğ‘–</ci></apply><apply id=\"A3.SS2.p1.1.m1.2.2.2.2.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.1.m1.2.2.2.2.1.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2\">subscript</csymbol><ci id=\"A3.SS2.p1.1.m1.2.2.2.2.2.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.2\">ğ’™</ci><ci id=\"A3.SS2.p1.1.m1.2.2.2.2.3.cmml\" xref=\"A3.SS2.p1.1.m1.2.2.2.2.3\">ğ‘—</ci></apply></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.1.m1.2c\">(\\bm{x}_{i},\\bm{x}_{j})</annotation></semantics></math> in longer document sequences, where <math id=\"A3.SS2.p1.2.m2.3\" class=\"ltx_Math\" alttext=\"T=50,100,200\" display=\"inline\"><semantics id=\"A3.SS2.p1.2.m2.3a\"><mrow id=\"A3.SS2.p1.2.m2.3.4\" xref=\"A3.SS2.p1.2.m2.3.4.cmml\"><mi id=\"A3.SS2.p1.2.m2.3.4.2\" xref=\"A3.SS2.p1.2.m2.3.4.2.cmml\">T</mi><mo id=\"A3.SS2.p1.2.m2.3.4.1\" xref=\"A3.SS2.p1.2.m2.3.4.1.cmml\">=</mo><mrow id=\"A3.SS2.p1.2.m2.3.4.3.2\" xref=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\"><mn id=\"A3.SS2.p1.2.m2.1.1\" xref=\"A3.SS2.p1.2.m2.1.1.cmml\">50</mn><mo id=\"A3.SS2.p1.2.m2.3.4.3.2.1\" xref=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\">,</mo><mn id=\"A3.SS2.p1.2.m2.2.2\" xref=\"A3.SS2.p1.2.m2.2.2.cmml\">100</mn><mo id=\"A3.SS2.p1.2.m2.3.4.3.2.2\" xref=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\">,</mo><mn id=\"A3.SS2.p1.2.m2.3.3\" xref=\"A3.SS2.p1.2.m2.3.3.cmml\">200</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.2.m2.3b\"><apply id=\"A3.SS2.p1.2.m2.3.4.cmml\" xref=\"A3.SS2.p1.2.m2.3.4\"><eq id=\"A3.SS2.p1.2.m2.3.4.1.cmml\" xref=\"A3.SS2.p1.2.m2.3.4.1\"></eq><ci id=\"A3.SS2.p1.2.m2.3.4.2.cmml\" xref=\"A3.SS2.p1.2.m2.3.4.2\">ğ‘‡</ci><list id=\"A3.SS2.p1.2.m2.3.4.3.1.cmml\" xref=\"A3.SS2.p1.2.m2.3.4.3.2\"><cn type=\"integer\" id=\"A3.SS2.p1.2.m2.1.1.cmml\" xref=\"A3.SS2.p1.2.m2.1.1\">50</cn><cn type=\"integer\" id=\"A3.SS2.p1.2.m2.2.2.cmml\" xref=\"A3.SS2.p1.2.m2.2.2\">100</cn><cn type=\"integer\" id=\"A3.SS2.p1.2.m2.3.3.cmml\" xref=\"A3.SS2.p1.2.m2.3.3\">200</cn></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.2.m2.3c\">T=50,100,200</annotation></semantics></math> respectively, in Figure <a href=\"#A3.F18\" title=\"Figure 18 â€£ Appendix C Additional Visualizations â€£ Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>. We use the 1B model and default hyperparameters. We observe that, as we increase the length of the document sequence, the highlight area near the center of the matrix is separated into two blobs, one in the top-left corner and the other in the bottom-right corner. We also observe a â€boundaryâ€ on the sides of the matrix where there is little or no recovery. The width of this â€boundaryâ€ stays relatively constant across different lengths of document sequences and is around 10 to 15 documents. This confirms our observation in the main text that the recovery on document <math id=\"A3.SS2.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j}\" display=\"inline\"><semantics id=\"A3.SS2.p1.3.m3.1a\"><msub id=\"A3.SS2.p1.3.m3.1.1\" xref=\"A3.SS2.p1.3.m3.1.1.cmml\"><mi id=\"A3.SS2.p1.3.m3.1.1.2\" xref=\"A3.SS2.p1.3.m3.1.1.2.cmml\">ğ’™</mi><mi id=\"A3.SS2.p1.3.m3.1.1.3\" xref=\"A3.SS2.p1.3.m3.1.1.3.cmml\">j</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.3.m3.1b\"><apply id=\"A3.SS2.p1.3.m3.1.1.cmml\" xref=\"A3.SS2.p1.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.3.m3.1.1.1.cmml\" xref=\"A3.SS2.p1.3.m3.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.3.m3.1.1.2.cmml\" xref=\"A3.SS2.p1.3.m3.1.1.2\">ğ’™</ci><ci id=\"A3.SS2.p1.3.m3.1.1.3.cmml\" xref=\"A3.SS2.p1.3.m3.1.1.3\">ğ‘—</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.3.m3.1c\">\\bm{x}_{j}</annotation></semantics></math> when fine-tuning on a proximal document <math id=\"A3.SS2.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{i}\" display=\"inline\"><semantics id=\"A3.SS2.p1.4.m4.1a\"><msub id=\"A3.SS2.p1.4.m4.1.1\" xref=\"A3.SS2.p1.4.m4.1.1.cmml\"><mi id=\"A3.SS2.p1.4.m4.1.1.2\" xref=\"A3.SS2.p1.4.m4.1.1.2.cmml\">ğ’™</mi><mi id=\"A3.SS2.p1.4.m4.1.1.3\" xref=\"A3.SS2.p1.4.m4.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.4.m4.1b\"><apply id=\"A3.SS2.p1.4.m4.1.1.cmml\" xref=\"A3.SS2.p1.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.4.m4.1.1.1.cmml\" xref=\"A3.SS2.p1.4.m4.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.4.m4.1.1.2.cmml\" xref=\"A3.SS2.p1.4.m4.1.1.2\">ğ’™</ci><ci id=\"A3.SS2.p1.4.m4.1.1.3.cmml\" xref=\"A3.SS2.p1.4.m4.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.4.m4.1c\">\\bm{x}_{i}</annotation></semantics></math> is highest when the model checkpoint is taken from document <math id=\"A3.SS2.p1.5.m5.1\" class=\"ltx_Math\" alttext=\"\\bm{x}_{j\\pm b}\" display=\"inline\"><semantics id=\"A3.SS2.p1.5.m5.1a\"><msub id=\"A3.SS2.p1.5.m5.1.1\" xref=\"A3.SS2.p1.5.m5.1.1.cmml\"><mi id=\"A3.SS2.p1.5.m5.1.1.2\" xref=\"A3.SS2.p1.5.m5.1.1.2.cmml\">ğ’™</mi><mrow id=\"A3.SS2.p1.5.m5.1.1.3\" xref=\"A3.SS2.p1.5.m5.1.1.3.cmml\"><mi id=\"A3.SS2.p1.5.m5.1.1.3.2\" xref=\"A3.SS2.p1.5.m5.1.1.3.2.cmml\">j</mi><mo id=\"A3.SS2.p1.5.m5.1.1.3.1\" xref=\"A3.SS2.p1.5.m5.1.1.3.1.cmml\">Â±</mo><mi id=\"A3.SS2.p1.5.m5.1.1.3.3\" xref=\"A3.SS2.p1.5.m5.1.1.3.3.cmml\">b</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.5.m5.1b\"><apply id=\"A3.SS2.p1.5.m5.1.1.cmml\" xref=\"A3.SS2.p1.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.SS2.p1.5.m5.1.1.1.cmml\" xref=\"A3.SS2.p1.5.m5.1.1\">subscript</csymbol><ci id=\"A3.SS2.p1.5.m5.1.1.2.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.2\">ğ’™</ci><apply id=\"A3.SS2.p1.5.m5.1.1.3.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3\"><csymbol cd=\"latexml\" id=\"A3.SS2.p1.5.m5.1.1.3.1.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3.1\">plus-or-minus</csymbol><ci id=\"A3.SS2.p1.5.m5.1.1.3.2.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3.2\">ğ‘—</ci><ci id=\"A3.SS2.p1.5.m5.1.1.3.3.cmml\" xref=\"A3.SS2.p1.5.m5.1.1.3.3\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.5.m5.1c\">\\bm{x}_{j\\pm b}</annotation></semantics></math> where <math id=\"A3.SS2.p1.6.m6.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"A3.SS2.p1.6.m6.1a\"><mi id=\"A3.SS2.p1.6.m6.1.1\" xref=\"A3.SS2.p1.6.m6.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"A3.SS2.p1.6.m6.1b\"><ci id=\"A3.SS2.p1.6.m6.1.1.cmml\" xref=\"A3.SS2.p1.6.m6.1.1\">ğ‘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.SS2.p1.6.m6.1c\">b</annotation></semantics></math> is a small number relative to the length of the document sequence.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>",
  "css": "",
  "arxiv_id": "2403.09613",
  "source": "ar5iv",
  "generated": "2025-10-13T02:29:37.301Z"
}
