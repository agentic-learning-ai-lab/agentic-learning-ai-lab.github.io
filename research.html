<!doctype html>
<html lang="en">
    
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>agentic learning ai lab</title>
        <meta name="description" content="" />
        <!-- <link
            rel="shortcut icon"
            href="./assets/logo.png"
            type="image/x-icon"
        /> -->
    
        <!-- Open Graph / Facebook -->
        <meta property="og:title" content="agentic learning ai lab" />
        <meta property="og:description" content="agentic learning ai lab" />
        <meta property="og:type" content="website" />
        <meta property="og:url" content="https://agenticlearning.ai/" />
        <!--Replace with the current website url-->
        <meta property="og:image" content="" />
    
        <link rel="stylesheet" href="./css/tailwind-runtime.css" />
        <!-- <link rel="stylesheet" href="./css/tailwind-build.css" /> -->
        <link rel="stylesheet" href="css/index.css" />
    
        <link
            rel="stylesheet"
            href="https://fonts.googleapis.com/icon?family=Material+Icons"
        />
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-icons/1.11.3/font/bootstrap-icons.min.css"
            integrity="sha512-dPXYcDub/aeb08c63jRq/k6GaKccl256JQy/AnOq7CAnEZ9FzSL9wSbcZkMp4R26vBsMLFYH4kQ67/bbV8XaCQ=="
            crossorigin="anonymous"
            referrerpolicy="no-referrer"
        />
        <script src="https://www.google.com/recaptcha/api.js" async defer></script>
        
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-44MVTGBV0D"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
    
          gtag('config', 'G-44MVTGBV0D');
        </script>
    
        <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@emailjs/browser@4/dist/email.min.js"></script>
        <script type="text/javascript">
            (function() {
                // https://dashboard.emailjs.com/admin/account
                emailjs.init({
                  publicKey: "y6ebNBhEpEzmyqS8F",
                });
            })();
        </script>
        
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.0/gsap.min.js"
            integrity="sha512-B1lby8cGcAUU3GR+Fd809/ZxgHbfwJMp0jLTVfHiArTuUt++VqSlJpaJvhNtRf3NERaxDNmmxkdx2o+aHd4bvw=="
            crossorigin="anonymous"
            referrerpolicy="no-referrer"
        ></script>
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.0/ScrollTrigger.min.js"
            integrity="sha512-AY2+JxnBETJ0wcXnLPCcZJIJx0eimyhz3OJ55k2Jx4RtYC+XdIi2VtJQ+tP3BaTst4otlGG1TtPJ9fKrAUnRdQ=="
            crossorigin="anonymous"
            referrerpolicy="no-referrer"
        ></script>
    </head>    <body class="tw-flex tw-min-h-[100vh] tw-flex-col tw-bg-[#fff] tw-font-mono">
        <header class="tw-absolute tw-top-0 tw-z-20 tw-flex tw-h-[120px] tw-w-full tw-bg-opacity-0 tw-px-[5%] max-lg:tw-mr-auto max-lg:tw-px-4 lg:tw-justify-around">
            <a class="tw-h-[120px] tw-p-[4px] tw-w-[100px] tw-text-2xl tw-font-medium" href="">
            agentic learning <br/> ai lab
            </a>
            <div class="collapsible-header animated-collapse max-lg:tw-shadow-md"
                id="collapsed-header-items">
                <div class="tw-flex tw-h-full tw-w-max tw-gap-5 tw-text-base tw-text-black max-lg:tw-mt-[30px] max-lg:tw-flex-col max-lg:tw-place-items-end max-lg:tw-gap-5 lg:tw-mx-auto lg:tw-place-items-center">
                    <a class="header-links" href="./index.html"> home </a>
                    <a class="header-links" href="./research.html"> research </a>
                    <a class="header-links" href="./people.html"> people </a>
                    <a class="header-links" href="./contact.html"> contact </a>
                </div>
                <div class="tw-mx-4 tw-flex tw-place-items-center tw-justify-end tw-gap-[20px] tw-text-base">
                        <div class="tw-flex tw-h-[30px] tw-w-[30px] tw-rounded-full tw-bg-white tw-font-semibold tw-text-black tw-place-items-center ">
                            <a href="">
                            <i class="bi bi-search"></i>
                            </a>
                        </div>
                </div>
            </div>
            <button
                class="bi bi-list tw-absolute tw-right-3 tw-top-3 tw-z-50 tw-text-3xl tw-text-black lg:tw-hidden"
                onclick="toggleHeader()"
                aria-label="menu"
                id="collapse-btn"
            ></button>
        </header>
        <section class="tw-flex tw-w-full tw-flex-col tw-p-[5%] tw-mt-[150px] max-lg:tw-p-4">
            <h3 class="text-left tw-text-4xl tw-font-medium max-md:tw-text-2xl">
                research
            </h3>
            <div class="tw-my-4 max-md:tw-h-[3px] max-md:tw-w-[40px] tw-h-[5px] tw-w-[60px] tw-bg-gray-300 ">
            </div>

            <div id="flexContainer" class="tw-mt-10 tw-flex tw-flex-wrap tw-place-content-center tw-gap-y-4 tw-gap-x-4 tw-justify-around">
            <div class="page-item">
            <a href="#">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/are_llms_prescient.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Are LLMs Prescient? A Continuous Evaluation using Daily News as Oracle</h3>
                    <p class="tw-mt-4">
                        Authors: Hui Dai, Ryan Teehan, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to model updates and an evolving information landscape. Moreover, they often lack the ability to assess how model performance evolves over time, as they consist of static questions without a temporal dimension. To address these, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict "future" events based on pre-training data. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) can enhance prediction accuracy, the degradation persists, highlighting the need for ongoing model updates.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-11-01
                    </p>
                    <a href=# class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2408.11208">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/poodle.webp
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">PooDLe: Pooled and Dense Self-Supervised Learning from Naturalistic Videos</h3>
                    <p class="tw-mt-4">
                        Authors: Alex N. Wang, Christopher Hoang, Yuwen Xiong, Yann LeCun, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos. We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-08-20
                    </p>
                    <a href=https://arxiv.org/abs/2408.11208 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2408.02226">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/procreate.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation</h3>
                    <p class="tw-mt-4">
                        Authors: Jack Lu, Ryan Teehan, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-08-05
                    </p>
                    <a href=https://arxiv.org/abs/2408.02226 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2404.19132">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/osiris.webp
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Integrating Present and Past in Unsupervised Continual Learning</h3>
                    <p class="tw-mt-4">
                        Authors: Yipeng Zhang, Laurent Charlin, Richard S. Zemel, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        We formulate a unifying framework for unsupervised continual learning (UCL), which disentangles learning objectives that are specific to the present and the past data, encompassing stability, plasticity, and cross-task consolidation. The framework reveals that many existing UCL approaches overlook cross-task consolidation and try to balance plasticity and stability in a shared embedding space. This results in worse performance due to a lack of within-task data diversity and reduced effectiveness in learning the current task. Our method, Osiris, which explicitly optimizes all three objectives on separate embedding spaces, achieves state-of-the-art performance on all benchmarks, including two novel benchmarks proposed in this paper featuring semantically structured task sequences. Compared to standard benchmarks, these two structured benchmarks more closely resemble visual signals received by humans and animals when navigating real-world environments. Finally, we show some preliminary evidence that continual models can benefit from such realistic learning scenarios.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-04-29
                    </p>
                    <a href=https://arxiv.org/abs/2404.19132 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2403.15362">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/college.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">CoLLEGe: Concept Embedding Generation for Large Language Models</h3>
                    <p class="tw-mt-4">
                        Authors: Ryan Teehan, Brenden M. Lake, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting without task-specific training.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-03-22
                    </p>
                    <a href=https://arxiv.org/abs/2403.15362 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2403.09613">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/reawakening.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Reawakening Knowledge: Anticipatory Recovery from Catastrophic Interference via Structured Training</h3>
                    <p class="tw-mt-4">
                        Authors: Yanlai Yang, Matt Jones, Michael C. Mozer, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-03-14
                    </p>
                    <a href=https://arxiv.org/abs/2403.09613 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2402.00300">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/ssl_childs_perspective.webp
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Self-Supervised Learning of Video Representations from a Child's Perspective</h3>
                    <p class="tw-mt-4">
                        Authors: A. Emin Orhan, Wentao Wang, Alex N. Wang, Mengye Ren, and Brenden M. Lake
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more accurate and more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2024-02-01
                    </p>
                    <a href=https://arxiv.org/abs/2402.00300 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2312.12736">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/learning_and_forgetting_llm.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">Learning and Forgetting Unsafe Examples in Large Language Models</h3>
                    <p class="tw-mt-4">
                        Authors: Jiachen Zhao, Zhun Deng, David Madras, James Zou, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the "ForgetFilter" algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2023-12-20
                    </p>
                    <a href=https://arxiv.org/abs/2312.12736 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            <div class="page-item">
            <a href="https://arxiv.org/abs/2312.12736">
            <div class="safari-padding-fix">
            <div class="tw-flex tw-h-fit tw-break-inside-avoid tw-flex-col tw-gap-2 tw-rounded-lg tw-bg-[#f3f3f3b4] tw-p-4 max-lg:tw-w-full hover:tw-shadow-lg tw-transition-shadow tw-duration-300 tw-overflow-hidden">
                <div class="tw-flex tw-place-items-center tw-gap-3">
                    <div class="tw-h-[300px] tw-w-full tw-overflow-hidden tw-rounded-lg">
                        <img src=./assets/images/papers/lifelong_memory.png
                            class="tw-h-full tw-w-full tw-object-cover"
                            alt="design"/>
                    </div>
                    <!--  tw-transition-transform tw-duration-300 tw-transform hover:tw-scale-110 -->
                </div>
                <div class="tw-flex tw-flex-col tw-gap-2">
                    <h3 class="tw-text-xl tw-font-medium tw-mt-4">LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos</h3>
                    <p class="tw-mt-4">
                        Authors: Ying Wang, Yanlai Yang, and Mengye Ren
                    </p>
                    <p class="tw-text-gray-600 tw-mt-4">
                        In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D.
                    </p>
                    <p class="tw-mt-4">
                    Publish Date: 2023-12-07
                    </p>
                    <a href=https://arxiv.org/abs/2312.12736 class="tw-mt-4">
                        <span>Learn more</span>
                        <i class="bi bi-arrow-right"></i>
                    </a>
                </div>
            </div>
            </div>
            </a>
            </div>
            </div>

            <div class="pagination-controls tw-mt-12 tw-flex tw-justify-center tw-items-center tw-gap-2">
                <button id="prevButton" class="tw-px-6 tw-py-1 tw-text-black disabled:tw-text-gray-300" onclick="prevPage()">Prev</button>
                <span id="pageNumbers" class="tw-flex tw-gap-2"></span>
                <button id="nextButton" class="tw-px-6 tw-py-1 tw-text-black disabled:tw-text-gray-300" onclick="nextPage()">Next</button>
            </div>

        </section>
        
        <hr class="tw-mt-4" />
        <footer class="tw-mt-auto tw-flex tw-p-[5%] tw-min-h-[100px] tw-w-full tw-place-items-center tw-gap-3 tw-text-black
        ">
        
            <div class="tw-flex tw-gap-6 tw-text-sm">
                60 Fifth Ave
                <br />
                New York, NY
                <br />
                
            </div>
            <div class="tw-flex tw-gap-6 tw-text-2xl tw-justify-end tw-ml-auto tw-px-2">
                <a href="https://github.com/agentic-learning-ai-lab" aria-label="Github">
                    <i class="bi bi-github"></i>
                </a>
                
                <a href="https://x.com/agentic_ai_lab" aria-label="X">
                    <i class="bi bi-twitter-x"></i>
                </a>
            </div>
        </footer>    </body>
    <script src="./index.js"></script>
    <script src="./research.js"></script>
</html>
