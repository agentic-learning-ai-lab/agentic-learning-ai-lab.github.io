- title: "Replay Can Provably Increase Forgetting"
  image: "/assets/images/papers/replay_can_provably_increase_forgetting.png"
  authors: 
    - "Yasaman Mahdaviyeh"
    - "James Lucas"
    - "Mengye Ren"
    - "Andreas S. Tolias"
    - "Richard Zemel"
    - "Toniann Pitassi"
  short_abstract: "We provide a theoretical analysis of sample replay in over-parameterized continual linear regression, and we show that replay can provably increase forgetting in the worst case even though the network has the capacity to memorize all tasks."
  abstract: "Continual learning seeks to enable machine learning systems to solve an increasing corpus of tasks sequentially. A critical challenge for continual learning is forgetting, where the performance on previously learned tasks decreases as new tasks are introduced. One of the commonly used techniques to mitigate forgetting, sample replay, has been shown empirically to reduce forgetting by retaining some examples from old tasks and including them in new training episodes. In this work, we provide a theoretical analysis of sample replay in an over-parameterized continual linear regression setting, where each task is given by a linear subspace and with enough replay samples, one would be able to eliminate forgetting. Our analysis focuses on sample replay and highlights the role of the replayed samples and the relationship between task subspaces. Surprisingly, we find that, even in a noiseless setting, forgetting can be non-monotonic with respect to the number of replay samples. We present tasks where replay can be harmful with respect to worst-case settings, and also in distributional settings where replay of randomly selected samples increases forgetting in expectation. We also give empirical evidence that harmful replay is not limited to training with linear models by showing similar behavior for a neural networks equipped with SGD. Through experiments on a commonly used benchmark, we provide additional evidence that, even in seemingly benign scenarios, performance of the replay heavily depends on the choice of replay samples and the relationship between tasks."
  arxiv: "https://arxiv.org/abs/2506.04377"
  pdf: "https://arxiv.org/pdf/2506.04377"
  webpage: "#"
  permalink: "replay-can-provably-increase-forgetting"
  research_areas:
    - "learning-from-visual-experience"
  date: 2025-06-04
  journal: "The Fourth Conference on Lifelong Learning Agents (CoLLAs 2025)"
  is_recent: true

-
  title: "Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos"
  image: "/assets/images/papers/memory_storyboard.png"
  authors:
    - "Yanlai Yang"
    - "Mengye Ren"
  short_abstract: "Memory Storyboard groups recent past frames into temporal segments and provides effective summarization of the past visual streams for memory replay."
  abstract: "Self-supervised learning holds the promise to learn good representations from real-world continuous uncurated data streams. However, most existing works in visual self-supervised learning focus on static images or artificial data streams. Towards exploring a more realistic learning substrate, we investigate streaming self-supervised learning from long-form real-world egocentric video streams. Inspired by the event segmentation mechanism in human perception and memory, we propose \"Memory Storyboard\" that groups recent past frames into temporal segments for more effective summarization of the past visual streams for memory replay. To accommodate efficient temporal segmentation, we propose a two-tier memory hierarchy: the recent past is stored in a short-term memory, and the storyboard temporal segments are then transferred to a long-term memory. Experiments on real-world egocentric video datasets including SAYCam and KrishnaCam show that contrastive learning objectives on top of storyboard frames result in semantically meaningful representations which outperform those produced by state-of-the-art unsupervised continual learning methods."
  arxiv: "https://arxiv.org/abs/2501.12254"
  pdf: "https://arxiv.org/pdf/2501.12254"
  webpage: "https://agenticlearning.ai/memory-storyboard/"
  permalink: "memory-storyboard"
  research_areas:
    - "learning-from-visual-experience"
  date: 2025-01-21
  journal: "The Fourth Conference on Lifelong Learning Agents (CoLLAs 2025)"
  is_recent: true

-
  title: "Are LLMs Prescient? A Continuous Evaluation using Daily News as Oracle"
  image: "/assets/images/papers/are_llms_prescient.png"
  authors:
    - "Amelia Dai"
    - "Ryan Teehan"
    - "Mengye Ren"
  short_abstract: "Our new benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict \"future\" events based on pre-training data."
  abstract: "Existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to model updates and an evolving information landscape. Moreover, they often lack the ability to assess how model performance evolves over time, as they consist of static questions without a temporal dimension. To address these, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict \"future\" events based on pre-training data. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) can enhance prediction accuracy, the degradation persists, highlighting the need for ongoing model updates."
  arxiv: "https://arxiv.org/abs/2411.08324"
  pdf: "https://arxiv.org/pdf/2411.08324"
  webpage: "https://agenticlearning.ai/daily-oracle/"
  permalink: "are-llms-prescient"
  research_areas:
    - "adaptive-foundation-models"
  date: 2024-11-13
  journal: "The 42nd International Conference on Machine Learning (ICML 2025)"
  is_recent: true

-
  title: "PooDLe: Pooled and Dense Self-Supervised Learning from Naturalistic Videos"
  image: "/assets/images/papers/poodle.webp"
  authors: 
    - "Alex N. Wang"
    - "Chris Hoang"
    - "Yuwen Xiong"
    - "Yann LeCun"
    - "Mengye Ren"
  short_abstract: "We propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping."
  abstract: "Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos. We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective."
  arxiv: "https://arxiv.org/abs/2408.11208"
  pdf: "https://arxiv.org/pdf/2408.11208"
  webpage: "https://agenticlearning.ai/poodle/"
  permalink: "poodle"
  research_areas:
    - "learning-from-visual-experience"
  date: 2024-08-20
  journal: "The 13th International Conference on Learning Representations (ICLR 2025)"
  is_recent: true

-
  title: "ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation"
  image: "/assets/images/papers/procreate.png"
  authors: 
    - "Jack Lu"
    - "Ryan Teehan"
    - "Mengye Ren"
  short_abstract: "ProCreate is a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction."
  abstract: "In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompassing different concepts, styles, and settings -- in which ProCreate achieves the highest sample diversity and fidelity. Furthermore, we show that ProCreate is effective at preventing replicating training data in a large-scale evaluation using training text prompts."
  arxiv: "https://arxiv.org/abs/2408.02226"
  pdf: "https://arxiv.org/pdf/2408.02226"
  webpage: "https://agenticlearning.ai/procreate-diffusion/"
  permalink: "procreate"
  research_areas:
    - "concept-learning-abstraction"
  date: 2024-08-05
  journal: "The 18th European Conference on Computer Vision (ECCV 2024)"
  is_recent: true

- 
  title: "Integrating Present and Past in Unsupervised Continual Learning"
  image: "/assets/images/papers/osiris.webp"
  authors: 
    - "Yipeng Zhang"
    - "Laurent Charlin"
    - "Richard S. Zemel"
    - "Mengye Ren"
  short_abstract: "We formulate Osiris, a unifying framework for unsupervised continual learning (UCL), which disentangles learning objectives that encompass stability, plasticity, and cross-task consolidation."
  abstract: "We formulate a unifying framework for unsupervised continual learning (UCL), which disentangles learning objectives that are specific to the present and the past data, encompassing stability, plasticity, and cross-task consolidation. The framework reveals that many existing UCL approaches overlook cross-task consolidation and try to balance plasticity and stability in a shared embedding space. This results in worse performance due to a lack of within-task data diversity and reduced effectiveness in learning the current task. Our method, Osiris, which explicitly optimizes all three objectives on separate embedding spaces, achieves state-of-the-art performance on all benchmarks, including two novel benchmarks proposed in this paper featuring semantically structured task sequences. Compared to standard benchmarks, these two structured benchmarks more closely resemble visual signals received by humans and animals when navigating real-world environments. Finally, we show some preliminary evidence that continual models can benefit from such realistic learning scenarios."
  arxiv: "https://arxiv.org/abs/2404.19132"
  pdf: "https://arxiv.org/pdf/2404.19132"
  webpage: "#"
  permalink: "osiris"
  research_areas:
    - "learning-from-visual-experience"
  date: 2024-04-29
  journal: "The Third Conference on Lifelong Learning Agents (CoLLAs 2024)"
  is_recent: true

- 
  title: "CoLLEGe: Concept Embedding Generation for Large Language Models"
  image: "/assets/images/papers/college.png"
  authors: 
    - "Ryan Teehan"
    - "Brenden M. Lake"
    - "Mengye Ren"
  short_abstract: "CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions."
  abstract: "Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting without task-specific training."
  arxiv: "https://arxiv.org/abs/2403.15362"
  pdf: "https://arxiv.org/pdf/2403.15362"
  webpage: "https://agenticlearning.ai/college/"
  permalink: "college"
  research_areas:
    - "adaptive-foundation-models"
    - "concept-learning-abstraction"
  date: 2024-03-22
  journal: "The First Conference on Language Modeling (COLM 2024)"
  is_recent: true

-
  title: "Reawakening Knowledge: Anticipatory Recovery from Catastrophic Interference via Structured Training"
  image: "/assets/images/papers/reawakening.png"
  authors:
    - "Yanlai Yang"
    - "Matt Jones"
    - "Michael C. Mozer"
    - "Mengye Ren"
  short_abstract: "We discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again."
  abstract: "We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments."
  arxiv: "https://arxiv.org/abs/2403.09613"
  pdf: "https://arxiv.org/pdf/2403.09613"
  webpage: "https://agenticlearning.ai/anticipatory-recovery/"
  permalink: "anticipatory-recovery"
  research_areas:
    - "adaptive-foundation-models"
  date: 2024-03-14
  journal: "Advances in Neural Information Processing Systems 37 (NeurIPS 2024)"
  is_recent: true

- 
  title: "Self-Supervised Learning of Video Representations from a Child's Perspective"
  image: "/assets/images/papers/ssl_childs_perspective.webp"
  authors:
    - "A. Emin Orhan"
    - "Wentao Wang"
    - "Alex N. Wang"
    - "Mengye Ren"
    - "Brenden M. Lake"
  short_abstract: "We train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development."
  abstract: "Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more accurate and more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases."
  arxiv: "https://arxiv.org/abs/2402.00300"
  pdf: "https://arxiv.org/pdf/2402.00300"
  webpage: "#"
  permalink: "video-ssl-from-child"
  research_areas:
    - "learning-from-visual-experience"
  date: 2024-02-01
  journal: "The 46th Annual Meeting of the Cognitive Science Society (CogSci 2024)"
  is_recent: false

-
  title: "Learning and Forgetting Unsafe Examples in Large Language Models"
  image: "/assets/images/papers/learning_and_forgetting_llm.png"
  authors:
    - "Jiachen Zhao"
    - "Zhun Deng"
    - "David Madras"
    - "James Zou"
    - "Mengye Ren"
  short_abstract: 'We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content and propose a simple filtering algorithm for detecting harmful content based on the phenomenon of selective forgetting.'
  abstract: "As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the \"ForgetFilter\" algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score."
  arxiv: "https://arxiv.org/abs/2312.12736"
  pdf: "https://arxiv.org/pdf/2312.12736"
  webpage: "#"
  permalink: "learning-forgetting-llms"
  research_areas:
    - "adaptive-foundation-models"
  date: 2023-12-20
  journal: "The 41st International Conference on Machine Learning (ICML 2024)"
  is_recent: false

-
  title: "LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos"
  image: "/assets/images/papers/lifelong_memory.png"
  authors:
    - "Ying Wang"
    - "Yanlai Yang"
    - "Mengye Ren"
  short_abstract: 'LifelongMemory is a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval.'
  abstract: "In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D."
  arxiv: "https://arxiv.org/abs/2312.05269"
  pdf: "https://arxiv.org/pdf/2312.05269"
  webpage: "https://agenticlearning.ai/lifelong-memory/"
  permalink: "lifelong-memory"
  research_areas:
    - "learning-from-visual-experience"
    - "adaptive-foundation-models"
  date: 2023-12-07
  journal: "CoRR"
  is_recent: false
